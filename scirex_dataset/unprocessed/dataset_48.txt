document	O
:	O
Density	Task
estimation	Task
using	O
Real	Method
NVP	Method
Unsupervised	Method
learning	Method
of	O
probabilistic	Method
models	Method
is	O
a	O
central	O
yet	O
challenging	O
problem	O
in	O
machine	Task
learning	Task
.	O
Specifically	O
,	O
designing	O
models	O
with	O
tractable	Task
learning	Task
,	O
sampling	Task
,	O
inference	Task
and	O
evaluation	Task
is	O
crucial	O
in	O
solving	O
this	O
task	O
.	O
We	O
extend	O
the	O
space	O
of	O
such	O
models	O
using	O
real	Method
-	Method
valued	Method
non	Method
-	Method
volume	Method
preserving	Method
(	O
real	Method
NVP	Method
)	O
transformations	O
,	O
a	O
set	O
of	O
powerful	O
,	O
stably	O
invertible	O
,	O
and	O
learnable	O
transformations	O
,	O
resulting	O
in	O
an	O
unsupervised	Method
learning	Method
algorithm	Method
with	O
exact	Task
log	Task
-	Task
likelihood	Task
computation	Task
,	O
exact	O
and	O
efficient	O
sampling	Task
,	O
exact	O
and	O
efficient	O
inference	Task
of	Task
latent	Task
variables	Task
,	O
and	O
an	O
interpretable	O
latent	O
space	O
.	O
We	O
demonstrate	O
its	O
ability	O
to	O
model	O
natural	O
images	O
on	O
four	O
datasets	O
through	O
sampling	Method
,	O
log	Method
-	Method
likelihood	Method
evaluation	Method
,	O
and	O
latent	Method
variable	Method
manipulations	Method
.	O
[	O
]	O
,	O
n	O
,	O
,	O
section	O
:	O
Introduction	O
The	O
domain	O
of	O
representation	Task
learning	Task
has	O
undergone	O
tremendous	O
advances	O
due	O
to	O
improved	O
supervised	Method
learning	Method
techniques	Method
.	O
However	O
,	O
unsupervised	Method
learning	Method
has	O
the	O
potential	O
to	O
leverage	O
large	O
pools	O
of	O
unlabeled	O
data	O
,	O
and	O
extend	O
these	O
advances	O
to	O
modalities	O
that	O
are	O
otherwise	O
impractical	O
or	O
impossible	O
.	O
One	O
principled	O
approach	O
to	O
unsupervised	Method
learning	Method
is	O
generative	Method
probabilistic	Method
modeling	Method
.	O
Not	O
only	O
do	O
generative	Method
probabilistic	Method
models	Method
have	O
the	O
ability	O
to	O
create	O
novel	O
content	O
,	O
they	O
also	O
have	O
a	O
wide	O
range	O
of	O
reconstruction	Task
related	Task
applications	Task
including	O
inpainting	Task
theis2015generative	O
,	O
oord2016pixel	O
,	O
DBLP	O
:	O
conf	O
/	O
icml	O
/	O
Sohl	O
-	O
DicksteinW15	O
,	O
denoising	Task
balle2015density	O
,	O
colorization	Task
zhang2016colorful	O
,	O
and	O
super	Task
-	Task
resolution	Task
bruna2015super	O
.	O
As	O
data	O
of	O
interest	O
are	O
generally	O
high	O
-	O
dimensional	O
and	O
highly	O
structured	O
,	O
the	O
challenge	O
in	O
this	O
domain	O
is	O
building	O
models	O
that	O
are	O
powerful	O
enough	O
to	O
capture	O
its	O
complexity	O
yet	O
still	O
trainable	O
.	O
We	O
address	O
this	O
challenge	O
by	O
introducing	O
real	Method
-	Method
valued	Method
non	Method
-	Method
volume	Method
preserving	Method
(	O
real	Method
NVP	Method
)	O
transformations	O
,	O
a	O
tractable	O
yet	O
expressive	O
approach	O
to	O
modeling	Task
high	Task
-	Task
dimensional	Task
data	Task
.	O
This	O
model	O
can	O
perform	O
efficient	O
and	O
exact	Task
inference	Task
,	O
sampling	Task
and	O
log	Task
-	Task
density	Task
estimation	Task
of	O
data	O
points	O
.	O
Moreover	O
,	O
the	O
architecture	O
presented	O
in	O
this	O
paper	O
enables	O
exact	O
and	O
efficient	O
reconstruction	Task
of	O
input	O
images	O
from	O
the	O
hierarchical	O
features	O
extracted	O
by	O
this	O
model	O
.	O
section	O
:	O
Related	O
work	O
Inference	Task
Generation	Task
Substantial	O
work	O
on	O
probabilistic	Method
generative	Method
models	Method
has	O
focused	O
on	O
training	O
models	O
using	O
maximum	Method
likelihood	Method
.	O
One	O
class	O
of	O
maximum	Method
likelihood	Method
models	Method
are	O
those	O
described	O
by	O
probabilistic	Method
undirected	Method
graphs	Method
,	O
such	O
as	O
Restricted	Method
Boltzmann	Method
Machines	Method
smolensky1986information	O
and	O
Deep	Method
Boltzmann	Method
Machines	Method
salakhutdinov2009deep	O
.	O
These	O
models	O
are	O
trained	O
by	O
taking	O
advantage	O
of	O
the	O
conditional	O
independence	O
property	O
of	O
their	O
bipartite	O
structure	O
to	O
allow	O
efficient	O
exact	Task
or	Task
approximate	Task
posterior	Task
inference	Task
on	Task
latent	Task
variables	Task
.	O
However	O
,	O
because	O
of	O
the	O
intractability	O
of	O
the	O
associated	O
marginal	O
distribution	O
over	O
latent	O
variables	O
,	O
their	O
training	O
,	O
evaluation	Task
,	O
and	O
sampling	Method
procedures	Method
necessitate	O
the	O
use	O
of	O
approximations	Method
like	O
Mean	Method
Field	Method
inference	Method
and	O
Markov	Method
Chain	Method
Monte	Method
Carlo	Method
,	O
whose	O
convergence	Metric
time	Metric
for	O
such	O
complex	O
models	O
remains	O
undetermined	O
,	O
often	O
resulting	O
in	O
generation	O
of	O
highly	O
correlated	O
samples	O
.	O
Furthermore	O
,	O
these	O
approximations	O
can	O
often	O
hinder	O
their	O
performance	O
berglund2013stochastic	O
.	O
Directed	Method
graphical	Method
models	Method
are	O
instead	O
defined	O
in	O
terms	O
of	O
an	O
ancestral	Method
sampling	Method
procedure	Method
,	O
which	O
is	O
appealing	O
both	O
for	O
its	O
conceptual	O
and	O
computational	Metric
simplicity	O
.	O
They	O
lack	O
,	O
however	O
,	O
the	O
conditional	O
independence	O
structure	O
of	O
undirected	Method
models	Method
,	O
making	O
exact	Task
and	Task
approximate	Task
posterior	Task
inference	Task
on	O
latent	O
variables	O
cumbersome	O
saul1996mean	O
.	O
Recent	O
advances	O
in	O
stochastic	Method
variational	Method
inference	Method
hoffman2013stochastic	O
and	O
amortized	Task
inference	Task
dayan1995helmholtz	O
,	O
mnih2014neural	O
,	O
kingma2013auto	O
,	O
rezende2014stochastic	O
,	O
allowed	O
efficient	O
approximate	Task
inference	Task
and	Task
learning	Task
of	Task
deep	Task
directed	Task
graphical	Task
models	Task
by	O
maximizing	O
a	O
variational	Metric
lower	Metric
bound	Metric
on	O
the	O
log	O
-	O
likelihood	O
neal1998view	O
.	O
In	O
particular	O
,	O
the	O
variational	Method
autoencoder	Method
algorithm	Method
kingma2013auto	O
,	O
rezende2014stochastic	O
simultaneously	O
learns	O
a	O
generative	Method
network	Method
,	O
that	O
maps	O
gaussian	O
latent	O
variables	O
to	O
samples	O
,	O
and	O
a	O
matched	Method
approximate	Method
inference	Method
network	Method
that	O
maps	O
samples	O
to	O
a	O
semantically	O
meaningful	O
latent	O
representation	O
,	O
by	O
exploiting	O
the	O
reparametrization	Method
trick	Method
williams1992simple	O
.	O
Its	O
success	O
in	O
leveraging	O
recent	O
advances	O
in	O
backpropagation	Method
rumelhart1988learning	O
,	O
lecun2012efficient	O
in	O
deep	Method
neural	Method
networks	Method
resulted	O
in	O
its	O
adoption	O
for	O
several	O
applications	O
ranging	O
from	O
speech	Task
synthesis	Task
chung2015recurrent	O
to	O
language	Task
modeling	Task
bowman2015generating	O
.	O
Still	O
,	O
the	O
approximation	O
in	O
the	O
inference	Method
process	Method
limits	O
its	O
ability	O
to	O
learn	O
high	Task
dimensional	Task
deep	Task
representations	Task
,	O
motivating	O
recent	O
work	O
in	O
improving	O
approximate	Task
inference	Task
maaloe2016auxiliary	O
,	O
rezende2015variational	O
,	O
salimans2014markov	O
,	O
tran2015variational	O
,	O
burda2015importance	O
,	O
DBLP	O
:	O
conf	O
/	O
icml	O
/	O
Sohl	O
-	O
DicksteinW15	O
,	O
kingma2016improving	O
.	O
Such	O
approximations	O
can	O
be	O
avoided	O
altogether	O
by	O
abstaining	O
from	O
using	O
latent	O
variables	O
.	O
Auto	Method
-	Method
regressive	Method
models	Method
frey1998graphical	O
,	O
bengio1999modeling	O
,	O
larochelle2011neural	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
GermainGML15	O
can	O
implement	O
this	O
strategy	O
while	O
typically	O
retaining	O
a	O
great	O
deal	O
of	O
flexibility	O
.	O
This	O
class	O
of	O
algorithms	O
tractably	O
models	O
the	O
joint	O
distribution	O
by	O
decomposing	O
it	O
into	O
a	O
product	O
of	O
conditionals	O
using	O
the	O
probability	Method
chain	Method
rule	Method
according	O
to	O
a	O
fixed	O
ordering	O
over	O
dimensions	O
,	O
simplifying	O
log	Method
-	Method
likelihood	Method
evaluation	Method
and	O
sampling	Task
.	O
Recent	O
work	O
in	O
this	O
line	O
of	O
research	O
has	O
taken	O
advantage	O
of	O
recent	O
advances	O
in	O
recurrent	Method
networks	Method
rumelhart1988learning	O
,	O
in	O
particular	O
long	Method
-	Method
short	Method
term	Method
memory	Method
DBLP	O
:	O
journals	O
/	O
neco	O
/	O
HochreiterS97	O
,	O
and	O
residual	Method
networks	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZR016	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZRS15	O
in	O
order	O
to	O
learn	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
generative	Method
image	Method
models	Method
theis2015generative	O
,	O
oord2016pixel	O
and	O
language	Method
models	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
JozefowiczVSSW16	O
.	O
The	O
ordering	O
of	O
the	O
dimensions	O
,	O
although	O
often	O
arbitrary	O
,	O
can	O
be	O
critical	O
to	O
the	O
training	O
of	O
the	O
model	O
vinyals2015order	O
.	O
The	O
sequential	O
nature	O
of	O
this	O
model	O
limits	O
its	O
computational	Metric
efficiency	O
.	O
For	O
example	O
,	O
its	O
sampling	Method
procedure	Method
is	O
sequential	O
and	O
non	O
-	O
parallelizable	O
,	O
which	O
can	O
become	O
cumbersome	O
in	O
applications	O
like	O
speech	Task
and	Task
music	Task
synthesis	Task
,	O
or	O
real	Task
-	Task
time	Task
rendering	Task
..	O
Additionally	O
,	O
there	O
is	O
no	O
natural	Method
latent	Method
representation	Method
associated	O
with	O
autoregressive	Method
models	Method
,	O
and	O
they	O
have	O
not	O
yet	O
been	O
shown	O
to	O
be	O
useful	O
for	O
semi	Task
-	Task
supervised	Task
learning	Task
.	O
Generative	Method
Adversarial	Method
Networks	Method
(	O
GANs	Method
)	O
DBLP	O
:	O
conf	O
/	O
nips	O
/	O
GoodfellowPMXWOCB14	O
on	O
the	O
other	O
hand	O
can	O
train	O
any	O
differentiable	Method
generative	Method
network	Method
by	O
avoiding	O
the	O
maximum	Method
likelihood	Method
principle	Method
altogether	O
.	O
Instead	O
,	O
the	O
generative	Method
network	Method
is	O
associated	O
with	O
a	O
discriminator	Method
network	Method
whose	O
task	O
is	O
to	O
distinguish	O
between	O
samples	O
and	O
real	O
data	O
.	O
Rather	O
than	O
using	O
an	O
intractable	O
log	O
-	O
likelihood	O
,	O
this	O
discriminator	Method
network	Method
provides	O
the	O
training	O
signal	O
in	O
an	O
adversarial	Method
fashion	Method
.	O
Successfully	O
trained	O
GAN	Method
models	Method
DBLP	O
:	O
conf	O
/	O
nips	O
/	O
GoodfellowPMXWOCB14	O
,	O
DBLP	O
:	O
conf	O
/	O
nips	O
/	O
DentonCSF15	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
RadfordMC15	O
can	O
consistently	O
generate	O
sharp	O
and	O
realistically	O
looking	O
samples	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
LarsenSW15	O
.	O
However	O
,	O
metrics	O
that	O
measure	O
the	O
diversity	O
in	O
the	O
generated	O
samples	O
are	O
currently	O
intractable	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
TheisOB15	O
,	O
gregor2016towards	O
,	O
im2016generating	O
.	O
Additionally	O
,	O
instability	O
in	O
their	O
training	O
process	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
RadfordMC15	O
requires	O
careful	O
hyperparameter	Method
tuning	Method
to	O
avoid	O
diverging	O
behavior	O
.	O
Training	O
such	O
a	O
generative	Method
network	Method
that	O
maps	O
latent	O
variable	O
to	O
a	O
sample	O
does	O
not	O
in	O
theory	O
require	O
a	O
discriminator	Method
network	Method
as	O
in	O
GANs	Method
,	O
or	O
approximate	Method
inference	Method
as	O
in	O
variational	Method
autoencoders	Method
.	O
Indeed	O
,	O
if	O
is	O
bijective	O
,	O
it	O
can	O
be	O
trained	O
through	O
maximum	Method
likelihood	Method
using	O
the	O
change	Method
of	Method
variable	Method
formula	Method
:	O
This	O
formula	O
has	O
been	O
discussed	O
in	O
several	O
papers	O
including	O
the	O
maximum	Method
likelihood	Method
formulation	Method
of	O
independent	Method
components	Method
analysis	Method
(	O
ICA	Method
)	O
bell1995information	O
,	O
hyvarinen2004independent	O
,	O
gaussianization	O
NIPS1994_901	O
,	O
chen2000gaussianization	O
and	O
deep	Method
density	Method
models	Method
bengio1991artificial	O
,	O
rippel2013high	O
,	O
dinh2014nice	O
,	O
balle2015density	O
.	O
As	O
the	O
existence	O
proof	O
of	O
nonlinear	Method
ICA	Method
solutions	O
hyvarinen1999nonlinear	O
suggests	O
,	O
auto	Method
-	Method
regressive	Method
models	Method
can	O
be	O
seen	O
as	O
tractable	O
instance	O
of	O
maximum	Method
likelihood	Method
nonlinear	Method
ICA	Method
,	O
where	O
the	O
residual	O
corresponds	O
to	O
the	O
independent	O
components	O
.	O
However	O
,	O
naive	O
application	O
of	O
the	O
change	Method
of	Method
variable	Method
formula	Method
produces	O
models	O
which	O
are	O
computationally	O
expensive	O
and	O
poorly	O
conditioned	O
,	O
and	O
so	O
large	O
scale	O
models	O
of	O
this	O
type	O
have	O
not	O
entered	O
general	O
use	O
.	O
section	O
:	O
Model	Task
definition	Task
In	O
this	O
paper	O
,	O
we	O
will	O
tackle	O
the	O
problem	O
of	O
learning	Method
highly	Method
nonlinear	Method
models	Method
in	O
high	Task
-	Task
dimensional	Task
continuous	Task
spaces	Task
through	O
maximum	Method
likelihood	Method
.	O
In	O
order	O
to	O
optimize	O
the	O
log	O
-	O
likelihood	O
,	O
we	O
introduce	O
a	O
more	O
flexible	O
class	O
of	O
architectures	O
that	O
enables	O
the	O
computation	Task
of	Task
log	Task
-	Task
likelihood	Task
on	O
continuous	O
data	O
using	O
the	O
change	O
of	O
variable	O
formula	O
.	O
Building	O
on	O
our	O
previous	O
work	O
in	O
dinh2014nice	O
,	O
we	O
define	O
a	O
powerful	O
class	O
of	O
bijective	Method
functions	Method
which	O
enable	O
exact	Task
and	Task
tractable	Task
density	Task
evaluation	Task
and	O
exact	Task
and	Task
tractable	Task
inference	Task
.	O
Moreover	O
,	O
the	O
resulting	O
cost	Method
function	Method
does	O
not	O
to	O
rely	O
on	O
a	O
fixed	Metric
form	Metric
reconstruction	Metric
cost	Metric
such	O
as	O
square	Metric
error	Metric
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
LarsenSW15	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
RadfordMC15	O
,	O
and	O
generates	O
sharper	O
samples	O
as	O
a	O
result	O
.	O
Also	O
,	O
this	O
flexibility	O
helps	O
us	O
leverage	O
recent	O
advances	O
in	O
batch	Method
normalization	Method
ioffe2015batch	O
and	O
residual	Method
networks	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZRS15	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZR016	O
to	O
define	O
a	O
very	O
deep	O
multi	Method
-	Method
scale	Method
architecture	Method
with	O
multiple	O
levels	O
of	O
abstraction	O
.	O
subsection	O
:	O
Change	O
of	O
variable	O
formula	O
Given	O
an	O
observed	O
data	O
variable	O
,	O
a	O
simple	O
prior	O
probability	O
distribution	O
on	O
a	O
latent	O
variable	O
,	O
and	O
a	O
bijection	O
(	O
with	O
)	O
,	O
the	O
change	O
of	O
variable	O
formula	O
defines	O
a	O
model	Method
distribution	Method
on	O
by	O
where	O
is	O
the	O
Jacobian	O
of	O
at	O
.	O
Exact	O
samples	O
from	O
the	O
resulting	O
distribution	O
can	O
be	O
generated	O
by	O
using	O
the	O
inverse	Method
transform	Method
sampling	Method
rule	Method
devroye1986sample	O
.	O
A	O
sample	O
is	O
drawn	O
in	O
the	O
latent	O
space	O
,	O
and	O
its	O
inverse	O
image	O
generates	O
a	O
sample	O
in	O
the	O
original	O
space	O
.	O
Computing	O
the	O
density	O
on	O
a	O
point	O
is	O
accomplished	O
by	O
computing	O
the	O
density	O
of	O
its	O
image	O
and	O
multiplying	O
by	O
the	O
associated	O
Jacobian	O
determinant	O
.	O
See	O
also	O
Figure	O
[	O
reference	O
]	O
.	O
Exact	O
and	O
efficient	O
inference	Task
enables	O
the	O
accurate	O
and	O
fast	O
evaluation	O
of	O
the	O
model	O
.	O
subsection	O
:	O
Coupling	O
layers	O
Computing	O
the	O
Jacobian	O
of	O
functions	O
with	O
high	O
-	O
dimensional	O
domain	O
and	O
codomain	O
and	O
computing	O
the	O
determinants	O
of	O
large	O
matrices	O
are	O
in	O
general	O
computationally	O
very	O
expensive	O
.	O
This	O
combined	O
with	O
the	O
restriction	O
to	O
bijective	O
functions	O
makes	O
Equation	O
[	O
reference	O
]	O
appear	O
impractical	O
for	O
modeling	O
arbitrary	O
distributions	O
.	O
As	O
shown	O
however	O
in	O
dinh2014nice	O
,	O
by	O
careful	O
design	O
of	O
the	O
function	O
,	O
a	O
bijective	Method
model	Method
can	O
be	O
learned	O
which	O
is	O
both	O
tractable	O
and	O
extremely	O
flexible	O
.	O
As	O
computing	O
the	O
Jacobian	O
determinant	O
of	O
the	O
transformation	O
is	O
crucial	O
to	O
effectively	O
train	O
using	O
this	O
principle	O
,	O
this	O
work	O
exploits	O
the	O
simple	O
observation	O
that	O
the	O
determinant	O
of	O
a	O
triangular	O
matrix	O
can	O
be	O
efficiently	O
computed	O
as	O
the	O
product	O
of	O
its	O
diagonal	O
terms	O
.	O
We	O
will	O
build	O
a	O
flexible	O
and	O
tractable	O
bijective	O
function	O
by	O
stacking	O
a	O
sequence	O
of	O
simple	O
bijections	O
.	O
In	O
each	O
simple	O
bijection	O
,	O
part	O
of	O
the	O
input	O
vector	O
is	O
updated	O
using	O
a	O
function	O
which	O
is	O
simple	O
to	O
invert	O
,	O
but	O
which	O
depends	O
on	O
the	O
remainder	O
of	O
the	O
input	O
vector	O
in	O
a	O
complex	O
way	O
.	O
We	O
refer	O
to	O
each	O
of	O
these	O
simple	O
bijections	O
as	O
an	O
affine	O
coupling	O
layer	O
.	O
Given	O
a	O
dimensional	O
input	O
and	O
,	O
the	O
output	O
of	O
an	O
affine	Method
coupling	Method
layer	Method
follows	O
the	O
equations	O
where	O
and	O
stand	O
for	O
scale	O
and	O
translation	O
,	O
and	O
are	O
functions	O
from	O
,	O
and	O
is	O
the	O
Hadamard	O
product	O
or	O
element	O
-	O
wise	O
product	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Properties	O
The	O
Jacobian	O
of	O
this	O
transformation	O
is	O
where	O
is	O
the	O
diagonal	O
matrix	O
whose	O
diagonal	O
elements	O
correspond	O
to	O
the	O
vector	O
.	O
Given	O
the	O
observation	O
that	O
this	O
Jacobian	O
is	O
triangular	O
,	O
we	O
can	O
efficiently	O
compute	O
its	O
determinant	O
as	O
.	O
Since	O
computing	O
the	O
Jacobian	O
determinant	O
of	O
the	O
coupling	Method
layer	Method
operation	Method
does	O
not	O
involve	O
computing	O
the	O
Jacobian	O
of	O
or	O
,	O
those	O
functions	O
can	O
be	O
arbitrarily	O
complex	O
.	O
We	O
will	O
make	O
them	O
deep	Method
convolutional	Method
neural	Method
networks	Method
.	O
Note	O
that	O
the	O
hidden	O
layers	O
of	O
and	O
can	O
have	O
more	O
features	O
than	O
their	O
input	O
and	O
output	O
layers	O
.	O
Another	O
interesting	O
property	O
of	O
these	O
coupling	Method
layers	Method
in	O
the	O
context	O
of	O
defining	O
probabilistic	Method
models	Method
is	O
their	O
invertibility	Method
.	O
Indeed	O
,	O
computing	O
the	O
inverse	O
is	O
no	O
more	O
complex	O
than	O
the	O
forward	Method
propagation	Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
meaning	O
that	O
sampling	Method
is	O
as	O
efficient	O
as	O
inference	Task
for	O
this	O
model	O
.	O
Note	O
again	O
that	O
computing	O
the	O
inverse	O
of	O
the	O
coupling	O
layer	O
does	O
not	O
require	O
computing	O
the	O
inverse	O
of	O
or	O
,	O
so	O
these	O
functions	O
can	O
be	O
arbitrarily	O
complex	O
and	O
difficult	O
to	O
invert	O
.	O
subsection	O
:	O
Masked	Method
convolution	Method
Partitioning	Method
can	O
be	O
implemented	O
using	O
a	O
binary	O
mask	O
,	O
and	O
using	O
the	O
functional	O
form	O
for	O
,	O
We	O
use	O
two	O
partitionings	O
that	O
exploit	O
the	O
local	O
correlation	O
structure	O
of	O
images	O
:	O
spatial	O
checkerboard	O
patterns	O
,	O
and	O
channel	Method
-	Method
wise	Method
masking	Method
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
The	O
spatial	O
checkerboard	O
pattern	O
mask	O
has	O
value	O
where	O
the	O
sum	O
of	O
spatial	O
coordinates	O
is	O
odd	O
,	O
and	O
otherwise	O
.	O
The	O
channel	O
-	O
wise	O
mask	O
is	O
for	O
the	O
first	O
half	O
of	O
the	O
channel	O
dimensions	O
and	O
for	O
the	O
second	O
half	O
.	O
For	O
the	O
models	O
presented	O
here	O
,	O
both	O
and	O
are	O
rectified	Method
convolutional	Method
networks	Method
.	O
subsection	O
:	O
Combining	O
coupling	O
layers	O
Although	O
coupling	O
layers	O
can	O
be	O
powerful	O
,	O
their	O
forward	Method
transformation	Method
leaves	O
some	O
components	O
unchanged	O
.	O
This	O
difficulty	O
can	O
be	O
overcome	O
by	O
composing	O
coupling	O
layers	O
in	O
an	O
alternating	O
pattern	O
,	O
such	O
that	O
the	O
components	O
that	O
are	O
left	O
unchanged	O
in	O
one	O
coupling	O
layer	O
are	O
updated	O
in	O
the	O
next	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
The	O
Jacobian	O
determinant	O
of	O
the	O
resulting	O
function	O
remains	O
tractable	O
,	O
relying	O
on	O
the	O
fact	O
that	O
Similarly	O
,	O
its	O
inverse	O
can	O
be	O
computed	O
easily	O
as	O
subsection	O
:	O
Multi	Method
-	Method
scale	Method
architecture	Method
We	O
implement	O
a	O
multi	Method
-	Method
scale	Method
architecture	Method
using	O
a	O
squeezing	Method
operation	Method
:	O
for	O
each	O
channel	O
,	O
it	O
divides	O
the	O
image	O
into	O
subsquares	O
of	O
shape	O
,	O
then	O
reshapes	O
them	O
into	O
subsquares	O
of	O
shape	O
.	O
The	O
squeezing	Method
operation	Method
transforms	O
an	O
tensor	O
into	O
an	O
tensor	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
effectively	O
trading	O
spatial	O
size	O
for	O
number	O
of	O
channels	O
.	O
At	O
each	O
scale	O
,	O
we	O
combine	O
several	O
operations	O
into	O
a	O
sequence	O
:	O
we	O
first	O
apply	O
three	O
coupling	Method
layers	Method
with	O
alternating	O
checkerboard	O
masks	O
,	O
then	O
perform	O
a	O
squeezing	Method
operation	Method
,	O
and	O
finally	O
apply	O
three	O
more	O
coupling	Method
layers	Method
with	O
alternating	Method
channel	Method
-	Method
wise	Method
masking	Method
.	O
The	O
channel	Method
-	Method
wise	Method
masking	Method
is	O
chosen	O
so	O
that	O
the	O
resulting	O
partitioning	O
is	O
not	O
redundant	O
with	O
the	O
previous	O
checkerboard	O
masking	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
For	O
the	O
final	O
scale	O
,	O
we	O
only	O
apply	O
four	O
coupling	Method
layers	Method
with	O
alternating	O
checkerboard	O
masks	O
.	O
Propagating	O
a	O
dimensional	O
vector	O
through	O
all	O
the	O
coupling	Method
layers	Method
would	O
be	O
cumbersome	O
,	O
in	O
terms	O
of	O
computational	Metric
and	O
memory	Metric
cost	Metric
,	O
and	O
in	O
terms	O
of	O
the	O
number	O
of	O
parameters	O
that	O
would	O
need	O
to	O
be	O
trained	O
.	O
For	O
this	O
reason	O
we	O
follow	O
the	O
design	O
choice	O
of	O
simonyan2014very	O
and	O
factor	O
out	O
half	O
of	O
the	O
dimensions	O
at	O
regular	O
intervals	O
(	O
see	O
Equation	O
[	O
reference	O
]	O
)	O
.	O
We	O
can	O
define	O
this	O
operation	O
recursively	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
In	O
our	O
experiments	O
,	O
we	O
use	O
this	O
operation	O
for	O
.	O
The	O
sequence	O
of	O
coupling	Method
-	Method
squeezing	Method
-	Method
coupling	Method
operations	Method
described	O
above	O
is	O
performed	O
per	O
layer	O
when	O
computing	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
.	O
At	O
each	O
layer	O
,	O
as	O
the	O
spatial	O
resolution	O
is	O
reduced	O
,	O
the	O
number	O
of	O
hidden	O
layer	O
features	O
in	O
and	O
is	O
doubled	O
.	O
All	O
variables	O
which	O
have	O
been	O
factored	O
out	O
at	O
different	O
scales	O
are	O
concatenated	O
to	O
obtain	O
the	O
final	O
transformed	O
output	O
(	O
Equation	O
[	O
reference	O
]	O
)	O
.	O
As	O
a	O
consequence	O
,	O
the	O
model	O
must	O
Gaussianize	O
units	O
which	O
are	O
factored	O
out	O
at	O
a	O
finer	O
scale	O
(	O
in	O
an	O
earlier	O
layer	O
)	O
before	O
those	O
which	O
are	O
factored	O
out	O
at	O
a	O
coarser	O
scale	O
(	O
in	O
a	O
later	O
layer	O
)	O
.	O
This	O
results	O
in	O
the	O
definition	O
of	O
intermediary	O
levels	O
of	O
representation	O
salakhutdinov2009deep	O
,	O
rezende2014stochastic	O
corresponding	O
to	O
more	O
local	O
,	O
fine	O
-	O
grained	O
features	O
as	O
shown	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
Moreover	O
,	O
Gaussianizing	O
and	O
factoring	O
out	O
units	O
in	O
earlier	O
layers	O
has	O
the	O
practical	O
benefit	O
of	O
distributing	O
the	O
loss	O
function	O
throughout	O
the	O
network	O
,	O
following	O
the	O
philosophy	O
similar	O
to	O
guiding	O
intermediate	O
layers	O
using	O
intermediate	Method
classifiers	Method
lee2014deeply	O
.	O
It	O
also	O
reduces	O
significantly	O
the	O
amount	O
of	O
computation	O
and	O
memory	O
used	O
by	O
the	O
model	O
,	O
allowing	O
us	O
to	O
train	O
larger	O
models	O
.	O
subsection	O
:	O
Batch	Method
normalization	Method
To	O
further	O
improve	O
the	O
propagation	Task
of	Task
training	Task
signal	Task
,	O
we	O
use	O
deep	Method
residual	Method
networks	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZRS15	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
HeZR016	O
with	O
batch	Method
normalization	Method
ioffe2015batch	O
and	O
weight	Method
normalization	Method
badrinarayanan2015understanding	O
,	O
salimans2016weight	O
in	O
and	O
.	O
As	O
described	O
in	O
Appendix	O
[	O
reference	O
]	O
we	O
introduce	O
and	O
use	O
a	O
novel	O
variant	O
of	O
batch	Method
normalization	Method
which	O
is	O
based	O
on	O
a	O
running	Method
average	Method
over	O
recent	O
minibatches	O
,	O
and	O
is	O
thus	O
more	O
robust	O
when	O
training	O
with	O
very	O
small	O
minibatches	O
.	O
We	O
also	O
use	O
apply	O
batch	Method
normalization	Method
to	O
the	O
whole	O
coupling	O
layer	O
output	O
.	O
The	O
effects	O
of	O
batch	Method
normalization	Method
are	O
easily	O
included	O
in	O
the	O
Jacobian	Task
computation	Task
,	O
since	O
it	O
acts	O
as	O
a	O
linear	Method
rescaling	Method
on	O
each	O
dimension	O
.	O
That	O
is	O
,	O
given	O
the	O
estimated	O
batch	O
statistics	O
and	O
,	O
the	O
rescaling	O
function	O
has	O
a	O
Jacobian	O
determinant	O
This	O
form	O
of	O
batch	Method
normalization	Method
can	O
be	O
seen	O
as	O
similar	O
to	O
reward	Task
normalization	Task
in	O
deep	Method
reinforcement	Method
learning	Method
mnih2015human	O
,	O
van2016learning	O
.	O
We	O
found	O
that	O
the	O
use	O
of	O
this	O
technique	O
not	O
only	O
allowed	O
training	O
with	O
a	O
deeper	O
stack	O
of	O
coupling	O
layers	O
,	O
but	O
also	O
alleviated	O
the	O
instability	Task
problem	Task
that	O
practitioners	O
often	O
encounter	O
when	O
training	O
conditional	O
distributions	O
with	O
a	O
scale	O
parameter	O
through	O
a	O
gradient	Method
-	Method
based	Method
approach	Method
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Procedure	O
The	O
algorithm	O
described	O
in	O
Equation	O
[	O
reference	O
]	O
shows	O
how	O
to	O
learn	O
distributions	O
on	O
unbounded	O
space	O
.	O
In	O
general	O
,	O
the	O
data	O
of	O
interest	O
have	O
bounded	O
magnitude	O
.	O
For	O
examples	O
,	O
the	O
pixel	O
values	O
of	O
an	O
image	O
typically	O
lie	O
in	O
after	O
application	O
of	O
the	O
recommended	O
jittering	Method
procedure	Method
uria2013rnade	O
,	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
TheisOB15	O
.	O
In	O
order	O
to	O
reduce	O
the	O
impact	O
of	O
boundary	O
effects	O
,	O
we	O
instead	O
model	O
the	O
density	O
of	O
,	O
where	O
is	O
picked	O
here	O
as	O
.	O
We	O
take	O
into	O
account	O
this	O
transformation	O
when	O
computing	O
log	O
-	O
likelihood	O
and	O
bits	O
per	O
dimension	O
.	O
We	O
also	O
augment	O
the	O
CIFAR	Material
-	Material
10	Material
,	O
CelebA	O
and	O
LSUN	O
datasets	O
during	O
training	O
to	O
also	O
include	O
horizontal	O
flips	O
of	O
the	O
training	O
examples	O
.	O
We	O
train	O
our	O
model	O
on	O
four	O
natural	O
image	O
datasets	O
:	O
CIFAR	Material
-	Material
10	Material
krizhevsky2009learning	O
,	O
Imagenet	O
russakovsky2015imagenet	O
,	O
Large	Task
-	Task
scale	Task
Scene	Task
Understanding	Task
(	O
LSUN	O
)	O
yu2015construction	O
,	O
CelebFaces	O
Attributes	O
(	O
CelebA	O
)	O
liu2015faceattributes	O
.	O
More	O
specifically	O
,	O
we	O
train	O
on	O
the	O
downsampled	O
to	O
and	O
versions	O
of	O
Imagenet	O
oord2016pixel	O
.	O
For	O
the	O
LSUN	O
dataset	O
,	O
we	O
train	O
on	O
the	O
bedroom	O
,	O
tower	O
and	O
church	O
outdoor	O
categories	O
.	O
The	O
procedure	O
for	O
LSUN	O
is	O
the	O
same	O
as	O
in	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
RadfordMC15	O
:	O
we	O
downsample	O
the	O
image	O
so	O
that	O
the	O
smallest	O
side	O
is	O
pixels	O
and	O
take	O
random	O
crops	O
of	O
.	O
For	O
CelebA	O
,	O
we	O
use	O
the	O
same	O
procedure	O
as	O
in	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
LarsenSW15	O
:	O
we	O
take	O
an	O
approximately	O
central	O
crop	O
of	O
then	O
resize	O
it	O
to	O
.	O
We	O
use	O
the	O
multi	Method
-	Method
scale	Method
architecture	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
and	O
use	O
deep	Method
convolutional	Method
residual	Method
networks	Method
in	O
the	O
coupling	O
layers	O
with	O
rectifier	O
nonlinearity	O
and	O
skip	O
-	O
connections	O
as	O
suggested	O
by	O
oord2016pixel	O
.	O
To	O
compute	O
the	O
scaling	O
functions	O
,	O
we	O
use	O
a	O
hyperbolic	Method
tangent	Method
function	Method
multiplied	O
by	O
a	O
learned	O
scale	O
,	O
whereas	O
the	O
translation	O
function	O
has	O
an	O
affine	O
output	O
.	O
Our	O
multi	Method
-	Method
scale	Method
architecture	Method
is	O
repeated	O
recursively	O
until	O
the	O
input	O
of	O
the	O
last	O
recursion	O
is	O
a	O
tensor	O
.	O
For	O
datasets	O
of	O
images	O
of	O
size	O
,	O
we	O
use	O
residual	O
blocks	O
with	O
hidden	O
feature	O
maps	O
for	O
the	O
first	O
coupling	Method
layers	Method
with	O
checkerboard	O
masking	O
.	O
Only	O
residual	O
blocks	O
are	O
used	O
for	O
images	O
of	O
size	O
.	O
We	O
use	O
a	O
batch	O
size	O
of	O
.	O
For	O
CIFAR	Material
-	Material
10	Material
,	O
we	O
use	O
residual	O
blocks	O
,	O
feature	O
maps	O
,	O
and	O
downscale	O
only	O
once	O
.	O
We	O
optimize	O
with	O
ADAM	O
kingma2014adam	O
with	O
default	O
hyperparameters	O
and	O
use	O
an	O
regularization	Method
on	O
the	O
weight	O
scale	O
parameters	O
with	O
coefficient	O
.	O
We	O
set	O
the	O
prior	O
to	O
be	O
an	O
isotropic	Method
unit	Method
norm	Method
Gaussian	Method
.	O
However	O
,	O
any	O
distribution	O
could	O
be	O
used	O
for	O
,	O
including	O
distributions	O
that	O
are	O
also	O
learned	O
during	O
training	O
,	O
such	O
as	O
from	O
an	O
auto	Method
-	Method
regressive	Method
model	Method
,	O
or	O
(	O
with	O
slight	O
modifications	O
to	O
the	O
training	O
objective	O
)	O
a	O
variational	Method
autoencoder	Method
.	O
subsection	O
:	O
Results	O
We	O
show	O
in	O
Table	O
[	O
reference	O
]	O
that	O
the	O
number	O
of	O
bits	O
per	O
dimension	O
,	O
while	O
not	O
improving	O
over	O
the	O
Pixel	Method
RNN	Method
oord2016pixel	O
baseline	O
,	O
is	O
competitive	O
with	O
other	O
generative	Method
methods	Method
.	O
As	O
we	O
notice	O
that	O
our	O
performance	O
increases	O
with	O
the	O
number	O
of	O
parameters	O
,	O
larger	O
models	O
are	O
likely	O
to	O
further	O
improve	O
performance	O
.	O
For	O
CelebA	O
and	O
LSUN	O
,	O
the	O
bits	O
per	O
dimension	O
for	O
the	O
validation	O
set	O
was	O
decreasing	O
throughout	O
training	O
,	O
so	O
little	O
overfitting	O
is	O
expected	O
.	O
We	O
show	O
in	O
Figure	O
[	O
reference	O
]	O
samples	O
generated	O
from	O
the	O
model	O
with	O
training	O
examples	O
from	O
the	O
dataset	O
for	O
comparison	O
.	O
As	O
mentioned	O
in	O
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
TheisOB15	O
,	O
gregor2016towards	O
,	O
maximum	Method
likelihood	Method
is	O
a	O
principle	O
that	O
values	O
diversity	O
over	O
sample	Metric
quality	Metric
in	O
a	O
limited	Task
capacity	Task
setting	Task
.	O
As	O
a	O
result	O
,	O
our	O
model	O
outputs	O
sometimes	O
highly	O
improbable	O
samples	O
as	O
we	O
can	O
notice	O
especially	O
on	O
CelebA.	O
As	O
opposed	O
to	O
variational	Method
autoencoders	Method
,	O
the	O
samples	O
generated	O
from	O
our	O
model	O
look	O
not	O
only	O
globally	O
coherent	O
but	O
also	O
sharp	O
.	O
Our	O
hypothesis	O
is	O
that	O
as	O
opposed	O
to	O
these	O
models	O
,	O
real	Method
NVP	Method
does	O
not	O
rely	O
on	O
fixed	O
form	O
reconstruction	O
cost	O
like	O
an	O
norm	O
which	O
tends	O
to	O
reward	O
capturing	O
low	O
frequency	O
components	O
more	O
heavily	O
than	O
high	O
frequency	O
components	O
.	O
Unlike	O
autoregressive	Method
models	Method
,	O
sampling	O
from	O
our	O
model	O
is	O
done	O
very	O
efficiently	O
as	O
it	O
is	O
parallelized	O
over	O
input	O
dimensions	O
.	O
On	O
Imagenet	O
and	O
LSUN	O
,	O
our	O
model	O
seems	O
to	O
have	O
captured	O
well	O
the	O
notion	O
of	O
background	O
/	O
foreground	O
and	O
lighting	O
interactions	O
such	O
as	O
luminosity	O
and	O
consistent	O
light	O
source	O
direction	O
for	O
reflectance	O
and	O
shadows	O
.	O
We	O
also	O
illustrate	O
the	O
smooth	O
semantically	O
consistent	O
meaning	O
of	O
our	O
latent	O
variables	O
.	O
In	O
the	O
latent	O
space	O
,	O
we	O
define	O
a	O
manifold	O
based	O
on	O
four	O
validation	O
examples	O
,	O
,	O
,	O
,	O
and	O
parametrized	O
by	O
two	O
parameters	O
and	O
by	O
,	O
We	O
project	O
the	O
resulting	O
manifold	O
back	O
into	O
the	O
data	O
space	O
by	O
computing	O
.	O
Results	O
are	O
shown	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
observe	O
that	O
the	O
model	O
seems	O
to	O
have	O
organized	O
the	O
latent	O
space	O
with	O
a	O
notion	O
of	O
meaning	O
that	O
goes	O
well	O
beyond	O
pixel	Method
space	Method
interpolation	Method
.	O
More	O
visualization	O
are	O
shown	O
in	O
the	O
Appendix	O
.	O
To	O
further	O
test	O
whether	O
the	O
latent	O
space	O
has	O
a	O
consistent	O
semantic	O
interpretation	O
,	O
we	O
trained	O
a	O
class	Method
-	Method
conditional	Method
model	Method
on	O
CelebA	O
,	O
and	O
found	O
that	O
the	O
learned	O
representation	O
had	O
a	O
consistent	O
semantic	O
meaning	O
across	O
class	O
labels	O
(	O
see	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O
section	O
:	O
Discussion	O
and	O
conclusion	O
In	O
this	O
paper	O
,	O
we	O
have	O
defined	O
a	O
class	O
of	O
invertible	Method
functions	Method
with	O
tractable	O
Jacobian	O
determinant	O
,	O
enabling	O
exact	Task
and	Task
tractable	Task
log	Task
-	Task
likelihood	Task
evaluation	Task
,	O
inference	Task
,	O
and	O
sampling	Task
.	O
We	O
have	O
shown	O
that	O
this	O
class	O
of	O
generative	Method
model	Method
achieves	O
competitive	O
performances	O
,	O
both	O
in	O
terms	O
of	O
sample	Metric
quality	Metric
and	O
log	Metric
-	Metric
likelihood	Metric
.	O
Many	O
avenues	O
exist	O
to	O
further	O
improve	O
the	O
functional	O
form	O
of	O
the	O
transformations	O
,	O
for	O
instance	O
by	O
exploiting	O
the	O
latest	O
advances	O
in	O
dilated	Method
convolutions	Method
yu2015multi	O
and	O
residual	Method
networks	Method
architectures	Method
DBLP	O
:	O
journals	O
/	O
corr	O
/	O
TargAL16	O
.	O
This	O
paper	O
presented	O
a	O
technique	O
bridging	O
the	O
gap	O
between	O
auto	Method
-	Method
regressive	Method
models	Method
,	O
variational	Method
autoencoders	Method
,	O
and	O
generative	Method
adversarial	Method
networks	Method
.	O
Like	O
auto	Method
-	Method
regressive	Method
models	Method
,	O
it	O
allows	O
tractable	O
and	O
exact	O
log	Task
-	Task
likelihood	Task
evaluation	Task
for	O
training	Task
.	O
It	O
allows	O
however	O
a	O
much	O
more	O
flexible	O
functional	O
form	O
,	O
similar	O
to	O
that	O
in	O
the	O
generative	Method
model	Method
of	Method
variational	Method
autoencoders	Method
.	O
This	O
allows	O
for	O
fast	O
and	O
exact	O
sampling	O
from	O
the	O
model	Method
distribution	Method
.	O
Like	O
GANs	Method
,	O
and	O
unlike	O
variational	Method
autoencoders	Method
,	O
our	O
technique	O
does	O
not	O
require	O
the	O
use	O
of	O
a	O
fixed	O
form	O
reconstruction	O
cost	O
,	O
and	O
instead	O
defines	O
a	O
cost	O
in	O
terms	O
of	O
higher	O
level	O
features	O
,	O
generating	O
sharper	O
images	O
.	O
Finally	O
,	O
unlike	O
both	O
variational	Method
autoencoders	Method
and	O
GANs	Method
,	O
our	O
technique	O
is	O
able	O
to	O
learn	O
a	O
semantically	O
meaningful	O
latent	O
space	O
which	O
is	O
as	O
high	O
dimensional	O
as	O
the	O
input	O
space	O
.	O
This	O
may	O
make	O
the	O
algorithm	O
particularly	O
well	O
suited	O
to	O
semi	Task
-	Task
supervised	Task
learning	Task
tasks	Task
,	O
as	O
we	O
hope	O
to	O
explore	O
in	O
future	O
work	O
.	O
Real	Method
NVP	Method
generative	Method
models	Method
can	O
additionally	O
be	O
conditioned	O
on	O
additional	O
variables	O
(	O
for	O
instance	O
class	O
labels	O
)	O
to	O
create	O
a	O
structured	Method
output	Method
algorithm	Method
.	O
More	O
so	O
,	O
as	O
the	O
resulting	O
class	O
of	O
invertible	O
transformations	O
can	O
be	O
treated	O
as	O
a	O
probability	O
distribution	O
in	O
a	O
modular	O
way	O
,	O
it	O
can	O
also	O
be	O
used	O
to	O
improve	O
upon	O
other	O
probabilistic	Method
models	Method
like	O
auto	Method
-	Method
regressive	Method
models	Method
and	O
variational	Method
autoencoders	Method
.	O
For	O
variational	Method
autoencoders	Method
,	O
these	O
transformations	O
could	O
be	O
used	O
both	O
to	O
enable	O
a	O
more	O
flexible	O
reconstruction	Metric
cost	Metric
DBLP	Metric
:	O
journals	O
/	O
corr	O
/	O
LarsenSW15	O
and	O
a	O
more	O
flexible	O
stochastic	Method
inference	Method
distribution	Method
rezende2015variational	O
.	O
Probabilistic	Method
models	Method
in	O
general	O
can	O
also	O
benefit	O
from	O
batch	Method
normalization	Method
techniques	Method
as	O
applied	O
in	O
this	O
paper	O
.	O
The	O
definition	O
of	O
powerful	O
and	O
trainable	O
invertible	Method
functions	Method
can	O
also	O
benefit	O
domains	O
other	O
than	O
generative	Method
unsupervised	Method
learning	Method
.	O
For	O
example	O
,	O
in	O
reinforcement	Task
learning	Task
,	O
these	O
invertible	O
functions	O
can	O
help	O
extend	O
the	O
set	O
of	O
functions	O
for	O
which	O
an	O
operation	O
is	O
tractable	O
for	O
continuous	Task
Q	Task
-	Task
learning	Task
gu2016continuous	O
or	O
find	O
representation	O
where	O
local	Method
linear	Method
Gaussian	Method
approximations	Method
are	O
more	O
appropriate	O
watter2015embed	O
.	O
section	O
:	O
Acknowledgments	O
The	O
authors	O
thank	O
the	O
developers	O
of	O
Tensorflow	Method
abadi2016tensorflow	O
.	O
We	O
thank	O
Sherry	O
Moore	O
,	O
David	O
Andersen	O
and	O
Jon	O
Shlens	O
for	O
their	O
help	O
in	O
implementing	O
the	O
model	O
.	O
We	O
thank	O
Aäron	O
van	O
den	O
Oord	O
,	O
Yann	O
Dauphin	O
,	O
Kyle	O
Kastner	O
,	O
Chelsea	O
Finn	O
,	O
Maithra	O
Raghu	O
,	O
David	O
Warde	O
-	O
Farley	O
,	O
Daniel	O
Jiwoong	O
I	O
m	O
and	O
Oriol	O
Vinyals	O
for	O
fruitful	O
discussions	O
.	O
Finally	O
,	O
we	O
thank	O
Ben	O
Poole	O
,	O
Rafal	O
Jozefowicz	O
and	O
George	O
Dahl	O
for	O
their	O
input	O
on	O
a	O
draft	O
of	O
the	O
paper	O
.	O
plus	O
0.12ex	O
bibliography	O
:	O
References	O
appendix	O
:	O
Samples	O
appendix	O
:	O
Manifold	O
appendix	O
:	O
Extrapolation	Task
Inspired	O
by	O
the	O
texture	Task
generation	Task
work	O
by	O
DBLP	O
:	O
conf	O
/	O
nips	O
/	O
GatysEB15	O
,	O
theis2015generative	O
and	O
extrapolation	Task
test	Task
with	O
DCGAN	Method
DBLP	Method
:	O
journals	O
/	O
corr	O
/	O
RadfordMC15	O
,	O
we	O
also	O
evaluate	O
the	O
statistics	Metric
captured	O
by	O
our	O
model	O
by	O
generating	O
images	O
twice	O
or	O
ten	O
times	O
as	O
large	O
as	O
present	O
in	O
the	O
dataset	O
.	O
As	O
we	O
can	O
observe	O
in	O
the	O
following	O
figures	O
,	O
our	O
model	O
seems	O
to	O
successfully	O
create	O
a	O
“	O
texture	Method
”	Method
representation	Method
of	O
the	O
dataset	O
while	O
maintaining	O
a	O
spatial	O
smoothness	O
through	O
the	O
image	O
.	O
Our	O
convolutional	Method
architecture	Method
is	O
only	O
aware	O
of	O
the	O
position	O
of	O
considered	O
pixel	O
through	O
edge	O
effects	O
in	O
convolutions	Method
,	O
therefore	O
our	O
model	O
is	O
similar	O
to	O
a	O
stationary	Method
process	Method
.	O
This	O
also	O
explains	O
why	O
these	O
samples	O
are	O
more	O
consistent	O
in	O
LSUN	O
,	O
where	O
the	O
training	O
data	O
was	O
obtained	O
using	O
random	O
crops	O
.	O
appendix	O
:	O
Latent	O
variables	O
semantic	O
As	O
in	O
gregor2016towards	O
,	O
we	O
further	O
try	O
to	O
grasp	O
the	O
semantic	O
of	O
our	O
learned	O
layers	O
latent	O
variables	O
by	O
doing	O
ablation	Method
tests	Method
.	O
We	O
infer	O
the	O
latent	O
variables	O
and	O
resample	O
the	O
lowest	O
levels	O
of	O
latent	O
variables	O
from	O
a	O
standard	O
gaussian	Method
,	O
increasing	O
the	O
highest	O
level	O
affected	O
by	O
this	O
resampling	O
.	O
As	O
we	O
can	O
see	O
in	O
the	O
following	O
figures	O
,	O
the	O
semantic	O
of	O
our	O
latent	O
space	O
seems	O
to	O
be	O
more	O
on	O
a	O
graphic	O
level	O
rather	O
than	O
higher	O
level	O
concept	O
.	O
Although	O
the	O
heavy	O
use	O
of	O
convolution	Method
improves	O
learning	Task
by	O
exploiting	O
image	O
prior	O
knowledge	O
,	O
it	O
is	O
also	O
likely	O
to	O
be	O
responsible	O
for	O
this	O
limitation	O
.	O
appendix	O
:	O
Batch	Method
normalization	Method
We	O
further	O
experimented	O
with	O
batch	Method
normalization	Method
by	O
using	O
a	O
weighted	Method
average	Method
of	O
a	O
moving	Method
average	Method
of	Method
the	Method
layer	Method
statistics	Method
and	O
the	O
current	O
batch	O
batch	O
statistics	O
,	O
where	O
is	O
the	O
momentum	O
.	O
When	O
using	O
,	O
we	O
only	O
propagate	O
gradient	O
through	O
the	O
current	O
batch	O
statistics	O
.	O
We	O
observe	O
that	O
using	O
this	O
lag	O
helps	O
the	O
model	O
train	O
with	O
very	O
small	O
minibatches	O
.	O
We	O
used	O
batch	Method
normalization	Method
with	O
a	O
moving	Method
average	Method
for	O
our	O
results	O
on	O
CIFAR	Material
-	Material
10	Material
.	O
appendix	O
:	O
Attribute	O
change	O
Additionally	O
,	O
we	O
exploit	O
the	O
attribute	O
information	O
in	O
CelebA	O
to	O
build	O
a	O
conditional	Method
model	Method
,	O
i.e.	O
the	O
invertible	O
function	O
from	O
image	O
to	O
latent	O
variable	O
uses	O
the	O
labels	O
in	O
to	O
define	O
its	O
parameters	O
.	O
In	O
order	O
to	O
observe	O
the	O
information	O
stored	O
in	O
the	O
latent	O
variables	O
,	O
we	O
choose	O
to	O
encode	O
a	O
batch	O
of	O
images	O
with	O
their	O
original	O
attribute	O
and	O
decode	O
them	O
using	O
a	O
new	O
set	O
of	O
attributes	O
,	O
build	O
by	O
shuffling	O
the	O
original	O
attributes	O
inside	O
the	O
batch	O
.	O
We	O
obtain	O
the	O
new	O
images	O
.	O
We	O
observe	O
that	O
,	O
although	O
the	O
faces	O
are	O
changed	O
as	O
to	O
respect	O
the	O
new	O
attributes	O
,	O
several	O
properties	O
remain	O
unchanged	O
like	O
position	O
and	O
background	O
.	O
