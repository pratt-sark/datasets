Neural	Task
Machine	Task
Translation	Task
of	Task
Rare	Task
Words	Task
with	O
Subword	Method
Units	Method
section	O
:	O
Abstract	O
Neural	Task
machine	Task
translation	Task
(	O
NMT	Task
)	O
models	O
typically	O
operate	O
with	O
a	O
fixed	O
vocabulary	O
,	O
but	O
translation	Task
is	O
an	O
open	Task
-	Task
vocabulary	Task
problem	Task
.	O
Previous	O
work	O
addresses	O
the	O
translation	Task
of	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
by	O
backing	O
off	O
to	O
a	O
dictionary	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
simpler	O
and	O
more	O
effective	O
approach	O
,	O
making	O
the	O
NMT	Task
model	O
capable	O
of	O
open	O
-	O
vocabulary	O
translation	Task
by	O
encoding	O
rare	O
and	O
unknown	O
words	O
as	O
sequences	O
of	O
subword	O
units	O
.	O
This	O
is	O
based	O
on	O
the	O
intuition	O
that	O
various	O
word	O
classes	O
are	O
translatable	O
via	O
smaller	O
units	O
than	O
words	O
,	O
for	O
instance	O
names	O
(	O
via	O
character	O
copying	O
or	O
transliteration	O
)	O
,	O
compounds	O
(	O
via	O
compositional	O
translation	Task
)	O
,	O
and	O
cognates	O
and	O
loanwords	O
(	O
via	O
phonological	O
and	O
morphological	O
transformations	O
)	O
.	O
We	O
discuss	O
the	O
suitability	O
of	O
different	O
word	Method
segmentation	Method
techniques	Method
,	O
including	O
simple	O
character	Method
ngram	Method
models	Method
and	O
a	O
segmentation	Method
based	O
on	O
the	O
byte	Method
pair	Method
encoding	Method
compression	Method
algorithm	Method
,	O
and	O
empirically	O
show	O
that	O
subword	Method
models	Method
improve	O
over	O
a	O
back	Method
-	Method
off	Method
dictionary	Method
baseline	Method
for	O
the	O
WMT	Material
15	Material
translation	Material
tasks	Material
English→German	Material
and	O
English→Russian	Material
by	O
up	O
to	O
1.1	O
and	O
1.3	O
BLEU	Metric
,	O
respectively	O
.	O
section	O
:	O
Introduction	O
Neural	Task
machine	Task
translation	Task
has	O
recently	O
shown	O
impressive	O
results	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
However	O
,	O
the	O
translation	Task
of	O
rare	O
words	O
is	O
an	O
open	O
problem	O
.	O
The	O
vocabulary	O
of	O
neural	Method
models	Method
is	O
typically	O
limited	O
to	O
30	O
000	O
-	O
50	O
000	O
words	O
,	O
but	O
translation	Task
is	O
an	O
open	Task
-	Task
vocabulary	Task
prob	Task
-	Task
The	O
research	O
presented	O
in	O
this	O
publication	O
was	O
conducted	O
in	O
cooperation	O
with	O
Samsung	O
Electronics	O
Polska	O
sp	O
.	O
z	O
o.o	O
.	O
-	O
Samsung	O
R	O
&	O
D	O
Institute	O
Poland	O
.	O
lem	Method
,	O
and	O
especially	O
for	O
languages	O
with	O
productive	Task
word	Task
formation	Task
processes	Task
such	O
as	O
agglutination	Task
and	O
compounding	Method
,	O
translation	Task
models	O
require	O
mechanisms	O
that	O
go	O
below	O
the	O
word	O
level	O
.	O
As	O
an	O
example	O
,	O
consider	O
compounds	O
such	O
as	O
the	O
German	O
Abwasser|behandlungs|anlange	O
'	O
sewage	O
water	O
treatment	O
plant	O
'	O
,	O
for	O
which	O
a	O
segmented	Method
,	Method
variable	Method
-	Method
length	Method
representation	Method
is	O
intuitively	O
more	O
appealing	O
than	O
encoding	O
the	O
word	O
as	O
a	O
fixed	O
-	O
length	O
vector	O
.	O
For	O
word	O
-	O
level	O
NMT	Task
models	O
,	O
the	O
translation	Task
of	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
has	O
been	O
addressed	O
through	O
a	O
back	O
-	O
off	O
to	O
a	O
dictionary	Method
look	Method
-	Method
up	Method
[	O
reference	O
][	O
reference	O
]	O
.	O
We	O
note	O
that	O
such	O
techniques	O
make	O
assumptions	O
that	O
often	O
do	O
not	O
hold	O
true	O
in	O
practice	O
.	O
For	O
instance	O
,	O
there	O
is	O
not	O
always	O
a	O
1	O
-	O
to	O
-	O
1	O
correspondence	O
between	O
source	O
and	O
target	O
words	O
because	O
of	O
variance	O
in	O
the	O
degree	O
of	O
morphological	O
synthesis	O
between	O
languages	O
,	O
like	O
in	O
our	O
introductory	O
compounding	O
example	O
.	O
Also	O
,	O
word	Method
-	Method
level	Method
models	Method
are	O
unable	O
to	O
translate	O
or	O
generate	O
unseen	O
words	O
.	O
Copying	O
unknown	O
words	O
into	O
the	O
target	O
text	O
,	O
as	O
done	O
by	O
[	O
reference	O
][	O
reference	O
]	O
,	O
is	O
a	O
reasonable	O
strategy	O
for	O
names	Task
,	O
but	O
morphological	O
changes	O
and	O
transliteration	Task
is	O
often	O
required	O
,	O
especially	O
if	O
alphabets	O
differ	O
.	O
We	O
investigate	O
NMT	Task
models	O
that	O
operate	O
on	O
the	O
level	O
of	O
subword	O
units	O
.	O
Our	O
main	O
goal	O
is	O
to	O
model	O
open	O
-	O
vocabulary	O
translation	Task
in	O
the	O
NMT	Task
network	O
itself	O
,	O
without	O
requiring	O
a	O
back	Method
-	Method
off	Method
model	Method
for	O
rare	O
words	O
.	O
In	O
addition	O
to	O
making	O
the	O
translation	Task
process	Task
simpler	O
,	O
we	O
also	O
find	O
that	O
the	O
subword	Method
models	Method
achieve	O
better	O
accuracy	Metric
for	O
the	O
translation	Task
of	O
rare	O
words	O
than	O
large	Method
-	Method
vocabulary	Method
models	Method
and	O
back	Method
-	Method
off	Method
dictionaries	Method
,	O
and	O
are	O
able	O
to	O
productively	O
generate	O
new	O
words	O
that	O
were	O
not	O
seen	O
at	O
training	O
time	O
.	O
Our	O
analysis	O
shows	O
that	O
the	O
neural	Method
networks	Method
are	O
able	O
to	O
learn	O
compounding	Task
and	O
transliteration	Task
from	O
subword	Method
representations	Method
.	O
This	O
paper	O
has	O
two	O
main	O
contributions	O
:	O
•	O
We	O
show	O
that	O
open	O
-	O
vocabulary	O
neural	O
machine	O
translation	Task
is	O
possible	O
by	O
encoding	O
(	O
rare	O
)	O
words	O
via	O
subword	O
units	O
.	O
We	O
find	O
our	O
architecture	O
simpler	O
and	O
more	O
effective	O
than	O
using	O
large	O
vocabularies	O
and	O
back	O
-	O
off	O
dictionaries	O
[	O
reference	O
][	O
reference	O
]	O
)	O
.	O
•	O
We	O
adapt	O
byte	Method
pair	Method
encoding	Method
(	O
BPE	Method
)	O
[	O
reference	O
]	O
,	O
a	O
compression	Method
algorithm	Method
,	O
to	O
the	O
task	O
of	O
word	Task
segmentation	Task
.	O
BPE	Method
allows	O
for	O
the	O
representation	O
of	O
an	O
open	O
vocabulary	O
through	O
a	O
fixed	O
-	O
size	O
vocabulary	O
of	O
variable	O
-	O
length	O
character	O
sequences	O
,	O
making	O
it	O
a	O
very	O
suitable	O
word	Method
segmentation	Method
strategy	Method
for	O
neural	Method
network	Method
models	Method
.	O
section	O
:	O
Neural	Task
Machine	Task
Translation	Task
We	O
follow	O
the	O
neural	O
machine	O
translation	Task
architecture	O
by	O
[	O
reference	O
]	O
,	O
which	O
we	O
will	O
briefly	O
summarize	O
here	O
.	O
However	O
,	O
we	O
note	O
that	O
our	O
approach	O
is	O
not	O
specific	O
to	O
this	O
architecture	O
.	O
The	O
neural	O
machine	O
translation	Task
system	O
is	O
implemented	O
as	O
an	O
encoder	Method
-	Method
decoder	Method
network	Method
with	O
recurrent	Method
neural	Method
networks	Method
.	O
The	O
encoder	Method
is	O
a	O
bidirectional	Method
neural	Method
network	Method
with	O
gated	Method
recurrent	Method
units	Method
[	O
reference	O
]	O
that	O
reads	O
an	O
input	O
sequence	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
m	O
)	O
and	O
calculates	O
a	O
forward	O
sequence	O
of	O
hidden	O
states	O
(	O
−	O
→	O
h	O
1	O
,	O
...	O
,	O
−	O
→	O
h	O
m	O
)	O
,	O
and	O
a	O
backward	O
sequence	O
The	O
hidden	O
states	O
−	O
→	O
h	O
j	O
and	O
←	O
−	O
h	O
j	O
are	O
concatenated	O
to	O
obtain	O
the	O
annotation	O
vector	O
h	O
j	O
.	O
The	O
decoder	Method
is	O
a	O
recurrent	Method
neural	Method
network	Method
that	O
predicts	O
a	O
target	O
sequence	O
y	O
=	O
(	O
y	O
1	O
,	O
...	O
,	O
y	O
n	O
)	O
.	O
Each	O
word	O
y	O
i	O
is	O
predicted	O
based	O
on	O
a	O
recurrent	O
hidden	O
state	O
s	O
i	O
,	O
the	O
previously	O
predicted	O
word	O
y	O
i−1	O
,	O
and	O
a	O
context	O
vector	O
c	O
i	O
.	O
c	O
i	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
annotations	O
h	O
j	O
.	O
The	O
weight	O
of	O
each	O
annotation	O
h	O
j	O
is	O
computed	O
through	O
an	O
alignment	Method
model	Method
α	Method
ij	Method
,	O
which	O
models	O
the	O
probability	O
that	O
y	O
i	O
is	O
aligned	O
to	O
x	O
j	O
.	O
The	O
alignment	Method
model	Method
is	O
a	O
singlelayer	Method
feedforward	Method
neural	Method
network	Method
that	O
is	O
learned	O
jointly	O
with	O
the	O
rest	O
of	O
the	O
network	O
through	O
backpropagation	Method
.	O
A	O
detailed	O
description	O
can	O
be	O
found	O
in	O
[	O
reference	O
]	O
.	O
Training	O
is	O
performed	O
on	O
a	O
parallel	O
corpus	O
with	O
stochastic	Method
gradient	Method
descent	Method
.	O
For	O
translation	Task
,	O
a	O
beam	Method
search	Method
with	O
small	O
beam	O
size	O
is	O
employed	O
.	O
section	O
:	O
Subword	Task
Translation	Task
The	O
main	O
motivation	O
behind	O
this	O
paper	O
is	O
that	O
the	O
translation	Task
of	O
some	O
words	O
is	O
transparent	O
in	O
that	O
they	O
are	O
translatable	O
by	O
a	O
competent	O
translator	O
even	O
if	O
they	O
are	O
novel	O
to	O
him	O
or	O
her	O
,	O
based	O
on	O
a	O
translation	Task
of	O
known	O
subword	O
units	O
such	O
as	O
morphemes	O
or	O
phonemes	O
.	O
Word	O
categories	O
whose	O
translation	Task
is	O
potentially	O
transparent	O
include	O
:	O
•	O
named	O
entities	O
.	O
Between	O
languages	O
that	O
share	O
an	O
alphabet	O
,	O
names	O
can	O
often	O
be	O
copied	O
from	O
source	O
to	O
target	O
text	O
.	O
Transcription	Task
or	O
transliteration	Task
may	O
be	O
required	O
,	O
especially	O
if	O
the	O
alphabets	O
or	O
syllabaries	O
differ	O
.	O
Example	O
:	O
Barack	O
Obama	O
(	O
English	O
;	O
German	O
)	O
Барак	O
Обама	O
(	O
Russian	O
)	O
バラク・オバマ	O
(	O
ba	O
-	O
ra	O
-	O
ku	O
o	O
-	O
ba	O
-	O
ma	O
)	O
(	O
Japanese	O
)	O
•	O
cognates	O
and	O
loanwords	O
.	O
Cognates	O
and	O
loanwords	O
with	O
a	O
common	O
origin	O
can	O
differ	O
in	O
regular	O
ways	O
between	O
languages	O
,	O
so	O
that	O
character	O
-	O
level	O
translation	Task
rules	O
are	O
sufficient	O
[	O
reference	O
]	O
.	O
Example	O
:	O
•	O
morphologically	O
complex	O
words	O
.	O
Words	O
containing	O
multiple	O
morphemes	O
,	O
for	O
instance	O
formed	O
via	O
compounding	O
,	O
affixation	O
,	O
or	O
inflection	O
,	O
may	O
be	O
translatable	O
by	O
translating	O
the	O
morphemes	O
separately	O
.	O
Example	O
:	O
solar	Task
system	Task
(	O
English	O
)	O
Sonnensystem	O
(	O
Sonne	Method
+	Method
System	Method
)	O
(	O
German	O
)	O
Naprendszer	O
(	O
Nap	O
+	O
Rendszer	O
)	O
(	O
Hungarian	O
)	O
In	O
an	O
analysis	O
of	O
100	O
rare	O
tokens	O
(	O
not	O
among	O
the	O
50	O
000	O
most	O
frequent	O
types	O
)	O
in	O
our	O
German	O
training	O
data	O
1	O
,	O
the	O
majority	O
of	O
tokens	O
are	O
potentially	O
translatable	O
from	O
English	O
through	O
smaller	O
units	O
.	O
We	O
find	O
56	O
compounds	O
,	O
21	O
names	O
,	O
6	O
loanwords	O
with	O
a	O
common	O
origin	O
(	O
emancipate→emanzipieren	O
)	O
,	O
5	O
cases	O
of	O
transparent	O
affixation	O
(	O
sweetish	O
'	O
sweet	O
'	O
+	O
'	O
-	O
ish	O
'	O
→	O
süßlich	O
'	O
süß	O
'	O
+	O
'	O
-	O
lich	O
'	O
)	O
,	O
1	O
number	O
and	O
1	O
computer	O
language	O
identifier	O
.	O
Our	O
hypothesis	O
is	O
that	O
a	O
segmentation	O
of	O
rare	O
words	O
into	O
appropriate	O
subword	O
units	O
is	O
sufficient	O
to	O
allow	O
for	O
the	O
neural	O
translation	Task
network	O
to	O
learn	O
transparent	Task
translations	Task
,	O
and	O
to	O
generalize	O
this	O
knowledge	O
to	O
translate	O
and	O
produce	O
unseen	O
words	O
.	O
[	O
reference	O
]	O
We	O
provide	O
empirical	O
support	O
for	O
this	O
hy	O
-	O
pothesis	O
in	O
Sections	O
4	O
and	O
5	O
.	O
First	O
,	O
we	O
discuss	O
different	O
subword	Method
representations	Method
.	O
section	O
:	O
Related	O
Work	O
For	O
Statistical	Task
Machine	Task
Translation	Task
(	O
SMT	Method
)	O
,	O
the	O
translation	Task
of	O
unknown	O
words	O
has	O
been	O
the	O
subject	O
of	O
intensive	O
research	O
.	O
A	O
large	O
proportion	O
of	O
unknown	O
words	O
are	O
names	O
,	O
which	O
can	O
just	O
be	O
copied	O
into	O
the	O
target	O
text	O
if	O
both	O
languages	O
share	O
an	O
alphabet	O
.	O
If	O
alphabets	O
differ	O
,	O
transliteration	O
is	O
required	O
[	O
reference	O
]	O
.	O
Character	O
-	O
based	O
translation	Task
has	O
also	O
been	O
investigated	O
with	O
phrase	Method
-	Method
based	Method
models	Method
,	O
which	O
proved	O
especially	O
successful	O
for	O
closely	O
related	O
languages	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
The	O
segmentation	Task
of	Task
morphologically	Task
complex	Task
words	Task
such	O
as	O
compounds	O
is	O
widely	O
used	O
for	O
SMT	Method
,	O
and	O
various	O
algorithms	O
for	O
morpheme	Task
segmentation	Task
have	O
been	O
investigated	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Segmentation	Method
algorithms	Method
commonly	O
used	O
for	O
phrase	O
-	O
based	O
SMT	Method
tend	O
to	O
be	O
conservative	O
in	O
their	O
splitting	O
decisions	O
,	O
whereas	O
we	O
aim	O
for	O
an	O
aggressive	Method
segmentation	Method
that	O
allows	O
for	O
open	O
-	O
vocabulary	O
translation	Task
with	O
a	O
compact	O
network	O
vocabulary	O
,	O
and	O
without	O
having	O
to	O
resort	O
to	O
back	O
-	O
off	O
dictionaries	O
.	O
The	O
best	O
choice	O
of	O
subword	O
units	O
may	O
be	O
taskspecific	O
.	O
For	O
speech	Task
recognition	Task
,	O
phone	Method
-	Method
level	Method
language	Method
models	Method
have	O
been	O
used	O
[	O
reference	O
]	O
.	O
[	O
reference	O
]	O
investigate	O
subword	Method
language	Method
models	Method
,	O
and	O
propose	O
to	O
use	O
syllables	O
.	O
For	O
multilingual	Task
segmentation	Task
tasks	Task
,	O
multilingual	Method
algorithms	Method
have	O
been	O
proposed	O
[	O
reference	O
]	O
.	O
We	O
find	O
these	O
intriguing	O
,	O
but	O
inapplicable	O
at	O
test	O
time	O
.	O
Various	O
techniques	O
have	O
been	O
proposed	O
to	O
produce	O
fixed	O
-	O
length	O
continuous	O
word	O
vectors	O
based	O
on	O
characters	O
or	O
morphemes	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
An	O
effort	O
to	O
apply	O
such	O
techniques	O
to	O
NMT	Task
,	O
parallel	O
to	O
ours	O
,	O
has	O
found	O
no	O
significant	O
improvement	O
over	O
word	Method
-	Method
based	Method
approaches	Method
[	O
reference	O
]	O
.	O
One	O
technical	O
difference	O
from	O
our	O
work	O
is	O
that	O
the	O
attention	Method
mechanism	Method
still	O
operates	O
on	O
the	O
level	O
of	O
words	O
in	O
the	O
model	O
by	O
[	O
reference	O
]	O
,	O
and	O
that	O
the	O
representation	O
of	O
each	O
word	O
is	O
fixed	O
-	O
length	O
.	O
We	O
expect	O
that	O
the	O
attention	Method
mechanism	Method
benefits	O
from	O
our	O
variable	Method
-	Method
length	Method
representation	Method
:	O
the	O
network	O
can	O
learn	O
to	O
place	O
attention	O
on	O
different	O
subword	O
units	O
at	O
each	O
step	O
.	O
Recall	O
our	O
introductory	O
example	O
Abwasserbehandlungsanlange	O
,	O
for	O
which	O
a	O
subword	Method
segmentation	Method
avoids	O
the	O
information	O
bottleneck	O
of	O
a	O
fixed	Method
-	Method
length	Method
representation	Method
.	O
Neural	Task
machine	Task
translation	Task
differs	O
from	O
phrasebased	Method
methods	Method
in	O
that	O
there	O
are	O
strong	O
incentives	O
to	O
minimize	O
the	O
vocabulary	O
size	O
of	O
neural	Method
models	Method
to	O
increase	O
time	Metric
and	Metric
space	Metric
efficiency	Metric
,	O
and	O
to	O
allow	O
for	O
translation	Task
without	O
back	Method
-	Method
off	Method
models	Method
.	O
At	O
the	O
same	O
time	O
,	O
we	O
also	O
want	O
a	O
compact	O
representation	O
of	O
the	O
text	O
itself	O
,	O
since	O
an	O
increase	O
in	O
text	O
length	O
reduces	O
efficiency	O
and	O
increases	O
the	O
distances	O
over	O
which	O
neural	Method
models	Method
need	O
to	O
pass	O
information	O
.	O
A	O
simple	O
method	O
to	O
manipulate	O
the	O
trade	O
-	O
off	O
between	O
vocabulary	Metric
size	Metric
and	O
text	O
size	O
is	O
to	O
use	O
shortlists	O
of	O
unsegmented	O
words	O
,	O
using	O
subword	O
units	O
only	O
for	O
rare	O
words	O
.	O
As	O
an	O
alternative	O
,	O
we	O
propose	O
a	O
segmentation	Method
algorithm	Method
based	O
on	O
byte	Method
pair	Method
encoding	Method
(	O
BPE	Method
)	O
,	O
which	O
lets	O
us	O
learn	O
a	O
vocabulary	O
that	O
provides	O
a	O
good	O
compression	Metric
rate	Metric
of	O
the	O
text	O
.	O
section	O
:	O
Byte	Method
Pair	Method
Encoding	Method
(	O
BPE	Method
)	O
Byte	Method
Pair	Method
Encoding	Method
(	O
BPE	Method
)	O
[	O
reference	O
]	O
)	O
is	O
a	O
simple	O
data	Method
compression	Method
technique	Method
that	O
iteratively	O
replaces	O
the	O
most	O
frequent	O
pair	O
of	O
bytes	O
in	O
a	O
sequence	O
with	O
a	O
single	O
,	O
unused	O
byte	O
.	O
We	O
adapt	O
this	O
algorithm	O
for	O
word	Task
segmentation	Task
.	O
Instead	O
of	O
merging	O
frequent	O
pairs	O
of	O
bytes	O
,	O
we	O
merge	O
characters	O
or	O
character	O
sequences	O
.	O
Firstly	O
,	O
we	O
initialize	O
the	O
symbol	O
vocabulary	O
with	O
the	O
character	O
vocabulary	O
,	O
and	O
represent	O
each	O
word	O
as	O
a	O
sequence	O
of	O
characters	O
,	O
plus	O
a	O
special	O
end	O
-	O
ofword	O
symbol	O
'	O
·	O
'	O
,	O
which	O
allows	O
us	O
to	O
restore	O
the	O
original	O
tokenization	O
after	O
translation	Task
.	O
We	O
iteratively	O
count	O
all	O
symbol	O
pairs	O
and	O
replace	O
each	O
occurrence	O
of	O
the	O
most	O
frequent	O
pair	O
(	O
'	O
A	O
'	O
,	O
'	O
B	O
'	O
)	O
with	O
a	O
new	O
symbol	O
'	O
AB	O
'	O
.	O
Each	O
merge	Method
operation	Method
produces	O
a	O
new	O
symbol	O
which	O
represents	O
a	O
character	O
n	O
-	O
gram	O
.	O
Frequent	O
character	O
n	O
-	O
grams	O
(	O
or	O
whole	O
words	O
)	O
are	O
eventually	O
merged	O
into	O
a	O
single	O
symbol	O
,	O
thus	O
BPE	Method
requires	O
no	O
shortlist	O
.	O
The	O
final	O
symbol	Metric
vocabulary	Metric
size	Metric
is	O
equal	O
to	O
the	O
size	O
of	O
the	O
initial	O
vocabulary	O
,	O
plus	O
the	O
number	O
of	O
merge	O
operations	O
-	O
the	O
latter	O
is	O
the	O
only	O
hyperparameter	O
of	O
the	O
algorithm	O
.	O
For	O
efficiency	O
,	O
we	O
do	O
not	O
consider	O
pairs	O
that	O
cross	O
word	O
boundaries	O
.	O
The	O
algorithm	O
can	O
thus	O
be	O
run	O
on	O
the	O
dictionary	O
extracted	O
from	O
a	O
text	O
,	O
with	O
each	O
word	O
being	O
weighted	O
by	O
its	O
frequency	O
.	O
A	O
minimal	O
Python	Method
implementation	Method
is	O
shown	O
in	O
Al	O
-	O
'	O
n	O
e	O
w	O
e	O
s	O
t	O
<	O
/	O
w>':6	O
,	O
'	O
w	O
i	O
d	O
e	O
s	O
t	O
<	O
/	O
w>':3	O
}	O
num_merges	Method
=	O
10	O
for	O
i	O
in	O
range	O
(	O
num_merges	O
)	O
:	O
pairs	O
=	O
get_stats	O
(	O
vocab	O
)	O
best	O
=	O
max	Method
(	Method
pairs	Method
,	O
key	O
=	O
pairs.get	O
)	O
vocab	O
=	O
merge_vocab	O
(	O
best	O
,	O
vocab	O
)	O
Figure	O
1	O
:	O
BPE	Method
merge	O
operations	O
learned	O
from	O
dictionary	O
{	O
'	O
low	O
'	O
,	O
'	O
lowest	O
'	O
,	O
'	O
newer	O
'	O
,	O
'	O
wider'}.	O
gorithm	O
1	O
.	O
In	O
practice	O
,	O
we	O
increase	O
efficiency	O
by	O
indexing	O
all	O
pairs	O
,	O
and	O
updating	O
data	Method
structures	Method
incrementally	O
.	O
The	O
main	O
difference	O
to	O
other	O
compression	Method
algorithms	Method
,	O
such	O
as	O
Huffman	Method
encoding	Method
,	O
which	O
have	O
been	O
proposed	O
to	O
produce	O
a	O
variable	Method
-	Method
length	Method
encoding	Method
of	Method
words	Method
for	O
NMT	Task
[	O
reference	O
]	O
,	O
is	O
that	O
our	O
symbol	O
sequences	O
are	O
still	O
interpretable	O
as	O
subword	O
units	O
,	O
and	O
that	O
the	O
network	O
can	O
generalize	O
to	O
translate	O
and	O
produce	O
new	O
words	O
(	O
unseen	O
at	O
training	O
time	O
)	O
on	O
the	O
basis	O
of	O
these	O
subword	O
units	O
.	O
Figure	O
1	O
shows	O
a	O
toy	O
example	O
of	O
learned	O
BPE	Method
operations	O
.	O
At	O
test	O
time	O
,	O
we	O
first	O
split	O
words	O
into	O
sequences	O
of	O
characters	O
,	O
then	O
apply	O
the	O
learned	O
operations	O
to	O
merge	O
the	O
characters	O
into	O
larger	O
,	O
known	O
symbols	O
.	O
This	O
is	O
applicable	O
to	O
any	O
word	O
,	O
and	O
allows	O
for	O
open	Method
-	Method
vocabulary	Method
networks	Method
with	O
fixed	O
symbol	O
vocabularies	O
.	O
[	O
reference	O
]	O
In	O
our	O
example	O
,	O
the	O
OOV	O
'	O
lower	O
'	O
would	O
be	O
segmented	O
into	O
'	O
low	O
er	O
·	O
'	O
.	O
We	O
evaluate	O
two	O
methods	O
of	O
applying	O
BPE	Method
:	O
learning	O
two	O
independent	Method
encodings	Method
,	O
one	O
for	O
the	O
source	O
,	O
one	O
for	O
the	O
target	O
vocabulary	O
,	O
or	O
learning	O
the	O
encoding	O
on	O
the	O
union	O
of	O
the	O
two	O
vocabularies	O
(	O
which	O
we	O
call	O
joint	O
BPE	Method
)	O
.	O
[	O
reference	O
]	O
The	O
former	O
has	O
the	O
advantage	O
of	O
being	O
more	O
compact	O
in	O
terms	O
of	O
text	O
and	O
vocabulary	Metric
size	Metric
,	O
and	O
having	O
stronger	O
guarantees	O
that	O
each	O
subword	O
unit	O
has	O
been	O
seen	O
in	O
the	O
training	O
text	O
of	O
the	O
respective	O
language	O
,	O
whereas	O
the	O
latter	O
improves	O
consistency	O
between	O
the	O
source	O
and	O
the	O
target	O
segmentation	O
.	O
If	O
we	O
apply	O
BPE	Method
independently	O
,	O
the	O
same	O
name	O
may	O
be	O
segmented	O
differently	O
in	O
the	O
two	O
languages	O
,	O
which	O
makes	O
it	O
harder	O
for	O
the	O
neural	Method
models	Method
to	O
learn	O
a	O
mapping	O
between	O
the	O
subword	O
units	O
.	O
To	O
increase	O
the	O
consistency	O
between	O
English	O
and	O
Russian	O
segmentation	O
despite	O
the	O
differing	O
alphabets	O
,	O
we	O
transliterate	O
the	O
Russian	O
vocabulary	O
into	O
Latin	O
characters	O
with	O
ISO	O
-	O
9	O
to	O
learn	O
the	O
joint	O
BPE	Method
encoding	O
,	O
then	O
transliterate	O
the	O
BPE	Method
merge	O
operations	O
back	O
into	O
Cyrillic	O
to	O
apply	O
them	O
to	O
the	O
Russian	Material
training	Material
text	Material
.	O
5	O
section	O
:	O
Evaluation	O
We	O
aim	O
to	O
answer	O
the	O
following	O
empirical	O
questions	O
:	O
•	O
Can	O
we	O
improve	O
the	O
translation	Task
of	O
rare	O
and	O
unseen	O
words	O
in	O
neural	O
machine	O
translation	Task
by	O
representing	O
them	O
via	O
subword	Method
units	Method
?	O
•	O
Which	O
segmentation	O
into	O
subword	O
units	O
performs	O
best	O
in	O
terms	O
of	O
vocabulary	Metric
size	Metric
,	O
text	Metric
size	Metric
,	O
and	O
translation	Task
quality	O
?	O
We	O
perform	O
experiments	O
on	O
data	O
from	O
the	O
shared	O
translation	Task
task	O
of	O
WMT	Material
2015	Material
.	O
For	O
English→German	Material
,	O
our	O
training	O
set	O
consists	O
of	O
4.2	O
million	O
sentence	O
pairs	O
,	O
or	O
approximately	O
100	O
million	O
tokens	O
.	O
For	O
English→Russian	Material
,	O
the	O
training	O
set	O
consists	O
of	O
2.6	O
million	O
sentence	O
pairs	O
,	O
or	O
approximately	O
50	O
million	O
tokens	O
.	O
We	O
tokenize	O
and	O
truecase	O
the	O
data	O
with	O
the	O
scripts	O
provided	O
in	O
Moses	O
[	O
reference	O
]	O
.	O
We	O
use	O
newstest2013	O
as	O
development	O
set	O
,	O
and	O
report	O
results	O
on	O
newstest2014	O
and	O
newstest2015	O
.	O
We	O
report	O
results	O
with	O
BLEU	Metric
(	O
mteval	O
-	O
v13a.pl	O
)	O
,	O
and	O
CHRF3	Method
[	O
reference	O
]	O
,	O
a	O
character	Metric
n	Metric
-	Metric
gram	Metric
F	Metric
3	Metric
score	Metric
which	O
was	O
found	O
to	O
correlate	O
well	O
with	O
human	O
judgments	O
,	O
especially	O
for	O
translations	O
out	O
of	O
English	O
[	O
reference	O
]	O
.	O
Since	O
our	O
main	O
claim	O
is	O
concerned	O
with	O
the	O
translation	Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
we	O
report	O
separate	O
statistics	O
for	O
these	O
.	O
We	O
measure	O
these	O
through	O
unigram	Metric
F	Metric
1	Metric
,	O
which	O
we	O
calculate	O
as	O
the	O
harmonic	Metric
mean	Metric
of	Metric
clipped	Metric
unigram	Metric
precision	Metric
and	Metric
recall	Metric
.	O
[	O
reference	O
]	O
We	O
perform	O
all	O
experiments	O
with	O
Groundhog	O
7	O
[	O
reference	O
]	O
.	O
We	O
generally	O
follow	O
settings	O
by	O
previous	O
work	O
[	O
reference	O
][	O
reference	O
]	O
.	O
All	O
networks	O
have	O
a	O
hidden	O
layer	O
size	O
of	O
1000	O
,	O
and	O
an	O
embedding	Metric
layer	Metric
size	Metric
of	O
620	O
.	O
Following	O
[	O
reference	O
]	O
,	O
we	O
only	O
keep	O
a	O
shortlist	O
of	O
τ	O
=	O
30000	O
words	O
in	O
memory	O
.	O
During	O
training	O
,	O
we	O
use	O
Adadelta	Method
(	O
Zeiler	O
,	O
2012	O
)	O
,	O
a	O
minibatch	O
size	O
of	O
80	O
,	O
and	O
reshuffle	O
the	O
training	O
set	O
between	O
epochs	O
.	O
We	O
train	O
a	O
network	O
for	O
approximately	O
7	O
days	O
,	O
then	O
take	O
the	O
last	O
4	O
saved	O
models	O
(	O
models	O
being	O
saved	O
every	O
12	O
hours	O
)	O
,	O
and	O
continue	O
training	O
each	O
with	O
a	O
fixed	O
embedding	Method
layer	Method
(	O
as	O
suggested	O
by	O
[	O
reference	O
]	O
)	O
for	O
12	O
hours	O
.	O
We	O
perform	O
two	O
independent	O
training	O
runs	O
for	O
each	O
models	O
,	O
once	O
with	O
cut	O
-	O
off	O
for	O
gradient	O
clipping	O
[	O
reference	O
]	O
of	O
5.0	O
,	O
once	O
with	O
a	O
cut	O
-	O
off	O
of	O
1.0	O
-	O
the	O
latter	O
produced	O
better	O
single	O
models	O
for	O
most	O
settings	O
.	O
We	O
report	O
results	O
of	O
the	O
system	O
that	O
performed	O
best	O
on	O
our	O
development	O
set	O
(	O
newstest2013	O
)	O
,	O
and	O
of	O
an	O
ensemble	O
of	O
all	O
8	O
models	O
.	O
We	O
use	O
a	O
beam	O
size	O
of	O
12	O
for	O
beam	Task
search	Task
,	O
with	O
probabilities	O
normalized	O
by	O
sentence	O
length	O
.	O
We	O
use	O
a	O
bilingual	O
dictionary	O
based	O
on	O
fast	Method
-	Method
align	Method
[	O
reference	O
]	O
.	O
For	O
our	O
baseline	O
,	O
this	O
serves	O
as	O
back	O
-	O
off	O
dictionary	O
for	O
rare	O
words	O
.	O
We	O
also	O
use	O
the	O
dictionary	O
to	O
speed	O
up	O
translation	Task
for	O
all	O
experiments	O
,	O
only	O
performing	O
the	O
softmax	Method
over	O
a	O
filtered	O
list	O
of	O
candidate	O
translations	O
(	O
like	O
[	O
reference	O
]	O
,	O
we	O
use	O
K	O
=	O
30000	O
;	O
K	O
′	O
=	O
10	O
)	O
.	O
section	O
:	O
Subword	Metric
statistics	Metric
Apart	O
from	O
translation	Task
quality	O
,	O
which	O
we	O
will	O
verify	O
empirically	O
,	O
our	O
main	O
objective	O
is	O
to	O
represent	O
an	O
open	O
vocabulary	O
through	O
a	O
compact	O
fixed	O
-	O
size	O
subword	O
vocabulary	O
,	O
and	O
allow	O
for	O
efficient	O
training	Task
and	Task
decoding	Task
.	O
[	O
reference	O
]	O
Statistics	O
for	O
different	O
segmentations	O
of	O
the	O
Ger	O
-	O
man	O
side	O
of	O
the	O
parallel	O
data	O
are	O
shown	O
in	O
Table	O
1	O
.	O
A	O
simple	O
baseline	O
is	O
the	O
segmentation	Task
of	Task
words	Task
into	O
character	Method
n	Method
-	Method
grams	Method
.	O
9	O
Character	Method
n	Method
-	Method
grams	Method
allow	O
for	O
different	O
trade	O
-	O
offs	O
between	O
sequence	O
length	O
(	O
#	O
tokens	O
)	O
and	O
vocabulary	O
size	O
(	O
#	O
types	O
)	O
,	O
depending	O
on	O
the	O
choice	O
of	O
n.	O
The	O
increase	O
in	O
sequence	O
length	O
is	O
substantial	O
;	O
one	O
way	O
to	O
reduce	O
sequence	O
length	O
is	O
to	O
leave	O
a	O
shortlist	O
of	O
the	O
k	O
most	O
frequent	O
word	O
types	O
unsegmented	O
.	O
Only	O
the	O
unigram	Method
representation	Method
is	O
truly	O
open	O
-	O
vocabulary	O
.	O
However	O
,	O
the	O
unigram	Method
representation	Method
performed	O
poorly	O
in	O
preliminary	O
experiments	O
,	O
and	O
we	O
report	O
translation	Task
results	O
with	O
a	O
bigram	Method
representation	Method
,	O
which	O
is	O
empirically	O
better	O
,	O
but	O
unable	O
to	O
produce	O
some	O
tokens	O
in	O
the	O
test	O
set	O
with	O
the	O
training	O
set	O
vocabulary	O
.	O
We	O
report	O
statistics	O
for	O
several	O
word	Method
segmentation	Method
techniques	Method
that	O
have	O
proven	O
useful	O
in	O
previous	O
SMT	Method
research	O
,	O
including	O
frequency	Method
-	Method
based	Method
compound	Method
splitting	Method
[	O
reference	O
]	O
,	O
rulebased	Method
hyphenation	Method
[	O
reference	O
]	O
,	O
and	O
Morfessor	Method
[	O
reference	O
]	O
.	O
We	O
find	O
that	O
they	O
only	O
moderately	O
reduce	O
vocabulary	Metric
size	Metric
,	O
and	O
do	O
not	O
solve	O
the	O
unknown	Task
word	Task
problem	Task
,	O
and	O
we	O
thus	O
find	O
them	O
unsuitable	O
for	O
our	O
goal	O
of	O
open	O
-	O
vocabulary	O
translation	Task
without	O
back	Task
-	Task
off	Task
dictionary	Task
.	O
BPE	Method
meets	O
our	O
goal	O
of	O
being	O
open	O
-	O
vocabulary	O
,	O
and	O
the	O
learned	O
merge	Method
operations	Method
can	O
be	O
applied	O
to	O
the	O
test	O
set	O
to	O
obtain	O
a	O
segmentation	Task
with	O
no	O
unknown	O
symbols	O
.	O
[	O
reference	O
]	O
Its	O
main	O
difference	O
from	O
the	O
character	Method
-	Method
level	Method
model	Method
is	O
that	O
the	O
more	O
compact	Method
representation	Method
of	O
BPE	Method
allows	O
for	O
shorter	O
sequences	O
,	O
and	O
that	O
the	O
attention	Method
model	Method
operates	O
on	O
variable	O
-	O
length	O
units	O
.	O
11	O
Table	O
1	O
shows	O
BPE	Method
with	O
59	O
500	O
merge	Method
operations	Method
,	O
and	O
joint	O
BPE	Method
with	O
89	O
500	O
operations	O
.	O
In	O
practice	O
,	O
we	O
did	O
not	O
include	O
infrequent	O
subword	O
units	O
in	O
the	O
NMT	Task
network	O
vocabulary	O
,	O
since	O
there	O
is	O
noise	O
in	O
the	O
subword	O
symbol	O
sets	O
,	O
e.g.	O
because	O
of	O
characters	O
from	O
foreign	O
alphabets	O
.	O
Hence	O
,	O
our	O
network	O
vocabularies	O
in	O
Table	O
2	O
are	O
typically	O
slightly	O
smaller	O
than	O
the	O
number	O
of	O
types	O
in	O
Table	O
1	O
.	O
9	O
Our	O
character	Method
n	Method
-	Method
grams	Method
do	O
not	O
cross	O
word	O
boundaries	O
.	O
We	O
mark	O
whether	O
a	O
subword	O
is	O
word	O
-	O
final	O
or	O
not	O
with	O
a	O
special	O
character	O
,	O
which	O
allows	O
us	O
to	O
restore	O
the	O
original	O
tokenization	O
.	O
10	O
Joint	O
BPE	Method
can	O
produce	O
segments	O
that	O
are	O
unknown	O
because	O
they	O
only	O
occur	O
in	O
the	O
English	O
training	O
text	O
,	O
but	O
these	O
are	O
rare	O
(	O
0.05	O
%	O
of	O
test	O
tokens	O
)	O
.	O
[	O
reference	O
]	O
We	O
highlighted	O
the	O
limitations	O
of	O
word	O
-	O
level	O
attention	O
in	O
section	O
3.1	O
.	O
At	O
the	O
other	O
end	O
of	O
the	O
spectrum	O
,	O
the	O
character	O
level	O
is	O
suboptimal	O
for	O
alignment	Task
[	O
reference	O
]	O
[	O
reference	O
]	O
;	O
⋄	O
:	O
[	O
reference	O
]	O
.	O
section	O
:	O
Translation	Task
experiments	Task
English→German	Material
translation	Task
results	O
are	O
shown	O
in	O
Table	O
2	O
;	O
English→Russian	Material
results	O
in	O
Table	O
3	O
.	O
Our	O
baseline	O
WDict	Method
is	O
a	O
word	Method
-	Method
level	Method
model	Method
with	O
a	O
back	Method
-	Method
off	Method
dictionary	Method
.	O
It	O
differs	O
from	O
WUnk	Method
in	O
that	O
the	O
latter	O
uses	O
no	O
back	O
-	O
off	O
dictionary	O
,	O
and	O
just	O
represents	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
as	O
UNK	O
12	O
.	O
The	O
back	Method
-	Method
off	Method
dictionary	Method
improves	O
unigram	Metric
F	Metric
1	Metric
for	O
rare	O
and	O
unseen	O
words	O
,	O
although	O
the	O
improvement	O
is	O
smaller	O
for	O
English→Russian	Material
,	O
since	O
the	O
back	O
-	O
off	O
dictionary	O
is	O
incapable	O
of	O
transliterating	O
names	O
.	O
All	O
subword	Method
systems	Method
operate	O
without	O
a	O
back	O
-	O
off	O
dictionary	O
.	O
We	O
first	O
focus	O
on	O
unigram	Task
F	Task
1	Task
,	O
where	O
all	O
systems	O
improve	O
over	O
the	O
baseline	O
,	O
especially	O
for	O
rare	O
words	O
(	O
36.8%→41.8	O
%	O
for	O
EN→DE	Material
;	O
26.5%→29.7	O
%	O
for	O
EN→RU	Material
)	O
.	O
For	O
OOVs	Task
,	O
the	O
baseline	O
strategy	O
of	O
copying	Task
unknown	Task
words	Task
works	O
well	O
for	O
English→German	Material
.	O
However	O
,	O
when	O
alphabets	O
differ	O
,	O
like	O
in	O
English→Russian	Material
,	O
the	O
subword	Method
models	Method
do	O
much	O
better	O
.	O
Unigram	Metric
F	Metric
1	Metric
scores	Metric
indicate	O
that	O
learning	O
the	O
BPE	Method
symbols	O
on	O
the	O
vocabulary	O
union	O
(	O
BPEJ90k	Method
)	O
is	O
more	O
effective	O
than	O
learning	O
them	O
separately	O
(	O
BPE	Method
-	O
60k	O
)	O
,	O
and	O
more	O
effective	O
than	O
using	O
character	Method
bigrams	Method
with	O
a	O
shortlist	O
of	O
50	O
000	O
unsegmented	O
words	O
(	O
C2	O
-	O
50k	O
)	O
,	O
but	O
all	O
reported	O
subword	Method
segmentations	Method
are	O
viable	O
choices	O
and	O
outperform	O
the	O
back	Method
-	Method
off	Method
dictionary	Method
baseline	Method
.	O
Our	O
subword	Method
representations	Method
cause	O
big	O
improvements	O
in	O
the	O
translation	Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
but	O
these	O
only	O
constitute	O
9	O
-	O
11	O
%	O
of	O
the	O
test	O
sets	O
.	O
Since	O
rare	O
words	O
tend	O
to	O
carry	O
central	O
information	O
in	O
a	O
sentence	O
,	O
we	O
suspect	O
that	O
BLEU	Metric
and	O
CHRF3	Method
underestimate	O
their	O
effect	O
on	O
translation	Task
quality	O
.	O
Still	O
,	O
we	O
also	O
see	O
improvements	O
over	O
the	O
baseline	O
in	O
total	Metric
unigram	Metric
F	Metric
1	Metric
,	O
as	O
well	O
as	O
BLEU	Metric
and	O
CHRF3	Method
,	O
and	O
the	O
subword	Method
ensembles	Method
outperform	O
the	O
WDict	Method
baseline	Method
by	O
0.3	O
-	O
1.3	O
BLEU	Metric
and	O
0.6	O
-	O
2	O
CHRF3	O
.	O
There	O
is	O
some	O
inconsistency	O
between	O
BLEU	Metric
and	O
CHRF3	Method
,	O
which	O
we	O
attribute	O
to	O
the	O
fact	O
that	O
BLEU	Metric
has	O
a	O
precision	Metric
bias	Metric
,	O
and	O
CHRF3	Method
a	O
recall	Metric
bias	Metric
.	O
For	O
English→German	Material
,	O
we	O
observe	O
the	O
best	O
BLEU	Metric
score	O
of	O
25.3	O
with	O
C2	Method
-	O
50k	O
,	O
but	O
the	O
best	O
CHRF3	Metric
score	Metric
of	O
54.1	O
with	O
BPE	Method
-	Method
J90k	Method
.	O
For	O
comparison	O
to	O
the	O
(	O
to	O
our	O
knowledge	O
)	O
best	O
non	Method
-	Method
neural	Method
MT	Method
system	Method
on	O
this	O
data	O
set	O
,	O
we	O
report	O
syntaxbased	O
SMT	Method
results	O
[	O
reference	O
]	O
.	O
We	O
observe	O
that	O
our	O
best	O
systems	O
outperform	O
the	O
syntax	Method
-	Method
based	Method
system	Method
in	O
terms	O
of	O
BLEU	Metric
,	O
but	O
not	O
in	O
terms	O
of	O
CHRF3	Method
.	O
Regarding	O
other	O
neural	Method
systems	Method
,	O
[	O
reference	O
]	O
report	O
a	O
BLEU	Metric
score	O
of	O
25.9	O
on	O
newstest2015	O
,	O
but	O
we	O
note	O
that	O
they	O
use	O
an	O
ensemble	O
of	O
8	O
independently	O
trained	O
models	O
,	O
and	O
also	O
report	O
strong	O
improvements	O
from	O
applying	O
dropout	Method
,	O
which	O
we	O
did	O
not	O
use	O
.	O
We	O
are	O
confident	O
that	O
our	O
improvements	O
to	O
the	O
translation	Task
of	O
rare	O
words	O
are	O
orthogonal	O
to	O
improvements	O
achievable	O
through	O
other	O
improvements	O
in	O
the	O
network	Method
architecture	Method
,	O
training	Method
algorithm	Method
,	O
or	O
better	O
ensembles	Method
.	O
For	O
English→Russian	Material
,	O
the	O
state	O
of	O
the	O
art	O
is	O
the	O
phrase	Method
-	Method
based	Method
system	Method
by	O
.	O
It	O
outperforms	O
our	O
WDict	Method
baseline	Method
by	O
1.5	O
BLEU	Metric
.	O
The	O
subword	Method
models	Method
are	O
a	O
step	O
towards	O
closing	O
this	O
gap	O
,	O
and	O
BPE	Method
-	Method
J90k	Method
yields	O
an	O
improvement	O
of	O
1.3	O
BLEU	Metric
,	O
and	O
2.0	O
CHRF3	Method
,	O
over	O
WDict	Method
.	O
As	O
a	O
further	O
comment	O
on	O
our	O
translation	Task
results	O
,	O
we	O
want	O
to	O
emphasize	O
that	O
performance	O
variability	O
is	O
still	O
an	O
open	O
problem	O
with	O
NMT	Task
.	O
On	O
our	O
development	O
set	O
,	O
we	O
observe	O
differences	O
of	O
up	O
to	O
1	O
BLEU	Metric
between	O
different	O
models	O
.	O
For	O
single	O
systems	O
,	O
we	O
report	O
the	O
results	O
of	O
the	O
model	O
that	O
performs	O
best	O
on	O
dev	O
(	O
out	O
of	O
8	O
)	O
,	O
which	O
has	O
a	O
stabilizing	O
effect	O
,	O
but	O
how	O
to	O
control	O
for	O
randomness	O
deserves	O
further	O
attention	O
in	O
future	O
research	O
.	O
section	O
:	O
Analysis	O
section	O
:	O
Unigram	Metric
accuracy	Metric
Our	O
main	O
claims	O
are	O
that	O
the	O
translation	Task
of	O
rare	O
and	O
unknown	O
words	O
is	O
poor	O
in	O
word	O
-	O
level	O
NMT	Task
models	O
,	O
and	O
that	O
subword	Method
models	Method
improve	O
the	O
translation	Task
of	O
these	O
word	O
types	O
.	O
To	O
further	O
illustrate	O
the	O
effect	O
of	O
different	O
subword	Method
segmentations	Method
on	O
the	O
translation	Task
of	O
rare	O
and	O
unseen	O
words	O
,	O
we	O
plot	O
target	O
-	O
side	O
words	O
sorted	O
by	O
their	O
frequency	O
in	O
the	O
training	O
set	O
.	O
[	O
reference	O
]	O
To	O
analyze	O
the	O
effect	O
of	O
vocabulary	Metric
size	Metric
,	O
we	O
also	O
include	O
the	O
system	Method
C2	Method
-	Method
3	Method
/	Method
500k	Method
,	O
which	O
is	O
a	O
system	O
with	O
the	O
same	O
vocabulary	O
size	O
as	O
the	O
WDict	Method
baseline	Method
,	O
and	O
character	Method
bigrams	Method
to	O
represent	O
unseen	O
words	O
.	O
Figure	O
2	O
shows	O
results	O
for	O
the	O
English	Method
-	Method
German	Method
ensemble	Method
systems	Method
on	O
newstest2015	O
.	O
Unigram	Metric
F	Metric
1	Metric
of	O
all	O
systems	O
tends	O
to	O
decrease	O
for	O
lowerfrequency	O
words	O
.	O
The	O
baseline	O
system	O
has	O
a	O
spike	O
in	O
F	Metric
1	Metric
for	O
OOVs	O
,	O
i.e.	O
words	O
that	O
do	O
not	O
occur	O
in	O
the	O
training	O
text	O
.	O
This	O
is	O
because	O
a	O
high	O
proportion	O
of	O
OOVs	O
are	O
names	O
,	O
for	O
which	O
a	O
copy	O
from	O
the	O
source	O
to	O
the	O
target	O
text	O
is	O
a	O
good	O
strategy	O
for	O
English→German	Material
.	O
The	O
systems	O
with	O
a	O
target	O
vocabulary	O
of	O
500	O
000	O
words	O
mostly	O
differ	O
in	O
how	O
well	O
they	O
translate	O
words	O
with	O
rank	O
>	O
500	O
000	O
.	O
A	O
back	O
-	O
off	O
dictionary	O
is	O
an	O
obvious	O
improvement	O
over	O
producing	O
UNK	Method
,	O
but	O
the	O
subword	Method
system	Method
C2	Method
-	Method
3	Method
/	Method
500k	Method
achieves	O
better	O
performance	O
.	O
Note	O
that	O
all	O
OOVs	O
that	O
the	O
backoff	O
dictionary	O
produces	O
are	O
words	O
that	O
are	O
copied	O
from	O
the	O
source	O
,	O
usually	O
names	O
,	O
while	O
the	O
subword	O
[	O
reference	O
]	O
We	O
perform	O
binning	O
of	O
words	O
with	O
the	O
same	O
training	O
set	O
frequency	O
,	O
and	O
apply	O
bezier	Method
smoothing	Method
to	O
the	O
graph	O
.	O
systems	O
can	O
productively	O
form	O
new	O
words	O
such	O
as	O
compounds	O
.	O
For	O
the	O
50	O
000	O
most	O
frequent	O
words	O
,	O
the	O
representation	O
is	O
the	O
same	O
for	O
all	O
neural	Method
networks	Method
,	O
and	O
all	O
neural	Method
networks	Method
achieve	O
comparable	O
unigram	Metric
F	Metric
1	Metric
for	O
this	O
category	O
.	O
For	O
the	O
interval	O
between	O
frequency	O
rank	O
50	O
000	O
and	O
500	O
000	O
,	O
the	O
comparison	O
between	O
C2	O
-	O
3	O
/	O
500k	O
and	O
C2	O
-	O
50k	O
unveils	O
an	O
interesting	O
difference	O
.	O
The	O
two	O
systems	O
only	O
differ	O
in	O
the	O
size	O
of	O
the	O
shortlist	O
,	O
with	O
C2	O
-	O
3	O
/	O
500k	O
representing	O
words	O
in	O
this	O
interval	O
as	O
single	O
units	O
,	O
and	O
C2	O
-	O
50k	O
via	O
subword	Method
units	Method
.	O
We	O
find	O
that	O
the	O
performance	O
of	O
C2	Method
-	Method
3	Method
/	Method
500k	Method
degrades	O
heavily	O
up	O
to	O
frequency	O
rank	O
500	O
000	O
,	O
at	O
which	O
point	O
the	O
model	O
switches	O
to	O
a	O
subword	Method
representation	Method
and	O
performance	O
recovers	O
.	O
The	O
performance	O
of	O
C2	O
-	O
50k	O
remains	O
more	O
stable	O
.	O
We	O
attribute	O
this	O
to	O
the	O
fact	O
that	O
subword	O
units	O
are	O
less	O
sparse	O
than	O
words	O
.	O
In	O
our	O
training	O
set	O
,	O
the	O
frequency	O
rank	O
50	O
000	O
corresponds	O
to	O
a	O
frequency	O
of	O
60	O
in	O
the	O
training	O
data	O
;	O
the	O
frequency	O
rank	O
500	O
000	O
to	O
a	O
frequency	O
of	O
2	O
.	O
Because	O
subword	Method
representations	Method
are	O
less	O
sparse	O
,	O
reducing	O
the	O
size	O
of	O
the	O
network	O
vocabulary	O
,	O
and	O
representing	O
more	O
words	O
via	O
subword	O
units	O
,	O
can	O
lead	O
to	O
better	O
performance	O
.	O
The	O
F	Metric
1	Metric
numbers	Metric
hide	O
some	O
qualitative	O
differences	O
between	O
systems	O
.	O
For	O
English→German	Material
,	O
WDict	O
produces	O
few	O
OOVs	O
(	O
26.5	O
%	O
recall	Metric
)	O
,	O
but	O
with	O
high	O
precision	Metric
(	O
60.6	O
%	O
)	O
,	O
whereas	O
the	O
subword	Method
systems	Method
achieve	O
higher	O
recall	Metric
,	O
but	O
lower	O
precision	Metric
.	O
We	O
note	O
that	O
the	O
character	Method
bigram	Method
model	Method
C2	Method
-	Method
50k	Method
produces	O
the	O
most	O
OOV	O
words	O
,	O
and	O
achieves	O
relatively	O
low	O
precision	Metric
of	O
29.1	O
%	O
for	O
this	O
category	O
.	O
However	O
,	O
it	O
outperforms	O
the	O
back	Method
-	Method
off	Method
dictionary	Method
in	O
recall	Metric
(	O
33.0	O
%	O
)	O
.	O
BPE	Method
-	O
60k	O
,	O
which	O
suffers	O
from	O
transliteration	O
(	O
or	O
copy	O
)	O
errors	O
due	O
to	O
segmentation	O
inconsistencies	O
,	O
obtains	O
a	O
slightly	O
better	O
precision	Metric
(	O
32.4	O
%	O
)	O
,	O
but	O
a	O
worse	O
recall	Metric
(	O
26.6	O
%	O
)	O
.	O
In	O
contrast	O
to	O
BPE	Method
-	O
60k	O
,	O
the	O
joint	O
BPE	Method
encoding	O
of	O
BPEJ90k	Method
improves	O
both	O
precision	Metric
(	O
38.6	O
%	O
)	O
and	O
recall	Metric
(	O
29.8	O
%	O
)	O
.	O
For	O
English→Russian	Material
,	O
unknown	O
names	O
can	O
only	O
rarely	O
be	O
copied	O
,	O
and	O
usually	O
require	O
transliteration	O
.	O
Consequently	O
,	O
the	O
WDict	Method
baseline	Method
performs	O
more	O
poorly	O
for	O
OOVs	Task
(	O
9.2	O
%	O
precision	Metric
;	O
5.2	O
%	O
recall	Metric
)	O
,	O
and	O
the	O
subword	Method
models	Method
improve	O
both	O
precision	Metric
and	O
recall	Metric
(	O
21.9	O
%	O
precision	Metric
and	O
15.6	O
%	O
recall	Metric
for	O
BPE	Method
-	Method
J90k	Method
)	O
.	O
The	O
full	O
unigram	Metric
F	Metric
1	Metric
plot	Metric
is	O
shown	O
in	O
Figure	O
3	O
.	O
24	O
.	O
Table	O
3	O
:	O
English→Russian	Material
translation	Task
performance	O
(	O
BLEU	Metric
,	O
CHRF3	Metric
and	O
unigram	Metric
F	Metric
1	Metric
)	O
on	O
newstest2015	O
.	O
Ens	O
-	O
8	O
:	O
ensemble	O
of	O
8	O
models	O
.	O
Best	O
NMT	Task
system	O
in	O
bold	O
.	O
Unigram	Metric
F	Metric
1	Metric
(	O
with	O
ensembles	Method
)	O
is	O
computed	O
for	O
all	O
words	O
(	O
n	O
=	O
55654	O
)	O
,	O
rare	O
words	O
(	O
not	O
among	O
top	O
50	O
000	O
in	O
training	O
set	O
;	O
n	O
=	O
5442	O
)	O
,	O
and	O
OOVs	O
(	O
not	O
in	O
training	O
set	O
;	O
n	O
=	O
851	O
)	O
.	O
The	O
English→Russian	Material
examples	O
show	O
that	O
the	O
subword	Method
systems	Method
are	O
capable	O
of	O
transliteration	Task
.	O
However	O
,	O
transliteration	Metric
errors	Metric
do	O
occur	O
,	O
either	O
due	O
to	O
ambiguous	O
transliterations	O
,	O
or	O
because	O
of	O
non	O
-	O
consistent	O
segmentations	O
between	O
source	O
and	O
target	O
text	O
which	O
make	O
it	O
hard	O
for	O
the	O
system	O
to	O
learn	O
a	O
transliteration	Task
mapping	Task
.	O
Note	O
that	O
the	O
BPE	Method
-	O
60k	O
system	O
encodes	O
Mirzayeva	O
inconsistently	O
for	O
the	O
two	O
language	O
pairs	O
(	O
Mirz|ayeva→Мир|за|ева	O
Mir|za|eva	O
)	O
.	O
This	O
example	O
is	O
still	O
translated	O
correctly	O
,	O
but	O
we	O
observe	O
spurious	O
insertions	O
and	O
deletions	O
of	O
characters	O
in	O
the	O
BPE	Method
-	O
60k	O
system	O
.	O
An	O
example	O
is	O
the	O
transliteration	O
of	O
rakfisk	Method
,	O
where	O
a	O
п	O
is	O
inserted	O
and	O
a	O
к	O
is	O
deleted	O
.	O
We	O
trace	O
this	O
error	O
back	O
to	O
translation	Task
pairs	O
in	O
the	O
training	O
data	O
with	O
inconsistent	O
segmentations	O
,	O
such	O
as	O
(	O
p|rak|ri|ti→пра|крит|и	O
system	O
sentence	O
source	O
health	O
research	O
institutes	O
reference	O
Gesundheitsforschungsinstitute	O
WDict	O
Forschungsinstitute	O
C2	O
-	O
50k	O
Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n	O
BPE	Method
-	O
60k	O
Gesundheits|forsch|ungsinstitu|ten	O
BPE	Method
-	O
J90k	O
Gesundheits|forsch|ungsin|stitute	O
source	O
asinine	O
situation	O
reference	O
dumme	O
Situation	O
WDict	O
asinine	O
situation	O
→	O
UNK	O
→	O
asinine	O
C2	O
-	O
50k	O
as|in|in|e	O
situation	O
→	O
As|in|en|si|tu|at|io|n	O
BPE	Method
-	O
60k	O
as|in|ine	O
situation	O
→	O
A|in|line	O
-	O
|Situation	O
BPE	Method
-	O
J90	O
K	O
as|in|ine	O
situation	O
→	O
As|in|in	O
-	O
|Situation	O
(	O
pra|krit|i	O
)	O
)	O
,	O
from	O
which	O
the	O
translation	Task
(	O
rak→пра	O
)	O
is	O
erroneously	O
learned	O
.	O
The	O
segmentation	Task
of	O
the	O
joint	O
BPE	Method
system	O
(	O
BPE	Method
-	Method
J90k	Method
)	O
is	O
more	O
consistent	O
(	O
pra|krit|i→пра|крит|и	O
(	O
pra|krit|i	O
)	O
)	O
.	O
section	O
:	O
Manual	Task
Analysis	Task
section	O
:	O
Conclusion	O
The	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
that	O
we	O
show	O
that	O
neural	O
machine	O
translation	Task
systems	O
are	O
capable	O
of	O
open	O
-	O
vocabulary	O
translation	Task
by	O
representing	O
rare	O
and	O
unseen	O
words	O
as	O
a	O
sequence	O
of	O
subword	O
units	O
.	O
14	O
This	O
is	O
both	O
simpler	O
and	O
more	O
effective	O
than	O
using	O
a	O
back	O
-	O
off	O
translation	Task
model	O
.	O
We	O
introduce	O
a	O
variant	O
of	O
byte	Method
pair	Method
encoding	Method
for	O
word	Task
segmentation	Task
,	O
which	O
is	O
capable	O
of	O
encoding	O
open	O
vocabularies	O
with	O
a	O
compact	O
symbol	O
vocabulary	O
of	O
variable	O
-	O
length	O
subword	O
units	O
.	O
We	O
show	O
performance	O
gains	O
over	O
the	O
baseline	O
with	O
both	O
BPE	Method
segmentation	Method
,	O
and	O
a	O
simple	O
character	Method
bigram	Method
segmentation	Method
.	O
Our	O
analysis	O
shows	O
that	O
not	O
only	O
out	O
-	O
ofvocabulary	O
words	O
,	O
but	O
also	O
rare	O
in	O
-	O
vocabulary	O
words	O
are	O
translated	O
poorly	O
by	O
our	O
baseline	O
NMT	Task
system	O
,	O
and	O
that	O
reducing	O
the	O
vocabulary	Metric
size	Metric
of	O
subword	Method
models	Method
can	O
actually	O
improve	O
performance	O
.	O
In	O
this	O
work	O
,	O
our	O
choice	O
of	O
vocabulary	O
size	O
is	O
somewhat	O
arbitrary	O
,	O
and	O
mainly	O
motivated	O
by	O
comparison	O
to	O
prior	O
work	O
.	O
One	O
avenue	O
of	O
future	O
research	O
is	O
to	O
learn	O
the	O
optimal	O
vocabulary	O
size	O
for	O
a	O
translation	Task
task	O
,	O
which	O
we	O
expect	O
to	O
depend	O
on	O
the	O
language	O
pair	O
and	O
amount	O
of	O
training	O
data	O
,	O
automatically	O
.	O
We	O
also	O
believe	O
there	O
is	O
further	O
potential	O
in	O
bilingually	Method
informed	Method
segmentation	Method
algorithms	Method
to	O
create	O
more	O
alignable	O
subword	O
units	O
,	O
although	O
the	O
segmentation	Method
algorithm	Method
can	O
not	O
rely	O
on	O
the	O
target	O
text	O
at	O
runtime	O
.	O
While	O
the	O
relative	O
effectiveness	O
will	O
depend	O
on	O
language	O
-	O
specific	O
factors	O
such	O
as	O
vocabulary	O
size	O
,	O
we	O
believe	O
that	O
subword	Method
segmentations	Method
are	O
suitable	O
for	O
most	O
language	O
pairs	O
,	O
eliminating	O
the	O
need	O
for	O
large	O
NMT	Task
vocabularies	O
or	O
back	Method
-	Method
off	Method
models	Method
.	O
section	O
:	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
Maja	O
Popović	O
for	O
her	O
implementation	O
of	O
CHRF	Method
,	O
with	O
which	O
we	O
verified	O
our	O
reimplementation	O
.	O
The	O
research	O
presented	O
in	O
this	O
publication	O
was	O
conducted	O
in	O
cooperation	O
with	O
Samsung	O
Electronics	O
Polska	O
sp	O
.	O
z	O
o.o	O
.	O
-	O
Samsung	O
R	O
&	O
D	O
Institute	O
Poland	O
.	O
This	O
project	O
received	O
funding	O
from	O
the	O
European	O
Union	O
's	O
Horizon	O
2020	O
research	O
and	O
innovation	O
programme	O
under	O
grant	O
agreement	O
645452	O
(	O
QT21	O
)	O
.	O
section	O
:	O
