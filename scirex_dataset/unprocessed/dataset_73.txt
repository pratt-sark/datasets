document	O
:	O
An	O
Actor	Method
-	Method
Critic	Method
Algorithm	Method
for	O
Sequence	Task
Prediction	Task
We	O
present	O
an	O
approach	O
to	O
training	O
neural	Method
networks	Method
to	O
generate	O
sequences	O
using	O
actor	Method
-	Method
critic	Method
methods	Method
from	O
reinforcement	Method
learning	Method
(	O
RL	Method
)	O
.	O
Current	O
log	Method
-	Method
likelihood	Method
training	Method
methods	Method
are	O
limited	O
by	O
the	O
discrepancy	O
between	O
their	O
training	O
and	O
testing	O
modes	O
,	O
as	O
models	O
must	O
generate	O
tokens	O
conditioned	O
on	O
their	O
previous	O
guesses	O
rather	O
than	O
the	O
ground	O
-	O
truth	O
tokens	O
.	O
We	O
address	O
this	O
problem	O
by	O
introducing	O
a	O
critic	Method
network	Method
that	O
is	O
trained	O
to	O
predict	O
the	O
value	O
of	O
an	O
output	O
token	O
,	O
given	O
the	O
policy	O
of	O
an	O
actor	Method
network	Method
.	O
This	O
results	O
in	O
a	O
training	Method
procedure	Method
that	O
is	O
much	O
closer	O
to	O
the	O
test	O
phase	O
,	O
and	O
allows	O
us	O
to	O
directly	O
optimize	O
for	O
a	O
task	Metric
-	Metric
specific	Metric
score	Metric
such	O
as	O
BLEU	Metric
.	O
Crucially	O
,	O
since	O
we	O
leverage	O
these	O
techniques	O
in	O
the	O
supervised	Task
learning	Task
setting	Task
rather	O
than	O
the	O
traditional	O
RL	Method
setting	Method
,	O
we	O
condition	O
the	O
critic	Method
network	Method
on	O
the	O
ground	O
-	O
truth	O
output	O
.	O
We	O
show	O
that	O
our	O
method	O
leads	O
to	O
improved	O
performance	O
on	O
both	O
a	O
synthetic	Task
task	Task
,	O
and	O
for	O
German	O
-	O
English	O
machine	Task
translation	Task
.	O
Our	O
analysis	O
paves	O
the	O
way	O
for	O
such	O
methods	O
to	O
be	O
applied	O
in	O
natural	Task
language	Task
generation	Task
tasks	Task
,	O
such	O
as	O
machine	Task
translation	Task
,	O
caption	Task
generation	Task
,	O
and	O
dialogue	Task
modelling	Task
.	O
section	O
:	O
Introduction	O
In	O
many	O
important	O
applications	O
of	O
machine	Task
learning	Task
,	O
the	O
task	O
is	O
to	O
develop	O
a	O
system	O
that	O
produces	O
a	O
sequence	O
of	O
discrete	O
tokens	O
given	O
an	O
input	O
.	O
Recent	O
work	O
has	O
shown	O
that	O
recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	Method
can	O
deliver	O
excellent	O
performance	O
in	O
many	O
such	O
tasks	O
when	O
trained	O
to	O
predict	O
the	O
next	O
output	O
token	O
given	O
the	O
input	O
and	O
previous	O
tokens	O
.	O
This	O
approach	O
has	O
been	O
applied	O
successfully	O
in	O
machine	Task
translation	Task
sutskever2014sequence	O
,	O
bahdanau2015neural	O
,	O
caption	Task
generation	Task
kiros2014unifying	O
,	O
donahue2015long	O
,	O
vinyals2015show	O
,	O
xu2015show	O
,	O
karpathy2015deep	O
,	O
and	O
speech	Task
recognition	Task
chorowski2015attention	O
,	O
chan2015listen	O
.	O
The	O
standard	O
way	O
to	O
train	O
RNNs	Method
to	O
generate	O
sequences	O
is	O
to	O
maximize	O
the	O
log	O
-	O
likelihood	O
of	O
the	O
“	O
correct	O
”	O
token	O
given	O
a	O
history	O
of	O
the	O
previous	O
“	O
correct	O
”	O
ones	O
,	O
an	O
approach	O
often	O
called	O
teacher	Method
forcing	Method
.	O
At	O
evaluation	O
time	O
,	O
the	O
output	O
sequence	O
is	O
often	O
produced	O
by	O
an	O
approximate	Method
search	Method
for	O
the	O
most	O
likely	O
candidate	O
according	O
to	O
the	O
learned	O
distribution	O
.	O
During	O
this	O
search	O
,	O
the	O
model	O
is	O
conditioned	O
on	O
its	O
own	O
guesses	O
,	O
which	O
may	O
be	O
incorrect	O
and	O
thus	O
lead	O
to	O
a	O
compounding	O
of	O
errors	O
bengio2015scheduled	O
.	O
This	O
can	O
become	O
especially	O
problematic	O
for	O
longer	O
sequences	O
.	O
Due	O
to	O
this	O
discrepancy	O
between	O
training	O
and	O
testing	O
conditions	O
,	O
it	O
has	O
been	O
shown	O
that	O
maximum	Method
likelihood	Method
training	Method
can	O
be	O
suboptimal	O
bengio2015scheduled	O
,	O
ranzato2015sequence	O
.	O
In	O
these	O
works	O
,	O
the	O
authors	O
argue	O
that	O
the	O
network	O
should	O
be	O
trained	O
to	O
continue	O
generating	O
correctly	O
given	O
the	O
outputs	O
already	O
produced	O
by	O
the	O
model	O
,	O
rather	O
than	O
the	O
ground	O
-	O
truth	O
reference	O
outputs	O
from	O
the	O
data	O
.	O
This	O
gives	O
rise	O
to	O
the	O
challenging	O
problem	O
of	O
determining	O
the	O
target	O
for	O
the	O
next	O
network	O
output	O
.	O
bengio2015scheduled	O
use	O
the	O
token	O
from	O
the	O
ground	O
-	O
truth	O
answer	O
as	O
the	O
target	O
for	O
the	O
network	O
at	O
step	O
,	O
whereas	O
ranzato2015sequence	O
rely	O
on	O
the	O
REINFORCE	Method
algorithm	Method
williams1992simple	O
to	O
decide	O
whether	O
or	O
not	O
the	O
tokens	O
from	O
a	O
sampled	O
prediction	O
lead	O
to	O
a	O
high	O
task	Metric
-	Metric
specific	Metric
score	Metric
,	O
such	O
as	O
BLEU	Metric
papineni2002bleu	O
or	O
ROUGE	Metric
lin2003automatic	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
and	O
study	O
an	O
alternative	O
procedure	O
for	O
training	O
sequence	Task
prediction	Task
networks	Task
that	O
aims	O
to	O
directly	O
improve	O
their	O
test	Metric
time	Metric
metrics	Metric
(	O
which	O
are	O
typically	O
not	O
the	O
log	O
-	O
likelihood	O
)	O
.	O
In	O
particular	O
,	O
we	O
train	O
an	O
additional	O
network	O
called	O
the	O
critic	Method
to	O
output	O
the	O
value	O
of	O
each	O
token	O
,	O
which	O
we	O
define	O
as	O
the	O
expected	O
task	Metric
-	Metric
specific	Metric
score	Metric
that	O
the	O
network	O
will	O
receive	O
if	O
it	O
outputs	O
the	O
token	O
and	O
continues	O
to	O
sample	O
outputs	O
according	O
to	O
its	O
probability	O
distribution	O
.	O
Furthermore	O
,	O
we	O
show	O
how	O
the	O
predicted	O
values	O
can	O
be	O
used	O
to	O
train	O
the	O
main	Method
sequence	Method
prediction	Method
network	Method
,	O
which	O
we	O
refer	O
to	O
as	O
the	O
actor	O
.	O
The	O
theoretical	O
foundation	O
of	O
our	O
method	O
is	O
that	O
,	O
under	O
the	O
assumption	O
that	O
the	O
critic	O
computes	O
exact	O
values	O
,	O
the	O
expression	O
that	O
we	O
use	O
to	O
train	O
the	O
actor	O
is	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
of	O
the	O
expected	Metric
task	Metric
-	Metric
specific	Metric
score	Metric
.	O
Our	O
approach	O
draws	O
inspiration	O
and	O
borrows	O
the	O
terminology	O
from	O
the	O
field	O
of	O
reinforcement	Method
learning	Method
(	O
RL	Method
)	O
sutton1998introduction	O
,	O
in	O
particular	O
from	O
the	O
actor	Method
-	Method
critic	Method
approach	Method
sutton1984temporal	O
,	O
sutton1999policy	O
,	O
barto1983neuronlike	O
.	O
RL	Method
studies	O
the	O
problem	O
of	O
acting	O
efficiently	O
based	O
only	O
on	O
weak	O
supervision	O
in	O
the	O
form	O
of	O
a	O
reward	O
given	O
for	O
some	O
of	O
the	O
agent	O
’s	O
actions	O
.	O
In	O
our	O
case	O
,	O
the	O
reward	O
is	O
analogous	O
to	O
the	O
task	Metric
-	Metric
specific	Metric
score	Metric
associated	O
with	O
a	O
prediction	O
.	O
However	O
,	O
the	O
tasks	O
we	O
consider	O
are	O
those	O
of	O
supervised	Task
learning	Task
,	O
and	O
we	O
make	O
use	O
of	O
this	O
crucial	O
difference	O
by	O
allowing	O
the	O
critic	Method
to	O
use	O
the	O
ground	O
-	O
truth	O
answer	O
as	O
an	O
input	O
.	O
In	O
other	O
words	O
,	O
the	O
critic	O
has	O
access	O
to	O
a	O
sequence	O
of	O
expert	O
actions	O
that	O
are	O
known	O
to	O
lead	O
to	O
high	O
(	O
or	O
even	O
optimal	O
)	O
returns	O
.	O
To	O
train	O
the	O
critic	Method
,	O
we	O
adapt	O
the	O
temporal	Method
difference	Method
methods	Method
from	O
the	O
RL	Method
literature	O
sutton1988learning	O
to	O
our	O
setup	O
.	O
While	O
RL	Method
methods	Method
with	O
non	Method
-	Method
linear	Method
function	Method
approximators	Method
are	O
not	O
new	O
tesauro1994td	O
,	O
miller1995neural	O
,	O
they	O
have	O
recently	O
surged	O
in	O
popularity	O
,	O
giving	O
rise	O
to	O
the	O
field	O
of	O
‘	O
deep	Method
RL	Method
’	O
mnih2015human	O
.	O
We	O
show	O
that	O
some	O
of	O
the	O
techniques	O
recently	O
developed	O
in	O
deep	Method
RL	Method
,	O
such	O
as	O
having	O
a	O
target	Method
network	Method
,	O
may	O
also	O
be	O
beneficial	O
for	O
sequence	Task
prediction	Task
.	O
The	O
contributions	O
of	O
the	O
paper	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
1	O
)	O
we	O
describe	O
how	O
RL	Method
methodology	Method
like	O
the	O
actor	Method
-	Method
critic	Method
approach	Method
can	O
be	O
applied	O
to	O
supervised	Task
learning	Task
problems	Task
with	O
structured	O
outputs	O
;	O
and	O
2	O
)	O
we	O
investigate	O
the	O
performance	O
and	O
behavior	O
of	O
the	O
new	O
method	O
on	O
both	O
a	O
synthetic	Task
task	Task
and	O
a	O
real	O
-	O
world	O
task	O
of	O
machine	Task
translation	Task
,	O
demonstrating	O
the	O
improvements	O
over	O
maximum	Metric
-	Metric
likelihood	Metric
and	O
REINFORCE	Method
brought	O
by	O
the	O
actor	Method
-	Method
critic	Method
training	Method
.	O
section	O
:	O
Background	O
We	O
consider	O
the	O
problem	O
of	O
learning	Task
to	O
produce	O
an	O
output	O
sequence	O
,	O
given	O
an	O
input	O
,	O
where	O
is	O
the	O
alphabet	O
of	O
output	O
tokens	O
.	O
We	O
will	O
often	O
use	O
notation	O
to	O
refer	O
to	O
subsequences	O
of	O
the	O
form	O
.	O
Two	O
sets	O
of	O
input	O
-	O
output	O
pairs	O
are	O
assumed	O
to	O
be	O
available	O
for	O
both	O
training	O
and	O
testing	Task
.	O
The	O
trained	O
predictor	O
is	O
evaluated	O
by	O
computing	O
the	O
average	Metric
task	Metric
-	Metric
specific	Metric
score	Metric
on	O
the	O
test	O
set	O
,	O
where	O
is	O
the	O
prediction	O
.	O
To	O
simplify	O
the	O
formulas	O
we	O
always	O
use	O
to	O
denote	O
the	O
length	O
of	O
an	O
output	O
sequence	O
,	O
ignoring	O
the	O
fact	O
that	O
the	O
output	O
sequences	O
may	O
have	O
different	O
length	O
.	O
paragraph	O
:	O
Recurrent	Method
neural	Method
networks	Method
A	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
produces	O
a	O
sequence	O
of	O
state	O
vectors	O
given	O
a	O
sequence	O
of	O
input	O
vectors	O
by	O
starting	O
from	O
an	O
initial	O
state	O
and	O
applying	O
times	O
the	O
transition	O
function	O
:	O
.	O
Popular	O
choices	O
for	O
the	O
mapping	O
are	O
the	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
hochreiter1997long	O
and	O
the	O
Gated	Method
Recurrent	Method
Units	Method
cho2014learning	O
,	O
the	O
latter	O
of	O
which	O
we	O
use	O
for	O
our	O
models	O
.	O
To	O
build	O
a	O
probabilistic	Method
model	Method
for	O
sequence	Task
generation	Task
with	O
an	O
RNN	Method
,	O
one	O
adds	O
a	O
stochastic	Method
output	Method
layer	Method
(	O
typically	O
a	O
softmax	O
for	O
discrete	O
outputs	O
)	O
that	O
generates	O
outputs	O
and	O
can	O
feed	O
these	O
outputs	O
back	O
by	O
replacing	O
them	O
with	O
their	O
embedding	O
:	O
Thus	O
,	O
the	O
RNN	Method
defines	O
a	O
probability	O
distribution	O
of	O
the	O
next	O
output	O
token	O
given	O
the	O
previous	O
tokens	O
.	O
Upon	O
adding	O
a	O
special	O
end	O
-	O
of	O
-	O
sequence	O
token	O
to	O
the	O
alphabet	O
,	O
the	O
RNN	Method
can	O
define	O
the	O
distribution	O
over	O
all	O
possible	O
sequences	O
as	O
.	O
paragraph	O
:	O
RNNs	Method
for	O
sequence	Task
prediction	Task
To	O
use	O
RNNs	Method
for	O
sequence	Task
prediction	Task
,	O
they	O
must	O
be	O
augmented	O
to	O
generate	O
conditioned	O
on	O
an	O
input	O
.	O
The	O
simplest	O
way	O
to	O
do	O
this	O
is	O
to	O
start	O
with	O
an	O
initial	O
state	O
sutskever2014sequence	O
,	O
cho2014learning	O
.	O
Alternatively	O
,	O
one	O
can	O
encode	O
as	O
a	O
variable	O
-	O
length	O
sequence	O
of	O
vectors	O
and	O
condition	O
the	O
RNN	Method
on	O
this	O
sequence	O
using	O
an	O
attention	Method
mechanism	Method
.	O
In	O
our	O
models	O
,	O
the	O
sequence	O
of	O
vectors	O
is	O
produced	O
by	O
either	O
a	O
bidirectional	O
RNN	Method
schuster1997bidirectional	O
or	O
a	O
convolutional	Method
encoder	Method
rush2015neural	O
.	O
We	O
use	O
a	O
soft	Method
attention	Method
mechanism	Method
bahdanau2015neural	O
that	O
computes	O
a	O
weighted	Method
sum	Method
of	O
a	O
sequence	O
of	O
vectors	O
.	O
The	O
attention	O
weights	O
determine	O
the	O
relative	O
importance	O
of	O
each	O
vector	O
.	O
More	O
formally	O
,	O
we	O
consider	O
the	O
following	O
equations	O
for	O
RNNs	Method
with	O
attention	O
:	O
where	O
is	O
the	O
attention	Method
mechanism	Method
that	O
produces	O
the	O
attention	O
weights	O
and	O
is	O
the	O
context	O
vector	O
,	O
or	O
‘	O
glimpse	O
’	O
,	O
for	O
time	O
step	O
.	O
The	O
attention	O
weights	O
are	O
computed	O
by	O
an	O
MLP	Method
that	O
takes	O
as	O
input	O
the	O
current	O
RNN	Method
state	O
and	O
each	O
individual	O
vector	O
to	O
focus	O
on	O
.	O
The	O
weights	O
are	O
typically	O
(	O
as	O
in	O
our	O
work	O
)	O
constrained	O
to	O
be	O
positive	O
and	O
sum	O
to	O
1	O
by	O
using	O
the	O
softmax	O
function	O
.	O
A	O
conditioned	O
RNN	Method
can	O
be	O
trained	O
for	O
sequence	Task
prediction	Task
by	O
gradient	Method
ascent	Method
on	O
the	O
log	O
-	O
likelihood	O
for	O
the	O
input	O
-	O
output	O
pairs	O
from	O
the	O
training	O
set	O
.	O
To	O
produce	O
a	O
prediction	O
for	O
a	O
test	O
input	O
sequence	O
,	O
an	O
approximate	O
beam	Method
search	Method
for	O
the	O
maximum	O
of	O
is	O
usually	O
conducted	O
.	O
During	O
this	O
search	O
the	O
probabilities	O
are	O
considered	O
,	O
where	O
the	O
previous	O
tokens	O
comprise	O
a	O
candidate	O
beginning	O
of	O
the	O
prediction	O
.	O
Actor	Method
-	Method
Critic	Method
Training	Method
for	O
Sequence	Task
Prediction	Task
[	O
1	O
]	O
A	O
critic	Method
and	O
an	O
actor	Method
with	O
weights	O
and	O
respectively	O
.	O
Initialize	O
delayed	Method
actor	Method
and	O
target	Method
critic	Method
with	O
same	O
weights	O
:	O
,	O
.	O
Not	O
Converged	O
Receive	O
a	O
random	O
example	O
.	O
Generate	O
a	O
sequence	O
of	O
actions	O
from	O
.	O
Compute	O
targets	O
for	O
the	O
critic	O
Update	O
the	O
critic	O
weights	O
using	O
the	O
gradient	Method
Update	Method
actor	Method
weights	Method
using	O
the	O
following	O
gradient	Method
estimate	Method
Update	Method
delayed	Method
actor	Method
and	O
target	Method
critic	Method
,	O
with	O
constants	O
,	O
Complete	O
Actor	Method
-	Method
Critic	Method
Algorithm	Method
for	O
Sequence	Task
Prediction	Task
[	O
1	O
]	O
Initialize	O
critic	Method
and	O
actor	Method
with	O
random	O
weights	O
and	O
respectively	O
.	O
Pre	O
-	O
train	O
the	O
actor	O
to	O
predict	O
given	O
by	O
maximizing	O
.	O
Pre	O
-	O
train	O
the	O
critic	Method
to	O
estimate	O
by	O
running	O
Algorithm	O
[	O
reference	O
]	O
with	O
fixed	Method
actor	Method
.	O
Run	Method
Algorithm	Method
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Value	O
functions	O
We	O
view	O
the	O
conditioned	O
RNN	Method
as	O
a	O
stochastic	Method
policy	Method
that	O
generates	O
actions	O
and	O
receives	O
the	O
task	O
score	O
(	O
e.g.	O
,	O
BLEU	Metric
score	Metric
)	O
as	O
the	O
return	O
.	O
We	O
furthermore	O
consider	O
the	O
case	O
when	O
the	O
return	O
is	O
partially	O
received	O
at	O
the	O
intermediate	O
steps	O
in	O
the	O
form	O
of	O
rewards	O
:	O
.	O
This	O
is	O
more	O
general	O
than	O
the	O
case	O
of	O
receiving	O
the	O
full	O
return	O
at	O
the	O
end	O
of	O
the	O
sequence	O
,	O
as	O
we	O
can	O
simply	O
define	O
all	O
rewards	O
other	O
than	O
to	O
be	O
zero	O
.	O
Receiving	O
intermediate	O
rewards	O
may	O
ease	O
the	O
learning	O
for	O
the	O
critic	Method
,	O
and	O
we	O
use	O
reward	Method
shaping	Method
as	O
explained	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Given	O
the	O
policy	Method
,	O
possible	O
actions	O
and	O
reward	O
function	O
,	O
the	O
value	O
represents	O
the	O
expected	O
future	O
return	O
as	O
a	O
function	O
of	O
the	O
current	O
state	O
of	O
the	O
system	O
,	O
which	O
in	O
our	O
case	O
is	O
uniquely	O
defined	O
by	O
the	O
sequence	O
of	O
actions	O
taken	O
so	O
far	O
,	O
.	O
We	O
define	O
the	O
value	O
of	O
an	O
unfinished	Task
prediction	Task
as	O
follows	O
:	O
We	O
define	O
the	O
value	O
of	O
a	O
candidate	O
next	O
token	O
for	O
an	O
unfinished	Task
prediction	Task
as	O
the	O
expected	O
future	O
return	O
after	O
generating	O
token	O
:	O
We	O
will	O
refer	O
to	O
the	O
candidate	O
next	O
tokens	O
as	O
actions	O
.	O
For	O
notational	O
simplicity	O
,	O
we	O
henceforth	O
drop	O
and	O
from	O
the	O
signature	O
of	O
,	O
,	O
,	O
and	O
,	O
assuming	O
it	O
is	O
clear	O
from	O
the	O
context	O
which	O
of	O
and	O
is	O
meant	O
.	O
We	O
will	O
also	O
use	O
without	O
arguments	O
for	O
the	O
expected	O
reward	O
of	O
a	O
random	Method
prediction	Method
.	O
section	O
:	O
Actor	Method
-	Method
Critic	Method
for	O
Sequence	Task
Prediction	Task
Let	O
be	O
the	O
parameters	O
of	O
the	O
conditioned	O
RNN	Method
,	O
which	O
we	O
will	O
also	O
refer	O
to	O
as	O
the	O
actor	O
.	O
Our	O
training	Method
algorithm	Method
is	O
based	O
on	O
the	O
following	O
way	O
of	O
rewriting	O
the	O
gradient	O
of	O
the	O
expected	O
return	O
:	O
This	O
equality	O
is	O
known	O
in	O
RL	Method
under	O
the	O
names	Method
policy	Method
gradient	Method
theorem	O
sutton1999policy	O
and	O
stochastic	Method
actor	Method
-	Method
critic	Method
sutton1984temporal	O
.	O
Note	O
that	O
we	O
use	O
the	O
probability	O
rather	O
than	O
the	O
log	O
probability	O
in	O
this	O
formula	O
(	O
which	O
is	O
more	O
typical	O
in	O
RL	Task
applications	Task
)	O
as	O
we	O
are	O
summing	O
over	O
actions	O
rather	O
than	O
taking	O
an	O
expectation	O
.	O
Intuitively	O
,	O
this	O
equality	O
corresponds	O
to	O
increasing	O
the	O
probability	O
of	O
actions	O
that	O
give	O
high	O
values	O
,	O
and	O
decreasing	O
the	O
probability	O
of	O
actions	O
that	O
give	O
low	O
values	O
.	O
Since	O
this	O
gradient	Method
expression	Method
is	O
an	O
expectation	O
,	O
it	O
is	O
trivial	O
to	O
build	O
an	O
unbiased	Method
estimate	Method
for	O
it	O
:	O
where	O
are	O
random	O
samples	O
from	O
.	O
By	O
replacing	O
with	O
a	O
parameteric	Method
estimate	Method
one	O
can	O
obtain	O
a	O
biased	Method
estimate	Method
with	O
relatively	O
low	O
variance	O
.	O
The	O
parameteric	Method
estimate	Method
is	O
called	O
the	O
critic	Method
.	O
The	O
above	O
formula	O
is	O
similar	O
in	O
spirit	O
to	O
the	O
REINFORCE	Method
learning	Method
rule	Method
that	O
ranzato2015sequence	O
use	O
in	O
the	O
same	O
context	O
:	O
where	O
the	O
scalar	O
is	O
called	O
baseline	O
or	O
control	O
variate	O
.	O
The	O
difference	O
is	O
that	O
in	O
REINFORCE	O
the	O
inner	O
sum	O
over	O
all	O
actions	O
is	O
replaced	O
by	O
its	O
1	Method
-	Method
sample	Method
estimate	Method
,	O
namely	O
,	O
where	O
the	O
log	O
probability	O
is	O
introduced	O
to	O
correct	O
for	O
the	O
sampling	O
of	O
.	O
Furthermore	O
,	O
instead	O
of	O
the	O
value	O
,	O
REINFORCE	O
uses	O
the	O
cumulative	O
reward	O
following	O
the	O
action	O
,	O
which	O
again	O
can	O
be	O
seen	O
as	O
a	O
1	O
-	O
sample	O
estimate	O
of	O
.	O
Due	O
to	O
these	O
simplifications	O
and	O
the	O
potential	O
high	O
variance	O
in	O
the	O
cumulative	O
reward	O
,	O
the	O
REINFORCE	Method
gradient	Method
estimator	Method
has	O
very	O
high	O
variance	O
.	O
In	O
order	O
to	O
improve	O
upon	O
it	O
,	O
we	O
consider	O
the	O
actor	Method
-	Method
critic	Method
estimate	Method
from	O
Equation	O
[	O
reference	O
]	O
,	O
which	O
has	O
a	O
lower	O
variance	O
at	O
the	O
cost	O
of	O
significant	O
bias	O
,	O
since	O
the	O
critic	Method
is	O
not	O
perfect	O
and	O
trained	O
simultaneously	O
with	O
the	O
actor	O
.	O
The	O
success	O
depends	O
on	O
our	O
ability	O
to	O
control	O
the	O
bias	O
by	O
designing	O
the	O
critic	Method
network	Method
and	O
using	O
an	O
appropriate	O
training	O
criterion	O
for	O
it	O
.	O
To	O
implement	O
the	O
critic	Method
,	O
we	O
propose	O
to	O
use	O
a	O
separate	O
RNN	Method
parameterized	O
by	O
.	O
The	O
critic	O
RNN	Method
is	O
run	O
in	O
parallel	O
with	O
the	O
actor	O
,	O
consumes	O
the	O
tokens	O
that	O
the	O
actor	O
outputs	O
and	O
produces	O
the	O
estimates	O
for	O
all	O
.	O
A	O
key	O
difference	O
between	O
the	O
critic	Method
and	O
the	O
actor	O
is	O
that	O
the	O
correct	O
answer	O
is	O
given	O
to	O
the	O
critic	O
as	O
an	O
input	O
,	O
similarly	O
to	O
how	O
the	O
actor	O
is	O
conditioned	O
on	O
.	O
Indeed	O
,	O
the	O
return	O
is	O
a	O
deterministic	O
function	O
of	O
,	O
and	O
we	O
argue	O
that	O
using	O
to	O
compute	O
should	O
be	O
of	O
great	O
help	O
.	O
We	O
can	O
do	O
this	O
because	O
the	O
values	O
are	O
only	O
required	O
during	O
training	O
and	O
we	O
do	O
not	O
use	O
the	O
critic	O
at	O
test	O
time	O
.	O
We	O
also	O
experimented	O
with	O
providing	O
the	O
actor	O
states	O
as	O
additional	O
inputs	O
to	O
the	O
critic	Method
.	O
See	O
Figure	O
[	O
reference	O
]	O
for	O
a	O
visual	Method
representation	Method
of	O
our	O
actor	Method
-	Method
critic	Method
architecture	Method
.	O
paragraph	O
:	O
Temporal	Method
-	Method
difference	Method
learning	Method
A	O
crucial	O
component	O
of	O
our	O
approach	O
is	O
policy	Task
evaluation	Task
,	O
that	O
is	O
the	O
training	O
of	O
the	O
critic	Method
to	O
produce	O
useful	O
estimates	O
of	O
.	O
With	O
a	O
naïve	O
Monte	Method
-	Method
Carlo	Method
method	Method
,	O
one	O
could	O
use	O
the	O
future	O
return	O
as	O
a	O
target	O
to	O
,	O
and	O
use	O
the	O
critic	O
parameters	O
to	O
minimize	O
the	O
square	O
error	O
between	O
these	O
two	O
values	O
.	O
However	O
,	O
like	O
with	O
REINFORCE	Method
,	O
using	O
such	O
a	O
target	O
yields	O
to	O
very	O
high	O
variance	O
which	O
quickly	O
grows	O
with	O
the	O
number	O
of	O
steps	O
.	O
We	O
use	O
a	O
temporal	Method
difference	Method
(	Method
TD	Method
)	Method
method	Method
for	O
policy	Method
evaluation	Method
sutton1988learning	O
.	O
Namely	O
,	O
we	O
use	O
the	O
right	O
-	O
hand	O
side	O
of	O
the	O
Bellman	Method
equation	Method
as	O
the	O
target	O
for	O
the	O
left	O
-	O
hand	O
.	O
paragraph	O
:	O
Applying	O
deep	Method
RL	Method
techniques	Method
It	O
has	O
been	O
shown	O
in	O
the	O
RL	O
literature	O
that	O
if	O
is	O
non	O
-	O
linear	O
(	O
like	O
in	O
our	O
case	O
)	O
,	O
the	O
TD	Task
policy	Task
evaluation	Task
might	O
diverge	O
tsitsiklis1997analysis	O
.	O
Previous	O
work	O
has	O
shown	O
that	O
this	O
problem	O
can	O
be	O
alleviated	O
by	O
using	O
an	O
additional	O
target	Method
network	Method
to	O
compute	O
,	O
which	O
is	O
updated	O
less	O
often	O
and	O
/	O
or	O
more	O
slowly	O
than	O
.	O
Similarly	O
to	O
lillicrap2015continuous	O
,	O
we	O
update	O
the	O
parameters	O
of	O
the	O
target	O
critic	O
by	O
linearly	O
interpolating	O
them	O
with	O
the	O
parameters	O
of	O
the	O
trained	O
one	O
.	O
Attempts	O
to	O
remove	O
the	O
target	Method
network	Method
by	O
propagating	O
the	O
gradient	O
through	O
resulted	O
in	O
a	O
lower	O
square	Metric
error	Metric
,	O
but	O
the	O
resulting	O
values	O
proved	O
very	O
unreliable	O
as	O
training	O
signals	O
for	O
the	O
actor	O
.	O
The	O
fact	O
that	O
both	O
actor	Method
and	O
critic	Method
use	O
outputs	O
of	O
each	O
other	O
for	O
training	O
creates	O
a	O
potentially	O
dangerous	O
feedback	O
loop	O
.	O
To	O
address	O
this	O
,	O
we	O
sample	O
predictions	O
from	O
a	O
delayed	Method
actor	Method
lillicrap2015continuous	O
,	O
whose	O
weights	O
are	O
slowly	O
updated	O
to	O
follow	O
the	O
actor	O
that	O
is	O
actually	O
trained	O
.	O
paragraph	O
:	O
Dealing	O
with	O
large	Task
action	Task
spaces	Task
One	O
of	O
the	O
challenges	O
of	O
our	O
work	O
is	O
that	O
the	O
action	O
space	O
is	O
very	O
large	O
(	O
as	O
is	O
typically	O
the	O
case	O
in	O
NLP	Task
tasks	Task
with	O
large	O
vocabularies	O
)	O
.	O
This	O
can	O
be	O
alleviated	O
by	O
putting	O
constraints	O
on	O
the	O
critic	O
values	O
for	O
actions	O
that	O
are	O
rarely	O
sampled	O
.	O
We	O
found	O
experimentally	O
that	O
shrinking	O
the	O
values	O
of	O
these	O
rare	O
actions	O
is	O
necessary	O
for	O
the	O
algorithm	O
to	O
converge	O
.	O
Specifically	O
,	O
we	O
add	O
a	O
term	O
for	O
every	O
step	O
to	O
the	O
critic	O
’s	O
optimization	Metric
objective	Metric
which	O
drives	O
all	O
value	O
predictions	O
of	O
the	O
critic	O
closer	O
to	O
their	O
mean	O
:	O
This	O
corresponds	O
to	O
penalizing	O
the	O
variance	O
of	O
the	O
outputs	O
of	O
the	O
critic	O
.	O
Without	O
this	O
penalty	O
the	O
values	O
of	O
rare	O
actions	O
can	O
be	O
severely	O
overestimated	O
,	O
which	O
biases	O
the	O
gradient	Method
estimates	Method
and	O
can	O
cause	O
divergence	O
.	O
A	O
similar	O
trick	O
was	O
used	O
in	O
the	O
context	O
of	O
learning	Method
simple	Method
algorithms	Method
with	O
Q	Method
-	Method
learning	Method
zaremba2015learning	O
.	O
paragraph	O
:	O
Reward	Method
shaping	Method
While	O
we	O
are	O
ultimately	O
interested	O
in	O
the	O
maximization	O
of	O
the	O
score	O
of	O
a	O
complete	O
prediction	O
,	O
simply	O
awarding	O
this	O
score	O
at	O
the	O
last	O
step	O
provides	O
a	O
very	O
sparse	O
training	O
signal	O
for	O
the	O
critic	O
.	O
For	O
this	O
reason	O
we	O
use	O
potential	Method
-	Method
based	Method
reward	Method
shaping	Method
with	O
potentials	O
for	O
incomplete	O
sequences	O
and	O
for	O
complete	O
ones	O
ng1999policy	O
.	O
Namely	O
,	O
for	O
a	O
predicted	O
sequence	O
we	O
compute	O
score	O
values	O
for	O
all	O
prefixes	O
to	O
obtain	O
the	O
sequence	O
of	O
scores	O
.	O
The	O
difference	O
between	O
the	O
consecutive	O
pairs	O
of	O
scores	O
is	O
then	O
used	O
as	O
the	O
reward	O
at	O
each	O
step	O
:	O
.	O
Using	O
the	O
shaped	O
reward	O
instead	O
of	O
awarding	O
the	O
whole	O
score	O
at	O
the	O
last	O
step	O
does	O
not	O
change	O
the	O
optimal	O
policy	O
ng1999policy	O
.	O
paragraph	O
:	O
Putting	O
it	O
all	O
together	O
Algorithm	O
[	O
reference	O
]	O
describes	O
the	O
proposed	O
method	O
in	O
detail	O
.	O
We	O
consider	O
adding	O
the	O
weighted	O
log	O
-	O
likelihood	O
gradient	O
to	O
the	O
actor	Method
’s	Method
gradient	Method
estimate	Method
.	O
This	O
is	O
in	O
line	O
with	O
the	O
prior	O
work	O
by	O
ranzato2015sequence	O
and	O
shen2015minimum	O
.	O
It	O
is	O
also	O
motivated	O
by	O
our	O
preliminary	O
experiments	O
that	O
showed	O
that	O
using	O
the	O
actor	Method
-	Method
critic	Method
estimate	Method
alone	O
can	O
lead	O
to	O
an	O
early	O
determinization	O
of	O
the	O
policy	Task
and	O
vanishing	O
gradients	O
(	O
also	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Starting	O
training	O
with	O
a	O
randomly	O
initialized	O
actor	O
and	O
critic	O
would	O
be	O
problematic	O
,	O
because	O
neither	O
the	O
actor	O
nor	O
the	O
critic	Method
would	O
provide	O
adequate	O
training	O
signals	O
for	O
one	O
another	O
.	O
The	O
actor	O
would	O
sample	O
completely	O
random	O
predictions	O
that	O
receive	O
very	O
little	O
reward	O
,	O
thus	O
providing	O
a	O
very	O
weak	O
training	O
signal	O
for	O
the	O
critic	Method
.	O
A	O
random	Method
critic	Method
would	O
be	O
similarly	O
useless	O
for	O
training	O
the	O
actor	O
.	O
Motivated	O
by	O
these	O
considerations	O
,	O
we	O
pre	O
-	O
train	O
the	O
actor	O
using	O
standard	O
log	Method
-	Method
likelihood	Method
training	Method
.	O
Furthermore	O
,	O
we	O
pre	O
-	O
train	O
the	O
critic	Method
by	O
feeding	O
it	O
samples	O
from	O
the	O
pre	O
-	O
trained	O
actor	O
,	O
while	O
the	O
actor	O
’s	O
parameters	O
are	O
frozen	O
.	O
The	O
complete	O
training	O
procedure	O
including	O
pre	Task
-	Task
training	Task
is	O
described	O
by	O
Algorithm	O
[	O
reference	O
]	O
.	O
section	O
:	O
Related	O
Work	O
In	O
other	O
recent	O
RL	Method
-	O
inspired	O
work	O
on	O
sequence	Task
prediction	Task
,	O
ranzato2015sequence	O
trained	O
a	O
translation	Method
model	Method
by	O
gradually	O
transitioning	O
from	O
maximum	Method
likelihood	Method
learning	Method
into	O
optimizing	O
BLEU	Metric
or	O
ROUGE	Metric
scores	Metric
using	O
the	O
REINFORCE	Method
algorithm	Method
.	O
However	O
,	O
REINFORCE	Method
is	O
known	O
to	O
have	O
very	O
high	O
variance	Metric
and	O
does	O
not	O
exploit	O
the	O
availability	O
of	O
the	O
ground	O
-	O
truth	O
like	O
the	O
critic	Method
network	Method
does	O
.	O
The	O
approach	O
also	O
relies	O
on	O
a	O
curriculum	Method
learning	Method
scheme	Method
.	O
Standard	O
value	Method
-	Method
based	Method
RL	Method
algorithms	Method
like	O
SARSA	Method
and	O
OLPOMDP	Method
have	O
also	O
been	O
applied	O
to	O
structured	Task
prediction	Task
maes2009structured	O
.	O
Again	O
,	O
these	O
systems	O
do	O
not	O
use	O
the	O
ground	O
-	O
truth	O
for	O
value	Task
prediction	Task
.	O
Imitation	Method
learning	Method
has	O
also	O
been	O
applied	O
to	O
structured	Task
prediction	Task
vlachos2012investigation	O
.	O
Methods	O
of	O
this	O
type	O
include	O
the	O
Searn	Method
daume2009search	O
and	O
DAgger	Method
ross2010reduction	O
algorithms	O
.	O
These	O
methods	O
rely	O
on	O
an	O
expert	Method
policy	Method
to	O
provide	O
action	O
sequences	O
that	O
the	O
policy	Method
learns	O
to	O
imitate	O
.	O
Unfortunately	O
,	O
it	O
’s	O
not	O
always	O
easy	O
or	O
even	O
possible	O
to	O
construct	O
an	O
expert	Method
policy	Method
for	O
a	O
task	Metric
-	Metric
specific	Metric
score	Metric
.	O
In	O
our	O
approach	O
,	O
the	O
critic	Method
plays	O
a	O
role	O
that	O
is	O
similar	O
to	O
the	O
expert	Method
policy	Method
,	O
but	O
is	O
learned	O
without	O
requiring	O
prior	O
knowledge	O
about	O
the	O
task	Metric
-	Metric
specific	Metric
score	Metric
.	O
The	O
recently	O
proposed	O
‘	O
scheduled	Method
sampling	Method
’	O
bengio2015scheduled	O
can	O
also	O
be	O
seen	O
as	O
imitation	Method
learning	Method
.	O
In	O
this	O
method	O
,	O
ground	O
-	O
truth	O
tokens	O
are	O
occasionally	O
replaced	O
by	O
samples	O
from	O
the	O
model	O
itself	O
during	O
training	O
.	O
A	O
limitation	O
is	O
that	O
the	O
token	O
for	O
the	O
ground	O
-	O
truth	O
answer	O
is	O
used	O
as	O
the	O
target	O
at	O
step	O
,	O
which	O
might	O
not	O
always	O
be	O
the	O
optimal	O
strategy	O
.	O
There	O
are	O
also	O
approaches	O
that	O
aim	O
to	O
approximate	O
the	O
gradient	O
of	O
the	O
expected	O
score	O
.	O
One	O
such	O
approach	O
is	O
‘	O
Direct	Method
Loss	Method
Minimization	Method
’	O
hazan2010	O
in	O
which	O
the	O
inference	Method
procedure	Method
is	O
adapted	O
to	O
take	O
both	O
the	O
model	O
likelihood	O
and	O
task	Metric
-	Metric
specific	Metric
score	Metric
into	O
account	O
.	O
Another	O
popular	O
approach	O
is	O
to	O
replace	O
the	O
domain	O
over	O
which	O
the	O
task	O
score	O
expectation	O
is	O
defined	O
with	O
a	O
small	O
subset	O
of	O
it	O
,	O
as	O
is	O
done	O
in	O
Minimum	O
(	O
Bayes	Method
)	Method
Risk	Method
Training	Method
goel2000minimum	O
,	O
shen2015minimum	O
,	O
och2003minimum	O
.	O
This	O
small	O
subset	O
is	O
typically	O
an	O
-	O
best	O
list	O
or	O
a	O
sample	O
(	O
like	O
in	O
REINFORCE	O
)	O
that	O
may	O
or	O
may	O
not	O
include	O
the	O
ground	O
-	O
truth	O
as	O
well	O
.	O
None	O
of	O
these	O
methods	O
provide	O
intermediate	O
targets	O
for	O
the	O
actor	O
during	O
training	O
,	O
and	O
shen2015minimum	O
report	O
that	O
as	O
many	O
as	O
100	O
samples	O
were	O
required	O
for	O
the	O
best	O
results	O
.	O
Another	O
recently	O
proposed	O
method	O
is	O
to	O
optimize	O
a	O
global	O
sequence	O
cost	O
with	O
respect	O
to	O
the	O
selection	O
and	O
pruning	O
behavior	O
of	O
the	O
beam	Method
search	Method
procedure	O
itself	O
wiseman2016sequence	O
.	O
This	O
method	O
follows	O
the	O
more	O
general	O
strategy	O
called	O
‘	O
learning	Method
as	Method
search	Method
optimization	Method
’	O
daume2005learning	O
.	O
This	O
is	O
an	O
interesting	O
alternative	O
to	O
our	O
approach	O
;	O
however	O
,	O
it	O
is	O
designed	O
specifically	O
for	O
the	O
precise	O
inference	Task
procedure	Task
involved	O
.	O
section	O
:	O
Experiments	O
To	O
validate	O
our	O
approach	O
,	O
we	O
performed	O
two	O
sets	O
of	O
experiments	O
.	O
First	O
,	O
we	O
trained	O
the	O
proposed	O
model	O
to	O
recover	O
strings	O
of	O
natural	O
text	O
from	O
their	O
corrupted	O
versions	O
.	O
Specifically	O
,	O
we	O
consider	O
each	O
character	O
in	O
a	O
natural	O
language	O
corpus	O
and	O
with	O
some	O
probability	O
replace	O
it	O
with	O
a	O
random	O
character	O
.	O
We	O
call	O
this	O
synthetic	Task
task	Task
spelling	Task
correction	Task
.	O
A	O
desirable	O
property	O
of	O
this	O
synthetic	Task
task	Task
is	O
that	O
data	O
is	O
essentially	O
infinite	O
and	O
overfitting	O
is	O
no	O
concern	O
.	O
Our	O
second	O
series	O
of	O
experiments	O
is	O
done	O
on	O
the	O
task	O
of	O
automatic	O
machine	Task
translation	Task
using	O
different	O
models	O
and	O
datasets	O
.	O
In	O
addition	O
to	O
maximum	Method
likelihood	Method
and	O
actor	Method
-	Method
critic	Method
training	Method
we	O
implemented	O
two	O
versions	O
of	O
the	O
REINFORCE	Method
gradient	Method
estimator	Method
.	O
In	O
the	O
first	O
version	O
,	O
we	O
use	O
a	O
linear	Method
baseline	Method
network	Method
that	O
takes	O
the	O
actor	O
states	O
as	O
input	O
,	O
exactly	O
as	O
in	O
ranzato2015sequence	O
.	O
We	O
also	O
propose	O
a	O
novel	O
extension	O
of	O
REINFORCE	Method
that	O
leverages	O
the	O
extra	O
information	O
available	O
in	O
the	O
ground	O
-	O
truth	O
output	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
estimates	O
produced	O
by	O
the	O
critic	Method
network	Method
as	O
the	O
baseline	O
for	O
the	O
REINFORCE	Method
algorithm	Method
.	O
The	O
motivation	O
behind	O
this	O
approach	O
is	O
that	O
using	O
the	O
ground	O
-	O
truth	O
output	O
should	O
produce	O
a	O
better	O
baseline	O
that	O
lowers	O
the	O
variance	O
of	O
REINFORCE	O
,	O
resulting	O
in	O
higher	O
task	Metric
-	Metric
specific	Metric
scores	Metric
.	O
We	O
refer	O
to	O
this	O
method	O
as	O
REINFORCE	Method
-	Method
critic	Method
.	O
subsection	O
:	O
Spelling	Task
Correction	Task
We	O
use	O
text	O
from	O
the	O
One	O
Billion	O
Word	O
dataset	O
for	O
the	O
spelling	Task
correction	Task
task	Task
chelba2013one	O
,	O
which	O
has	O
pre	O
-	O
defined	O
training	O
and	O
testing	O
sets	O
.	O
The	O
training	O
data	O
was	O
abundant	O
,	O
and	O
we	O
never	O
used	O
any	O
example	O
twice	O
.	O
We	O
evaluate	O
trained	O
models	O
on	O
a	O
section	O
of	O
the	O
test	O
data	O
that	O
comprises	O
6075	O
sentences	O
.	O
To	O
speed	O
up	O
experiments	O
,	O
we	O
clipped	O
all	O
sentences	O
to	O
the	O
first	O
10	O
or	O
30	O
characters	O
.	O
For	O
the	O
spelling	Task
correction	Task
actor	Task
network	Task
,	O
we	O
use	O
an	O
RNN	Method
with	O
100	O
Gated	Method
Recurrent	Method
Units	Method
(	O
GRU	Method
)	O
and	O
a	O
bidirectional	Method
GRU	Method
network	Method
for	O
the	O
encoder	Method
.	O
We	O
use	O
the	O
same	O
attention	Method
mechanism	Method
as	O
proposed	O
in	O
bahdanau2015neural	O
,	O
which	O
effectively	O
makes	O
our	O
actor	Method
network	Method
a	O
smaller	O
version	O
of	O
the	O
model	O
used	O
in	O
that	O
work	O
.	O
For	O
the	O
critic	Method
network	Method
,	O
we	O
employed	O
a	O
model	O
with	O
the	O
same	O
architecture	O
as	O
the	O
actor	O
.	O
We	O
use	O
character	Metric
error	Metric
rate	Metric
(	O
CER	Metric
)	O
to	O
measure	O
performance	O
on	O
the	O
spelling	Task
task	Task
,	O
which	O
we	O
define	O
as	O
the	O
ratio	O
between	O
the	O
total	O
of	O
Levenshtein	O
distances	O
between	O
predictions	O
and	O
ground	O
-	O
truth	O
outputs	O
and	O
the	O
total	O
length	O
of	O
the	O
ground	O
-	O
truth	O
outputs	O
.	O
This	O
is	O
a	O
corpus	Metric
-	Metric
level	Metric
metric	Metric
for	O
which	O
a	O
lower	O
value	O
is	O
better	O
.	O
We	O
use	O
it	O
as	O
the	O
return	O
by	O
negating	O
per	O
-	O
sentence	O
ratios	O
.	O
At	O
the	O
evaluation	O
time	O
greedy	Method
search	Method
is	O
used	O
to	O
extract	O
predictions	O
from	O
the	O
model	O
.	O
We	O
use	O
the	O
ADAM	Method
optimizer	Method
kingma2015method	O
to	O
train	O
all	O
the	O
networks	O
with	O
the	O
parameters	O
recommended	O
in	O
the	O
original	O
paper	O
,	O
with	O
the	O
exception	O
of	O
the	O
scale	O
parameter	O
.	O
The	O
latter	O
is	O
first	O
set	O
to	O
and	O
then	O
annealed	O
to	O
for	O
log	Task
-	Task
likelihood	Task
training	Task
.	O
For	O
the	O
pre	Task
-	Task
training	Task
stage	Task
of	O
the	O
actor	Method
-	Method
critic	Method
,	O
we	O
use	O
and	O
decrease	O
it	O
to	O
for	O
the	O
joint	Method
actor	Method
-	Method
critic	Method
training	Method
.	O
We	O
pretrain	O
the	O
actor	O
until	O
its	O
score	O
on	O
the	O
development	O
set	O
stops	O
improving	O
.	O
We	O
pretrain	O
the	O
critic	O
until	O
its	O
TD	Metric
error	Metric
stabilizes	O
.	O
We	O
used	O
sample	O
for	O
both	O
actor	Method
-	Method
critic	Method
and	O
REINFORCE	Method
.	O
For	O
exact	O
hyperparameter	O
settings	O
we	O
refer	O
the	O
reader	O
to	O
Appendix	O
[	O
reference	O
]	O
.	O
We	O
start	O
REINFORCE	Method
training	Method
from	O
a	O
pretrained	O
actor	O
,	O
but	O
we	O
do	O
not	O
use	O
the	O
curriculum	Method
learning	Method
employed	O
in	O
MIXER	Method
.	O
The	O
critic	O
is	O
trained	O
in	O
the	O
same	O
way	O
for	O
both	O
REINFORCE	Method
and	Method
actor	Method
-	Method
critic	Method
,	O
including	O
the	O
pretraining	Method
stage	Method
.	O
We	O
report	O
results	O
obtained	O
with	O
the	O
reward	Method
shaping	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
as	O
we	O
found	O
that	O
it	O
slightly	O
improves	O
REINFORCE	Task
performance	O
.	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
results	O
on	O
the	O
spelling	Task
correction	Task
task	Task
.	O
We	O
observe	O
an	O
improvement	O
in	O
CER	Metric
over	O
log	Method
-	Method
likelihood	Method
training	Method
for	O
all	O
four	O
settings	O
considered	O
.	O
Without	O
simultaneous	Method
log	Method
-	Method
likelihood	Method
training	Method
,	O
actor	Method
-	Method
critic	Method
training	Method
results	O
in	O
a	O
better	O
CER	Metric
than	O
REINFORCE	Method
-	Method
critic	Method
in	O
three	O
out	O
of	O
four	O
settings	O
.	O
In	O
the	O
fourth	O
case	O
,	O
actor	Method
-	Method
critic	Method
and	O
REINFORCE	Method
-	Method
critic	Method
have	O
similar	O
performance	O
.	O
Adding	O
the	O
log	O
-	O
likelihood	O
gradient	O
with	O
a	O
cofficient	O
helps	O
both	O
of	O
the	O
methods	O
,	O
but	O
actor	Method
-	Method
critic	Method
still	O
retains	O
a	O
margin	O
of	O
improvement	O
over	O
REINFORCE	Method
-	Method
critic	Method
.	O
subsection	O
:	O
Machine	Task
Translation	Task
For	O
our	O
first	O
translation	Task
experiment	O
,	O
we	O
use	O
data	O
from	O
the	O
German	Material
-	Material
English	Material
machine	Material
translation	Material
track	Material
of	Material
the	Material
IWSLT	Material
2014	Material
evaluation	Material
campaign	Material
cettolo2014report	O
,	O
as	O
used	O
in	O
ranzato2015sequence	O
,	O
and	O
closely	O
follow	O
the	O
pre	O
-	O
processing	O
described	O
in	O
that	O
work	O
.	O
The	O
training	O
data	O
comprises	O
about	O
153	O
,	O
000	O
German	Material
-	Material
English	Material
sentence	Material
pairs	Material
.	O
In	O
addition	O
we	O
considered	O
a	O
larger	O
WMT14	O
English	O
-	O
French	O
dataset	O
with	O
more	O
than	O
12	O
million	O
examples	O
.	O
For	O
further	O
information	O
about	O
the	O
data	O
we	O
refer	O
the	O
reader	O
to	O
Appendix	O
[	O
reference	O
]	O
.	O
The	O
return	O
is	O
defined	O
as	O
a	O
smoothed	O
and	O
rescaled	O
version	O
of	O
the	O
BLEU	Metric
score	Metric
.	O
Specifically	O
,	O
we	O
start	O
all	O
n	O
-	O
gram	O
counts	O
from	O
1	O
instead	O
of	O
0	O
,	O
and	O
multiply	O
the	O
resulting	O
score	O
by	O
the	O
length	O
of	O
the	O
ground	O
-	O
truth	O
translation	O
.	O
Smoothing	Method
is	O
a	O
common	O
practice	O
when	O
sentence	Metric
-	Metric
level	Metric
BLEU	Metric
score	Metric
is	O
considered	O
,	O
and	O
it	O
has	O
been	O
used	O
to	O
apply	O
REINFORCE	Method
in	O
similar	O
settings	O
ranzato2015sequence	O
.	O
paragraph	O
:	O
IWSLT	Method
2014	Method
with	O
a	O
convolutional	Method
encoder	Method
In	O
our	O
first	O
experiment	O
we	O
use	O
a	O
convolutional	Method
encoder	Method
in	O
the	O
actor	O
to	O
make	O
our	O
results	O
more	O
comparable	O
with	O
ranzato2015sequence	O
.	O
For	O
the	O
same	O
reason	O
,	O
we	O
use	O
256	O
hidden	O
units	O
in	O
the	O
networks	O
.	O
For	O
the	O
critic	Method
,	O
we	O
replaced	O
the	O
convolutional	Method
network	Method
with	O
a	O
bidirectional	Method
GRU	Method
network	Method
.	O
For	O
training	O
this	O
model	O
we	O
mostly	O
used	O
the	O
same	O
hyperparameter	O
values	O
as	O
in	O
the	O
spelling	Task
correction	Task
experiments	O
,	O
with	O
a	O
few	O
differences	O
highlighted	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
For	O
decoding	Task
we	O
used	O
greedy	Method
search	Method
and	O
beam	Method
search	Method
with	O
a	O
beam	O
size	O
of	O
10	O
.	O
We	O
found	O
that	O
penalizing	O
candidate	O
sentences	O
that	O
are	O
too	O
short	O
was	O
required	O
to	O
obtain	O
the	O
best	O
results	O
.	O
Similarly	O
to	O
hannun2014first	O
,	O
we	O
subtracted	O
from	O
the	O
negative	O
log	O
-	O
likelihood	O
of	O
each	O
candidate	O
sentence	O
,	O
where	O
is	O
the	O
candidate	O
’s	O
length	O
,	O
and	O
is	O
a	O
hyperparameter	O
tuned	O
on	O
the	O
validation	O
set	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
report	O
a	O
significant	O
improvement	O
of	O
BLEU	Metric
points	Metric
over	O
the	O
log	Method
-	Method
likelihood	Method
baseline	Method
when	O
greedy	Method
search	Method
is	O
used	O
for	O
decoding	Task
.	O
Surprisingly	O
,	O
the	O
best	O
performing	O
method	O
is	O
REINFORCE	O
with	O
critic	Method
,	O
with	O
an	O
additional	O
BLEU	Metric
point	Metric
advantage	Metric
over	O
the	O
actor	Method
-	Method
critic	Method
.	O
When	O
beam	Method
-	Method
search	Method
is	O
used	O
,	O
the	O
ranking	O
of	O
the	O
compared	O
approaches	O
is	O
the	O
same	O
,	O
but	O
the	O
margin	O
between	O
the	O
proposed	O
methods	O
and	O
log	Method
-	Method
likelihood	Method
training	Method
becomes	O
smaller	O
.	O
The	O
final	O
performances	O
of	O
the	O
actor	Method
-	Method
critic	Method
and	O
the	O
REINFORCE	Method
-	Method
critic	Method
with	O
greedy	Method
search	Method
are	O
also	O
and	O
BLEU	Metric
points	O
respectively	O
better	O
than	O
what	O
ranzato2015sequence	O
report	O
for	O
their	O
MIXER	Method
approach	Method
.	O
This	O
comparison	O
should	O
be	O
treated	O
with	O
caution	O
,	O
because	O
our	O
log	Method
-	Method
likelihood	Method
baseline	Method
is	O
BLEU	Metric
points	O
stronger	O
than	O
its	O
equivalent	O
from	O
ranzato2015sequence	O
.	O
The	O
performance	O
of	O
REINFORCE	Method
with	O
a	O
simple	O
baseline	O
matches	O
the	O
score	O
reported	O
for	O
MIXER	Method
in	O
ranzato2015sequence	O
.	O
To	O
better	O
understand	O
the	O
IWSLT	O
2014	O
results	O
we	O
provide	O
the	O
learning	O
curves	O
for	O
the	O
considered	O
approaches	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
can	O
clearly	O
see	O
that	O
the	O
training	Method
methods	Method
that	O
use	O
generated	O
predictions	O
have	O
a	O
strong	O
regularization	O
effect	O
—	O
that	O
is	O
,	O
better	O
progress	O
on	O
the	O
validation	O
set	O
in	O
exchange	O
for	O
slower	O
or	O
negative	O
progress	O
on	O
the	O
training	O
set	O
.	O
The	O
effect	O
is	O
stronger	O
for	O
both	O
REINFORCE	O
varieties	O
,	O
especially	O
for	O
the	O
one	O
without	O
a	O
critic	O
.	O
The	O
actor	Method
-	Method
critic	Method
training	Method
does	O
a	O
much	O
better	O
job	O
of	O
fitting	O
the	O
training	O
set	O
than	O
REINFORCE	Method
and	O
is	O
the	O
only	O
method	O
except	O
log	Method
-	Method
likelihood	Method
that	O
shows	O
a	O
clear	O
overfitting	O
,	O
which	O
is	O
a	O
healthy	O
behaviour	O
for	O
such	O
a	O
small	O
dataset	O
.	O
In	O
addition	O
,	O
we	O
performed	O
an	O
ablation	Task
study	Task
.	O
We	O
found	O
that	O
using	O
a	O
target	Method
network	Method
was	O
crucial	O
;	O
while	O
the	O
joint	Method
actor	Method
-	Method
critic	Method
training	Method
was	O
still	O
progressing	O
with	O
,	O
with	O
it	O
did	O
not	O
work	O
at	O
all	O
.	O
Similarly	O
important	O
was	O
the	O
value	O
penalty	O
described	O
in	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
.	O
We	O
found	O
that	O
good	O
values	O
of	O
the	O
coefficient	O
were	O
in	O
the	O
range	O
.	O
Other	O
techniques	O
,	O
such	O
as	O
reward	Method
shaping	Method
and	O
a	O
delayed	Method
actor	Method
,	O
brought	O
moderate	O
performance	O
gains	O
.	O
We	O
refer	O
the	O
reader	O
to	O
Appendix	O
[	O
reference	O
]	O
for	O
more	O
details	O
.	O
paragraph	O
:	O
IWSLT	O
2014	O
with	O
a	O
bidirectional	Method
GRU	Method
encoder	Method
In	O
order	O
to	O
compare	O
our	O
results	O
with	O
those	O
reported	O
by	O
wiseman2016sequence	O
we	O
repeated	O
our	O
IWSLT	O
2014	O
investigation	O
with	O
a	O
different	O
encoder	Method
,	O
a	O
bidirectional	O
RNN	Method
with	O
256	O
GRU	Method
units	Method
.	O
In	O
this	O
round	O
of	O
experiments	O
we	O
also	O
tried	O
to	O
used	O
combined	O
training	O
objectives	O
in	O
the	O
same	O
way	O
as	O
in	O
our	O
spelling	Task
correction	Task
experiments	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
One	O
can	O
see	O
that	O
the	O
actor	Method
-	Method
critic	Method
training	Method
,	O
especially	O
its	O
AC	Method
+	Method
LL	Method
version	Method
,	O
yields	O
significant	O
improvements	O
(	O
1.7	O
with	O
greedy	Method
search	Method
and	O
1.0	O
with	O
beam	Method
search	Method
)	O
upon	O
the	O
pure	Method
log	Method
-	Method
likelihood	Method
training	Method
,	O
which	O
are	O
comparable	O
to	O
those	O
brought	O
by	O
Beam	Method
Search	Method
Optimization	Method
(	O
BSO	Method
)	O
,	O
even	O
though	O
our	O
log	Method
-	Method
likelihood	Method
baseline	Method
is	O
much	O
stronger	O
.	O
In	O
this	O
round	O
of	O
experiments	O
actor	Method
-	Method
critic	Method
and	O
REINFORCE	Method
-	Method
critic	Method
performed	O
on	O
par	O
.	O
paragraph	O
:	O
WMT	O
14	O
Finally	O
we	O
report	O
our	O
results	O
on	O
a	O
very	O
popular	O
large	O
WMT14	O
English	O
-	O
French	O
dataset	O
cho2014learning	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Our	O
model	O
closely	O
follows	O
the	O
achitecture	O
from	O
bahdanau2015neural	O
,	O
however	O
we	O
achieved	O
a	O
higher	O
baseline	O
performance	O
by	O
annealing	O
the	O
learning	Metric
rate	Metric
and	O
penalizing	O
output	O
sequences	O
that	O
were	O
too	O
short	O
during	O
beam	Method
search	Method
.	O
The	O
actor	Method
-	Method
critic	Method
training	Method
brings	O
a	O
significant	O
1.5	O
BLEU	Metric
improvement	Metric
with	O
greedy	Method
search	Method
and	O
a	O
noticeable	O
0.4	Metric
BLEU	Metric
improvement	Metric
with	O
beam	Method
search	Method
.	O
In	O
previous	O
work	O
report	O
a	O
higher	O
improvement	O
of	O
1.4	O
BLEU	Metric
with	O
beam	Method
search	Method
,	O
however	O
they	O
use	O
100	O
samples	O
for	O
each	O
training	O
example	O
,	O
whereas	O
we	O
use	O
just	O
one	O
.	O
We	O
note	O
that	O
in	O
this	O
experiment	O
,	O
which	O
is	O
perhaps	O
the	O
most	O
realistic	O
settings	O
,	O
the	O
actor	Method
-	Method
critic	Method
enjoys	O
a	O
significant	O
advantage	O
over	O
the	O
REINFORCE	Method
-	Method
critic	Method
.	O
section	O
:	O
Discussion	O
We	O
proposed	O
an	O
actor	Method
-	Method
critic	Method
approach	Method
to	O
sequence	Task
prediction	Task
.	O
Our	O
method	O
takes	O
the	O
task	O
objective	O
into	O
account	O
during	O
training	O
and	O
uses	O
the	O
ground	O
-	O
truth	O
output	O
to	O
aid	O
the	O
critic	Method
in	O
its	O
prediction	Task
of	Task
intermediate	Task
targets	Task
for	O
the	O
actor	O
.	O
We	O
showed	O
that	O
our	O
method	O
leads	O
to	O
significant	O
improvements	O
over	O
maximum	Method
likelihood	Method
training	Method
on	O
both	O
a	O
synthetic	Task
task	Task
and	O
a	O
machine	Task
translation	Task
benchmark	O
.	O
Compared	O
to	O
REINFORCE	Method
training	Method
on	O
machine	Task
translation	Task
,	O
actor	Method
-	Method
critic	Method
fits	O
the	O
training	O
data	O
much	O
faster	O
,	O
although	O
in	O
some	O
of	O
our	O
experiments	O
we	O
were	O
able	O
to	O
significantly	O
reduce	O
the	O
gap	O
in	O
the	O
training	Metric
speed	Metric
and	O
achieve	O
a	O
better	O
test	Metric
error	Metric
using	O
our	O
critic	Method
network	Method
as	O
the	O
baseline	O
for	O
REINFORCE	Method
.	O
One	O
interesting	O
observation	O
we	O
made	O
from	O
the	O
machine	Task
translation	Task
results	O
is	O
that	O
the	O
training	Method
methods	Method
that	O
use	O
generated	O
predictions	O
have	O
a	O
strong	O
regularization	O
effect	O
.	O
Our	O
understanding	O
is	O
that	O
conditioning	O
on	O
the	O
sampled	O
outputs	O
effectively	O
increases	O
the	O
diversity	O
of	O
training	O
data	O
.	O
This	O
phenomenon	O
makes	O
it	O
harder	O
to	O
judge	O
whether	O
the	O
actor	Method
-	Method
critic	Method
training	Method
meets	O
our	O
expectations	O
,	O
because	O
a	O
noisier	Method
gradient	Method
estimate	Method
yielded	O
a	O
better	O
test	O
set	O
performance	O
.	O
We	O
argue	O
that	O
the	O
spelling	Task
correction	Task
results	O
obtained	O
on	O
a	O
virtually	O
infinite	O
dataset	O
in	O
conjuction	O
with	O
better	O
machine	Task
translation	Task
performance	O
on	O
the	O
large	O
WMT	Material
14	Material
dataset	Material
provide	O
convincing	O
evidence	O
that	O
the	O
actor	Method
-	Method
training	Method
can	O
be	O
effective	O
.	O
In	O
future	O
work	O
we	O
will	O
consider	O
larger	O
machine	Task
translation	Task
datasets	O
.	O
We	O
ran	O
into	O
several	O
optimization	Task
issues	Task
.	O
The	O
critic	O
would	O
sometimes	O
assign	O
very	O
high	O
values	O
to	O
actions	O
with	O
a	O
very	O
low	O
probability	O
according	O
to	O
the	O
actor	O
.	O
We	O
were	O
able	O
to	O
resolve	O
this	O
by	O
penalizing	O
the	O
critic	O
’s	O
variance	O
.	O
Additionally	O
,	O
the	O
actor	O
would	O
sometimes	O
have	O
trouble	O
to	O
adapt	O
to	O
the	O
demands	O
of	O
the	O
critic	O
.	O
We	O
noticed	O
that	O
the	O
action	O
distribution	O
tends	O
to	O
saturate	O
and	O
become	O
deterministic	O
,	O
causing	O
the	O
gradient	O
to	O
vanish	O
.	O
We	O
found	O
that	O
combining	O
an	O
RL	Method
training	Method
objective	Method
with	O
log	Method
-	Method
likelihood	Method
can	O
help	O
,	O
but	O
in	O
general	O
we	O
think	O
this	O
issue	O
deserves	O
further	O
investigation	O
.	O
For	O
example	O
,	O
one	O
can	O
look	O
for	O
suitable	O
training	Metric
criteria	Metric
that	O
have	O
a	O
well	O
-	O
behaved	O
gradient	O
even	O
when	O
the	O
policy	O
has	O
little	O
or	O
no	O
stochasticity	O
.	O
In	O
a	O
concurrent	O
work	O
wu2016google	O
show	O
that	O
a	O
version	O
of	O
REINFORCE	Method
with	O
the	O
baseline	O
computed	O
using	O
multiple	O
samples	O
can	O
improve	O
performance	O
of	O
a	O
very	O
strong	O
machine	Task
translation	Task
system	O
.	O
This	O
result	O
,	O
and	O
our	O
REINFORCE	Method
-	Method
critic	Method
experiments	O
,	O
suggest	O
that	O
often	O
the	O
variance	O
of	O
REINFORCE	Method
can	O
be	O
reduced	O
enough	O
to	O
make	O
its	O
application	O
practical	O
.	O
That	O
said	O
,	O
we	O
would	O
like	O
to	O
emphasize	O
that	O
this	O
paper	O
attacks	O
the	O
problem	O
of	O
gradient	Task
estimation	Task
from	O
a	O
very	O
different	O
angle	O
as	O
it	O
aims	O
for	O
low	Task
-	Task
variance	Task
but	Task
potentially	Task
high	Task
-	Task
bias	Task
estimates	Task
.	O
The	O
idea	O
of	O
using	O
the	O
ground	O
-	O
truth	O
output	O
that	O
we	O
proposed	O
is	O
an	O
absolutely	O
necessary	O
first	O
step	O
in	O
this	O
direction	O
.	O
Future	O
work	O
could	O
focus	O
on	O
further	O
reducing	O
the	O
bias	O
of	O
the	O
actor	Method
-	Method
critic	Method
estimate	Method
,	O
for	O
example	O
,	O
by	O
using	O
a	O
multi	Method
-	Method
sample	Method
training	Method
criterion	Method
for	O
the	O
critic	Method
.	O
subsubsection	O
:	O
Acknowledgments	O
We	O
thank	O
the	O
developers	O
of	O
Theano	O
team2016theano	O
and	O
Blocks	Method
blocksfuel	Method
for	O
their	O
great	O
work	O
.	O
We	O
thank	O
NSERC	O
,	O
Compute	O
Canada	O
,	O
Calcul	O
Quebéc	O
,	O
Canada	O
Research	O
Chairs	O
,	O
CIFAR	O
,	O
CHISTERA	O
project	O
M2CR	O
(	O
PCIN	O
-	O
2015	O
-	O
226	O
)	O
and	O
Samsung	O
Institute	O
of	O
Advanced	O
Techonology	O
for	O
their	O
financial	O
support	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Hyperparameters	O
For	O
machine	Task
translation	Task
experiments	O
the	O
variance	O
penalty	O
coefficient	O
was	O
set	O
to	O
,	O
and	O
the	O
delay	O
coefficients	O
and	O
were	O
both	O
set	O
to	O
.	O
For	O
REINFORCE	O
with	O
the	O
critic	O
we	O
did	O
not	O
use	O
a	O
delayed	O
actor	O
,	O
i.e.	O
was	O
set	O
to	O
1	O
.	O
For	O
the	O
spelling	Task
correction	Task
task	Task
we	O
used	O
the	O
same	O
and	O
but	O
a	O
different	O
.	O
When	O
we	O
used	O
a	O
combined	O
training	Metric
criterion	Metric
,	O
the	O
weight	O
of	O
the	O
log	O
-	O
likelihood	O
gradient	O
was	O
always	O
0.1	O
.	O
All	O
initial	O
weights	O
were	O
sampled	O
from	O
a	O
centered	O
uniform	O
distribution	O
with	O
width	O
.	O
In	O
some	O
of	O
our	O
experiments	O
we	O
provided	O
the	O
actor	O
states	O
as	O
additional	O
inputs	O
to	O
the	O
critic	Method
.	O
Specifically	O
,	O
we	O
did	O
so	O
in	O
our	O
spelling	Task
correction	Task
experiments	O
and	O
in	O
our	O
WMT	O
14	O
machine	Task
translation	Task
study	O
.	O
All	O
the	O
other	O
results	O
were	O
obtained	O
without	O
this	O
technique	O
.	O
For	O
decoding	Task
with	O
beam	Method
search	Method
we	O
substracted	O
the	O
length	O
of	O
a	O
candidate	O
times	O
from	O
the	O
log	O
-	O
likelihood	O
cost	O
.	O
The	O
exact	O
value	O
of	O
was	O
selected	O
on	O
the	O
validation	O
set	O
and	O
was	O
equal	O
to	O
for	O
models	O
trained	O
by	O
log	Method
-	Method
likelihood	Method
and	O
REINFORCE	Method
and	O
to	O
for	O
models	O
trained	O
by	O
actor	Method
-	Method
critic	Method
and	O
REINFORCE	Method
-	Method
critic	Method
.	O
For	O
some	O
of	O
the	O
hyperparameters	O
we	O
performed	O
an	O
ablation	Task
study	Task
.	O
The	O
results	O
are	O
reported	O
in	O
Table	O
[	O
reference	O
]	O
.	O
appendix	O
:	O
Data	O
For	O
the	O
IWSLT	O
2014	O
data	O
the	O
sizes	O
of	O
validation	O
and	O
tests	O
set	O
were	O
6	O
,	O
969	O
and	O
6	O
,	O
750	O
,	O
respectively	O
.	O
We	O
limited	O
the	O
number	O
of	O
words	O
in	O
the	O
English	O
and	O
German	O
vocabularies	O
to	O
the	O
22	O
,	O
822	O
and	O
32	O
,	O
009	O
most	O
frequent	O
words	O
,	O
respectively	O
,	O
and	O
replaced	O
all	O
other	O
words	O
with	O
a	O
special	O
token	O
.	O
The	O
maximum	O
sentence	O
length	O
in	O
our	O
dataset	O
was	O
50	O
.	O
For	O
WMT14	O
we	O
used	O
vocabularies	O
of	O
30	O
,	O
000	O
words	O
for	O
both	O
English	O
and	O
French	O
,	O
and	O
the	O
maximum	O
sentence	O
length	O
was	O
also	O
50	O
.	O
appendix	O
:	O
Generated	O
Q	O
-	O
values	O
In	O
Table	O
[	O
reference	O
]	O
we	O
provide	O
an	O
example	O
of	O
value	O
predictions	O
that	O
the	O
critic	Method
outputs	O
for	O
candidate	O
next	O
words	O
.	O
One	O
can	O
see	O
that	O
the	O
critic	Method
has	O
indeed	O
learnt	O
to	O
assign	O
larger	O
values	O
for	O
the	O
appropriate	O
next	O
words	O
.	O
While	O
the	O
critic	Method
does	O
not	O
always	O
produce	O
sensible	O
estimates	O
and	O
can	O
often	O
predict	O
a	O
high	O
return	O
for	O
irrelevant	O
rare	O
words	O
,	O
this	O
is	O
greatly	O
reduced	O
using	O
the	O
variance	O
penalty	O
term	O
from	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
.	O
appendix	O
:	O
Proof	O
of	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
