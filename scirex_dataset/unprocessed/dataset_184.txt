document	O
:	O
Sequence	Task
-	Task
to	Task
-	Task
Sequence	Task
Learning	O
as	O
Beam	Task
-	Task
Search	Task
Optimization	Task
Sequence	Task
-	Task
to	Task
-	Task
Sequence	Task
(	O
seq2seq	Task
)	O
modeling	O
has	O
rapidly	O
become	O
an	O
important	O
general	O
-	O
purpose	O
NLP	Method
tool	Method
that	O
has	O
proven	O
effective	O
for	O
many	O
text	Task
-	Task
generation	Task
and	Task
sequence	Task
-	Task
labeling	Task
tasks	Task
.	O
Seq2seq	Method
builds	O
on	O
deep	Method
neural	Method
language	Method
modeling	Method
and	O
inherits	O
its	O
remarkable	O
accuracy	Metric
in	O
estimating	Task
local	Task
,	Task
next	Task
-	Task
word	Task
distributions	Task
.	O
In	O
this	O
work	O
,	O
we	O
introduce	O
a	O
model	Method
and	Method
beam	Method
-	Method
search	Method
training	Method
scheme	Method
,	O
based	O
on	O
the	O
work	O
of	O
daume05learning	Method
,	O
that	O
extends	O
seq2seq	Task
to	O
learn	O
global	O
sequence	O
scores	O
.	O
This	O
structured	O
approach	O
avoids	O
classical	O
biases	O
associated	O
with	O
local	Method
training	Method
and	O
unifies	O
the	O
training	Metric
loss	Metric
with	O
the	O
test	Metric
-	Metric
time	Metric
usage	Metric
,	O
while	O
preserving	O
the	O
proven	O
model	Method
architecture	Method
of	O
seq2seq	Task
and	O
its	O
efficient	O
training	Method
approach	Method
.	O
We	O
show	O
that	O
our	O
system	O
outperforms	O
a	O
highly	O
-	O
optimized	O
attention	O
-	O
based	O
seq2seq	Task
system	O
and	O
other	O
baselines	O
on	O
three	O
different	O
sequence	Task
to	Task
sequence	Task
tasks	Task
:	O
word	Task
ordering	Task
,	O
parsing	Task
,	O
and	O
machine	Task
translation	Task
.	O
succ	Metric
topK	Metric
score	Metric
section	O
:	O
Introduction	O
Sequence	Task
-	Task
to	Task
-	Task
Sequence	Task
learning	O
with	O
deep	Method
neural	Method
networks	Method
(	O
herein	O
,	O
seq2seq	Task
)	O
has	O
rapidly	O
become	O
a	O
very	O
useful	O
and	O
surprisingly	O
general	O
-	O
purpose	O
tool	O
for	O
natural	Task
language	Task
processing	Task
.	O
In	O
addition	O
to	O
demonstrating	O
impressive	O
results	O
for	O
machine	Task
translation	Task
,	O
roughly	O
the	O
same	O
model	O
and	O
training	O
have	O
also	O
proven	O
to	O
be	O
useful	O
for	O
sentence	Task
compression	Task
,	O
parsing	Task
,	O
and	O
dialogue	Task
systems	Task
,	O
and	O
they	O
additionally	O
underlie	O
other	O
text	Task
generation	Task
applications	Task
,	O
such	O
as	O
image	Task
or	O
video	Task
captioning	Task
.	O
The	O
dominant	O
approach	O
to	O
training	O
a	O
seq2seq	Task
system	O
is	O
as	O
a	O
conditional	Method
language	Method
model	Method
,	O
with	O
training	O
maximizing	O
the	O
likelihood	O
of	O
each	O
successive	O
target	O
word	O
conditioned	O
on	O
the	O
input	O
sequence	O
and	O
the	O
gold	O
history	O
of	O
target	O
words	O
.	O
Thus	O
,	O
training	Task
uses	O
a	O
strictly	O
word	O
-	O
level	O
loss	O
,	O
usually	O
cross	O
-	O
entropy	O
over	O
the	O
target	O
vocabulary	O
.	O
This	O
approach	O
has	O
proven	O
to	O
be	O
very	O
effective	O
and	O
efficient	O
for	O
training	O
neural	Method
language	Method
models	Method
,	O
and	O
seq2seq	Task
models	O
similarly	O
obtain	O
impressive	O
perplexities	Metric
for	O
word	Task
-	Task
generation	Task
tasks	Task
.	O
Notably	O
,	O
however	O
,	O
seq2seq	Task
models	O
are	O
not	O
used	O
as	O
conditional	Method
language	Method
models	Method
at	O
test	O
-	O
time	O
;	O
they	O
must	O
instead	O
generate	O
fully	O
-	O
formed	O
word	O
sequences	O
.	O
In	O
practice	O
,	O
generation	Task
is	O
accomplished	O
by	O
searching	O
over	O
output	O
sequences	O
greedily	O
or	O
with	O
beam	Method
search	Method
.	O
In	O
this	O
context	O
,	O
ranzato16sequence	O
note	O
that	O
the	O
combination	O
of	O
the	O
training	Method
and	Method
generation	Method
scheme	Method
just	O
described	O
leads	O
to	O
at	O
least	O
two	O
major	O
issues	O
:	O
Exposure	O
Bias	O
:	O
the	O
model	O
is	O
never	O
exposed	O
to	O
its	O
own	O
errors	O
during	O
training	O
,	O
and	O
so	O
the	O
inferred	O
histories	O
at	O
test	O
-	O
time	O
do	O
not	O
resemble	O
the	O
gold	O
training	O
histories	O
.	O
Loss	Task
-	Task
Evaluation	Task
Mismatch	Task
:	O
training	Task
uses	O
a	O
word	O
-	O
level	O
loss	O
,	O
while	O
at	O
test	O
-	O
time	O
we	O
target	O
improving	O
sequence	Metric
-	Metric
level	Metric
evaluation	Metric
metrics	Metric
,	O
such	O
as	O
BLEU	Metric
.	O
We	O
might	O
additionally	O
add	O
the	O
concern	O
of	O
label	O
bias	O
to	O
the	O
list	O
,	O
since	O
word	O
-	O
probabilities	O
at	O
each	O
time	O
-	O
step	O
are	O
locally	O
normalized	O
,	O
guaranteeing	O
that	O
successors	O
of	O
incorrect	O
histories	O
receive	O
the	O
same	O
mass	O
as	O
do	O
the	O
successors	O
of	O
the	O
true	O
history	O
.	O
In	O
this	O
work	O
we	O
develop	O
a	O
non	Method
-	Method
probabilistic	Method
variant	Method
of	O
the	O
seq2seq	Task
model	O
that	O
can	O
assign	O
a	O
score	O
to	O
any	O
possible	O
target	O
sequence	O
,	O
and	O
we	O
propose	O
a	O
training	Method
procedure	Method
,	O
inspired	O
by	O
the	O
learning	Method
as	Method
search	Method
optimization	Method
(	O
LaSO	Method
)	O
framework	O
of	O
daume05learning	O
,	O
that	O
defines	O
a	O
loss	O
function	O
in	O
terms	O
of	O
errors	O
made	O
during	O
beam	Method
search	Method
.	O
Furthermore	O
,	O
we	O
provide	O
an	O
efficient	O
algorithm	O
to	O
back	O
-	O
propagate	O
through	O
the	O
beam	Method
-	Method
search	Method
procedure	Method
during	O
seq2seq	Task
training	O
.	O
This	O
approach	O
offers	O
a	O
possible	O
solution	O
to	O
each	O
of	O
the	O
three	O
aforementioned	O
issues	O
,	O
while	O
largely	O
maintaining	O
the	O
model	Method
architecture	Method
and	O
training	Metric
efficiency	Metric
of	O
standard	O
seq2seq	Task
learning	O
.	O
Moreover	O
,	O
by	O
scoring	O
sequences	O
rather	O
than	O
words	O
,	O
our	O
approach	O
also	O
allows	O
for	O
enforcing	O
hard	O
-	O
constraints	O
on	O
sequence	Task
generation	Task
at	O
training	O
time	O
.	O
To	O
test	O
out	O
the	O
effectiveness	O
of	O
the	O
proposed	O
approach	O
,	O
we	O
develop	O
a	O
general	O
-	O
purpose	O
seq2seq	Task
system	O
with	O
beam	Method
search	Method
optimization	Method
.	O
We	O
run	O
experiments	O
on	O
three	O
very	O
different	O
problems	O
:	O
word	Task
ordering	Task
,	O
syntactic	Task
parsing	Task
,	O
and	O
machine	Task
translation	Task
,	O
and	O
compare	O
to	O
a	O
highly	O
-	O
tuned	O
seq2seq	Task
system	O
with	O
attention	Method
.	O
The	O
version	O
with	O
beam	Method
search	Method
optimization	Method
shows	O
significant	O
improvements	O
on	O
all	O
three	O
tasks	O
,	O
and	O
particular	O
improvements	O
on	O
tasks	O
that	O
require	O
difficult	Task
search	Task
.	O
section	O
:	O
Related	O
Work	O
The	O
issues	O
of	O
exposure	Task
bias	Task
and	O
label	Task
bias	Task
have	O
received	O
much	O
attention	O
from	O
authors	O
in	O
the	O
structured	Task
prediction	Task
community	Task
,	O
and	O
we	O
briefly	O
review	O
some	O
of	O
this	O
work	O
here	O
.	O
One	O
prominent	O
approach	O
to	O
combating	O
exposure	Task
bias	Task
is	O
that	O
of	O
SEARN	Method
,	O
a	O
meta	Method
-	Method
training	Method
algorithm	Method
that	O
learns	O
a	O
search	Method
policy	Method
in	O
the	O
form	O
of	O
a	O
cost	Method
-	Method
sensitive	Method
classifier	Method
trained	O
on	O
examples	O
generated	O
from	O
an	O
interpolation	O
of	O
an	O
oracle	Method
policy	Method
and	O
the	O
model	O
’s	O
current	O
(	O
learned	O
)	O
policy	O
.	O
Thus	O
,	O
SEARN	Method
explicitly	O
targets	O
the	O
mismatch	O
between	O
oracular	Method
training	Method
and	O
non	O
-	O
oracular	O
(	O
often	O
greedy	O
)	O
test	Method
-	Method
time	Method
inference	Method
by	O
training	O
on	O
the	O
output	O
of	O
the	O
model	O
’s	O
own	O
policy	O
.	O
DAgger	Method
is	O
a	O
similar	O
approach	O
,	O
which	O
differs	O
in	O
terms	O
of	O
how	O
training	O
examples	O
are	O
generated	O
and	O
aggregated	O
,	O
and	O
there	O
have	O
additionally	O
been	O
important	O
refinements	O
to	O
this	O
style	O
of	O
training	O
over	O
the	O
past	O
several	O
years	O
.	O
When	O
it	O
comes	O
to	O
training	O
RNNs	Method
,	O
SEARN	Method
/	O
DAgger	Method
has	O
been	O
applied	O
under	O
the	O
name	O
“	O
scheduled	Task
sampling	Task
”	O
,	O
which	O
involves	O
training	O
an	O
RNN	Method
to	O
generate	O
the	O
’	O
st	O
token	O
in	O
a	O
target	O
sequence	O
after	O
consuming	O
either	O
the	O
true	O
’	O
th	O
token	O
,	O
or	O
,	O
with	O
probability	O
that	O
increases	O
throughout	O
training	O
,	O
the	O
predicted	O
’	O
th	O
token	O
.	O
Though	O
technically	O
possible	O
,	O
it	O
is	O
uncommon	O
to	O
use	O
beam	Method
search	Method
when	O
training	O
with	O
SEARN	Method
/	O
DAgger	Method
.	O
The	O
early	O
-	O
update	O
and	O
LaSO	Method
training	O
strategies	O
,	O
however	O
,	O
explicitly	O
account	O
for	O
beam	Task
search	Task
,	O
and	O
describe	O
strategies	O
for	O
updating	O
parameters	O
when	O
the	O
gold	O
structure	O
becomes	O
unreachable	O
during	O
search	Task
.	O
Early	Method
update	Method
and	O
LaSO	Method
differ	O
primarily	O
in	O
that	O
the	O
former	O
discards	O
a	O
training	O
example	O
after	O
the	O
first	O
search	O
error	O
,	O
whereas	O
LaSO	Method
resumes	O
searching	O
after	O
an	O
error	O
from	O
a	O
state	O
that	O
includes	O
the	O
gold	O
partial	O
structure	O
.	O
In	O
the	O
context	O
of	O
feed	Method
-	Method
forward	Method
neural	Method
network	Method
training	Method
,	O
early	Method
update	Method
training	Method
has	O
been	O
recently	O
explored	O
in	O
a	O
feed	Method
-	Method
forward	Method
setting	Method
by	O
zhou15a	O
and	O
andor16globally	O
.	O
Our	O
work	O
differs	O
in	O
that	O
we	O
adopt	O
a	O
LaSO	Method
-	O
like	O
paradigm	O
(	O
with	O
some	O
minor	O
modifications	O
)	O
,	O
and	O
apply	O
it	O
to	O
the	O
training	O
of	O
seq2seq	Task
RNNs	O
(	O
rather	O
than	O
feed	Method
-	Method
forward	Method
networks	Method
)	O
.	O
We	O
also	O
note	O
that	O
watanabe15transition	O
apply	O
maximum	Method
-	Method
violation	Method
training	Method
,	O
which	O
is	O
similar	O
to	O
early	Method
-	Method
update	Method
,	O
to	O
a	O
parsing	Method
model	Method
with	O
recurrent	Method
components	Method
,	O
and	O
that	O
yazdani15incremental	O
use	O
beam	Method
-	Method
search	Method
in	O
training	O
a	O
discriminative	Method
,	Method
locally	Method
normalized	Method
dependency	Method
parser	Method
with	O
recurrent	Method
components	Method
.	O
Recently	O
authors	O
have	O
also	O
proposed	O
alleviating	O
exposure	O
bias	O
using	O
techniques	O
from	O
reinforcement	Method
learning	Method
.	O
ranzato16sequence	O
follow	O
this	O
approach	O
to	O
train	O
RNN	Method
decoders	O
in	O
a	O
seq2seq	Task
model	O
,	O
and	O
they	O
obtain	O
consistent	O
improvements	O
in	O
performance	O
,	O
even	O
over	O
models	O
trained	O
with	O
scheduled	Method
sampling	Method
.	O
As	O
daume05learning	O
note	O
,	O
LaSO	Method
is	O
similar	O
to	O
reinforcement	Method
learning	Method
,	O
except	O
it	O
does	O
not	O
require	O
“	O
exploration	O
”	O
in	O
the	O
same	O
way	O
.	O
Such	O
exploration	O
may	O
be	O
unnecessary	O
in	O
supervised	Task
text	Task
-	Task
generation	Task
,	O
since	O
we	O
typically	O
know	O
the	O
gold	O
partial	O
sequences	O
at	O
each	O
time	O
-	O
step	O
.	O
shen16mrt	Method
use	O
minimum	Method
risk	Method
training	Method
(	O
approximated	O
by	O
sampling	Method
)	O
to	O
address	O
the	O
issues	O
of	O
exposure	Metric
bias	Metric
and	O
loss	Metric
-	Metric
evaluation	Metric
mismatch	Metric
for	O
seq2seq	Task
MT	O
,	O
and	O
show	O
impressive	O
performance	O
gains	O
.	O
Whereas	O
exposure	O
bias	O
results	O
from	O
training	O
in	O
a	O
certain	O
way	O
,	O
label	O
bias	O
results	O
from	O
properties	O
of	O
the	O
model	O
itself	O
.	O
In	O
particular	O
,	O
label	O
bias	O
is	O
likely	O
to	O
affect	O
structured	Method
models	Method
that	O
make	O
sub	O
-	O
structure	O
predictions	O
using	O
locally	O
-	O
normalized	O
scores	O
.	O
Because	O
the	O
neural	O
and	O
non	O
-	O
neural	O
literature	O
on	O
this	O
point	O
has	O
recently	O
been	O
reviewed	O
by	O
andor16globally	O
,	O
we	O
simply	O
note	O
here	O
that	O
RNN	Method
models	O
are	O
typically	O
locally	O
normalized	O
,	O
and	O
we	O
are	O
unaware	O
of	O
any	O
specifically	O
seq2seq	Task
work	O
with	O
RNNs	Method
that	O
does	O
not	O
use	O
locally	O
-	O
normalized	O
scores	O
.	O
The	O
model	O
we	O
introduce	O
here	O
,	O
however	O
,	O
is	O
not	O
locally	O
normalized	O
,	O
and	O
so	O
should	O
not	O
suffer	O
from	O
label	O
bias	O
.	O
We	O
also	O
note	O
that	O
there	O
are	O
some	O
(	O
non	O
-	O
seq2seq	Task
)	O
exceptions	O
to	O
the	O
trend	O
of	O
locally	Method
normalized	Method
RNNs	Method
,	O
such	O
as	O
the	O
work	O
of	O
sak14sequence	O
and	O
voigtlaender15sequence	O
,	O
who	O
train	O
LSTMs	Method
in	O
the	O
context	O
of	O
HMMs	Method
for	O
speech	Task
recognition	Task
using	O
sequence	O
-	O
level	O
objectives	O
;	O
their	O
work	O
does	O
not	O
consider	O
search	Task
,	O
however	O
.	O
section	O
:	O
Background	O
and	O
Notation	O
In	O
the	O
simplest	O
seq2seq	Task
scenario	O
,	O
we	O
are	O
given	O
a	O
collection	O
of	O
source	O
-	O
target	O
sequence	O
pairs	O
and	O
tasked	O
with	O
learning	Task
to	O
generate	O
target	O
sequences	O
from	O
source	O
sequences	O
.	O
For	O
instance	O
,	O
we	O
might	O
view	O
machine	Task
translation	Task
in	O
this	O
way	O
,	O
where	O
in	O
particular	O
we	O
attempt	O
to	O
generate	O
English	Material
sentences	Material
from	O
(	O
corresponding	O
)	O
French	Material
sentences	Material
.	O
Seq2seq	Method
models	Method
are	O
part	O
of	O
the	O
broader	O
class	O
of	O
“	O
encoder	Method
-	Method
decoder	Method
”	Method
models	Method
,	O
which	O
first	O
use	O
an	O
encoding	Method
model	Method
to	O
transform	O
a	O
source	O
object	O
into	O
an	O
encoded	Method
representation	Method
.	O
Many	O
different	O
sequential	Method
(	O
and	O
non	Method
-	Method
sequential	Method
)	O
encoders	Method
have	O
proven	O
to	O
be	O
effective	O
for	O
different	O
source	O
domains	O
.	O
In	O
this	O
work	O
we	O
are	O
agnostic	O
to	O
the	O
form	O
of	O
the	O
encoding	Method
model	Method
,	O
and	O
simply	O
assume	O
an	O
abstract	Method
source	Method
representation	Method
.	O
Once	O
the	O
input	O
sequence	O
is	O
encoded	O
,	O
seq2seq	Task
models	O
generate	O
a	O
target	O
sequence	O
using	O
a	O
decoder	Method
.	O
The	O
decoder	O
is	O
tasked	O
with	O
generating	O
a	O
target	O
sequence	O
of	O
words	O
from	O
a	O
target	O
vocabulary	O
.	O
In	O
particular	O
,	O
words	O
are	O
generated	O
sequentially	O
by	O
conditioning	O
on	O
the	O
input	O
representation	O
and	O
on	O
the	O
previously	O
generated	O
words	O
or	O
history	O
.	O
We	O
use	O
the	O
notation	O
to	O
refer	O
to	O
an	O
arbitrary	O
word	O
sequence	O
of	O
length	O
,	O
and	O
the	O
notation	O
to	O
refer	O
to	O
the	O
gold	O
(	O
i.e.	O
,	O
correct	O
)	O
target	O
word	O
sequence	O
for	O
an	O
input	O
.	O
Most	O
seq2seq	Task
systems	O
utilize	O
a	O
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
for	O
the	O
decoder	Method
model	Method
.	O
Formally	O
,	O
a	O
recurrent	Method
neural	Method
network	Method
is	O
a	O
parameterized	Method
non	Method
-	Method
linear	Method
function	Method
that	O
recursively	O
maps	O
a	O
sequence	O
of	O
vectors	O
to	O
a	O
sequence	O
of	O
hidden	O
states	O
.	O
Let	O
be	O
a	O
sequence	O
of	O
vectors	O
,	O
and	O
let	O
be	O
some	O
initial	O
state	O
vector	O
.	O
Applying	O
an	O
RNN	Method
to	O
any	O
such	O
sequence	O
yields	O
hidden	O
states	O
at	O
each	O
time	O
-	O
step	O
,	O
as	O
follows	O
:	O
_	O
t	O
RNN	Method
(	O
_	O
t	O
,	O
_	O
t	O
-	O
1	O
;	O
)	O
,	O
where	O
is	O
the	O
set	O
of	O
model	O
parameters	O
,	O
which	O
are	O
shared	O
over	O
time	O
.	O
In	O
this	O
work	O
,	O
the	O
vectors	O
will	O
always	O
correspond	O
to	O
the	O
embeddings	O
of	O
a	O
target	O
word	O
sequence	O
,	O
and	O
so	O
we	O
will	O
also	O
write	O
,	O
with	O
standing	O
in	O
for	O
its	O
embedding	Task
.	O
RNN	Method
decoders	O
are	O
typically	O
trained	O
to	O
act	O
as	O
conditional	Method
language	Method
models	Method
.	O
That	O
is	O
,	O
one	O
attempts	O
to	O
model	O
the	O
probability	O
of	O
the	O
’	O
th	O
target	O
word	O
conditioned	O
on	O
and	O
the	O
target	O
history	O
by	O
stipulating	O
that	O
,	O
for	O
some	O
parameterized	O
function	O
typically	O
computed	O
with	O
an	O
affine	Method
layer	Method
followed	O
by	O
a	O
softmax	Method
.	O
In	O
computing	O
these	O
probabilities	O
,	O
the	O
state	O
represents	O
the	O
target	O
history	O
,	O
and	O
is	O
typically	O
set	O
to	O
be	O
some	O
function	O
of	O
.	O
The	O
complete	O
model	O
(	O
including	O
encoder	Method
)	O
is	O
trained	O
,	O
analogously	O
to	O
a	O
neural	Method
language	Method
model	Method
,	O
to	O
minimize	O
the	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
at	O
each	O
time	O
-	O
step	O
while	O
conditioning	O
on	O
the	O
gold	O
history	O
in	O
the	O
training	O
data	O
.	O
That	O
is	O
,	O
the	O
model	O
is	O
trained	O
to	O
minimize	O
.	O
Once	O
the	O
decoder	Method
is	O
trained	O
,	O
discrete	Task
sequence	Task
generation	Task
can	O
be	O
performed	O
by	O
approximately	O
maximizing	O
the	O
probability	O
of	O
the	O
target	O
sequence	O
under	O
the	O
conditional	O
distribution	O
,	O
,	O
where	O
we	O
use	O
the	O
notation	O
to	O
emphasize	O
that	O
the	O
decoding	Method
process	Method
requires	O
heuristic	Method
search	Method
,	O
since	O
the	O
RNN	Method
model	O
is	O
non	O
-	O
Markovian	O
.	O
In	O
practice	O
,	O
a	O
simple	O
beam	Method
search	Method
procedure	Method
that	O
explores	O
prospective	O
histories	O
at	O
each	O
time	O
-	O
step	O
has	O
proven	O
to	O
be	O
an	O
effective	O
decoding	Method
approach	Method
.	O
However	O
,	O
as	O
noted	O
above	O
,	O
decoding	Task
in	O
this	O
manner	O
after	O
conditional	Method
language	Method
-	Method
model	Method
style	Method
training	Method
potentially	O
suffers	O
from	O
the	O
issues	O
of	O
exposure	O
bias	O
and	O
label	O
bias	O
,	O
which	O
motivates	O
the	O
work	O
of	O
this	O
paper	O
.	O
section	O
:	O
Beam	Task
Search	Task
Optimization	Task
We	O
begin	O
by	O
making	O
one	O
small	O
change	O
to	O
the	O
seq2seq	Task
modeling	O
framework	O
.	O
Instead	O
of	O
predicting	O
the	O
probability	O
of	O
the	O
next	O
word	O
,	O
we	O
instead	O
learn	O
to	O
produce	O
(	O
non	O
-	O
probabilistic	O
)	O
scores	O
for	O
ranking	O
sequences	O
.	O
Define	O
the	O
score	O
of	O
a	O
sequence	O
consisting	O
of	O
history	O
followed	O
by	O
a	O
single	O
word	O
as	O
,	O
where	O
is	O
a	O
parameterized	O
function	O
examining	O
the	O
current	O
hidden	O
-	O
state	O
of	O
the	O
relevant	O
RNN	Method
at	O
time	O
as	O
well	O
as	O
the	O
input	O
representation	O
.	O
In	O
experiments	O
,	O
our	O
will	O
have	O
an	O
identical	O
form	O
to	O
but	O
without	O
the	O
final	O
softmax	Method
transformation	Method
(	O
which	O
transforms	O
unnormalized	O
scores	O
into	O
probabilities	O
)	O
,	O
thereby	O
allowing	O
the	O
model	O
to	O
avoid	O
issues	O
associated	O
with	O
the	O
label	Task
bias	Task
problem	Task
.	O
More	O
importantly	O
,	O
we	O
also	O
modify	O
how	O
this	O
model	O
is	O
trained	O
.	O
Ideally	O
we	O
would	O
train	O
by	O
comparing	O
the	O
gold	O
sequence	O
to	O
the	O
highest	O
-	O
scoring	O
complete	O
sequence	O
.	O
However	O
,	O
because	O
finding	O
the	O
argmax	O
sequence	O
according	O
to	O
this	O
model	O
is	O
intractable	O
,	O
we	O
propose	O
to	O
adopt	O
a	O
LaSO	Method
-	O
like	O
scheme	O
to	O
train	O
,	O
which	O
we	O
will	O
refer	O
to	O
as	O
beam	Method
search	Method
optimization	Method
(	O
BSO	Method
)	O
.	O
In	O
particular	O
,	O
we	O
define	O
a	O
loss	Method
that	O
penalizes	O
the	O
gold	O
sequence	O
falling	O
off	O
the	O
beam	O
during	O
training	O
.	O
The	O
proposed	O
training	Method
approach	Method
is	O
a	O
simple	O
way	O
to	O
expose	O
the	O
model	O
to	O
incorrect	O
histories	O
and	O
to	O
match	O
the	O
training	Method
procedure	Method
to	O
test	Task
generation	Task
.	O
Furthermore	O
we	O
show	O
that	O
it	O
can	O
be	O
implemented	O
efficiently	O
without	O
changing	O
the	O
asymptotic	Metric
run	Metric
-	Metric
time	Metric
of	Metric
training	Metric
,	O
beyond	O
a	O
factor	O
of	O
the	O
beam	O
size	O
.	O
subsection	O
:	O
Search	Method
-	Method
Based	Method
Loss	Method
We	O
now	O
formalize	O
this	O
notion	O
of	O
a	O
search	Method
-	Method
based	Method
loss	Method
for	O
RNN	Method
training	O
.	O
Assume	O
we	O
have	O
a	O
set	O
of	O
candidate	O
sequences	O
of	O
length	O
.	O
We	O
can	O
calculate	O
a	O
score	O
for	O
each	O
sequence	O
in	O
using	O
a	O
scoring	Method
function	Method
parameterized	O
with	O
an	O
RNN	Method
,	O
as	O
above	O
,	O
and	O
we	O
define	O
the	O
sequence	O
to	O
be	O
the	O
’	O
th	O
ranked	O
sequence	O
in	O
according	O
to	O
.	O
That	O
is	O
,	O
assuming	O
distinct	O
scores	O
,	O
—	O
{	O
^y:1t	O
(	O
k	O
)	O
∈S_t	O
∣f	O
(	O
^y_t^	O
(	O
k	O
)	O
,	O
^	O
¿	O
f	O
(	O
^y_t^	O
(	O
K	O
)	O
,	O
^	O
=	O
K	O
-	O
1	O
,	O
where	O
is	O
the	O
’	O
th	O
token	O
in	O
,	O
is	O
the	O
RNN	Method
state	O
corresponding	O
to	O
its	O
’	O
st	O
step	O
,	O
and	O
where	O
we	O
have	O
omitted	O
the	O
argument	O
to	O
for	O
brevity	O
.	O
We	O
now	O
define	O
a	O
loss	Method
function	Method
that	O
gives	O
loss	O
each	O
time	O
the	O
score	O
of	O
the	O
gold	O
prefix	O
does	O
not	O
exceed	O
that	O
of	O
by	O
a	O
margin	O
:	O
(	O
f	O
)	O
=	O
_	O
t=1^T	O
(	O
)	O
1	O
-	O
f	O
(	O
y_t	O
,	O
_	O
t	O
-	O
1	O
)	O
+	O
f	O
(	O
_	O
t^	O
(	O
K	O
)	O
,	O
_	O
t	O
-	O
1^	O
(	O
K	O
)	O
)	O
.	O
Above	O
,	O
the	O
term	O
denotes	O
a	O
mistake	O
-	O
specific	O
cost	O
-	O
function	O
,	O
which	O
allows	O
us	O
to	O
scale	O
the	O
loss	O
depending	O
on	O
the	O
severity	O
of	O
erroneously	O
predicting	O
;	O
it	O
is	O
assumed	O
to	O
return	O
0	O
when	O
the	O
margin	O
requirement	O
is	O
satisfied	O
,	O
and	O
a	O
positive	O
number	O
otherwise	O
.	O
It	O
is	O
this	O
term	O
that	O
allows	O
us	O
to	O
use	O
sequence	O
-	O
rather	O
than	O
word	O
-	O
level	O
costs	O
in	O
training	Task
(	O
addressing	O
the	O
2nd	O
issue	O
in	O
the	O
introduction	O
)	O
.	O
For	O
instance	O
,	O
when	O
training	O
a	O
seq2seq	Task
model	O
for	O
machine	Task
translation	Task
,	O
it	O
may	O
be	O
desirable	O
to	O
have	O
be	O
inversely	O
related	O
to	O
the	O
partial	O
sentence	O
-	O
level	O
BLEU	Metric
score	O
of	O
with	O
;	O
we	O
experiment	O
along	O
these	O
lines	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Finally	O
,	O
because	O
we	O
want	O
the	O
full	O
gold	O
sequence	O
to	O
be	O
at	O
the	O
top	O
of	O
the	O
beam	O
at	O
the	O
end	O
of	O
search	O
,	O
when	O
we	O
modify	O
the	O
loss	O
to	O
require	O
the	O
score	O
of	O
to	O
exceed	O
the	O
score	O
of	O
the	O
highest	O
ranked	O
incorrect	O
prediction	O
by	O
a	O
margin	O
.	O
We	O
can	O
optimize	O
the	O
loss	O
using	O
a	O
two	O
-	O
step	O
process	O
:	O
(	O
1	O
)	O
in	O
a	O
forward	Method
pass	Method
,	O
we	O
compute	O
candidate	O
sets	O
and	O
record	O
margin	O
violations	O
(	O
sequences	O
with	O
non	O
-	O
zero	O
loss	O
)	O
;	O
(	O
2	O
)	O
in	O
a	O
backward	O
pass	O
,	O
we	O
back	O
-	O
propagate	O
the	O
errors	O
through	O
the	O
seq2seq	Task
RNNs	O
.	O
Unlike	O
standard	O
seq2seq	Task
training	O
,	O
the	O
first	O
-	O
step	O
requires	O
running	Method
search	Method
(	O
in	O
our	O
case	O
beam	Method
search	Method
)	O
to	O
find	O
margin	O
violations	O
.	O
The	O
second	O
step	O
can	O
be	O
done	O
by	O
adapting	O
back	Method
-	Method
propagation	Method
through	Method
time	Method
(	O
BPTT	Method
)	O
.	O
We	O
next	O
discuss	O
the	O
details	O
of	O
this	O
process	O
.	O
subsection	O
:	O
Forward	O
:	O
Find	O
Violations	O
In	O
order	O
to	O
minimize	O
this	O
loss	O
,	O
we	O
need	O
to	O
specify	O
a	O
procedure	O
for	O
constructing	O
candidate	O
sequences	O
at	O
each	O
time	O
step	O
so	O
that	O
we	O
find	O
margin	O
violations	O
.	O
We	O
follow	O
LaSO	Method
(	O
rather	O
than	O
early	Method
-	Method
update	Method
;	O
see	O
Section	O
[	O
reference	O
]	O
)	O
and	O
build	O
candidates	O
in	O
a	O
recursive	O
manner	O
.	O
If	O
there	O
was	O
no	O
margin	O
violation	O
at	O
,	O
then	O
is	O
constructed	O
using	O
a	O
standard	O
beam	Method
search	Method
update	Method
.	O
If	O
there	O
was	O
a	O
margin	O
violation	O
,	O
is	O
constructed	O
as	O
the	O
best	O
sequences	O
assuming	O
the	O
gold	O
history	O
through	O
time	O
-	O
step	O
.	O
Formally	O
,	O
assume	O
the	O
function	O
maps	O
a	O
sequence	O
to	O
the	O
set	O
of	O
all	O
valid	O
sequences	O
of	O
length	O
that	O
can	O
be	O
formed	O
by	O
appending	O
to	O
it	O
a	O
valid	O
word	O
.	O
In	O
the	O
simplest	O
,	O
unconstrained	O
case	O
,	O
we	O
will	O
have	O
(	O
)	O
=	O
{	O
,	O
w	O
w	O
}	O
.	O
As	O
an	O
important	O
aside	O
,	O
note	O
that	O
for	O
some	O
problems	O
it	O
may	O
be	O
preferable	O
to	O
define	O
a	O
function	O
which	O
imposes	O
hard	O
constraints	O
on	O
successor	O
sequences	O
.	O
For	O
instance	O
,	O
if	O
we	O
would	O
like	O
to	O
use	O
seq2seq	Task
models	O
for	O
parsing	Task
(	O
by	O
emitting	O
a	O
constituency	O
or	O
dependency	O
structure	O
encoded	O
into	O
a	O
sequence	O
in	O
some	O
way	O
)	O
,	O
we	O
will	O
have	O
hard	O
constraints	O
on	O
the	O
sequences	O
the	O
model	O
can	O
output	O
,	O
namely	O
,	O
that	O
they	O
represent	O
valid	O
parses	Method
.	O
While	O
hard	O
constraints	O
such	O
as	O
these	O
would	O
be	O
difficult	O
to	O
add	O
to	O
standard	O
seq2seq	Task
at	O
training	O
time	O
,	O
in	O
our	O
framework	O
they	O
can	O
naturally	O
be	O
added	O
to	O
the	O
function	O
,	O
allowing	O
us	O
to	O
train	O
with	O
hard	O
constraints	O
;	O
we	O
experiment	O
along	O
these	O
lines	O
in	O
Section	O
[	O
reference	O
]	O
,	O
where	O
we	O
refer	O
to	O
a	O
model	O
trained	O
with	O
constrained	Method
beam	Method
search	Method
as	O
ConBSO	Method
.	O
Having	O
defined	O
an	O
appropriate	O
function	O
,	O
we	O
specify	O
the	O
candidate	O
set	O
as	O
:	O
S_t	O
=	O
(	O
)	O
violation	O
at	O
t	O
-	O
1	O
_	O
k=1^K	O
(	O
)	O
otherwise	O
,	O
where	O
we	O
have	O
a	O
margin	O
violation	O
at	O
iff	O
,	O
and	O
where	O
considers	O
the	O
scores	O
given	O
by	O
.	O
This	O
search	Method
procedure	Method
is	O
illustrated	O
in	O
the	O
top	O
portion	O
of	O
Figure	O
[	O
reference	O
]	O
.	O
In	O
the	O
forward	O
pass	O
of	O
our	O
training	Method
algorithm	Method
,	O
shown	O
as	O
the	O
first	O
part	O
of	O
Algorithm	O
[	O
reference	O
]	O
,	O
we	O
run	O
this	O
version	O
of	O
beam	Method
search	Method
and	O
collect	O
all	O
sequences	O
and	O
their	O
hidden	O
states	O
that	O
lead	O
to	O
losses	O
.	O
subsection	O
:	O
Backward	O
:	O
Merge	O
Sequences	O
Once	O
we	O
have	O
collected	O
margin	O
violations	O
we	O
can	O
run	O
backpropagation	Method
to	O
compute	O
parameter	Task
updates	Task
.	O
Assume	O
a	O
margin	O
violation	O
occurs	O
at	O
time	O
-	O
step	O
between	O
the	O
predicted	O
history	O
and	O
the	O
gold	O
history	O
.	O
As	O
in	O
standard	O
seq2seq	Task
training	O
we	O
must	O
back	O
-	O
propagate	O
this	O
error	O
through	O
the	O
gold	O
history	O
;	O
however	O
,	O
unlike	O
seq2seq	Task
we	O
also	O
have	O
a	O
gradient	O
for	O
the	O
wrongly	O
predicted	O
history	O
.	O
Recall	O
that	O
to	O
back	O
-	O
propagate	O
errors	O
through	O
an	O
RNN	Method
we	O
run	O
a	O
recursive	Method
backward	Method
procedure	Method
—	O
denoted	O
below	O
by	O
—	O
at	O
each	O
time	O
-	O
step	O
,	O
which	O
accumulates	O
the	O
gradients	O
of	O
next	O
-	O
step	O
and	O
future	O
losses	O
with	O
respect	O
to	O
.	O
We	O
have	O
:	O
_	O
_	O
t	O
BRNN	O
(	O
_	O
_	O
t	O
_	O
t	O
+	O
1	O
,	O
_	O
_	O
t	O
+	O
1	O
)	O
,	O
where	O
is	O
the	O
loss	O
at	O
step	O
,	O
deriving	O
,	O
for	O
instance	O
,	O
from	O
the	O
score	O
.	O
Running	O
this	O
procedure	O
from	O
to	O
is	O
known	O
as	O
back	Method
-	Method
propagation	Method
through	Method
time	Method
(	O
BPTT	Method
)	O
.	O
In	O
determining	O
the	O
total	O
computational	Metric
cost	Metric
of	O
back	Task
-	Task
propagation	Task
here	O
,	O
first	O
note	O
that	O
in	O
the	O
worst	O
case	O
there	O
is	O
one	O
violation	O
at	O
each	O
time	O
-	O
step	O
,	O
which	O
leads	O
to	O
independent	O
,	O
incorrect	O
sequences	O
.	O
Since	O
we	O
need	O
to	O
call	O
times	O
for	O
each	O
sequence	O
,	O
a	O
naive	O
strategy	O
of	O
running	O
BPTT	Method
for	O
each	O
incorrect	O
sequence	O
would	O
lead	O
to	O
an	O
backward	O
pass	O
,	O
rather	O
than	O
the	O
time	O
required	O
for	O
the	O
standard	O
seq2seq	Task
approach	O
.	O
Fortunately	O
,	O
our	O
combination	O
of	O
search	Method
-	Method
strategy	Method
and	O
loss	Method
make	O
it	O
possible	O
to	O
efficiently	O
share	O
operations	O
.	O
This	O
shared	O
structure	O
comes	O
naturally	O
from	O
the	O
LaSO	Method
update	O
,	O
which	O
resets	O
the	O
beam	O
in	O
a	O
convenient	O
way	O
.	O
We	O
informally	O
illustrate	O
the	O
process	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
top	O
of	O
the	O
diagram	O
shows	O
a	O
possible	O
sequence	O
of	O
formed	O
during	O
search	O
with	O
a	O
beam	O
of	O
size	O
3	O
for	O
the	O
target	O
sequence	O
“	O
a	O
red	O
dog	O
runs	O
quickly	O
today	O
.	O
”	O
When	O
the	O
gold	O
sequence	O
falls	O
off	O
the	O
beam	O
at	O
,	O
search	O
resumes	O
with	O
,	O
and	O
so	O
all	O
subsequent	O
predicted	O
sequences	O
have	O
as	O
a	O
prefix	O
and	O
are	O
thus	O
functions	O
of	O
.	O
Moreover	O
,	O
because	O
our	O
loss	O
function	O
only	O
involves	O
the	O
scores	O
of	O
the	O
gold	O
prefix	O
and	O
the	O
violating	O
prefix	O
,	O
we	O
end	O
up	O
with	O
the	O
relatively	O
simple	O
computation	O
tree	O
shown	O
at	O
the	O
bottom	O
of	O
Figure	O
[	O
reference	O
]	O
.	O
It	O
is	O
evident	O
that	O
we	O
can	O
backpropagate	O
in	O
a	O
single	O
pass	O
,	O
accumulating	O
gradients	O
from	O
sequences	O
that	O
diverge	O
from	O
the	O
gold	O
at	O
the	O
time	O
-	O
step	O
that	O
precedes	O
their	O
divergence	O
.	O
The	O
second	O
half	O
of	O
Algorithm	O
[	O
reference	O
]	O
shows	O
this	O
explicitly	O
for	O
a	O
single	O
sequence	O
,	O
though	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
algorithm	O
to	O
operate	O
in	O
batch	O
.	O
[	O
t	O
!	O
]	O
{	O
algorithmic}	O
[	O
1	O
]	O
empty	O
storage	O
^y:1	O
T	O
and	O
^	O
init	O
S1	O
←⁢violations{0	O
}	O
if	O
≠tT	O
else	O
⁢	O
t	O
to	O
⁢violations	O
/	O
*Backward*	O
/	O
←⁢grad_^	O
Seq2seq	Task
Beam	Task
-	Task
Search	Task
Optimization	Task
section	O
:	O
Data	O
and	O
Methods	O
We	O
run	O
experiments	O
on	O
three	O
different	O
tasks	O
,	O
comparing	O
our	O
approach	O
to	O
the	O
seq2seq	Task
baseline	Method
,	O
and	O
to	O
other	O
relevant	O
baselines	O
.	O
subsection	O
:	O
Model	O
While	O
the	O
method	O
we	O
describe	O
applies	O
to	O
seq2seq	Task
RNNs	O
in	O
general	O
,	O
for	O
all	O
experiments	O
we	O
use	O
the	O
global	Method
attention	Method
model	Method
of	O
luong15effective	O
—	O
which	O
consists	O
of	O
an	O
LSTM	Method
encoder	Method
and	O
an	O
LSTM	Method
decoder	Method
with	O
a	O
global	Method
attention	Method
model	Method
—	O
as	O
both	O
the	O
baseline	Method
seq2seq	Task
model	O
(	O
i.e.	O
,	O
as	O
the	O
model	O
that	O
computes	O
the	O
in	O
Section	O
[	O
reference	O
]	O
)	O
and	O
as	O
the	O
model	O
that	O
computes	O
our	O
sequence	O
-	O
scores	O
.	O
As	O
in	O
luong15effective	O
,	O
we	O
also	O
use	O
“	O
input	O
feeding	O
,	O
”	O
which	O
involves	O
feeding	O
the	O
attention	O
distribution	O
from	O
the	O
previous	O
time	O
-	O
step	O
into	O
the	O
decoder	O
at	O
the	O
current	O
step	O
.	O
This	O
model	O
architecture	O
has	O
been	O
found	O
to	O
be	O
highly	O
performant	O
for	O
neural	O
machine	Task
translation	Task
and	O
other	O
seq2seq	Task
tasks	O
.	O
To	O
distinguish	O
the	O
models	O
we	O
refer	O
to	O
our	O
system	O
as	O
BSO	Method
(	O
beam	Method
search	Method
optimization	Method
)	O
and	O
to	O
the	O
baseline	Method
as	O
seq2seq	Task
.	O
When	O
we	O
apply	O
constrained	Method
training	Method
(	O
as	O
discussed	O
in	O
Section	O
[	O
reference	O
]	O
)	O
,	O
we	O
refer	O
to	O
the	O
model	O
as	O
ConBSO	Method
.	O
In	O
providing	O
results	O
we	O
also	O
distinguish	O
between	O
the	O
beam	O
size	O
with	O
which	O
the	O
model	O
is	O
trained	O
,	O
and	O
the	O
beam	O
size	O
which	O
is	O
used	O
at	O
test	O
-	O
time	O
.	O
In	O
general	O
,	O
if	O
we	O
plan	O
on	O
evaluating	O
with	O
a	O
beam	O
of	O
size	O
it	O
makes	O
sense	O
to	O
train	O
with	O
a	O
beam	O
of	O
size	O
,	O
since	O
our	O
objective	O
requires	O
the	O
gold	O
sequence	O
to	O
be	O
scored	O
higher	O
than	O
the	O
last	O
sequence	O
on	O
the	O
beam	O
.	O
subsection	O
:	O
Methodology	O
Here	O
we	O
detail	O
additional	O
techniques	O
we	O
found	O
necessary	O
to	O
ensure	O
the	O
model	O
learned	O
effectively	O
.	O
First	O
,	O
we	O
found	O
that	O
the	O
model	O
failed	O
to	O
learn	O
when	O
trained	O
from	O
a	O
random	Method
initialization	Method
.	O
We	O
therefore	O
found	O
it	O
necessary	O
to	O
pre	O
-	O
train	O
the	O
model	O
using	O
a	O
standard	O
,	O
word	Method
-	Method
level	Method
cross	Method
-	Method
entropy	Method
loss	Method
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
The	O
necessity	O
of	O
pre	Task
-	Task
training	Task
in	O
this	O
instance	O
is	O
consistent	O
with	O
the	O
findings	O
of	O
other	O
authors	O
who	O
train	O
non	Method
-	Method
local	Method
neural	Method
models	Method
.	O
Similarly	O
,	O
it	O
is	O
clear	O
that	O
the	O
smaller	O
the	O
beam	O
used	O
in	O
training	O
is	O
,	O
the	O
less	O
room	O
the	O
model	O
has	O
to	O
make	O
erroneous	O
predictions	O
without	O
running	O
afoul	O
of	O
the	O
margin	O
loss	O
.	O
Accordingly	O
,	O
we	O
also	O
found	O
it	O
useful	O
to	O
use	O
a	O
“	O
curriculum	Method
beam	Method
”	Method
strategy	Method
in	O
training	Task
,	O
whereby	O
the	O
size	O
of	O
the	O
beam	O
is	O
increased	O
gradually	O
during	O
training	O
.	O
In	O
particular	O
,	O
given	O
a	O
desired	O
training	O
beam	O
size	O
,	O
we	O
began	O
training	O
with	O
a	O
beam	O
of	O
size	O
2	O
,	O
and	O
increased	O
it	O
by	O
1	O
every	O
2	O
epochs	O
until	O
reaching	O
.	O
Finally	O
,	O
it	O
has	O
been	O
established	O
that	O
dropout	Method
regularization	Method
improves	O
the	O
performance	O
of	O
LSTMs	Method
,	O
and	O
in	O
our	O
experiments	O
we	O
run	O
beam	Method
search	Method
under	O
dropout	Method
.	O
For	O
all	O
experiments	O
,	O
we	O
trained	O
both	O
seq2seq	Task
and	O
BSO	Method
models	Method
with	O
mini	Method
-	Method
batch	Method
Adagrad	Method
(	O
using	O
batches	O
of	O
size	O
64	O
)	O
,	O
and	O
we	O
renormalized	O
all	O
gradients	O
so	O
they	O
did	O
not	O
exceed	O
5	O
before	O
updating	O
parameters	O
.	O
We	O
did	O
not	O
extensively	O
tune	O
learning	Metric
-	Metric
rates	Metric
,	O
but	O
we	O
found	O
initial	O
rates	O
of	O
0.02	O
for	O
the	O
encoder	O
and	O
decoder	Method
LSTMs	Method
,	O
and	O
a	O
rate	O
of	O
0.1	O
or	O
0.2	O
for	O
the	O
final	O
linear	Method
layer	Method
(	O
i.e.	O
,	O
the	O
layer	O
tasked	O
with	O
making	O
word	Task
-	Task
predictions	Task
at	O
each	O
time	O
-	O
step	O
)	O
to	O
work	O
well	O
across	O
all	O
the	O
tasks	O
we	O
considered	O
.	O
Code	O
implementing	O
the	O
experiments	O
described	O
below	O
can	O
be	O
found	O
at	O
.	O
subsection	O
:	O
Tasks	O
and	O
Results	O
Our	O
experiments	O
are	O
primarily	O
intended	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
beam	Method
search	Method
optimization	Method
over	O
standard	O
seq2seq	Task
training	O
.	O
As	O
such	O
,	O
we	O
run	O
experiments	O
with	O
the	O
same	O
model	O
across	O
three	O
very	O
different	O
problems	O
:	O
word	Task
ordering	Task
,	O
dependency	Task
parsing	Task
,	O
and	O
machine	Task
translation	Task
.	O
While	O
we	O
do	O
not	O
include	O
all	O
the	O
features	O
and	O
extensions	O
necessary	O
to	O
reach	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
even	O
the	O
baseline	Method
seq2seq	Task
model	O
is	O
generally	O
quite	O
performant	O
.	O
paragraph	O
:	O
Word	Task
Ordering	Task
The	O
task	O
of	O
correctly	O
ordering	O
the	O
words	O
in	O
a	O
shuffled	O
sentence	O
has	O
recently	O
gained	O
some	O
attention	O
as	O
a	O
way	O
to	O
test	O
the	O
(	O
syntactic	Task
)	Task
capabilities	Task
of	O
text	Method
-	Method
generation	Method
systems	Method
.	O
We	O
cast	O
this	O
task	O
as	O
seq2seq	Task
problem	O
by	O
viewing	O
a	O
shuffled	O
sentence	O
as	O
a	O
source	O
sentence	O
,	O
and	O
the	O
correctly	O
ordered	O
sentence	O
as	O
the	O
target	O
.	O
While	O
word	Task
ordering	Task
is	O
a	O
somewhat	O
synthetic	Task
task	Task
,	O
it	O
has	O
two	O
interesting	O
properties	O
for	O
our	O
purposes	O
.	O
First	O
,	O
it	O
is	O
a	O
task	O
which	O
plausibly	O
requires	O
search	Task
(	O
due	O
to	O
the	O
exponentially	O
many	O
possible	O
orderings	O
)	O
,	O
and	O
,	O
second	O
,	O
there	O
is	O
a	O
clear	O
hard	O
constraint	O
on	O
output	O
sequences	O
,	O
namely	O
,	O
that	O
they	O
be	O
a	O
permutation	O
of	O
the	O
source	O
sequence	O
.	O
For	O
both	O
the	O
baseline	Method
and	O
BSO	Method
models	Method
we	O
enforce	O
this	O
constraint	O
at	O
test	O
-	O
time	O
.	O
However	O
,	O
we	O
also	O
experiment	O
with	O
constraining	O
the	O
BSO	Method
model	Method
during	O
training	O
,	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
,	O
by	O
defining	O
the	O
function	O
to	O
only	O
allow	O
successor	O
sequences	O
containing	O
un	O
-	O
used	O
words	O
in	O
the	O
source	O
sentence	O
.	O
For	O
experiments	O
,	O
we	O
use	O
the	O
same	O
PTB	Material
dataset	Material
(	O
with	O
the	O
standard	O
training	O
,	O
development	O
,	O
and	O
test	O
splits	O
)	O
and	O
evaluation	O
procedure	O
as	O
in	O
zhang15discriminative	O
and	O
later	O
work	O
,	O
with	O
performance	O
reported	O
in	O
terms	O
of	O
BLEU	Metric
score	O
with	O
the	O
correctly	O
ordered	O
sentences	O
.	O
For	O
all	O
word	Task
-	Task
ordering	Task
experiments	O
we	O
use	O
2	Method
-	Method
layer	Method
encoder	Method
and	O
decoder	Method
LSTMs	Method
,	O
each	O
with	O
256	O
hidden	O
units	O
,	O
and	O
dropout	Method
with	O
a	O
rate	O
of	O
0.2	O
between	O
LSTM	Method
layers	Method
.	O
We	O
use	O
simple	O
0	O
/	O
1	O
costs	O
in	O
defining	O
the	O
function	O
.	O
We	O
show	O
our	O
test	O
-	O
set	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
see	O
that	O
on	O
this	O
task	O
there	O
is	O
a	O
large	O
improvement	O
at	O
each	O
beam	O
size	O
from	O
switching	O
to	O
BSO	Method
,	O
and	O
a	O
further	O
improvement	O
from	O
using	O
the	O
constrained	Method
model	Method
.	O
Inspired	O
by	O
a	O
similar	O
analysis	O
in	O
daume05learning	O
,	O
we	O
further	O
examine	O
the	O
relationship	O
between	O
and	O
when	O
training	O
with	O
ConBSO	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
see	O
that	O
larger	O
hurt	O
greedy	Task
inference	Task
,	O
but	O
that	O
results	O
continue	O
to	O
improve	O
,	O
at	O
least	O
initially	O
,	O
when	O
using	O
a	O
that	O
is	O
(	O
somewhat	O
)	O
bigger	O
than	O
.	O
paragraph	O
:	O
Dependency	Method
Parsing	Method
We	O
next	O
apply	O
our	O
model	O
to	O
dependency	Task
parsing	Task
,	O
which	O
also	O
has	O
hard	O
constraints	O
and	O
plausibly	O
benefits	O
from	O
search	Task
.	O
We	O
treat	O
dependency	Task
parsing	Task
with	O
arc	O
-	O
standard	O
transitions	O
as	O
a	O
seq2seq	Task
task	O
by	O
attempting	O
to	O
map	O
from	O
a	O
source	O
sentence	O
to	O
a	O
target	O
sequence	O
of	O
source	O
sentence	O
words	O
interleaved	O
with	O
the	O
arc	O
-	O
standard	O
,	O
reduce	O
-	O
actions	O
in	O
its	O
parse	O
.	O
For	O
example	O
,	O
we	O
attempt	O
to	O
map	O
the	O
source	O
sentence	O
But	O
it	O
was	O
the	O
Quotron	O
problems	O
that	O
…	O
to	O
the	O
target	O
sequence	O
But	O
it	O
was	O
@L_SBJ	O
@L_DEP	O
the	O
Quotron	Task
problems	Task
@L_NMOD	O
@L_NMOD	O
that	O
…	O
We	O
use	O
the	O
standard	O
Penn	Material
Treebank	Material
dataset	Material
splits	Material
with	O
Stanford	Material
dependency	Material
labels	Material
,	O
and	O
the	O
standard	O
UAS	O
/	O
LAS	O
evaluation	O
metric	O
(	O
excluding	O
punctuation	O
)	O
following	O
chen14fast	O
.	O
All	O
models	O
thus	O
see	O
only	O
the	O
words	O
in	O
the	O
source	O
and	O
,	O
when	O
decoding	O
,	O
the	O
actions	O
it	O
has	O
emitted	O
so	O
far	O
;	O
no	O
other	O
features	O
are	O
used	O
.	O
We	O
use	O
2	Method
-	Method
layer	Method
encoder	Method
and	O
decoder	Method
LSTMs	Method
with	O
300	O
hidden	O
units	O
per	O
layer	O
and	O
dropout	Method
with	O
a	O
rate	O
of	O
0.3	O
between	O
LSTM	Method
layers	Method
.	O
We	O
replace	O
singleton	O
words	O
in	O
the	O
training	O
set	O
with	O
an	O
UNK	O
token	O
,	O
normalize	O
digits	O
to	O
a	O
single	O
symbol	O
,	O
and	O
initialize	O
word	Method
embeddings	Method
for	O
both	O
source	O
and	O
target	O
words	O
from	O
the	O
publicly	O
available	O
word2vec	Method
embeddings	Method
.	O
We	O
use	O
simple	O
0	O
/	O
1	O
costs	O
in	O
defining	O
the	O
function	O
.	O
As	O
in	O
the	O
word	Task
-	Task
ordering	Task
case	Task
,	O
we	O
also	O
experiment	O
with	O
modifying	O
the	O
function	O
in	O
order	O
to	O
train	O
under	O
hard	O
constraints	O
,	O
namely	O
,	O
that	O
the	O
emitted	O
target	O
sequence	O
be	O
a	O
valid	O
parse	O
.	O
In	O
particular	O
,	O
we	O
constrain	O
the	O
output	O
at	O
each	O
time	O
-	O
step	O
to	O
obey	O
the	O
stack	O
constraint	O
,	O
and	O
we	O
ensure	O
words	O
in	O
the	O
source	O
are	O
emitted	O
in	O
order	O
.	O
We	O
show	O
results	O
on	O
the	O
test	O
-	O
set	O
in	O
Table	O
[	O
reference	O
]	O
.	O
BSO	Method
and	O
ConBSO	Method
both	O
show	O
significant	O
improvements	O
over	O
seq2seq	Task
,	O
with	O
ConBSO	Method
improving	O
most	O
on	O
UAS	Task
,	O
and	O
BSO	Method
improving	O
most	O
on	O
LAS	Task
.	O
We	O
achieve	O
a	O
reasonable	O
final	O
score	O
of	O
91.57	O
UAS	Metric
,	O
which	O
lags	O
behind	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
but	O
is	O
promising	O
for	O
a	O
general	O
-	O
purpose	O
,	O
word	Method
-	Method
only	Method
model	Method
.	O
paragraph	O
:	O
Translation	Task
We	O
finally	O
evaluate	O
our	O
model	O
on	O
a	O
small	O
machine	Task
translation	Task
dataset	O
,	O
which	O
allows	O
us	O
to	O
experiment	O
with	O
a	O
cost	Method
function	Method
that	O
is	O
not	O
0	O
/	O
1	O
,	O
and	O
to	O
consider	O
other	O
baselines	O
that	O
attempt	O
to	O
mitigate	O
exposure	O
bias	O
in	O
the	O
seq2seq	Task
setting	O
.	O
We	O
use	O
the	O
dataset	O
from	O
the	O
work	O
of	O
ranzato16sequence	O
,	O
which	O
uses	O
data	O
from	O
the	O
German	Material
-	Material
to	Material
-	Material
English	Material
portion	Material
of	O
the	O
IWSLT	O
2014	O
machine	Task
translation	Task
evaluation	O
campaign	O
.	O
The	O
data	O
comes	O
from	O
translated	O
TED	O
talks	O
,	O
and	O
the	O
dataset	O
contains	O
roughly	O
153	O
K	O
training	O
sentences	O
,	O
7	O
K	O
development	O
sentences	O
,	O
and	O
7	O
K	O
test	O
sentences	O
.	O
We	O
use	O
the	O
same	O
preprocessing	O
and	O
dataset	O
splits	O
as	O
ranzato16sequence	O
,	O
and	O
like	O
them	O
we	O
also	O
use	O
a	O
single	Method
-	Method
layer	Method
LSTM	Method
decoder	Method
with	O
256	Method
units	Method
.	O
We	O
also	O
use	O
dropout	Method
with	O
a	O
rate	O
of	O
0.2	O
between	O
each	O
LSTM	Method
layer	Method
.	O
We	O
emphasize	O
,	O
however	O
,	O
that	O
while	O
our	O
decoder	Method
LSTM	Method
is	O
of	O
the	O
same	O
size	O
as	O
that	O
of	O
ranzato16sequence	O
,	O
our	O
results	O
are	O
not	O
directly	O
comparable	O
,	O
because	O
we	O
use	O
an	O
LSTM	Method
encoder	Method
(	O
rather	O
than	O
a	O
convolutional	Method
encoder	Method
as	O
they	O
do	O
)	O
,	O
a	O
slightly	O
different	O
attention	Method
mechanism	Method
,	O
and	O
input	Method
feeding	Method
.	O
For	O
our	O
main	O
MT	Task
results	O
,	O
we	O
set	O
to	O
,	O
where	O
is	O
the	O
last	O
margin	O
violation	O
and	O
denotes	O
smoothed	O
,	O
sentence	O
-	O
level	O
BLEU	Metric
.	O
This	O
setting	O
of	O
should	O
act	O
to	O
penalize	O
erroneous	O
predictions	O
with	O
a	O
relatively	O
low	O
sentence	O
-	O
level	O
BLEU	Metric
score	O
more	O
than	O
those	O
with	O
a	O
relatively	O
high	O
sentence	O
-	O
level	O
BLEU	Metric
score	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
show	O
our	O
final	O
results	O
and	O
those	O
from	O
ranzato16sequence	O
.	O
While	O
we	O
start	O
with	O
an	O
improved	O
baseline	Method
,	O
we	O
see	O
similarly	O
large	O
increases	O
in	O
accuracy	Metric
as	O
those	O
obtained	O
by	O
DAD	Method
and	O
MIXER	Method
,	O
in	O
particular	O
when	O
.	O
We	O
further	O
investigate	O
the	O
utility	O
of	O
these	O
sequence	O
-	O
level	O
costs	O
in	O
Table	O
[	O
reference	O
]	O
,	O
which	O
compares	O
using	O
sentence	O
-	O
level	O
BLEU	Metric
costs	O
in	O
defining	O
with	O
using	O
0	O
/	O
1	O
costs	O
.	O
We	O
see	O
that	O
the	O
more	O
sophisticated	O
sequence	O
-	O
level	O
costs	O
have	O
a	O
moderate	O
effect	O
on	O
BLEU	Metric
score	O
.	O
paragraph	O
:	O
Timing	O
Given	O
Algorithm	O
[	O
reference	O
]	O
,	O
we	O
would	O
expect	O
training	Metric
time	Metric
to	O
increase	O
linearly	O
with	O
the	O
size	O
of	O
the	O
beam	O
.	O
On	O
the	O
above	O
MT	Task
task	Task
,	O
our	O
highly	O
tuned	O
seq2seq	Task
baseline	Method
processes	O
an	O
average	O
of	O
13	O
,	O
038	O
tokens	O
/	O
second	O
(	O
including	O
both	O
source	O
and	O
target	O
tokens	O
)	O
on	O
a	O
GTX	O
970	O
GPU	O
.	O
For	O
beams	O
of	O
size	O
=	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
and	O
6	O
,	O
our	O
implementation	O
processes	O
on	O
average	O
1	O
,	O
985	O
,	O
1	O
,	O
768	O
,	O
1	O
,	O
709	O
,	O
1	O
,	O
521	O
,	O
and	O
1	O
,	O
458	O
tokens	O
/	O
second	O
,	O
respectively	O
.	O
Thus	O
,	O
we	O
appear	O
to	O
pay	O
an	O
initial	O
constant	O
factor	O
of	O
due	O
to	O
the	O
more	O
complicated	O
forward	O
and	O
backward	O
passes	O
,	O
and	O
then	O
training	Task
scales	O
with	O
the	O
size	O
of	O
the	O
beam	O
.	O
Because	O
we	O
batch	O
beam	O
predictions	O
on	O
a	O
GPU	O
,	O
however	O
,	O
we	O
find	O
that	O
in	O
practice	O
training	Metric
time	Metric
scales	O
sub	O
-	O
linearly	O
with	O
the	O
beam	O
-	O
size	O
.	O
section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
a	O
variant	O
of	O
seq2seq	Task
and	O
an	O
associated	O
beam	Method
search	Method
training	Method
scheme	Method
,	O
which	O
addresses	O
exposure	O
bias	O
as	O
well	O
as	O
label	O
bias	O
,	O
and	O
moreover	O
allows	O
for	O
both	O
training	O
with	O
sequence	O
-	O
level	O
cost	O
functions	O
as	O
well	O
as	O
with	O
hard	O
constraints	O
.	O
Future	O
work	O
will	O
examine	O
scaling	O
this	O
approach	O
to	O
much	O
larger	O
datasets	O
.	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
Yoon	O
Kim	O
for	O
helpful	O
discussions	O
and	O
for	O
providing	O
the	O
initial	O
seq2seq	Task
code	O
on	O
which	O
our	O
implementations	O
are	O
based	O
.	O
We	O
thank	O
Allen	O
Schmaltz	O
for	O
help	O
with	O
the	O
word	Task
ordering	Task
experiments	Task
.	O
We	O
also	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
a	O
Google	O
Research	O
Award	O
.	O
bibliography	O
:	O
References	O
