Multi	Task
-	Task
Task	Task
Learning	Task
as	O
Multi	Task
-	Task
Objective	Task
Optimization	Task
section	O
:	O
Abstract	O
In	O
multi	Task
-	Task
task	Task
learning	Task
,	O
multiple	O
tasks	O
are	O
solved	O
jointly	O
,	O
sharing	O
inductive	O
bias	O
between	O
them	O
.	O
Multi	Task
-	Task
task	Task
learning	Task
is	O
inherently	O
a	O
multi	Task
-	Task
objective	Task
problem	Task
because	O
different	O
tasks	O
may	O
conflict	O
,	O
necessitating	O
a	O
trade	O
-	O
off	O
.	O
A	O
common	O
compromise	O
is	O
to	O
optimize	O
a	O
proxy	O
objective	O
that	O
minimizes	O
a	O
weighted	O
linear	O
combination	O
of	O
pertask	O
losses	O
.	O
However	O
,	O
this	O
workaround	O
is	O
only	O
valid	O
when	O
the	O
tasks	O
do	O
not	O
compete	O
,	O
which	O
is	O
rarely	O
the	O
case	O
.	O
In	O
this	O
paper	O
,	O
we	O
explicitly	O
cast	O
multi	Task
-	Task
task	Task
learning	Task
as	O
multi	Method
-	Method
objective	Method
optimization	Method
,	O
with	O
the	O
overall	O
objective	O
of	O
finding	O
a	O
Pareto	O
optimal	O
solution	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
algorithms	O
developed	O
in	O
the	O
gradient	Method
-	Method
based	Method
multiobjective	Method
optimization	Method
literature	Method
.	O
These	O
algorithms	O
are	O
not	O
directly	O
applicable	O
to	O
large	Task
-	Task
scale	Task
learning	Task
problems	Task
since	O
they	O
scale	O
poorly	O
with	O
the	O
dimensionality	O
of	O
the	O
gradients	O
and	O
the	O
number	O
of	O
tasks	O
.	O
We	O
therefore	O
propose	O
an	O
upper	Method
bound	Method
for	O
the	O
multi	Metric
-	Metric
objective	Metric
loss	Metric
and	O
show	O
that	O
it	O
can	O
be	O
optimized	O
efficiently	O
.	O
We	O
further	O
prove	O
that	O
optimizing	O
this	O
upper	Method
bound	Method
yields	O
a	O
Pareto	Method
optimal	Method
solution	Method
under	O
realistic	O
assumptions	O
.	O
We	O
apply	O
our	O
method	O
to	O
a	O
variety	O
of	O
multi	Task
-	Task
task	Task
deep	Task
learning	Task
problems	Task
including	O
digit	Task
classification	Task
,	O
scene	Task
understanding	Task
(	O
joint	Task
semantic	Task
segmentation	Task
,	O
instance	Task
segmentation	Task
,	O
and	O
depth	Task
estimation	Task
)	O
,	O
and	O
multilabel	Task
classification	Task
.	O
Our	O
method	O
produces	O
higher	O
-	O
performing	O
models	O
than	O
recent	O
multi	Task
-	Task
task	Task
learning	Task
formulations	O
or	O
per	Task
-	Task
task	Task
training	Task
.	O
section	O
:	O
The	O
problem	O
of	O
finding	Task
Pareto	Task
optimal	Task
solutions	Task
given	O
multiple	O
criteria	O
is	O
called	O
multi	Method
-	Method
objective	Method
optimization	Method
.	O
A	O
variety	O
of	O
algorithms	O
for	O
multi	Method
-	Method
objective	Method
optimization	Method
exist	O
.	O
One	O
such	O
approach	O
is	O
the	O
multiple	Method
-	Method
gradient	Method
descent	Method
algorithm	Method
(	O
MGDA	Method
)	Method
,	O
which	O
uses	O
gradient	Method
-	Method
based	Method
optimization	Method
and	O
provably	O
converges	O
to	O
a	O
point	O
on	O
the	O
Pareto	O
set	O
[	O
reference	O
]	O
.	O
MGDA	Method
is	O
well	O
-	O
suited	O
for	O
multi	Task
-	Task
task	Task
learning	Task
with	O
deep	Method
networks	Method
.	O
It	O
can	O
use	O
the	O
gradients	O
of	O
each	O
task	O
and	O
solve	O
an	O
optimization	Task
problem	Task
to	O
decide	O
on	O
an	O
update	O
over	O
the	O
shared	O
parameters	O
.	O
However	O
,	O
there	O
are	O
two	O
technical	O
problems	O
that	O
hinder	O
the	O
applicability	O
of	O
MGDA	Method
on	O
a	O
large	O
scale	O
.	O
(	O
i	O
)	O
The	O
underlying	O
optimization	Task
problem	Task
does	O
not	O
scale	O
gracefully	O
to	O
high	O
-	O
dimensional	O
gradients	O
,	O
which	O
arise	O
naturally	O
in	O
deep	Task
networks	Task
.	O
(	O
ii	O
)	O
The	O
algorithm	O
requires	O
explicit	O
computation	O
of	O
gradients	O
per	O
task	O
,	O
which	O
results	O
in	O
linear	O
scaling	O
of	O
the	O
number	O
of	O
backward	O
passes	O
and	O
roughly	O
multiplies	O
the	O
training	Metric
time	Metric
by	O
the	O
number	O
of	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
Frank	Method
-	Method
Wolfe	Method
-	Method
based	Method
optimizer	Method
that	O
scales	O
to	O
high	Task
-	Task
dimensional	Task
problems	Task
.	O
Furthermore	O
,	O
we	O
provide	O
an	O
upper	Metric
bound	Metric
for	O
the	O
MGDA	Method
optimization	O
objective	O
and	O
show	O
that	O
it	O
can	O
be	O
computed	O
via	O
a	O
single	O
backward	Method
pass	Method
without	O
explicit	O
task	O
-	O
specific	O
gradients	O
,	O
thus	O
making	O
the	O
computational	Metric
overhead	Metric
of	O
the	O
method	O
negligible	O
.	O
We	O
prove	O
that	O
using	O
our	O
upper	Method
bound	Method
yields	O
a	O
Pareto	Method
optimal	Method
solution	Method
under	O
realistic	O
assumptions	O
.	O
The	O
result	O
is	O
an	O
exact	O
algorithm	O
for	O
multi	Method
-	Method
objective	Method
optimization	Method
of	O
deep	O
networks	O
with	O
negligible	O
computational	Metric
overhead	Metric
.	O
We	O
empirically	O
evaluate	O
the	O
presented	O
method	O
on	O
three	O
different	O
problems	O
.	O
First	O
,	O
we	O
perform	O
an	O
extensive	O
evaluation	O
on	O
multi	Task
-	Task
digit	Task
classification	Task
with	O
MultiMNIST	Material
[	O
reference	O
]	O
.	O
Second	O
,	O
we	O
cast	O
multi	Task
-	Task
label	Task
classification	Task
as	O
MTL	Task
and	O
conduct	O
experiments	O
with	O
the	O
CelebA	Material
dataset	Material
[	O
reference	O
]	O
.	O
Lastly	O
,	O
we	O
apply	O
the	O
presented	O
method	O
to	O
scene	Task
understanding	Task
;	O
specifically	O
,	O
we	O
perform	O
joint	Task
semantic	Task
segmentation	Task
,	O
instance	Task
segmentation	Task
,	O
and	O
depth	Task
estimation	Task
on	O
the	O
Cityscapes	Material
dataset	Material
[	O
reference	O
]	O
.	O
The	O
number	O
of	O
tasks	O
in	O
our	O
evaluation	O
varies	O
from	O
2	O
to	O
40	O
.	O
Our	O
method	O
clearly	O
outperforms	O
all	O
baselines	O
.	O
section	O
:	O
Related	O
Work	O
Multi	Task
-	Task
task	Task
learning	Task
.	O
We	O
summarize	O
the	O
work	O
most	O
closely	O
related	O
to	O
ours	O
and	O
refer	O
the	O
interested	O
reader	O
to	O
reviews	O
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
additional	O
background	O
.	O
Multi	Task
-	Task
task	Task
learning	Task
(	O
MTL	Task
)	O
is	O
typically	O
conducted	O
via	O
hard	Method
or	Method
soft	Method
parameter	Method
sharing	Method
.	O
In	O
hard	Task
parameter	Task
sharing	Task
,	O
a	O
subset	O
of	O
the	O
parameters	O
is	O
shared	O
between	O
tasks	O
while	O
other	O
parameters	O
are	O
task	O
-	O
specific	O
.	O
In	O
soft	Task
parameter	Task
sharing	Task
,	O
all	O
parameters	O
are	O
task	O
-	O
specific	O
but	O
they	O
are	O
jointly	O
constrained	O
via	O
Bayesian	O
priors	O
[	O
reference	O
][	O
reference	O
]	O
or	O
a	O
joint	O
dictionary	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
We	O
focus	O
on	O
hard	Method
parameter	Method
sharing	Method
with	O
gradient	Method
-	Method
based	Method
optimization	Method
,	O
following	O
the	O
success	O
of	O
deep	O
MTL	Task
in	O
computer	Task
vision	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
natural	Task
language	Task
processing	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
speech	Task
processing	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
and	O
even	O
seemingly	O
unrelated	O
domains	O
over	O
multiple	O
modalities	O
[	O
reference	O
]	O
.	O
[	O
reference	O
]	O
theoretically	O
analyze	O
the	O
MTL	Task
problem	Task
as	O
interaction	O
between	O
individual	O
learners	O
and	O
a	O
meta	Method
-	Method
algorithm	Method
.	O
Each	O
learner	O
is	O
responsible	O
for	O
one	O
task	O
and	O
a	O
meta	Method
-	Method
algorithm	Method
decides	O
how	O
the	O
shared	O
parameters	O
are	O
updated	O
.	O
All	O
aforementioned	O
MTL	Task
algorithms	O
use	O
weighted	Method
summation	Method
as	O
the	O
meta	Method
-	Method
algorithm	Method
.	O
Meta	Method
-	Method
algorithms	Method
that	O
go	O
beyond	O
weighted	Method
summation	Method
have	O
also	O
been	O
explored	O
.	O
[	O
reference	O
]	O
consider	O
the	O
case	O
where	O
each	O
individual	O
learner	Method
is	O
based	O
on	O
kernel	Method
learning	Method
and	O
utilize	O
multi	Method
-	Method
objective	Method
optimization	Method
.	O
[	O
reference	O
]	O
consider	O
the	O
case	O
where	O
each	O
learner	Method
is	O
a	O
linear	Method
model	Method
and	O
use	O
a	O
task	Method
affinity	Method
matrix	Method
.	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
use	O
the	O
assumption	O
that	O
tasks	O
share	O
a	O
dictionary	O
and	O
develop	O
an	O
expectation	Method
-	Method
maximization	Method
-	Method
like	Method
metaalgorithm	Method
.	O
de	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
use	O
swarm	Method
optimization	Method
.	O
None	O
of	O
these	O
methods	O
apply	O
to	O
gradient	Method
-	Method
based	Method
learning	Method
of	Method
high	Method
-	Method
capacity	Method
models	Method
such	O
as	O
modern	O
deep	Method
networks	Method
.	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
propose	O
heuristics	Method
based	O
on	O
uncertainty	O
and	O
gradient	O
magnitudes	O
,	O
respectively	O
,	O
and	O
apply	O
their	O
methods	O
to	O
convolutional	Method
neural	Method
networks	Method
.	O
Another	O
recent	O
work	O
uses	O
multi	Method
-	Method
agent	Method
reinforcement	Method
learning	Method
[	O
reference	O
]	O
.	O
Multi	Task
-	Task
objective	Task
optimization	Task
.	O
Multi	Task
-	Task
objective	Task
optimization	Task
addresses	O
the	O
problem	O
of	O
optimizing	O
a	O
set	O
of	O
possibly	O
contrasting	O
objectives	O
.	O
We	O
recommend	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
for	O
surveys	O
of	O
this	O
field	O
.	O
Of	O
particular	O
relevance	O
to	O
our	O
work	O
is	O
gradient	Method
-	Method
based	Method
multi	Method
-	Method
objective	Method
optimization	Method
,	O
as	O
developed	O
by	O
[	O
reference	O
]	O
,	O
[	O
reference	O
][	O
reference	O
]	O
.	O
These	O
methods	O
use	O
multi	O
-	O
objective	O
Karush	Method
-	Method
Kuhn	Method
-	Method
Tucker	Method
(	O
KKT	Method
)	O
conditions	O
[	O
reference	O
]	O
and	O
find	O
a	O
descent	O
direction	O
that	O
decreases	O
all	O
objectives	O
.	O
This	O
approach	O
was	O
extended	O
to	O
stochastic	Method
gradient	Method
descent	Method
by	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
In	O
machine	Task
learning	Task
,	O
these	O
methods	O
have	O
been	O
applied	O
to	O
multi	Task
-	Task
agent	Task
learning	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
,	O
kernel	Method
learning	Method
[	O
reference	O
]	O
,	O
sequential	Task
decision	Task
making	Task
[	O
reference	O
]	O
,	O
and	O
Bayesian	Task
optimization	Task
[	O
reference	O
][	O
reference	O
]	O
.	O
Our	O
work	O
applies	O
gradient	Method
-	Method
based	Method
multi	Method
-	Method
objective	Method
optimization	Method
to	O
multi	Task
-	Task
task	Task
learning	Task
.	O
3	O
Multi	Task
-	Task
Task	Task
Learning	Task
as	O
Multi	Task
-	Task
Objective	Task
Optimization	Task
Consider	O
a	O
multi	Task
-	Task
task	Task
learning	Task
(	O
MTL	Task
)	O
problem	O
over	O
an	O
input	O
space	O
X	O
and	O
a	O
collection	O
of	O
task	O
spaces	O
is	O
given	O
where	O
T	O
is	O
the	O
number	O
of	O
tasks	O
,	O
N	O
is	O
the	O
number	O
of	O
data	O
points	O
,	O
and	O
y	O
t	O
i	O
is	O
the	O
label	O
of	O
the	O
t	O
th	O
task	O
for	O
the	O
i	O
th	O
data	O
point	O
.	O
[	O
reference	O
]	O
We	O
further	O
consider	O
a	O
parametric	O
hypothesis	O
class	O
per	O
task	O
as	O
f	O
t	O
(	O
x	O
;	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
:	O
X	O
→	O
Y	O
t	O
,	O
such	O
that	O
some	O
parameters	O
(	O
θ	O
sh	O
)	O
are	O
shared	O
between	O
tasks	O
and	O
some	O
(	O
θ	O
t	O
)	O
are	O
task	O
-	O
specific	O
.	O
We	O
also	O
consider	O
task	O
-	O
specific	O
loss	O
functions	O
Although	O
many	O
hypothesis	O
classes	O
and	O
loss	Method
functions	Method
have	O
been	O
proposed	O
in	O
the	O
MTL	Task
literature	O
,	O
they	O
generally	O
yield	O
the	O
following	O
empirical	Method
risk	Method
minimization	Method
formulation	Method
:	O
for	O
some	O
static	O
or	O
dynamically	O
computed	O
weights	O
c	O
t	O
per	O
task	O
,	O
whereL	O
t	O
(	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
is	O
the	O
empirical	O
loss	O
of	O
the	O
task	O
t	O
,	O
defined	O
asL	O
.	O
Although	O
the	O
weighted	Method
summation	Method
formulation	Method
(	O
1	O
)	O
is	O
intuitively	O
appealing	O
,	O
it	O
typically	O
either	O
requires	O
an	O
expensive	O
grid	Method
search	Method
over	O
various	O
scalings	O
or	O
the	O
use	O
of	O
a	O
heuristic	O
[	O
reference	O
][	O
reference	O
]	O
.	O
A	O
basic	O
justification	O
for	O
scaling	O
is	O
that	O
it	O
is	O
not	O
possible	O
to	O
define	O
global	O
optimality	O
in	O
the	O
MTL	Task
setting	O
.	O
Consider	O
two	O
sets	O
of	O
solutions	O
θ	O
andθ	O
such	O
thatL	O
,	O
for	O
some	O
tasks	O
t	O
1	O
and	O
t	O
2	O
.	O
In	O
other	O
words	O
,	O
solution	O
θ	O
is	O
better	O
for	O
task	O
t	O
1	O
whereasθ	O
is	O
better	O
for	O
t	O
2	O
.	O
It	O
is	O
not	O
possible	O
to	O
compare	O
these	O
two	O
solutions	O
without	O
a	O
pairwise	O
importance	O
of	O
tasks	O
,	O
which	O
is	O
typically	O
not	O
available	O
.	O
Alternatively	O
,	O
MTL	Task
can	O
be	O
formulated	O
as	O
multi	Method
-	Method
objective	Method
optimization	Method
:	O
optimizing	O
a	O
collection	O
of	O
possibly	O
conflicting	O
objectives	O
.	O
This	O
is	O
the	O
approach	O
we	O
take	O
.	O
We	O
specify	O
the	O
multi	Method
-	Method
objective	Method
optimization	Method
formulation	O
of	O
MTL	Task
using	O
a	O
vector	Method
-	Method
valued	Method
loss	Method
L	Method
:	O
The	O
goal	O
of	O
multi	Method
-	Method
objective	Method
optimization	Method
is	O
achieving	O
Pareto	O
optimality	O
.	O
Definition	O
1	O
(	O
Pareto	O
optimality	O
for	O
MTL	Task
)	O
(	O
b	O
)	O
A	O
solution	O
θ	O
is	O
called	O
Pareto	O
optimal	O
if	O
there	O
exists	O
no	O
solution	O
θ	O
that	O
dominates	O
θ	O
.	O
The	O
set	O
of	O
Pareto	O
optimal	O
solutions	O
is	O
called	O
the	O
Pareto	O
set	O
(	O
P	O
θ	O
)	O
and	O
its	O
image	O
is	O
called	O
the	O
Pareto	O
front	O
(	O
P	O
L	O
=	O
{	O
L	O
(	O
θ	O
)	O
}	O
θ∈P	O
θ	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
gradient	Method
-	Method
based	Method
multi	Method
-	Method
objective	Method
optimization	Method
due	O
to	O
its	O
direct	O
relevance	O
to	O
gradient	O
-	O
based	O
MTL	Task
.	O
In	O
the	O
rest	O
of	O
this	O
section	O
,	O
we	O
first	O
summarize	O
in	O
Section	O
3.1	O
how	O
multi	Method
-	Method
objective	Method
optimization	Method
can	O
be	O
performed	O
with	O
gradient	Method
descent	Method
.	O
Then	O
,	O
we	O
suggest	O
in	O
Section	O
3.2	O
a	O
practical	O
algorithm	O
for	O
performing	O
multi	Method
-	Method
objective	Method
optimization	Method
over	O
very	O
large	O
parameter	O
spaces	O
.	O
Finally	O
,	O
in	O
Section	O
3.3	O
we	O
propose	O
an	O
efficient	O
solution	O
for	O
multi	Method
-	Method
objective	Method
optimization	Method
designed	O
directly	O
for	O
high	Task
-	Task
capacity	Task
deep	Task
networks	Task
.	O
Our	O
method	O
scales	O
to	O
very	O
large	O
models	O
and	O
a	O
high	O
number	O
of	O
tasks	O
with	O
negligible	O
overhead	O
.	O
section	O
:	O
Multiple	Method
Gradient	Method
Descent	Method
Algorithm	Method
As	O
in	O
the	O
single	Task
-	Task
objective	Task
case	Task
,	O
multi	Method
-	Method
objective	Method
optimization	Method
can	O
be	O
solved	O
to	O
local	O
optimality	O
via	O
gradient	Method
descent	Method
.	O
In	O
this	O
section	O
,	O
we	O
summarize	O
one	O
such	O
approach	O
,	O
called	O
the	O
multiple	Method
gradient	Method
descent	Method
algorithm	Method
(	O
MGDA	Method
)	O
[	O
reference	O
]	O
.	O
MGDA	Method
leverages	O
the	O
Karush	Method
-	Method
Kuhn	Method
-	Method
Tucker	Method
(	O
KKT	Method
)	O
conditions	O
,	O
which	O
are	O
necessary	O
for	O
optimality	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
We	O
now	O
state	O
the	O
KKT	Method
conditions	O
for	O
both	O
task	O
-	O
specific	O
and	O
shared	O
parameters	O
:	O
•	O
There	O
exist	O
α	O
1	O
,	O
.	O
.	O
.	O
,	O
α	O
T	O
≥	O
0	O
such	O
that	O
Any	O
solution	O
that	O
satisfies	O
these	O
conditions	O
is	O
called	O
a	O
Pareto	O
stationary	O
point	O
.	O
Although	O
every	O
Pareto	O
optimal	O
point	O
is	O
Pareto	O
stationary	O
,	O
the	O
reverse	O
may	O
not	O
be	O
true	O
.	O
Consider	O
the	O
optimization	Task
problem	Task
Désidéri	O
(	O
2012	O
)	O
showed	O
that	O
either	O
the	O
solution	O
to	O
this	O
optimization	Task
problem	Task
is	O
0	O
and	O
the	O
resulting	O
point	O
satisfies	O
the	O
KKT	Method
conditions	O
,	O
or	O
the	O
solution	O
gives	O
a	O
descent	O
direction	O
that	O
improves	O
all	O
tasks	O
.	O
Hence	O
,	O
the	O
resulting	O
MTL	Task
algorithm	O
would	O
be	O
gradient	Method
descent	Method
on	O
the	O
task	O
-	O
specific	O
parameters	O
followed	O
by	O
solving	O
(	O
3	O
)	O
and	O
applying	O
the	O
solution	O
(	O
as	O
a	O
gradient	O
update	O
to	O
shared	O
parameters	O
.	O
We	O
discuss	O
how	O
to	O
solve	O
(	O
3	O
)	O
for	O
an	O
arbitrary	O
model	O
in	O
Section	O
3.2	O
and	O
present	O
an	O
efficient	O
solution	O
when	O
the	O
underlying	O
model	O
is	O
an	O
encoder	Method
-	Method
decoder	Method
in	O
Section	O
3.3	O
.	O
section	O
:	O
Solving	O
the	O
Optimization	Task
Problem	Task
The	O
optimization	Task
problem	Task
defined	O
in	O
(	O
3	O
)	O
is	O
equivalent	O
to	O
finding	O
a	O
minimum	O
-	O
norm	O
point	O
in	O
the	O
convex	O
hull	O
of	O
the	O
set	O
of	O
input	O
points	O
.	O
This	O
problem	O
arises	O
naturally	O
in	O
computational	Task
geometry	Task
:	O
it	O
is	O
equivalent	O
to	O
finding	O
the	O
closest	O
point	O
within	O
a	O
convex	O
hull	O
to	O
a	O
given	O
query	O
point	O
.	O
It	O
has	O
been	O
studied	O
extensively	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Although	O
many	O
algorithms	O
have	O
been	O
proposed	O
,	O
they	O
do	O
not	O
apply	O
in	O
our	O
setting	O
because	O
the	O
assumptions	O
they	O
make	O
do	O
not	O
hold	O
.	O
Algorithms	O
proposed	O
in	O
the	O
computational	Task
geometry	Task
literature	O
address	O
the	O
problem	O
of	O
finding	Task
minimum	Task
-	Task
norm	Task
points	Task
in	O
the	O
convex	O
hull	O
of	O
a	O
large	O
number	O
of	O
points	O
in	O
a	O
low	O
-	O
dimensional	O
space	O
(	O
typically	O
of	O
dimensionality	O
2	O
or	O
3	O
)	O
.	O
In	O
our	O
setting	O
,	O
the	O
number	O
of	O
points	O
is	O
the	O
number	O
of	O
tasks	O
and	O
is	O
typically	O
low	O
;	O
in	O
contrast	O
,	O
the	O
dimensionality	Metric
is	O
the	O
number	O
of	O
shared	O
parameters	O
and	O
can	O
be	O
in	O
the	O
millions	O
.	O
We	O
therefore	O
use	O
a	O
different	O
approach	O
based	O
on	O
convex	Task
optimization	Task
,	O
since	O
(	O
3	O
)	O
is	O
a	O
convex	Task
quadratic	Task
problem	Task
with	O
linear	O
constraints	O
.	O
Before	O
we	O
tackle	O
the	O
general	O
case	O
,	O
let	O
's	O
consider	O
the	O
case	O
of	O
two	O
tasks	O
.	O
The	O
optimization	Task
problem	Task
can	O
be	O
defined	O
as	O
,	O
which	O
is	O
a	O
onedimensional	Method
quadratic	Method
function	Method
of	Method
α	Method
with	O
an	O
analytical	Method
solution	Method
:	O
where	O
.	O
We	O
further	O
visualize	O
this	O
solution	O
in	O
Figure	O
1	O
.	O
Although	O
this	O
is	O
only	O
applicable	O
when	O
T	O
=	O
2	O
,	O
this	O
enables	O
efficient	O
application	O
of	O
the	O
Frank	Method
-	Method
Wolfe	Method
algorithm	Method
[	O
reference	O
]	O
since	O
the	O
line	Method
search	Method
can	O
be	O
solved	O
analytically	O
.	O
Hence	O
,	O
we	O
use	O
Frank	Method
-	Method
Wolfe	Method
to	O
solve	O
the	O
constrained	Task
optimization	Task
problem	Task
,	O
using	O
(	O
4	O
)	O
as	O
a	O
subroutine	O
for	O
the	O
line	Method
search	Method
.	O
We	O
give	O
all	O
the	O
update	Method
equations	Method
for	O
the	O
Frank	Method
-	Method
Wolfe	Method
solver	Method
in	O
Algorithm	O
2	O
.	O
Gradient	Method
descent	Method
on	O
task	O
-	O
specific	O
parameters	O
3	O
:	O
end	O
for	O
Gradient	Method
descent	Method
on	O
shared	O
parameters	O
6	O
:	O
procedure	O
FRANKWOLFESOLVER	Method
(	Method
θ	Method
)	O
7	O
:	O
repeat	O
10:t	O
=	O
arg	O
min	O
r	O
t	O
α	O
t	O
M	O
rt	O
untilγ	O
∼	O
0	O
or	O
Number	O
of	O
Iterations	O
Limit	O
14	O
:	O
return	O
α	O
1	O
,	O
.	O
.	O
.	O
,	O
α	O
T	O
15	O
:	O
end	O
procedure	O
section	O
:	O
Efficient	O
Optimization	Task
for	O
Encoder	Task
-	Task
Decoder	Task
Architectures	Task
The	O
MTL	Task
update	O
described	O
in	O
Algorithm	O
2	O
is	O
applicable	O
to	O
any	O
problem	O
that	O
uses	O
optimization	Task
based	O
on	O
gradient	Method
descent	Method
.	O
Our	O
experiments	O
also	O
suggest	O
that	O
the	O
Frank	Method
-	Method
Wolfe	Method
solver	Method
is	O
efficient	O
and	O
accurate	O
as	O
it	O
typically	O
converges	O
in	O
a	O
modest	O
number	O
of	O
iterations	O
with	O
negligible	O
effect	O
on	O
training	Metric
time	Metric
.	O
However	O
,	O
the	O
algorithm	O
we	O
described	O
needs	O
to	O
compute	O
∇	O
θ	O
shL	O
t	O
(	O
θ	O
sh	O
,	O
θ	O
t	O
)	O
for	O
each	O
task	O
t	O
,	O
which	O
requires	O
a	O
backward	O
pass	O
over	O
the	O
shared	O
parameters	O
for	O
each	O
task	O
.	O
Hence	O
,	O
the	O
resulting	O
gradient	Task
computation	Task
would	O
be	O
the	O
forward	O
pass	O
followed	O
by	O
T	O
backward	O
passes	O
.	O
Considering	O
the	O
fact	O
that	O
computation	O
of	O
the	O
backward	Method
pass	Method
is	O
typically	O
more	O
expensive	O
than	O
the	O
forward	Method
pass	Method
,	O
this	O
results	O
in	O
linear	Metric
scaling	Metric
of	Metric
the	Metric
training	Metric
time	Metric
and	O
can	O
be	O
prohibitive	O
for	O
problems	O
with	O
more	O
than	O
a	O
few	O
tasks	O
.	O
We	O
now	O
propose	O
an	O
efficient	O
method	O
that	O
optimizes	O
an	O
upper	O
bound	O
of	O
the	O
objective	O
and	O
requires	O
only	O
a	O
single	O
backward	O
pass	O
.	O
We	O
further	O
show	O
that	O
optimizing	O
this	O
upper	Method
bound	Method
yields	O
a	O
Pareto	Method
optimal	Method
solution	Method
under	O
realistic	O
assumptions	O
.	O
The	O
architectures	O
we	O
address	O
conjoin	O
a	O
shared	Method
representation	Method
function	Method
with	O
task	O
-	O
specific	O
decision	O
functions	O
.	O
This	O
class	O
of	O
architectures	O
covers	O
most	O
of	O
the	O
existing	O
deep	O
MTL	Task
models	O
and	O
can	O
be	O
formally	O
defined	O
by	O
constraining	O
the	O
hypothesis	O
class	O
as	O
where	O
g	O
is	O
the	O
representation	O
function	O
shared	O
by	O
all	O
tasks	O
and	O
f	O
t	O
are	O
the	O
task	O
-	O
specific	O
functions	O
that	O
take	O
this	O
representation	O
as	O
input	O
.	O
If	O
we	O
denote	O
the	O
representations	O
as	O
Z	O
=	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
N	O
,	O
where	O
z	O
i	O
=	O
g	O
(	O
x	O
i	O
;	O
θ	O
sh	O
)	O
,	O
we	O
can	O
state	O
the	O
following	O
upper	O
bound	O
as	O
a	O
direct	O
consequence	O
of	O
the	O
chain	Method
rule	Method
:	O
term	O
since	O
it	O
does	O
not	O
affect	O
the	O
optimization	Task
.	O
The	O
resulting	O
optimization	Task
problem	Task
is	O
We	O
refer	O
to	O
this	O
problem	O
as	O
MGDA	Method
-	Method
UB	Method
(	O
Multiple	Method
Gradient	Method
Descent	Method
Algorithm	Method
-	Method
Upper	Method
Bound	Method
)	O
.	O
In	O
practice	O
,	O
MGDA	Method
-	Method
UB	Method
corresponds	O
to	O
using	O
the	O
gradients	O
of	O
the	O
task	O
losses	O
with	O
respect	O
to	O
the	O
representations	O
instead	O
of	O
the	O
shared	O
parameters	O
.	O
We	O
use	O
Algorithm	O
2	O
with	O
only	O
this	O
change	O
as	O
the	O
final	O
method	O
.	O
Although	O
MGDA	Method
-	Method
UB	Method
is	O
an	O
approximation	O
of	O
the	O
original	O
optimization	Task
problem	Task
,	O
we	O
now	O
state	O
a	O
theorem	O
that	O
shows	O
that	O
our	O
method	O
produces	O
a	O
Pareto	Task
optimal	Task
solution	Task
under	O
mild	O
assumptions	O
.	O
The	O
proof	O
is	O
given	O
in	O
the	O
supplement	O
.	O
section	O
:	O
Theorem	O
1	O
Assume	O
∂Z	Method
∂θ	Method
sh	Method
is	O
full	O
-	O
rank	O
.	O
If	O
α	O
1	O
,	O
...	O
,	O
T	O
is	O
the	O
solution	O
of	O
MGDA	Method
-	Method
UB	Method
,	O
one	O
of	O
the	O
following	O
is	O
true	O
:	O
=	O
0	O
and	O
the	O
current	O
parameters	O
are	O
Pareto	O
stationary	O
.	O
This	O
result	O
follows	O
from	O
the	O
fact	O
that	O
as	O
long	O
as	O
∂Z	O
∂θ	O
sh	O
is	O
full	O
rank	O
,	O
optimizing	O
the	O
upper	O
bound	O
corresponds	O
to	O
minimizing	O
the	O
norm	O
of	O
the	O
convex	O
combination	O
of	O
the	O
gradients	O
using	O
the	O
Mahalonobis	O
norm	O
defined	O
by	O
∂Z	O
∂θ	O
sh	O
∂Z	O
∂θ	O
sh	O
.	O
The	O
non	O
-	O
singularity	O
assumption	O
is	O
reasonable	O
as	O
singularity	O
implies	O
that	O
tasks	O
are	O
linearly	O
related	O
and	O
a	O
trade	O
-	O
off	O
is	O
not	O
necessary	O
.	O
In	O
summary	O
,	O
our	O
method	O
provably	O
finds	O
a	O
Pareto	O
stationary	O
point	O
with	O
negligible	O
computational	Metric
overhead	Metric
and	O
can	O
be	O
applied	O
to	O
any	O
deep	Task
multi	Task
-	Task
objective	Task
problem	Task
with	O
an	O
encoder	Method
-	Method
decoder	Method
model	Method
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
presented	O
MTL	Task
method	O
on	O
a	O
number	O
of	O
problems	O
.	O
First	O
,	O
we	O
use	O
MultiMNIST	Method
[	O
reference	O
]	O
,	O
an	O
MTL	Task
adaptation	O
of	O
MNIST	Method
[	O
reference	O
]	O
.	O
Next	O
,	O
we	O
tackle	O
multi	Task
-	Task
label	Task
classification	Task
on	O
the	O
CelebA	Material
dataset	Material
[	O
reference	O
]	O
by	O
considering	O
each	O
label	O
as	O
a	O
distinct	O
binary	Task
classification	Task
task	Task
.	O
These	O
problems	O
include	O
both	O
classification	Task
and	Task
regression	Task
,	O
with	O
the	O
number	O
of	O
tasks	O
ranging	O
from	O
2	O
to	O
40	O
.	O
Finally	O
,	O
we	O
experiment	O
with	O
scene	Task
understanding	Task
,	O
jointly	O
tackling	O
the	O
tasks	O
of	O
semantic	Task
segmentation	Task
,	O
instance	Task
segmentation	Task
,	O
and	O
depth	Task
estimation	Task
on	O
the	O
Cityscapes	Material
dataset	Material
[	O
reference	O
]	O
.	O
We	O
discuss	O
each	O
experiment	O
separately	O
in	O
the	O
following	O
subsections	O
.	O
The	O
baselines	O
we	O
consider	O
are	O
(	O
i	O
)	O
uniform	Method
scaling	Method
:	O
minimizing	O
a	O
uniformly	Method
weighted	Method
sum	Method
of	Method
loss	Method
functions	Method
1	O
T	O
t	O
L	O
t	O
,	O
(	O
ii	O
)	O
single	O
task	O
:	O
solving	O
tasks	O
independently	O
,	O
(	O
iii	O
)	O
grid	Method
search	Method
:	O
exhaustively	O
trying	O
various	O
values	O
from	O
{	O
c	O
t	O
∈	O
[	O
0	O
,	O
1	O
]	O
|	O
t	O
c	O
t	O
=	O
1	O
}	O
and	O
optimizing	O
for	O
[	O
reference	O
]	O
:	O
using	O
the	O
uncertainty	Method
weighting	Method
proposed	O
by	O
[	O
reference	O
]	O
,	O
and	O
(	O
v	O
)	O
GradNorm	Method
:	O
using	O
the	O
normalization	Method
proposed	O
by	O
[	O
reference	O
]	O
.	O
section	O
:	O
MultiMNIST	O
Our	O
initial	O
experiments	O
are	O
on	O
MultiMNIST	Material
,	O
an	O
MTL	Task
version	O
of	O
the	O
MNIST	Material
dataset	Material
[	O
reference	O
]	O
.	O
In	O
order	O
to	O
convert	O
digit	Task
classification	Task
into	O
a	O
multi	Task
-	Task
task	Task
problem	Task
,	O
[	O
reference	O
]	O
overlaid	O
multiple	O
images	O
together	O
.	O
We	O
use	O
a	O
similar	O
construction	O
.	O
For	O
each	O
image	O
,	O
a	O
different	O
one	O
is	O
chosen	O
uniformly	O
in	O
random	O
.	O
Then	O
one	O
of	O
these	O
images	O
is	O
put	O
at	O
the	O
top	O
-	O
left	O
and	O
the	O
other	O
one	O
is	O
at	O
the	O
bottom	O
-	O
right	O
.	O
The	O
resulting	O
tasks	O
are	O
:	O
classifying	O
the	O
digit	O
on	O
the	O
top	O
-	O
left	O
(	O
task	O
-	O
L	O
)	O
and	O
classifying	O
the	O
digit	O
on	O
the	O
bottom	O
-	O
right	O
(	O
task	O
-	O
R	O
)	O
.	O
We	O
use	O
60	O
K	O
examples	O
and	O
directly	O
apply	O
existing	O
single	Method
-	Method
task	Method
MNIST	Method
models	Method
.	O
The	O
MultiMNIST	Material
dataset	Material
is	O
illustrated	O
in	O
the	O
supplement	O
.	O
We	O
use	O
the	O
LeNet	Method
architecture	Method
[	O
reference	O
]	O
.	O
We	O
treat	O
all	O
layers	O
except	O
the	O
last	O
as	O
the	O
representation	O
function	O
g	O
and	O
put	O
two	O
fully	O
-	O
connected	O
layers	O
as	O
task	O
-	O
specific	O
functions	O
(	O
see	O
the	O
[	O
reference	O
]	O
.	O
Lower	O
is	O
better	O
.	O
We	O
divide	O
attributes	O
into	O
two	O
sets	O
for	O
legibility	O
:	O
easy	O
on	O
the	O
left	O
,	O
hard	O
on	O
the	O
right	O
.	O
Zoom	O
in	O
for	O
details	O
.	O
supplement	O
for	O
details	O
)	O
.	O
We	O
visualize	O
the	O
performance	Metric
profile	Metric
as	O
a	O
scatter	Metric
plot	Metric
of	Metric
accuracies	Metric
on	O
task	O
-	O
L	O
and	O
task	O
-	O
R	O
in	O
Figure	O
3	O
,	O
and	O
list	O
the	O
results	O
in	O
Table	O
3	O
.	O
In	O
this	O
setup	O
,	O
any	O
static	Method
scaling	Method
results	O
in	O
lower	O
accuracy	Metric
than	O
solving	O
each	O
task	O
separately	O
(	O
the	O
singletask	Method
baseline	Method
)	O
.	O
The	O
two	O
tasks	O
appear	O
to	O
compete	O
for	O
model	O
capacity	O
,	O
since	O
increase	O
in	O
the	O
accuracy	Metric
of	O
one	O
task	O
results	O
in	O
decrease	O
in	O
the	O
accuracy	Metric
of	O
the	O
other	O
.	O
Uncertainty	Task
weighting	Task
[	O
reference	O
]	O
and	O
GradNorm	Method
[	O
reference	O
]	O
find	O
solutions	O
that	O
are	O
slightly	O
better	O
than	O
grid	Method
search	Method
but	O
distinctly	O
worse	O
than	O
the	O
single	O
-	O
task	O
baseline	O
.	O
In	O
contrast	O
,	O
our	O
method	O
finds	O
a	O
solution	O
that	O
efficiently	O
utilizes	O
the	O
model	O
capacity	O
and	O
yields	O
accuracies	Metric
that	O
are	O
as	O
good	O
as	O
the	O
single	Method
-	Method
task	Method
solutions	Method
.	O
This	O
experiment	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
method	O
as	O
well	O
as	O
the	O
necessity	O
of	O
treating	O
MTL	Task
as	O
multi	Method
-	Method
objective	Method
optimization	Method
.	O
Even	O
after	O
a	O
large	O
hyper	O
-	O
parameter	O
search	O
,	O
any	O
scaling	O
of	O
tasks	O
does	O
not	O
approach	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
Next	O
,	O
we	O
tackle	O
multi	Task
-	Task
label	Task
classification	Task
.	O
Given	O
a	O
set	O
of	O
attributes	O
,	O
multi	Task
-	Task
label	Task
classification	Task
calls	O
for	O
deciding	O
whether	O
each	O
attribute	O
holds	O
for	O
the	O
input	O
.	O
We	O
use	O
the	O
CelebA	Material
dataset	Material
[	O
reference	O
]	O
,	O
which	O
includes	O
200	O
K	O
face	O
images	O
annotated	O
with	O
40	O
attributes	O
.	O
Each	O
attribute	O
gives	O
rise	O
to	O
a	O
binary	Task
classification	Task
task	Task
and	O
we	O
cast	O
this	O
as	O
a	O
40	O
-	O
way	O
MTL	Task
problem	O
.	O
We	O
use	O
ResNet	Method
-	Method
18	Method
[	O
reference	O
]	O
without	O
the	O
final	O
layer	O
as	O
a	O
shared	O
representation	O
function	O
,	O
and	O
attach	O
a	O
linear	Method
layer	Method
for	O
each	O
attribute	O
(	O
see	O
the	O
supplement	O
for	O
further	O
details	O
)	O
.	O
section	O
:	O
Multi	Task
-	Task
Label	Task
Classification	Task
We	O
plot	O
the	O
resulting	O
error	Metric
for	O
each	O
binary	Task
classification	Task
task	Task
as	O
a	O
radar	O
chart	O
in	O
Figure	O
2	O
.	O
The	O
average	O
over	O
them	O
is	O
listed	O
in	O
Table	O
1	O
.	O
We	O
skip	O
grid	Method
search	Method
since	O
it	O
is	O
not	O
feasible	O
over	O
40	O
tasks	O
.	O
Although	O
uniform	Method
scaling	Method
is	O
the	O
norm	O
in	O
the	O
multi	Task
-	Task
label	Task
classification	Task
literature	Task
,	O
single	Task
-	Task
task	Task
performance	O
is	O
significantly	O
better	O
.	O
Our	O
method	O
outperforms	O
baselines	O
for	O
significant	O
majority	O
of	O
tasks	O
and	O
achieves	O
comparable	O
performance	O
in	O
rest	O
.	O
This	O
experiment	O
also	O
shows	O
that	O
our	O
method	O
remains	O
effective	O
when	O
the	O
number	O
of	O
tasks	O
is	O
high	O
.	O
section	O
:	O
Scene	Task
Understanding	Task
To	O
evaluate	O
our	O
method	O
in	O
a	O
more	O
realistic	O
setting	O
,	O
we	O
use	O
scene	Task
understanding	Task
.	O
Given	O
an	O
RGB	O
image	O
,	O
we	O
solve	O
three	O
tasks	O
:	O
semantic	Task
segmentation	Task
(	O
assigning	O
pixel	O
-	O
level	O
class	O
labels	O
)	O
,	O
instance	Method
segmentation	Method
(	O
assigning	O
pixel	O
-	O
level	O
instance	O
labels	O
)	O
,	O
and	O
monocular	Task
depth	Task
estimation	Task
(	O
estimating	O
continuous	O
disparity	O
per	O
pixel	O
)	O
.	O
We	O
follow	O
the	O
experimental	O
procedure	O
of	O
[	O
reference	O
]	O
and	O
use	O
an	O
encoder	Method
-	Method
decoder	Method
architecture	Method
.	O
The	O
encoder	Method
is	O
based	O
on	O
ResNet	Method
-	Method
50	Method
[	O
reference	O
]	O
and	O
is	O
shared	O
by	O
all	O
three	O
tasks	O
.	O
The	O
decoders	O
are	O
task	O
-	O
specific	O
and	O
are	O
based	O
on	O
the	O
pyramid	Method
pooling	Method
module	Method
[	O
reference	O
]	O
)	O
(	O
see	O
the	O
supplement	O
for	O
further	O
implementation	O
details	O
)	O
.	O
Since	O
the	O
output	O
space	O
of	O
instance	Task
segmentation	Task
is	O
unconstrained	O
(	O
the	O
number	O
of	O
instances	O
is	O
not	O
known	O
in	O
advance	O
)	O
,	O
we	O
use	O
a	O
proxy	Method
problem	Method
as	O
in	O
[	O
reference	O
]	O
.	O
For	O
each	O
pixel	O
,	O
we	O
estimate	O
the	O
location	O
of	O
the	O
center	O
of	O
mass	O
of	O
the	O
instance	O
that	O
encompasses	O
the	O
pixel	O
.	O
These	O
center	O
votes	O
can	O
then	O
be	O
clustered	O
to	O
extract	O
the	O
instances	O
.	O
In	O
our	O
experiments	O
,	O
we	O
directly	O
report	O
the	O
MSE	Metric
in	O
the	O
proxy	Task
task	Task
.	O
Figure	O
4	O
shows	O
the	O
performance	O
profile	O
for	O
each	O
pair	O
of	O
tasks	O
,	O
although	O
we	O
perform	O
all	O
experiments	O
on	O
all	O
three	O
tasks	O
jointly	O
.	O
The	O
pairwise	Metric
performance	Metric
profiles	Metric
shown	O
in	O
Figure	O
4	O
are	O
simply	O
2D	O
projections	O
of	O
the	O
three	O
-	O
dimensional	O
profile	O
,	O
presented	O
this	O
way	O
for	O
legibility	O
.	O
The	O
results	O
are	O
also	O
listed	O
in	O
Table	O
4	O
.	O
MTL	Task
outperforms	O
single	O
-	O
task	Metric
accuracy	Metric
,	O
indicating	O
that	O
the	O
tasks	O
cooperate	O
and	O
help	O
each	O
other	O
.	O
Our	O
method	O
outperforms	O
all	O
baselines	O
on	O
all	O
tasks	O
.	O
section	O
:	O
Role	O
of	O
the	O
Approximation	O
In	O
order	O
to	O
understand	O
the	O
role	O
of	O
the	O
approximation	O
proposed	O
in	O
Section	O
3.3	O
,	O
we	O
compare	O
the	O
final	O
performance	O
and	O
training	Metric
time	Metric
of	O
our	O
algorithm	O
with	O
and	O
without	O
the	O
presented	O
approximation	O
in	O
Table	O
2	O
(	O
runtime	Metric
measured	O
on	O
a	O
single	O
Titan	O
Xp	O
GPU	O
)	O
.	O
For	O
a	O
small	O
number	O
of	O
tasks	O
(	O
3	O
for	O
scene	Task
understanding	Task
)	O
,	O
training	Metric
time	Metric
is	O
reduced	O
by	O
40	O
%	O
.	O
For	O
the	O
multi	Task
-	Task
label	Task
classification	Task
experiment	O
(	O
40	O
tasks	O
)	O
,	O
the	O
presented	O
approximation	O
accelerates	O
learning	Task
by	O
a	O
factor	O
of	O
25	O
.	O
On	O
the	O
accuracy	Metric
side	Metric
,	O
we	O
expect	O
both	O
methods	O
to	O
perform	O
similarly	O
as	O
long	O
as	O
the	O
full	O
-	O
rank	O
assumption	O
is	O
satisfied	O
.	O
As	O
expected	O
,	O
the	O
accuracy	Metric
of	O
both	O
methods	O
is	O
very	O
similar	O
.	O
Somewhat	O
surprisingly	O
,	O
our	O
approximation	O
results	O
in	O
slightly	O
improved	O
accuracy	Metric
in	O
all	O
experiments	O
.	O
While	O
counter	O
-	O
intuitive	O
at	O
first	O
,	O
we	O
hypothesize	O
that	O
this	O
is	O
related	O
to	O
the	O
use	O
of	O
SGD	Method
in	O
the	O
learning	Method
algorithm	Method
.	O
Stability	Task
analysis	Task
in	O
convex	Task
optimization	Task
suggests	O
that	O
if	O
gradients	O
are	O
computed	O
with	O
an	O
error∇	Metric
θ	O
L	O
t	O
=	O
∇	O
θ	O
L	O
t	O
+	O
e	O
t	O
(	O
θ	O
corresponds	O
to	O
θ	O
sh	O
in	O
(	O
3	O
)	O
)	O
,	O
as	O
opposed	O
to	O
Z	O
in	O
the	O
approximate	Task
problem	Task
in	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
,	O
the	O
error	Metric
in	O
the	O
solution	O
is	O
bounded	O
as	O
α	O
−	O
α	O
2	O
≤	O
O	O
(	O
max	O
t	O
e	O
t	O
2	O
)	O
.	O
Considering	O
the	O
fact	O
that	O
the	O
gradients	O
are	O
computed	O
over	O
the	O
full	O
parameter	O
set	O
(	O
millions	O
of	O
dimensions	O
)	O
for	O
the	O
original	O
problem	O
and	O
over	O
a	O
smaller	O
space	O
for	O
the	O
approximation	O
(	O
batch	Method
size	Method
times	Method
representation	Method
which	O
is	O
in	O
the	O
thousands	O
)	O
,	O
the	O
dimension	O
of	O
the	O
error	Metric
vector	O
is	O
significantly	O
higher	O
in	O
the	O
original	O
problem	O
.	O
We	O
expect	O
the	O
l	O
2	O
norm	O
of	O
such	O
a	O
random	O
vector	O
to	O
depend	O
on	O
the	O
dimension	O
.	O
In	O
summary	O
,	O
our	O
quantitative	O
analysis	O
of	O
the	O
approximation	O
suggests	O
that	O
(	O
i	O
)	O
the	O
approximation	O
does	O
not	O
cause	O
an	O
accuracy	O
drop	O
and	O
(	O
ii	O
)	O
by	O
solving	O
an	O
equivalent	Task
problem	Task
in	O
a	O
lower	O
-	O
dimensional	O
space	O
,	O
our	O
method	O
achieves	O
both	O
better	O
computational	Metric
efficiency	Metric
and	O
higher	O
stability	Metric
.	O
section	O
:	O
Conclusion	O
We	O
described	O
an	O
approach	O
to	O
multi	Task
-	Task
task	Task
learning	Task
.	O
Our	O
approach	O
is	O
based	O
on	O
multi	Method
-	Method
objective	Method
optimization	Method
.	O
In	O
order	O
to	O
apply	O
multi	Method
-	Method
objective	Method
optimization	Method
to	O
MTL	Task
,	O
we	O
described	O
an	O
efficient	O
algorithm	O
as	O
well	O
as	O
specific	O
approximations	O
that	O
yielded	O
a	O
deep	O
MTL	Task
algorithm	O
with	O
almost	O
no	O
computational	Metric
overhead	Metric
.	O
Our	O
experiments	O
indicate	O
that	O
the	O
resulting	O
algorithm	O
is	O
effective	O
for	O
a	O
wide	O
range	O
of	O
multi	Task
-	Task
task	Task
scenarios	Task
.	O
Figure	O
3	O
:	O
MultiMNIST	Metric
accuracy	Metric
profile	Metric
.	O
We	O
plot	O
the	O
obtained	O
accuracy	Metric
in	O
detecting	O
the	O
left	O
and	O
right	O
digits	O
for	O
all	O
baselines	O
.	O
The	O
grid	Task
-	Task
search	Task
results	O
suggest	O
that	O
the	O
tasks	O
compete	O
for	O
model	Task
capacity	Task
.	O
Our	O
method	O
is	O
the	O
only	O
one	O
that	O
finds	O
a	O
solution	O
that	O
is	O
as	O
good	O
as	O
training	O
a	O
dedicated	O
model	O
for	O
each	O
task	O
.	O
Top	O
-	O
right	O
is	O
better	O
.	O
Figure	O
4	O
:	O
Cityscapes	Metric
performance	Metric
profile	Metric
.	O
We	O
plot	O
the	O
performance	O
of	O
all	O
baselines	O
for	O
the	O
tasks	O
of	O
semantic	Task
segmentation	Task
,	O
instance	Task
segmentation	Task
,	O
and	O
depth	Task
estimation	Task
.	O
We	O
use	O
mIoU	Metric
for	O
semantic	Task
segmentation	Task
,	O
error	Metric
of	O
per	Method
-	Method
pixel	Method
regression	Method
(	O
normalized	O
to	O
image	O
size	O
)	O
for	O
instance	Task
segmentation	Task
,	O
and	O
disparity	O
error	Metric
for	O
depth	Task
estimation	Task
.	O
To	O
convert	O
errors	O
to	O
performance	Metric
measures	Metric
,	O
we	O
use	O
1	O
−	O
instance	O
error	Metric
and	O
1	O
/	O
disparity	O
error	Metric
.	O
We	O
plot	O
2D	O
projections	O
of	O
the	O
performance	O
profile	O
for	O
each	O
pair	O
of	O
tasks	O
.	O
Although	O
we	O
plot	O
pairwise	O
projections	O
for	O
visualization	Task
,	O
each	O
point	O
in	O
the	O
plots	O
solves	O
all	O
tasks	O
.	O
Top	O
-	O
right	O
is	O
better	O
.	O
The	O
KKT	Method
condition	O
for	O
this	O
Lagrangian	O
yields	O
the	O
desired	O
result	O
as	O
section	O
:	O
B	O
Additional	O
Results	O
on	O
Multi	Task
-	Task
label	Task
Classification	Task
In	O
this	O
section	O
,	O
we	O
present	O
the	O
experimental	O
results	O
we	O
did	O
not	O
include	O
in	O
the	O
main	O
text	O
.	O
In	O
the	O
main	O
text	O
,	O
we	O
plotted	O
a	O
radar	Method
chart	Method
of	O
the	O
binary	Metric
attribute	Metric
classification	Metric
errors	Metric
.	O
However	O
,	O
we	O
did	O
not	O
include	O
the	O
tabulated	O
results	O
due	O
to	O
the	O
space	O
limitations	O
.	O
Here	O
we	O
list	O
the	O
binary	O
classification	O
error	Metric
of	O
each	O
attribute	O
for	O
each	O
algorithm	O
in	O
Table	O
5	O
.	O
C	O
Implementation	O
Details	O
section	O
:	O
C.1	O
MultiMNIST	O
We	O
use	O
the	O
MultiMNIST	Material
dataset	Material
,	O
which	O
overlays	O
multiple	O
images	O
together	O
[	O
reference	O
]	O
.	O
For	O
each	O
image	O
,	O
a	O
different	O
one	O
is	O
chosen	O
uniformly	O
in	O
random	O
.	O
One	O
of	O
these	O
images	O
is	O
placed	O
at	O
the	O
top	O
-	O
left	O
and	O
the	O
other	O
at	O
the	O
bottom	O
-	O
right	O
.	O
We	O
show	O
sample	O
MultiMNIST	Material
images	Material
in	O
Figure	O
6	O
.	O
For	O
the	O
MultiMNIST	O
experiments	O
,	O
we	O
use	O
an	O
architecture	O
based	O
on	O
[	O
reference	O
]	O
.	O
We	O
use	O
all	O
layers	O
except	O
the	O
final	O
one	O
as	O
a	O
shared	Method
encoder	Method
.	O
We	O
use	O
the	O
fully	Method
-	Method
connected	Method
layer	Method
as	O
a	O
task	O
-	O
specific	O
function	O
for	O
the	O
left	Task
and	Task
right	Task
tasks	Task
by	O
simply	O
adding	O
two	O
independent	O
fully	Method
-	Method
connected	Method
layers	Method
,	O
each	O
taking	O
the	O
output	O
of	O
the	O
shared	Method
encoder	Method
as	O
input	O
.	O
As	O
a	O
task	O
-	O
specific	O
loss	O
function	O
,	O
we	O
use	O
the	O
cross	Method
-	Method
entropy	Method
loss	Method
with	O
a	O
softmax	Method
for	O
both	O
tasks	O
.	O
The	O
architecture	O
is	O
visualized	O
in	O
Figure	O
5	O
.	O
The	O
implementation	O
uses	O
PyTorch	Method
[	O
reference	O
]	O
.	O
For	O
all	O
baselines	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	Metric
rates	Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
validation	Metric
accuracy	Metric
.	O
We	O
used	O
SGD	Method
with	O
momentum	Method
,	O
halving	O
the	O
learning	Metric
rate	Metric
every	O
30	O
epochs	O
.	O
We	O
use	O
batch	O
size	O
256	O
and	O
train	O
for	O
100	O
epochs	O
.	O
We	O
report	O
test	Metric
accuracy	Metric
.	O
section	O
:	O
C.2	O
Multi	Task
-	Task
label	Task
classification	Task
For	O
multi	Task
-	Task
label	Task
classification	Task
experiments	Task
,	O
we	O
use	O
ResNet	Method
-	Method
18	Method
[	O
reference	O
]	O
without	O
the	O
final	O
layer	O
as	O
a	O
shared	O
representation	O
function	O
.	O
Since	O
there	O
are	O
40	O
attributes	O
,	O
we	O
add	O
40	O
separate	O
2048	O
×	O
2	O
dimensional	O
fully	O
-	O
connected	O
layers	O
as	O
task	O
-	O
specific	O
functions	O
.	O
The	O
final	O
two	O
-	O
dimensional	O
output	O
is	O
passed	O
through	O
a	O
2	Method
-	Method
class	Method
softmax	Method
to	O
get	O
binary	O
attribute	O
classification	O
probabilities	O
.	O
We	O
use	O
cross	O
-	O
entropy	O
as	O
a	O
task	Task
-	Task
specific	Task
loss	Task
.	O
The	O
architecture	O
is	O
visualized	O
in	O
Figure	O
7	O
.	O
The	O
implementation	O
uses	O
PyTorch	Method
[	O
reference	O
]	O
.	O
We	O
resize	O
each	O
CelebA	Material
image	Material
[	O
reference	O
]	O
to	O
64	O
×	O
64	O
×	O
3	O
.	O
For	O
all	O
experiments	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	Metric
rates	Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
validation	Metric
accuracy	Metric
.	O
We	O
used	O
SGD	Method
with	O
momentum	Method
,	O
halving	O
the	O
learning	Metric
rate	Metric
every	O
30	O
epochs	O
.	O
We	O
use	O
batch	O
size	O
256	O
and	O
train	O
for	O
100	O
epochs	O
.	O
We	O
report	O
attribute	Metric
-	Metric
wise	Metric
binary	Metric
accuracies	Metric
on	O
the	O
test	O
set	O
as	O
well	O
as	O
the	O
average	Metric
accuracy	Metric
.	O
section	O
:	O
C.3	O
Scene	Task
understanding	Task
For	O
scene	Task
understanding	Task
experiments	Task
,	O
we	O
use	O
the	O
Cityscapes	Material
dataset	Material
[	O
reference	O
]	O
.	O
We	O
resize	O
all	O
images	O
to	O
resolution	O
256	O
×	O
512	O
for	O
computational	Metric
efficiency	Metric
.	O
As	O
a	O
shared	Method
representation	Method
function	Method
(	O
encoder	Method
)	O
,	O
we	O
use	O
the	O
ResNet	Method
-	Method
50	Method
architecture	Method
[	O
reference	O
]	O
in	O
fully	Method
-	Method
convolutional	Method
fashion	Method
.	O
We	O
take	O
the	O
ResNet	Method
-	Method
50	Method
architecture	Method
and	O
only	O
use	O
layers	O
prior	O
to	O
average	Method
pooling	Method
that	O
are	O
fully	Method
convolutional	Method
.	O
As	O
a	O
decoder	O
,	O
we	O
use	O
the	O
pyramid	Method
pooling	Method
module	Method
[	O
reference	O
]	O
and	O
set	O
the	O
output	O
sizes	O
to	O
256	O
×	O
512	O
×	O
19	O
for	O
semantic	Task
segmentation	Task
(	O
19	O
classes	O
)	O
,	O
256	O
×	O
512	O
×	O
2	O
for	O
instance	Task
segmentation	Task
(	O
one	O
output	O
channel	O
for	O
the	O
x	O
-	O
offset	O
of	O
the	O
center	O
location	O
and	O
another	O
channel	O
for	O
the	O
y	O
-	O
offset	O
)	O
,	O
and	O
256	O
×	O
512	O
×	O
1	O
for	O
monocular	Task
depth	Task
estimation	Task
.	O
For	O
instance	Task
segmentation	Task
,	O
we	O
use	O
the	O
proxy	Task
task	Task
of	O
estimating	O
the	O
offset	O
for	O
the	O
center	O
location	O
of	O
the	O
instance	O
that	O
encompasses	O
the	O
pixel	O
.	O
We	O
directly	O
estimate	O
disparity	O
instead	O
of	O
depth	O
and	O
later	O
convert	O
it	O
to	O
depth	O
using	O
the	O
provided	O
camera	O
intrinsics	O
.	O
As	O
a	O
loss	O
function	O
,	O
we	O
use	O
cross	Method
-	Method
entropy	Method
with	O
a	O
softmax	Method
for	O
semantic	Task
segmentation	Task
,	O
and	O
MSE	Metric
for	O
depth	Task
and	Task
instance	Task
segmentation	Task
.	O
We	O
visualize	O
the	O
architecture	O
in	O
Figure	O
8	O
.	O
We	O
initialize	O
the	O
encoder	O
with	O
a	O
model	O
pretrained	O
on	O
ImageNet	Material
[	O
reference	O
]	O
.	O
We	O
use	O
the	O
implementation	O
of	O
the	O
pyramidal	Method
pooling	Method
network	Method
with	O
bilinear	Method
interpolation	Method
shared	O
by	O
[	O
reference	O
]	O
.	O
Ground	O
-	O
truth	O
results	O
for	O
the	O
Cityscapes	Material
test	Material
set	Material
are	O
not	O
publicly	O
available	O
.	O
Therefore	O
,	O
we	O
report	O
numbers	O
on	O
the	O
validation	O
set	O
.	O
As	O
a	O
validation	O
set	O
for	O
hyperparameter	Task
search	Task
,	O
we	O
randomly	O
choose	O
275	O
images	O
from	O
the	O
training	O
set	O
.	O
After	O
the	O
best	O
hyperparameters	O
are	O
chosen	O
,	O
we	O
retrain	O
with	O
the	O
full	O
training	O
set	O
and	O
report	O
the	O
metrics	O
on	O
the	O
Cityscapes	Material
validation	Material
set	Material
,	O
which	O
our	O
algorithm	O
never	O
sees	O
during	O
training	O
or	O
hyperparameter	Method
search	Method
.	O
As	O
metrics	O
,	O
we	O
use	O
mean	Metric
intersection	Metric
over	Metric
union	Metric
(	O
mIoU	Metric
)	O
for	O
semantic	Task
segmentation	Task
,	O
MSE	Metric
for	O
instance	Task
segmentation	Task
,	O
and	O
MSE	Metric
for	O
disparities	Task
(	O
depth	Task
estimation	Task
)	O
.	O
We	O
directly	O
report	O
the	O
metric	O
in	O
the	O
proxy	Task
task	Task
for	O
instance	Task
segmentation	Task
instead	O
of	O
performing	O
a	O
further	O
clustering	Method
operation	Method
.	O
For	O
all	O
experiments	O
,	O
we	O
searched	O
over	O
the	O
set	O
LR	O
=	O
{	O
1e−4	O
,	O
5e−4	O
,	O
1e−3	O
,	O
5e−3	O
,	O
1e−2	O
,	O
5e−2	O
}	O
of	O
learning	Metric
rates	Metric
and	O
chose	O
the	O
model	O
with	O
the	O
highest	O
Figure	O
6	O
:	O
Sample	O
MultiMNIST	Material
images	Material
.	O
In	O
each	O
image	O
,	O
one	O
task	O
(	O
task	O
-	O
L	O
)	O
is	O
classifying	O
the	O
digit	O
on	O
the	O
top	O
-	O
left	O
and	O
the	O
second	O
task	O
(	O
task	O
-	O
R	O
)	O
is	O
classifying	O
the	O
digit	O
on	O
the	O
bottom	O
-	O
right	O
.	O
validation	Metric
accuracy	Metric
.	O
We	O
used	O
SGD	Method
with	O
momentum	Method
,	O
halving	O
the	O
learning	Metric
rate	Metric
every	O
30	O
epochs	O
.	O
We	O
use	O
batch	O
size	O
8	O
and	O
train	O
for	O
250	O
epochs	O
.	O
section	O
:	O
Convolutional	Method
ResNet	Method
50	O
:	O
ResNet	Method
50	O
without	O
final	O
average	Method
poling	Method
and	O
fully	Method
connected	Method
layer	Method
.	O
section	O
:	O
Pyramid	Method
Pooling	Method
section	O
:	O
section	O
:	O
A	O
Proof	O
of	O
Theorem	O
1	O
Proof	O
.	O
We	O
begin	O
by	O
showing	O
that	O
if	O
the	O
optimum	O
value	O
of	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
is	O
0	O
,	O
so	O
is	O
the	O
optimum	O
value	O
of	O
(	O
3	O
)	O
.	O
This	O
shows	O
the	O
first	O
case	O
of	O
the	O
theorem	O
.	O
Then	O
,	O
we	O
will	O
show	O
the	O
second	O
part	O
.	O
If	O
the	O
optimum	O
value	O
of	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
is	O
0	O
,	O
Hence	O
α	O
1	O
,	O
.	O
.	O
.	O
,	O
α	O
T	O
is	O
the	O
solution	O
of	O
(	O
3	O
)	O
and	O
the	O
optimal	O
value	O
of	O
(	O
3	O
)	O
is	O
0	O
.	O
This	O
proves	O
the	O
first	O
case	O
of	O
the	O
theorem	O
.	O
Before	O
we	O
move	O
to	O
the	O
second	O
case	O
,	O
we	O
state	O
a	O
straightforward	O
corollary	O
.	O
Since	O
∂Z	O
∂θ	O
sh	O
is	O
full	O
rank	O
,	O
this	O
equivalence	O
is	O
bi	O
-	O
directional	O
.	O
In	O
other	O
words	O
,	O
if	O
α	O
1	O
,	O
.	O
.	O
.	O
,	O
α	O
T	O
is	O
the	O
solution	O
of	O
(	O
3	O
)	O
,	O
it	O
is	O
the	O
solution	O
of	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
as	O
well	O
.	O
Hence	O
,	O
both	O
formulations	O
completely	O
agree	O
on	O
Pareto	O
stationarity	O
.	O
In	O
order	O
to	O
prove	O
the	O
second	O
case	O
,	O
we	O
need	O
to	O
show	O
that	O
the	O
resulting	O
descent	O
direction	O
computed	O
by	O
solving	O
(	O
MGDA	Method
-	O
UB	O
)	O
does	O
not	O
increase	O
any	O
of	O
the	O
loss	O
functions	O
.	O
Formally	O
,	O
we	O
need	O
to	O
show	O
that	O
This	O
condition	O
is	O
equivalent	O
to	O
where	O
M	O
=	O
∂Z	O
∂θ	O
sh	O
∂Z	O
∂θ	O
sh	O
.	O
Since	O
M	O
is	O
positive	O
definite	O
(	O
following	O
the	O
assumption	O
)	O
,	O
this	O
is	O
further	O
equivalent	O
to	O
We	O
show	O
that	O
this	O
follows	O
from	O
the	O
optimality	O
conditions	O
for	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
.	O
The	O
Lagrangian	O
of	O
(	O
MGDA	Method
-	Method
UB	Method
)	O
is	O
section	O
:	O
