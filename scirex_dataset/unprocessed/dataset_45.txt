document	O
:	O
Implicit	Task
3D	Task
Orientation	Task
Learning	Task
for	O
6D	Task
Object	Task
Detection	Task
from	O
RGB	O
Images	O
We	O
propose	O
a	O
real	Method
-	Method
time	Method
RGB	Method
-	Method
based	Method
pipeline	Method
for	O
object	Task
detection	Task
and	O
6D	Task
pose	Task
estimation	Task
.	O
Our	O
novel	O
3D	Task
orientation	Task
estimation	Task
is	O
based	O
on	O
a	O
variant	O
of	O
the	O
Denoising	Method
Autoencoder	Method
that	O
is	O
trained	O
on	O
simulated	O
views	O
of	O
a	O
3D	Method
model	Method
using	O
Domain	Method
Randomization	Method
.	O
This	O
so	O
-	O
called	O
Augmented	Method
Autoencoder	Method
has	O
several	O
advantages	O
over	O
existing	O
methods	O
:	O
It	O
does	O
not	O
require	O
real	O
,	O
pose	O
-	O
annotated	O
training	O
data	O
,	O
generalizes	O
to	O
various	O
test	O
sensors	O
and	O
inherently	O
handles	O
object	O
and	O
view	O
symmetries	O
.	O
Instead	O
of	O
learning	O
an	O
explicit	O
mapping	O
from	O
input	O
images	O
to	O
object	O
poses	O
,	O
it	O
provides	O
an	O
implicit	O
representation	O
of	O
object	O
orientations	O
defined	O
by	O
samples	O
in	O
a	O
latent	O
space	O
.	O
Experiments	O
on	O
the	O
T	Material
-	Material
LESS	Material
and	O
LineMOD	O
datasets	O
show	O
that	O
our	O
method	O
outperforms	O
similar	O
model	Method
-	Method
based	Method
approaches	Method
and	O
competes	O
with	O
state	O
-	O
of	O
-	O
the	O
art	O
approaches	O
that	O
require	O
real	O
pose	O
-	O
annotated	O
images	O
.	O
VAEname	O
=	O
VAE	Method
,	O
description	O
=	O
VariationalAutoencoder	Method
,	O
first=	O
VAE	Method
(	O
VAE	Method
)	O
AEname	O
=	O
AE	Method
,	O
description	O
=	O
Autoencoder	Method
,	O
first=	O
AE	Method
(	O
AE	Method
),	O
plural	O
=	O
AEs	O
,	O
descriptionplural	O
=	O
Autoencoders	Method
,	O
firstplural=	O
AE	Method
(	O
AE	Method
)	O
AAEname	Method
=	O
AAE	Method
,	O
description	O
=	O
AugmentedAutoencoder	Method
,	O
first=	O
AAE	Method
(	O
AAE	Method
),	O
plural	O
=	O
AAEs	Method
,	O
descriptionplural	O
=	O
AugmentedAutoencoders	O
,	O
firstplural=	O
AAE	Method
(	O
AAE	Method
)	O
CVAEname	O
=	O
CVAE	Method
,	O
description	O
=	O
ConditionalVariationalAutoencoder	Method
,	O
first=	O
CVAE	O
(	O
CVAE	O
)	O
SSDname	Method
=	O
SSD	Method
,	O
description	O
=	O
SingleShotMultiboxDetector	Method
,	O
first=	Method
SSD	Method
(	Method
SSD	Method
)	Method
CNNname	Method
=	O
CNN	Method
,	O
description	Method
=	O
ConvolutionalNeuralNetwork	Method
,	O
first=	Method
CNN	Method
(	Method
CNN	Method
),	Method
plural	Method
=	O
CNNs	Method
,	O
descriptionplural	Method
=	O
ConvolutionalNeuralNetworks	Method
,	O
firstplural=	Method
CNN	Method
(	Method
CNN	Method
)	O
SIFTname	Method
=	O
SIFT	Method
,	O
description	O
=	O
ScaledInvariantFeatureTransform	Method
,	O
first	O
=	O
ScaledInvariantFeatureTransform	Method
(	Method
SIFT	Method
)	O
SURFname	O
=	O
SURF	O
,	O
description	O
=	O
SpeededUpRobustFeatures	O
,	O
first	O
=	O
SpeededUpRobustFeatures	Method
(	Method
SURF	Method
)	Method
PnPname	Method
=	O
PnP	Method
,	O
description	O
=	O
Perspective	O
-	O
n	O
-	O
Point	O
,	O
first	O
=	O
Perspective	O
-	O
n	O
-	O
Point	O
(	O
PnP	O
)	O
RFname	O
=	O
RF	O
,	O
description	O
=	O
RandomForrest	O
,	O
first	O
=	O
RandomForest	Method
(	Method
RF	Method
),	Method
plural	Method
=	Method
RFs	Method
,	O
descriptionplural	Method
=	O
RandomForests	Method
,	O
firstplural=	O
RF	O
(	O
RF	O
)	O
RANSACname	Method
=	O
RANSAC	Method
,	O
description	O
=	O
RandomSampleConsensus	Method
,	O
first	O
=	O
RANSAC	Method
,	O
PCAname	Method
=	O
PCA	Method
,	O
description	O
=	O
PrincipalComponentAnalysis	Method
,	O
first	O
=	O
PrincipalComponentAnalysis	Method
(	Method
PCA	Method
)	O
LIDARname	Method
=	O
LIDAR	Method
,	O
description	O
=	O
LightDetectionAndRanging	O
,	O
first=	O
LIDAR	O
(	O
LIDAR	Method
)	O
kNNname	Method
=	O
kNN	O
,	O
description	O
=	O
k	O
-	O
Nearest	Method
-	Method
Neighbor	Method
,	O
first=	Method
kNN	Method
(	O
kNN	Method
)	O
MLPname	Method
=	O
MLP	Method
,	O
description	O
=	O
MultilayerPerceptron	Method
,	O
first=	Method
MLP	Method
(	Method
MLP	Method
),	Method
plural	Method
=	Method
MLPs	Method
,	O
descriptionplural	Method
=	O
MultilayerPerceptrons	Method
,	O
firstplural=	Method
MLP	Method
(	Method
MLP	Method
)	O
EMname	Method
=	O
EM	O
,	O
description	O
=	O
ExpectationMaximization	Method
,	O
first=	O
EM	Method
(	Method
EM	Method
)	O
6DOFname=6DOF	O
,	O
description	O
=	O
sixdegreesoffreedom	O
,	O
first=	O
6DOF	O
(	O
6DOF	O
)	O
ICPname	O
=	O
ICP	Method
,	O
description	O
=	O
IterativeClosestPoint	Method
,	O
first=	O
ICP	Method
(	O
ICP	Method
)	O
KLname	O
=	O
KL	O
,	O
description	O
=	O
Kullback	Method
-	Method
Leibler	Method
,	O
first=	Method
KL	Method
(	O
KL	O
)	O
VSDname=	O
,	O
description	O
=	O
VisibleSurfaceDiscrepancy	O
,	O
first=	O
VSD	O
(	O
VSD	O
)	O
mAPname	O
=	O
mAP	Method
,	O
description	O
=	O
meanAveragePrecision	O
,	O
first=	O
mAP	Method
(	O
mAP	Method
)	O
DAname	O
=	O
DA	O
,	O
description	O
=	O
DomainAdaptation	O
,	O
first=	O
DA	O
(	O
DA	O
)	O
DRname	O
=	O
DR	Method
,	O
description	O
=	O
DomainRandomization	Method
,	O
first=	O
DR	Method
(	O
DR	Method
)	O
GANname	Method
=	O
GAN	Method
,	O
description	O
=	O
GenerativeAdversarialNetwork	O
,	O
first=	O
GAN	Method
(	O
GAN	Method
),	O
plural	O
=	O
GANs	O
,	O
descriptionplural	Method
=	O
GenerativeAdversarialNetworks	Method
,	O
firstplural=	O
GAN	Method
(	O
GAN	Method
)	O
PPFname	Method
=	O
PPF	Method
,	O
description	O
=	O
PointPairFeatures	O
,	O
first=	O
PPF	Method
(	O
PPF	Method
)	O
section	O
:	O
Introduction	O
One	O
of	O
the	O
most	O
important	O
components	O
of	O
modern	O
computer	Method
vision	Method
systems	Method
for	O
applications	O
such	O
as	O
mobile	Task
robotic	Task
manipulation	Task
and	O
augmented	Task
reality	Task
is	O
a	O
reliable	O
and	O
fast	O
6D	Task
object	Task
detection	Task
module	Task
.	O
Although	O
,	O
there	O
are	O
very	O
encouraging	O
recent	O
results	O
,	O
a	O
flexible	O
,	O
general	O
,	O
robust	O
and	O
fast	O
solution	O
is	O
not	O
available	O
,	O
yet	O
.	O
The	O
reasons	O
for	O
this	O
are	O
manifold	O
.	O
First	O
and	O
foremost	O
,	O
current	O
solutions	O
are	O
not	O
robust	O
enough	O
against	O
typical	O
challenges	O
such	O
as	O
object	O
occlusions	O
,	O
different	O
kinds	O
of	O
background	O
clutter	O
,	O
and	O
dynamic	O
changes	O
of	O
the	O
environment	O
.	O
Second	O
,	O
existing	O
methods	O
often	O
require	O
certain	O
object	O
properties	O
such	O
as	O
enough	O
textural	O
surface	O
structure	O
or	O
an	O
asymmetric	O
shape	O
to	O
avoid	O
confusions	O
.	O
And	O
finally	O
,	O
current	O
systems	O
are	O
not	O
efficient	O
in	O
terms	O
of	O
run	Metric
-	Metric
time	Metric
and	O
in	O
the	O
amount	O
of	O
annotated	O
training	O
data	O
they	O
require	O
.	O
Therefore	O
,	O
we	O
propose	O
a	O
novel	O
approach	O
that	O
directly	O
addresses	O
these	O
issues	O
.	O
Concretely	O
,	O
our	O
method	O
operates	O
on	O
single	O
RGB	O
images	O
,	O
which	O
significantly	O
increases	O
the	O
usability	O
as	O
no	O
depth	O
information	O
is	O
required	O
.	O
We	O
note	O
though	O
that	O
depth	O
maps	O
may	O
be	O
incorporated	O
optionally	O
to	O
refine	O
the	O
estimation	Task
.	O
As	O
a	O
first	O
step	O
,	O
we	O
apply	O
a	O
SSD	Method
that	O
provides	O
object	O
bounding	O
boxes	O
and	O
identifiers	O
.	O
On	O
the	O
resulting	O
scene	O
crops	O
,	O
we	O
employ	O
our	O
novel	O
3D	Task
orientation	Task
estimation	Task
algorithm	O
,	O
which	O
is	O
based	O
on	O
a	O
previously	O
trained	O
deep	Method
network	Method
architecture	Method
.	O
While	O
deep	Method
networks	Method
are	O
also	O
used	O
in	O
existing	O
approaches	O
,	O
our	O
approach	O
differs	O
in	O
that	O
we	O
do	O
not	O
explicitly	O
learn	O
from	O
3D	O
pose	O
annotations	O
during	O
training	O
.	O
Instead	O
,	O
we	O
implicitly	O
learn	O
representations	O
from	O
rendered	O
3D	O
model	O
views	O
.	O
This	O
is	O
accomplished	O
by	O
training	O
a	O
generalized	O
version	O
of	O
the	O
Denoising	Method
Autoencoder	Method
,	O
that	O
we	O
call	O
’	O
,	O
using	O
a	O
novel	O
Domain	Method
Randomization	Method
strategy	Method
.	O
Our	O
approach	O
has	O
several	O
advantages	O
:	O
First	O
,	O
since	O
the	O
training	Task
is	O
independent	O
from	O
concrete	O
representations	O
of	O
object	O
orientations	O
within	O
(	O
e.g.	O
quaternions	O
)	O
,	O
we	O
can	O
handle	O
ambiguous	O
poses	O
caused	O
by	O
symmetric	O
views	O
because	O
we	O
avoid	O
one	O
-	O
to	O
-	O
many	O
mappings	O
from	O
images	O
to	O
orientations	O
.	O
Second	O
,	O
we	O
learn	O
representations	O
that	O
specifically	O
encode	O
3D	O
orientations	O
while	O
achieving	O
robustness	Metric
against	O
occlusion	O
,	O
cluttered	O
backgrounds	O
and	O
generalizing	O
to	O
different	O
environments	O
and	O
test	O
sensors	O
.	O
Finally	O
,	O
the	O
AAE	Method
does	O
not	O
require	O
any	O
real	O
pose	O
-	O
annotated	O
training	O
data	O
.	O
Instead	O
,	O
it	O
is	O
trained	O
to	O
encode	O
3D	O
model	O
views	O
in	O
a	O
self	Method
-	Method
supervised	Method
way	Method
,	O
overcoming	O
the	O
need	O
of	O
a	O
large	O
pose	O
-	O
annotated	O
dataset	O
.	O
A	O
schematic	O
overview	O
of	O
the	O
approach	O
is	O
shown	O
in	O
Fig	O
[	O
reference	O
]	O
.	O
width=	O
section	O
:	O
Related	O
Work	O
Depth	Method
-	Method
based	Method
methods	Method
(	O
e.g.	O
using	O
PPF	Method
)	O
have	O
shown	O
robust	O
pose	O
estimation	Task
performance	O
on	O
multiple	O
datasets	O
,	O
winning	O
the	O
SIXD	O
challenge	O
2017	O
.	O
However	O
,	O
they	O
usually	O
rely	O
on	O
the	O
computationally	O
expensive	O
evaluation	Task
of	Task
many	Task
pose	Task
hypotheses	Task
.	O
Furthermore	O
,	O
existing	O
depth	Method
sensors	Method
are	O
often	O
more	O
sensitive	O
to	O
sunlight	O
or	O
specular	O
object	O
surfaces	O
than	O
RGB	O
cameras	O
.	O
CNN	Method
have	O
revolutionized	O
2D	Task
object	Task
detection	Task
from	O
RGB	O
images	O
.	O
But	O
,	O
in	O
comparison	O
to	O
2D	Task
bounding	Task
box	Task
annotation	Task
,	O
the	O
effort	O
of	O
labeling	O
real	O
images	O
with	O
full	O
6D	Task
object	Task
poses	Task
is	O
magnitudes	O
higher	O
,	O
requires	O
expert	O
knowledge	O
and	O
a	O
complex	O
setup	O
.	O
Nevertheless	O
,	O
the	O
majority	O
of	O
learning	O
-	O
based	O
pose	O
estimation	Task
methods	O
use	O
real	O
labeled	O
images	O
and	O
are	O
thus	O
restricted	O
to	O
pose	O
-	O
annotated	O
datasets	O
.	O
In	O
consequence	O
,	O
some	O
works	O
have	O
proposed	O
to	O
train	O
on	O
synthetic	O
images	O
rendered	O
from	O
a	O
3D	Method
model	Method
,	O
yielding	O
a	O
great	O
data	O
source	O
with	O
pose	O
labels	O
free	O
of	O
charge	O
.	O
However	O
,	O
naive	Method
training	Method
on	O
synthetic	O
data	O
does	O
not	O
typically	O
generalize	O
to	O
real	O
test	O
images	O
.	O
Therefore	O
,	O
a	O
main	O
challenge	O
is	O
to	O
bridge	O
the	O
domain	O
gap	O
that	O
separates	O
simulated	O
views	O
from	O
real	O
camera	O
recordings	O
.	O
subsection	O
:	O
Simulation	Task
to	O
Reality	Task
Transfer	Task
There	O
exist	O
three	O
major	O
strategies	O
to	O
generalize	O
from	O
synthetic	O
to	O
real	O
data	O
:	O
subsubsection	O
:	O
Photo	Task
-	Task
Realistic	Task
Rendering	Task
of	O
object	O
views	O
and	O
backgrounds	O
has	O
shown	O
mixed	O
generalization	Metric
performance	O
for	O
tasks	O
like	O
object	Task
detection	Task
and	O
viewpoint	Task
estimation	Task
.	O
It	O
is	O
suitable	O
for	O
simple	O
environments	O
and	O
performs	O
well	O
if	O
jointly	O
trained	O
with	O
a	O
relatively	O
small	O
amount	O
of	O
real	O
annotated	O
images	O
.	O
However	O
,	O
photo	Method
-	Method
realistic	Method
modeling	Method
is	O
always	O
imperfect	O
and	O
requires	O
much	O
effort	O
.	O
subsubsection	O
:	O
DA	O
refers	O
to	O
leveraging	O
training	O
data	O
from	O
a	O
source	O
domain	O
to	O
a	O
target	O
domain	O
of	O
which	O
a	O
small	O
portion	O
of	O
labeled	O
data	O
(	O
supervised	Method
DA	Method
)	O
or	O
unlabeled	O
data	O
(	O
unsupervised	Method
DA	Method
)	O
is	O
available	O
.	O
GAN	Method
have	O
been	O
deployed	O
for	O
unsupervised	Task
DA	Task
by	O
generating	O
realistic	O
from	O
synthetic	O
images	O
to	O
train	O
classifiers	Method
,	O
3D	Method
pose	Method
estimators	Method
and	O
grasping	Method
algorithms	Method
.	O
While	O
constituting	O
a	O
promising	O
approach	O
,	O
GAN	Method
often	O
yield	O
fragile	O
training	O
results	O
.	O
Supervised	Method
DA	Method
can	O
lower	O
the	O
need	O
for	O
real	O
annotated	O
data	O
,	O
but	O
does	O
not	O
abstain	O
from	O
it	O
.	O
subsubsection	O
:	O
Domain	Method
Randomization	Method
(	O
DR	Method
)	O
builds	O
upon	O
the	O
hypothesis	O
that	O
by	O
training	O
a	O
model	O
on	O
rendered	O
views	O
in	O
a	O
variety	O
of	O
semi	O
-	O
realistic	O
settings	O
(	O
augmented	O
with	O
random	O
lighting	O
conditions	O
,	O
backgrounds	O
,	O
saturation	O
,	O
etc	O
.	O
)	O
,	O
it	O
will	O
also	O
generalize	O
to	O
real	O
images	O
.	O
Tobin	O
et	O
al	O
.	O
demonstrated	O
the	O
potential	O
of	O
the	O
DR	Method
paradigm	O
for	O
3D	Task
shape	Task
detection	Task
using	O
CNN	Method
.	O
Hinterstoisser	O
et	O
al	O
.	O
showed	O
that	O
by	O
training	O
only	O
the	O
head	Method
network	Method
of	O
FasterRCNN	Method
with	O
randomized	O
synthetic	O
views	O
of	O
a	O
textured	Method
3D	Method
model	Method
,	O
it	O
also	O
generalizes	O
well	O
to	O
real	O
images	O
.	O
It	O
must	O
be	O
noted	O
,	O
that	O
their	O
rendering	O
is	O
almost	O
photo	O
-	O
realistic	O
as	O
the	O
textured	Method
3D	Method
models	Method
have	O
very	O
high	O
quality	O
.	O
Recently	O
,	O
Kehl	O
et	O
al	O
.	O
pioneered	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
CNN	Method
,	O
called	O
’	O
SSD6D	Method
’	Method
,	O
for	O
6D	Task
object	Task
detection	Task
that	O
uses	O
a	O
moderate	O
DR	Method
strategy	O
to	O
utilize	O
synthetic	O
training	O
data	O
.	O
The	O
authors	O
render	O
views	O
of	O
textured	Method
3D	Method
object	Method
reconstructions	Method
at	O
random	O
poses	O
on	O
top	O
of	O
MS	O
COCO	O
background	O
images	O
while	O
varying	O
brightness	O
and	O
contrast	O
.	O
This	O
lets	O
the	O
network	O
generalize	O
to	O
real	O
images	O
and	O
enables	O
6D	Task
detection	Task
at	O
10Hz	O
.	O
Like	O
us	O
,	O
for	O
very	O
accurate	O
distance	O
estimation	Task
they	O
rely	O
on	O
ICP	Method
post	O
-	O
processing	O
using	O
depth	O
data	O
.	O
In	O
contrast	O
,	O
we	O
do	O
not	O
treat	O
3D	Task
orientation	Task
estimation	Task
as	O
a	O
classification	Task
task	Task
.	O
subsection	O
:	O
Learning	Task
representations	Task
of	Task
3D	Task
orientations	Task
We	O
describe	O
the	O
difficulties	O
of	O
training	O
with	O
fixed	O
SO	Method
(	Method
3	Method
)	Method
parameterizations	Method
which	O
will	O
motivate	O
the	O
learning	Task
of	Task
object	Task
-	Task
specific	Task
representations	Task
.	O
subsubsection	O
:	O
Regression	Method
.	O
Since	O
rotations	O
live	O
in	O
a	O
continuous	O
space	O
,	O
it	O
seems	O
natural	O
to	O
directly	O
regress	O
a	O
fixed	O
SO	O
(	O
3	O
)	O
parameterizations	O
like	O
quaternions	O
.	O
However	O
,	O
representational	O
constraints	O
and	O
pose	O
ambiguities	O
can	O
introduce	O
convergence	O
issues	O
.	O
In	O
practice	O
,	O
direct	Method
regression	Method
approaches	Method
for	O
full	O
3D	O
object	O
orientation	O
estimation	Task
have	O
not	O
been	O
very	O
successful	O
.	O
subsubsection	O
:	O
Classification	Task
of	O
3D	O
object	O
orientations	O
requires	O
a	O
discretization	Method
of	Method
SO	Method
(	Method
3	Method
)	O
.	O
Even	O
rather	O
coarse	O
intervals	O
of	O
lead	O
to	O
over	O
50.000	O
possible	O
classes	O
.	O
Since	O
each	O
class	O
appears	O
only	O
sparsely	O
in	O
the	O
training	O
data	O
,	O
this	O
hinders	O
convergence	O
.	O
In	O
SSD6D	Method
the	O
3D	O
orientation	O
is	O
learned	O
by	O
separately	O
classifying	O
a	O
discretized	O
viewpoint	O
and	O
in	O
-	O
plane	O
rotation	O
,	O
thus	O
reducing	O
the	O
complexity	Metric
to	O
.	O
However	O
,	O
for	O
non	O
-	O
canonical	O
views	O
,	O
e.g.	O
if	O
an	O
object	O
is	O
seen	O
from	O
above	O
,	O
a	O
change	O
of	O
viewpoint	O
can	O
be	O
nearly	O
equivalent	O
to	O
a	O
change	O
of	O
in	O
-	O
plane	O
rotation	O
which	O
yields	O
ambiguous	O
class	O
combinations	O
.	O
In	O
general	O
,	O
the	O
relation	O
between	O
different	O
orientations	O
is	O
ignored	O
when	O
performing	O
one	Task
-	Task
hot	Task
classification	Task
.	O
subsubsection	O
:	O
Symmetries	O
are	O
a	O
severe	O
issue	O
when	O
relying	O
on	O
fixed	O
representations	O
of	O
3D	O
orientations	O
since	O
they	O
cause	O
pose	O
ambiguities	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
If	O
not	O
manually	O
addressed	O
,	O
identical	O
training	O
images	O
can	O
have	O
different	O
orientation	O
labels	O
assigned	O
which	O
can	O
significantly	O
disturb	O
the	O
learning	Task
process	Task
.	O
In	O
order	O
to	O
cope	O
with	O
ambiguous	O
objects	O
,	O
most	O
approaches	O
in	O
literature	O
are	O
manually	O
adapted	O
.	O
The	O
strategies	O
reach	O
from	O
ignoring	O
one	O
axis	O
of	O
rotation	O
over	O
adapting	O
the	O
discretization	O
according	O
to	O
the	O
object	O
to	O
the	O
training	O
of	O
an	O
extra	O
CNN	Method
to	O
predict	O
symmetries	O
.	O
These	O
depict	O
tedious	O
,	O
manual	O
ways	O
to	O
filter	O
out	O
object	O
symmetries	O
(	O
[	O
reference	O
]	O
)	O
in	O
advance	O
,	O
but	O
treating	O
ambiguities	O
due	O
to	O
self	O
-	O
occlusions	O
(	O
[	O
reference	O
]	O
)	O
and	O
occlusions	O
(	O
[	O
reference	O
]	O
)	O
are	O
harder	O
to	O
address	O
.	O
Symmetries	Method
do	O
not	O
only	O
affect	O
regression	Method
and	Method
classification	Method
methods	Method
,	O
but	O
any	O
learning	Method
-	Method
based	Method
algorithm	Method
that	O
discriminates	O
object	O
views	O
solely	O
by	O
fixed	O
SO	Method
(	Method
3	Method
)	Method
representations	Method
.	O
[	O
Object	O
symmetries	O
]	O
width=0.23	O
,	O
justification	O
=	O
raggedright	O
[	O
Self	O
-	O
occlusion	O
induced	O
symmetries	O
]	O
width=0.205justification	O
=	O
raggedright	O
[	O
Occlusion	O
induced	O
symmetries	O
]	O
width=0.9	O
subsubsection	O
:	O
Descriptor	Method
Learning	Method
can	O
be	O
used	O
to	O
learn	O
a	O
representation	O
that	O
relates	O
object	O
views	O
in	O
a	O
low	O
-	O
dimensional	O
space	O
.	O
Wohlhart	O
et	O
al	O
.	O
introduced	O
a	O
CNN	Method
-	Method
based	Method
descriptor	Method
learning	Method
approach	Method
using	O
a	O
triplet	Method
loss	Method
that	O
minimizes	O
/	O
maximizes	O
the	O
Euclidean	O
distance	O
between	O
similar	O
/	O
dissimilar	O
object	O
orientations	O
.	O
Although	O
mixing	O
in	O
synthetic	O
data	O
,	O
the	O
training	O
also	O
relies	O
on	O
pose	O
-	O
annotated	O
sensor	O
data	O
.	O
Furthermore	O
,	O
the	O
approach	O
is	O
not	O
immune	O
against	O
symmetries	O
because	O
the	O
loss	O
can	O
be	O
dominated	O
by	O
ambiguous	O
object	O
views	O
that	O
appear	O
the	O
same	O
but	O
have	O
opposite	O
orientations	O
.	O
Baltnas	O
et	O
al	O
.	O
extended	O
this	O
work	O
by	O
enforcing	O
proportionality	O
between	O
descriptor	O
and	O
pose	O
distances	O
.	O
They	O
acknowledge	O
the	O
problem	O
of	O
object	O
symmetries	O
by	O
weighting	O
the	O
pose	O
distance	O
loss	O
with	O
the	O
depth	O
difference	O
of	O
the	O
object	O
at	O
the	O
considered	O
poses	O
.	O
This	O
heuristic	O
increases	O
the	O
accuracy	Metric
on	O
symmetric	O
objects	O
with	O
respect	O
to	O
.	O
Our	O
work	O
is	O
also	O
based	O
on	O
learning	Task
descriptors	Task
,	O
but	O
we	O
train	O
self	Method
-	Method
supervised	Method
Augmented	Method
Autoencoders	Method
(	O
AAEs	Method
)	O
such	O
that	O
the	O
learning	Method
process	Method
itself	O
is	O
independent	O
of	O
any	O
fixed	O
SO	Method
(	Method
3	Method
)	Method
representation	Method
.	O
This	O
means	O
that	O
descriptors	O
are	O
learned	O
solely	O
based	O
on	O
the	O
appearance	O
of	O
object	O
views	O
and	O
thus	O
symmetrical	O
ambiguities	O
are	O
inherently	O
regarded	O
.	O
Assigning	O
3D	O
orientations	O
to	O
the	O
descriptors	O
only	O
happens	O
after	O
the	O
training	O
.	O
Furthermore	O
,	O
unlike	O
we	O
can	O
abstain	O
from	O
the	O
use	O
of	O
real	O
labeled	O
data	O
for	O
training	O
.	O
Kehl	O
et	O
al	O
.	O
train	O
an	O
Autoencoder	Method
architecture	O
on	O
random	O
RGB	O
-	O
D	O
scene	O
patches	O
from	O
the	O
LineMOD	O
dataset	O
.	O
At	O
test	O
time	O
,	O
descriptors	O
from	O
scene	O
and	O
object	O
patches	O
are	O
compared	O
to	O
find	O
the	O
6D	Task
pose	Task
.	O
Since	O
the	O
approach	O
requires	O
the	O
evaluation	O
of	O
a	O
lot	O
of	O
patches	O
,	O
it	O
takes	O
about	O
670ms	O
per	O
prediction	O
.	O
Furthermore	O
,	O
using	O
local	O
patches	O
means	O
to	O
ignore	O
holistic	O
relations	O
between	O
object	O
features	O
which	O
is	O
crucial	O
if	O
few	O
texture	O
exists	O
.	O
Instead	O
we	O
train	O
on	O
holistic	O
object	O
views	O
and	O
explicitly	O
learn	O
domain	O
invariance	O
.	O
section	O
:	O
Method	O
In	O
the	O
following	O
,	O
we	O
mainly	O
focus	O
on	O
the	O
novel	O
3D	Task
orientation	Task
estimation	Task
technique	O
based	O
on	O
the	O
Augmented	Method
Autoencoder	Method
(	O
AAE	Method
)	O
.	O
subsection	O
:	O
Autoencoders	Method
The	O
original	O
AE	Method
,	O
introduced	O
by	O
Hinton	O
et	O
al	O
.	O
,	O
is	O
a	O
dimensionality	Method
reduction	Method
technique	Method
for	O
high	O
dimensional	O
data	O
such	O
as	O
images	O
,	O
audio	O
or	O
depth	O
.	O
It	O
consists	O
of	O
an	O
Encoder	Method
and	O
a	O
Decoder	Method
,	O
both	O
arbitrary	O
learnable	Method
function	Method
approximators	Method
which	O
are	O
usually	O
neural	Method
networks	Method
.	O
The	O
training	O
objective	O
is	O
to	O
reconstruct	O
the	O
input	O
after	O
passing	O
through	O
a	O
low	Method
-	Method
dimensional	Method
bottleneck	Method
,	O
referred	O
to	O
as	O
the	O
latent	Method
representation	Method
with	O
:	O
The	O
per	Metric
-	Metric
sample	Metric
loss	Metric
is	O
simply	O
a	O
sum	O
over	O
the	O
pixel	O
-	O
wise	O
L2	O
distance	O
The	O
resulting	O
latent	O
space	O
can	O
,	O
for	O
example	O
,	O
be	O
used	O
for	O
unsupervised	Task
clustering	Task
.	O
Denoising	Method
Autoencoders	Method
have	O
a	O
modified	O
training	Method
procedure	Method
.	O
Here	O
,	O
artificial	O
random	O
noise	O
is	O
applied	O
to	O
the	O
input	O
images	O
while	O
the	O
reconstruction	O
target	O
stays	O
clean	O
.	O
The	O
trained	O
model	O
can	O
be	O
used	O
to	O
reconstruct	O
denoised	O
test	O
images	O
.	O
But	O
how	O
is	O
the	O
latent	Method
representation	Method
affected	O
?	O
Hypothesis	O
1	O
:	O
The	O
Denoising	O
AE	Method
produces	O
latent	Method
representations	Method
which	O
are	O
invariant	O
to	O
noise	O
because	O
it	O
facilitates	O
the	O
reconstruction	Task
of	Task
de	Task
-	Task
noised	Task
images	Task
.	O
We	O
will	O
demonstrate	O
that	O
this	O
training	Method
strategy	Method
actually	O
enforces	O
invariance	O
not	O
only	O
against	O
noise	O
but	O
against	O
a	O
variety	O
of	O
different	O
input	O
augmentations	O
.	O
Finally	O
,	O
it	O
allows	O
us	O
to	O
bridge	O
the	O
domain	O
gap	O
between	O
simulated	O
and	O
real	O
data	O
.	O
subsection	O
:	O
Augmented	Method
Autoencoder	Method
justification	O
=	O
centering	O
,	O
font	O
=	O
scriptsize	O
,	O
aboveskip=0.15	O
cm	O
,	O
belowskip=0.25	O
cm	O
(	O
a	O
)	O
X	O
=	O
s1.0	O
,	O
=t⁢xy0.0	O
,	O
∈r	O
[	O
0	O
,	O
⁢2π	O
]	O
(	O
b	O
)	O
X	O
=	O
s0.6	O
,	O
=t⁢xy0.0	O
,	O
∈r	O
[	O
0	O
,	O
⁢2π	O
]	O
(	O
c	O
)	O
X	O
=	O
s1.0	O
,	O
∼t⁢xy⁢U	O
(-	O
1	O
,	O
1	O
),	O
∈r	O
[	O
0	O
,	O
⁢2π	O
]	O
(	O
d	O
)	O
X∼s⁢U	O
(	O
0.5	O
,	O
1	O
),	O
∼t⁢xy⁢U	O
(-	O
1	O
,	O
1	O
),	O
∈r	O
[	O
0	O
,	O
⁢2π	O
]	O
justification	O
=	O
centering	O
,	O
aboveskip=0.03	O
cm	O
,	O
belowskip=0.12	O
cm	O
z1	O
z2	O
(	O
1	O
)	O
Autoencoder	Method
⟶	O
(	O
a	O
)(	O
a	O
)	O
z1	O
z2	O
(	O
2	O
)	O
Autoencoder	Method
⟶	O
(	O
d	O
)(	O
d	O
)	O
rotation	O
angle	O
[	O
deg	O
]	O
z1	O
rotation	O
angle	O
[	O
deg	O
]	O
z2	O
(	O
3	O
)	O
Augmented	Method
Autoencoder	Method
⟶	O
(	O
d	O
)(	O
a	O
)	O
The	O
motivation	O
behind	O
the	O
AAE	Method
is	O
to	O
control	O
what	O
the	O
latent	Method
representation	Method
encodes	O
and	O
which	O
properties	O
are	O
ignored	O
.	O
We	O
apply	O
random	Method
augmentations	Method
to	O
the	O
input	O
images	O
against	O
which	O
the	O
encoding	O
shall	O
become	O
invariant	O
.	O
The	O
reconstruction	O
target	O
remains	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
but	O
eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
becomes	O
To	O
make	O
evident	O
that	O
Hypothesis	O
1	O
holds	O
for	O
geometric	O
transformations	O
,	O
we	O
learn	O
latent	Method
representations	Method
of	O
binary	O
images	O
depicting	O
a	O
2D	O
square	O
at	O
different	O
scales	O
,	O
in	O
-	O
plane	O
translations	O
and	O
rotations	O
.	O
Our	O
goal	O
is	O
to	O
encode	O
only	O
the	O
in	O
-	O
plane	O
rotations	O
in	O
a	O
two	O
dimensional	O
latent	O
space	O
independent	O
of	O
scale	O
or	O
translation	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
depicts	O
the	O
results	O
after	O
training	O
a	O
CNN	O
-	O
based	O
AE	Method
architecture	O
similar	O
to	O
the	O
model	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
It	O
can	O
be	O
observed	O
that	O
the	O
AE	Method
trained	O
on	O
reconstructing	O
squares	O
at	O
fixed	O
scale	O
and	O
translation	O
(	O
1	O
)	O
or	O
random	O
scale	O
and	O
translation	O
(	O
2	O
)	O
do	O
not	O
clearly	O
encode	O
rotation	O
alone	O
,	O
but	O
are	O
also	O
sensitive	O
to	O
other	O
latent	O
factors	O
.	O
Instead	O
,	O
the	O
encoding	O
of	O
the	O
AAE	Method
(	O
3	O
)	O
becomes	O
invariant	O
to	O
translation	O
and	O
scale	O
such	O
that	O
all	O
squares	O
with	O
coinciding	O
orientation	O
are	O
mapped	O
to	O
the	O
same	O
code	O
.	O
Furthermore	O
,	O
the	O
latent	Method
representation	Method
is	O
much	O
smoother	O
and	O
the	O
latent	O
dimensions	O
imitate	O
a	O
shifted	O
sine	O
and	O
cosine	O
function	O
with	O
frequency	O
respectively	O
.	O
The	O
reason	O
is	O
that	O
the	O
square	O
has	O
two	O
perpendicular	O
axes	O
of	O
symmetry	O
,	O
i.e.	O
after	O
rotating	O
the	O
square	O
appears	O
the	O
same	O
.	O
This	O
property	O
of	O
representing	O
the	O
orientation	O
based	O
on	O
the	O
appearance	O
of	O
an	O
object	O
rather	O
than	O
on	O
a	O
fixed	O
parametrization	O
is	O
valuable	O
to	O
avoid	O
ambiguities	O
due	O
to	O
symmetries	O
when	O
teaching	O
3D	O
object	O
orientations	O
.	O
subsection	O
:	O
Learning	Task
3D	Task
Orientation	Task
from	O
Synthetic	O
Object	O
Views	O
Our	O
toy	O
problem	O
showed	O
that	O
we	O
can	O
explicitly	O
learn	O
representations	Task
of	Task
object	Task
in	Task
-	Task
plane	Task
rotations	Task
using	O
a	O
geometric	Method
augmentation	Method
technique	Method
.	O
Applying	O
the	O
same	O
geometric	Method
input	Method
augmentations	Method
we	O
can	O
encode	O
the	O
whole	O
SO	O
(	O
3	O
)	O
space	O
of	O
views	O
from	O
a	O
3D	Method
object	Method
model	Method
(	O
CAD	Method
or	O
3D	Task
reconstruction	Task
)	O
while	O
being	O
robust	O
against	O
inaccurate	O
object	O
detections	O
.	O
However	O
,	O
the	O
encoder	Method
would	O
still	O
be	O
unable	O
to	O
relate	O
image	O
crops	O
from	O
real	O
RGB	O
sensors	O
because	O
(	O
1	O
)	O
the	O
3D	Method
model	Method
and	O
the	O
real	O
object	O
differ	O
,	O
(	O
2	O
)	O
simulated	O
and	O
real	O
lighting	O
conditions	O
differ	O
,	O
(	O
3	O
)	O
the	O
network	O
ca	O
n’t	O
distinguish	O
the	O
object	O
from	O
background	O
clutter	O
and	O
foreground	O
occlusions	O
.	O
Instead	O
of	O
trying	O
to	O
imitate	O
every	O
detail	O
of	O
specific	O
real	O
sensor	O
recordings	O
in	O
simulation	O
we	O
propose	O
a	O
Domain	O
Randomization	O
(	O
DR	Method
)	O
technique	O
within	O
the	O
AAE	Method
framework	Method
to	O
make	O
the	O
encodings	O
invariant	O
to	O
insignificant	O
environment	O
and	O
sensor	O
variations	O
.	O
The	O
goal	O
is	O
that	O
the	O
trained	O
encoder	Method
treats	O
the	O
differences	O
to	O
real	O
camera	O
images	O
as	O
just	O
another	O
irrelevant	O
variation	O
.	O
Therefore	O
,	O
while	O
keeping	O
reconstruction	O
targets	O
clean	O
,	O
we	O
randomly	O
apply	O
additional	O
augmentations	Method
to	O
the	O
input	O
training	O
views	O
:	O
(	O
1	O
)	O
rendering	Method
with	O
random	O
light	O
positions	O
and	O
randomized	O
diffuse	O
and	O
specular	O
reflection	O
(	O
simple	O
Phong	Method
model	Method
in	O
OpenGL	Method
)	O
,	O
(	O
2	O
)	O
inserting	O
random	O
background	O
images	O
from	O
the	O
Pascal	O
VOC	O
dataset	O
,	O
(	O
3	O
)	O
varying	O
image	O
contrast	O
,	O
brightness	O
,	O
Gaussian	O
blur	O
and	O
color	O
distortions	O
,	O
(	O
4	O
)	O
applying	O
occlusions	O
using	O
random	O
object	O
masks	O
or	O
black	O
squares	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
depicts	O
an	O
exemplary	O
training	Method
process	Method
for	O
synthetic	Task
views	Task
of	Task
object	Task
5	Task
from	O
T	Material
-	Material
LESS	Material
.	O
width=0.9	O
subsection	O
:	O
Network	Method
Architecture	Method
and	O
Training	O
Details	O
The	O
convolutional	O
Autoencoder	Method
architecture	O
that	O
is	O
used	O
in	O
our	O
experiments	O
is	O
depicted	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
We	O
use	O
a	O
bootstrapped	Method
pixel	Method
-	Method
wise	Method
L2	Method
loss	Method
which	O
is	O
only	O
computed	O
on	O
the	O
pixels	O
with	O
the	O
largest	O
errors	O
(	O
per	O
image	O
bootstrap	O
factor	O
b=4	O
)	O
.	O
Thereby	O
,	O
finer	O
details	O
are	O
reconstructed	O
and	O
the	O
training	Method
does	O
not	O
converge	O
to	O
local	O
minima	O
.	O
Using	O
OpenGL	Method
,	O
we	O
render	O
20000	O
views	O
of	O
each	O
object	O
uniformly	O
at	O
random	O
3D	O
orientations	O
and	O
constant	O
distance	O
along	O
the	O
camera	O
axis	O
(	O
700	O
mm	O
)	O
.	O
The	O
resulting	O
images	O
are	O
quadratically	O
cropped	O
and	O
resized	O
to	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
All	O
geometric	O
and	O
color	O
input	O
augmentations	O
besides	O
the	O
rendering	O
with	O
random	O
lighting	O
are	O
applied	O
online	O
during	O
training	O
at	O
uniform	O
random	O
strength	O
,	O
parameters	O
are	O
found	O
in	O
the	O
supplement	O
.	O
We	O
use	O
the	O
Adam	Method
optimizer	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
,	O
Xavier	Method
initialization	Method
,	O
a	O
batch	O
size	O
=	O
64	O
and	O
30000	O
iterations	O
which	O
takes	O
hours	O
on	O
a	O
single	O
Nvidia	O
Geforce	O
GTX	O
1080	O
.	O
width=	O
subsection	O
:	O
Codebook	Task
Creation	Task
and	O
Test	O
Procedure	O
After	O
training	O
,	O
the	O
AAE	Method
is	O
able	O
to	O
extract	O
a	O
3D	O
object	O
from	O
real	O
scene	O
crops	O
of	O
many	O
different	O
camera	O
sensors	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
clarity	O
and	O
orientation	O
of	O
the	O
decoder	Method
reconstruction	Method
is	O
an	O
indicator	O
of	O
the	O
encoding	Metric
quality	Metric
.	O
To	O
determine	O
3D	O
object	O
orientations	O
from	O
test	O
scene	O
crops	O
we	O
create	O
a	O
codebook	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
top	O
)	O
)	O
:	O
Render	O
clean	O
,	O
synthetic	O
object	O
views	O
at	O
equidistant	O
viewpoints	O
from	O
a	O
full	O
view	O
-	O
sphere	O
(	O
based	O
on	O
a	O
refined	O
icosahedron	O
)	O
Rotate	O
each	O
view	O
in	O
-	O
plane	O
at	O
fixed	O
intervals	O
to	O
cover	O
the	O
whole	O
SO	O
(	O
3	O
)	O
Create	O
a	O
codebook	Method
by	O
generating	O
latent	Method
codes	Method
for	O
all	O
resulting	O
images	O
and	O
assigning	O
their	O
corresponding	O
rotation	O
At	O
test	O
time	O
,	O
the	O
considered	O
object	O
(	O
s	O
)	O
are	O
first	O
detected	O
in	O
an	O
RGB	O
scene	O
.	O
The	O
area	O
is	O
quadratically	O
cropped	O
and	O
resized	O
to	O
match	O
the	O
encoder	Method
input	O
size	O
.	O
After	O
encoding	O
we	O
compute	O
the	O
cosine	O
similarity	O
between	O
the	O
test	O
code	O
and	O
all	O
codes	O
from	O
the	O
codebook	O
:	O
The	O
highest	O
similarities	O
are	O
determined	O
in	O
a	O
kNN	Method
search	Method
and	O
the	O
corresponding	O
rotation	O
matrices	O
from	O
the	O
codebook	O
are	O
returned	O
as	O
estimates	O
of	O
the	O
3D	O
object	O
orientation	O
.	O
We	O
use	O
cosine	O
similarity	O
because	O
(	O
1	O
)	O
it	O
can	O
be	O
very	O
efficiently	O
computed	O
on	O
a	O
single	O
GPU	O
even	O
for	O
large	O
codebooks	O
.	O
In	O
our	O
experiments	O
we	O
have	O
2562	O
equidistant	O
viewpoints	O
36	O
in	O
-	O
plane	O
rotation	O
=	O
92232	O
total	O
entries	O
.	O
(	O
2	O
)	O
We	O
observed	O
that	O
,	O
presumably	O
due	O
to	O
the	O
circular	O
nature	O
of	O
rotations	O
,	O
scaling	O
a	O
latent	Method
test	Method
code	Method
does	O
not	O
change	O
the	O
object	O
orientation	O
of	O
the	O
decoder	Method
reconstruction	Method
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
width=0.93	O
width=0.99	O
width=0.95	O
subsection	O
:	O
Extending	O
to	O
6D	Task
Object	Task
Detection	Task
subsubsection	O
:	O
Training	O
the	O
Object	Method
Detector	Method
.	O
We	O
finetune	O
SSD	Method
with	O
VGG16	Method
base	Method
using	O
object	O
recordings	O
on	O
black	O
background	O
from	O
different	O
viewpoints	O
which	O
are	O
provided	O
in	O
the	O
training	O
datasets	O
of	O
LineMOD	O
and	O
T	Material
-	Material
LESS	Material
.	O
We	O
also	O
train	O
RetinaNet	Method
with	O
ResNet50	Method
backbone	Method
which	O
is	O
slower	O
but	O
more	O
accurate	O
.	O
Multiple	O
objects	O
are	O
copied	O
in	O
a	O
scene	O
at	O
random	O
orientation	O
,	O
scale	O
and	O
translation	O
.	O
Bounding	O
box	O
annotations	O
are	O
adapted	O
accordingly	O
.	O
As	O
for	O
the	O
AAE	Method
,	O
the	O
black	O
background	O
is	O
replaced	O
with	O
Pascal	O
VOC	O
images	O
.	O
During	O
training	O
with	O
60000	O
scenes	O
,	O
we	O
apply	O
various	O
color	Method
and	Method
geometric	Method
augmentations	Method
.	O
subsubsection	O
:	O
Projective	Method
Distance	Method
Estimation	Method
.	O
We	O
estimate	O
the	O
full	O
3D	O
translation	O
from	O
camera	O
to	O
object	O
center	O
,	O
similar	O
to	O
.	O
Therefore	O
,	O
for	O
each	O
synthetic	O
object	O
view	O
in	O
the	O
codebook	O
,	O
we	O
save	O
the	O
diagonal	O
length	O
of	O
its	O
2D	O
bounding	O
box	O
.	O
At	O
test	O
time	O
,	O
we	O
compute	O
the	O
ratio	O
between	O
the	O
detected	O
bounding	O
box	O
diagonal	O
and	O
the	O
corresponding	O
codebook	O
diagonal	O
,	O
i.e.	O
at	O
similar	O
orientation	O
.	O
The	O
pinhole	Method
camera	Method
model	Method
yields	O
the	O
distance	Method
estimate	Method
with	O
synthetic	O
rendering	O
distance	O
and	O
focal	O
lengths	O
,	O
of	O
the	O
test	O
sensor	O
and	O
synthetic	O
views	O
.	O
It	O
follows	O
that	O
with	O
principal	O
points	O
and	O
bounding	O
box	O
centers	O
.	O
In	O
contrast	O
to	O
,	O
we	O
can	O
predict	O
the	O
3D	O
translation	O
for	O
different	O
test	O
intrinsics	O
.	O
subsubsection	O
:	O
ICP	Method
Refinement	O
.	O
Optionally	O
,	O
the	O
estimate	O
is	O
refined	O
on	O
depth	O
data	O
using	O
a	O
standard	O
ICP	Method
approach	O
taking	O
on	O
CPU	Method
.	O
Details	O
in	O
supplement	O
.	O
subsubsection	O
:	O
Inference	O
Time	O
.	O
SSD	Method
with	O
VGG16	O
base	O
and	O
31	O
classes	O
plus	O
the	O
AAE	Method
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
with	O
a	O
codebook	O
size	O
of	O
yield	O
the	O
average	Metric
inference	Metric
times	Metric
depicted	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
conclude	O
that	O
the	O
RGB	Method
-	Method
based	Method
pipeline	Method
is	O
real	O
-	O
time	O
capable	O
at	O
42Hz	O
on	O
a	O
Nvidia	O
GTX	O
1080	O
.	O
This	O
enables	O
augmented	Task
reality	Task
and	O
robotic	Task
applications	Task
and	O
leaves	O
room	O
for	O
tracking	Method
algorithms	Method
.	O
Multiple	O
encoders	Method
and	O
corresponding	O
codebooks	O
fit	O
into	O
the	O
GPU	O
memory	O
,	O
making	O
multi	Task
-	Task
object	Task
pose	Task
estimation	Task
feasible	O
.	O
width=0.8	O
width=	O
section	O
:	O
Evaluation	O
We	O
evaluate	O
the	O
AAE	Method
and	O
the	O
whole	O
6D	Method
detection	Method
pipeline	Method
on	O
the	O
T	Material
-	Material
LESS	Material
and	O
LineMOD	O
datasets	O
.	O
Example	O
sequences	O
are	O
found	O
in	O
the	O
supplement	O
.	O
subsection	O
:	O
Test	O
Conditions	O
Few	O
RGB	O
-	O
based	O
pose	O
estimation	Task
approaches	O
(	O
e.g.	O
)	O
only	O
rely	O
on	O
3D	O
model	O
information	O
.	O
Most	O
methods	O
make	O
use	O
of	O
real	O
pose	O
annotated	O
data	O
and	O
often	O
even	O
train	O
and	O
test	O
on	O
the	O
same	O
scenes	O
(	O
e.g.	O
at	O
slightly	O
different	O
viewpoints	O
)	O
.	O
It	O
is	O
common	O
practice	O
to	O
ignore	O
in	O
-	O
plane	O
rotations	O
or	O
only	O
consider	O
object	O
poses	O
that	O
appear	O
in	O
the	O
dataset	O
which	O
also	O
limits	O
applicability	O
.	O
Symmetric	O
object	O
views	O
are	O
often	O
individually	O
treated	O
or	O
ignored	O
.	O
The	O
SIXD	Task
challenge	Task
is	O
an	O
attempt	O
to	O
make	O
fair	O
comparisons	O
between	O
6D	Method
localization	Method
algorithms	Method
by	O
prohibiting	O
the	O
use	O
of	O
test	O
scene	O
pixels	O
.	O
We	O
follow	O
these	O
strict	O
evaluation	O
guidelines	O
,	O
but	O
treat	O
the	O
harder	O
problem	O
of	O
6D	Task
detection	Task
where	O
it	O
is	O
unknown	O
which	O
of	O
the	O
considered	O
objects	O
are	O
present	O
in	O
the	O
scene	O
.	O
This	O
is	O
especially	O
difficult	O
in	O
the	O
T	Material
-	Material
LESS	Material
dataset	Material
since	O
objects	O
are	O
very	O
similar	O
.	O
justification	O
=	O
centering	O
,	O
width=	O
max	O
width=	O
Train	O
RGBTest	Method
RGBdyn	Method
.	O
lightaddcontrastmultiplyinvertAUC	O
ReconstructionPrimesense	O
✳	O
0.472	O
(	O
±	O
0.013	O
)	O
✳	O
✳	O
0.611	O
(	O
±	O
0.030	O
)	O
✳	O
✳	O
✳	O
0.825	O
(	O
±	O
0.015	O
)	O
✳	O
✳	O
✳	O
✳	O
0.876	O
(	O
±	O
0.019	O
)	O
✳	O
✳	O
✳	O
✳	O
✳	O
0.877	O
(	O
±	O
0.005	O
)	O
✳	O
✳	O
✳	O
0.861	O
(	O
±	O
0.014	O
)	O
PrimesensePrimesense	O
✳	O
✳	O
✳	O
0.890	O
(	O
±	O
0.003	O
)	O
3D	O
ReconstructionKinect	O
✳	O
0.461	O
(	O
±	O
0.022	O
)	O
✳	O
✳	O
0.580	O
(	O
±	O
0.014	O
)	O
✳	O
✳	O
✳	O
0.701	O
(	O
±	O
0.046	O
)	O
✳	O
✳	O
✳	O
✳	O
0.855	O
(	O
±	O
0.016	O
)	O
✳	O
✳	O
✳	O
✳	O
✳	O
0.897	O
(	O
±	O
0.008	O
)	O
✳	O
✳	O
✳	O
0.903	O
(	O
±	O
0.016	O
)	O
KinectKinect	O
✳	O
✳	O
✳	O
0.917	O
(	O
±	O
0.007	O
)	O
[	O
Effect	O
of	O
latent	O
space	O
size	O
,	O
standard	O
deviation	O
in	O
red	O
]	O
[	O
Training	O
on	O
CAD	Method
model	Method
(	O
bottom	O
)	O
vs.	O
textured	Task
3D	Task
reconstruction	Task
(	O
top	O
)	O
]	O
subsection	O
:	O
Metrics	Metric
The	O
VSD	Method
is	O
an	O
ambiguity	O
-	O
invariant	O
pose	O
error	O
function	O
that	O
is	O
determined	O
by	O
the	O
distance	O
between	O
the	O
estimated	O
and	O
ground	O
truth	O
visible	O
object	O
depth	O
surfaces	O
.	O
As	O
in	O
the	O
SIXD	Task
challenge	Task
,	O
we	O
report	O
the	O
recall	O
of	O
correct	O
6D	Task
object	Task
poses	Task
at	O
with	O
tolerance	O
and	O
object	O
visibility	O
.	O
Although	O
the	O
Average	O
Distance	O
of	O
Model	O
Points	O
(	O
ADD	O
)	O
)	O
metric	O
ca	O
n’t	O
handle	O
pose	O
ambiguities	O
,	O
we	O
also	O
present	O
it	O
for	O
the	O
LineMOD	O
dataset	O
following	O
the	O
protocol	O
in	O
.	O
For	O
objects	O
with	O
symmetric	O
views	O
(	O
eggbox	O
,	O
glue	O
)	O
,	O
computes	O
the	O
average	O
distance	O
to	O
the	O
closest	O
model	O
point	O
.	O
In	O
our	O
ablation	Task
studies	Task
we	O
also	O
report	O
the	O
,	O
which	O
represents	O
the	O
area	O
under	O
the	O
’	O
vs.	O
recall	O
’	O
curve	O
:	O
justification	O
=	O
centering	O
max	O
width=	O
6D	O
Detection	O
-	O
SSD6D	Method
Detection	Method
-	O
Retina6D	O
Localizationw	O
/	O
GT	O
2D	O
BBsObjectOURSOURSOURSOURSKehl	O
[	O
]	O
Vidal	O
[	O
]	O
OURSOURSRGB	O
+	O
Depth	O
(	O
ICP	Method
)	O
RGB	O
+	O
Depth	O
(	O
ICP	Method
)	O
RGB	O
-	O
D	O
+	O
ICPDepth	Method
+	O
ICPRGB	O
+	O
Depth	O
(	O
ICP	Method
)	O
15.6515.798.8722.32	O
-	O
4312.3328.0525.4622.1413.2229.49	O
-	O
4711.2337.3037.0532.6512.4738.26	O
-	O
6913.1146.1544.6118.586.5623.07	O
-	O
6312.7135.30536.4569.3934.8076.10	O
-	O
6966.7090.29623.1561.3220.2467.64	O
-	O
6752.3088.28715.9768.4516.2173.88	O
-	O
7736.5881.75810.8643.1819.7467.02	O
-	O
7922.0582.65919.5967.1236.2178.24	O
-	O
9046.4984.381010.4758.6111.5577.65	O
-	O
6814.3183.12114.3532.526.3135.89	O
-	O
6915.0157.26127.8040.538.1549.30	O
-	O
8231.3473.75133.3029.314.9142.50	O
-	O
5613.6065.01142.8526.124.6130.53	O
-	O
4745.3276.05157.9052.3426.7183.73	O
-	O
5250.0090.561613.0661.6421.7367.42	O
-	O
8136.0970.571741.7077.4664.8486.17	O
-	O
8381.1190.491847.1781.0814.3084.34	O
-	O
8052.6287.471915.9545.4822.4650.54	O
-	O
5550.7582.50202.177.605.2714.75	O
-	O
4737.7553.842119.7738.9817.9340.31	O
-	O
6350.8972.102211.0125.4218.6335.23	O
-	O
7047.6061.74237.9830.2418.6342.52	O
-	O
8535.1854.65244.7449.484.2359.54	O
-	O
7011.2481.342521.9150.0018.7670.89	O
-	O
4837.1288.542610.0457.8512.6266.20	O
-	O
5528.3390.66277.4247.2221.1373.51	O
-	O
6021.8677.632821.7844.8023.0761.20	O
-	O
6942.5867.102915.3353.7126.6573.05	O
-	O
6557.0187.683034.6386.3429.5892.90	O
-	O
8470.4296.45Mean14.6746.5118.3557.1435.966.336.7972.76	O
subsection	O
:	O
Ablation	Task
Studies	Task
To	O
assess	O
the	O
AAE	Method
alone	O
,	O
in	O
this	O
subsection	O
we	O
only	O
predict	O
the	O
3D	O
orientation	O
of	O
Object	O
5	O
from	O
the	O
T	Material
-	Material
LESS	Material
dataset	Material
on	O
Primesense	O
and	O
Kinect	O
RGB	O
scene	O
crops	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
influence	O
of	O
different	O
input	O
augmentations	O
.	O
It	O
can	O
be	O
seen	O
that	O
the	O
effect	O
of	O
different	O
color	Method
augmentations	Method
is	O
cumulative	O
.	O
For	O
textureless	O
objects	O
,	O
even	O
the	O
inversion	O
of	O
color	O
channels	O
seems	O
to	O
be	O
beneficial	O
since	O
it	O
prevents	O
overfitting	O
to	O
synthetic	O
color	O
information	O
.	O
Furthermore	O
,	O
training	O
with	O
real	O
object	O
recordings	O
provided	O
in	O
T	Material
-	Material
LESS	Material
with	O
random	O
Pascal	O
VOC	O
background	O
and	O
augmentations	Method
yields	O
only	O
slightly	O
better	O
performance	O
than	O
the	O
training	O
with	O
synthetic	O
data	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
depicts	O
the	O
effect	O
of	O
different	O
latent	O
space	O
sizes	O
on	O
the	O
3D	O
pose	O
estimation	Task
accuracy	Metric
.	O
Performance	O
starts	O
to	O
saturate	O
at	O
.	O
In	O
Fig	O
.	O
[	O
reference	O
]	O
we	O
demonstrate	O
that	O
our	O
Domain	Method
Randomization	Method
strategy	Method
even	O
allows	O
the	O
generalization	Task
from	O
untextured	Method
CAD	Method
models	Method
.	O
subsection	O
:	O
6D	Task
Object	Task
Detection	Task
First	O
,	O
we	O
report	O
RGB	O
-	O
only	O
results	O
consisting	O
of	O
2D	Task
detection	Task
,	O
3D	Task
orientation	Task
estimation	Task
and	O
projective	O
distance	O
estimation	Task
.	O
Although	O
these	O
results	O
are	O
visually	O
appealing	O
,	O
the	O
distance	O
estimation	Task
is	O
refined	O
using	O
a	O
simple	O
cloud	O
-	O
based	O
ICP	Method
to	O
compete	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
depth	Method
-	Method
based	Method
methods	Method
.	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
6D	Task
detection	Task
evaluation	Task
on	O
all	O
scenes	O
of	O
the	O
T	Material
-	Material
LESS	Material
dataset	Material
,	O
which	O
contains	O
a	O
lot	O
of	O
pose	O
ambiguities	O
.	O
Our	O
refined	O
results	O
outperform	O
the	O
recent	O
local	Method
patch	Method
descriptor	Method
approach	Method
from	O
Kehl	O
et	O
al	O
.	O
even	O
though	O
they	O
only	O
do	O
6D	Task
localization	Task
.	O
The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
in	O
terms	O
of	O
average	O
accuracy	Metric
in	O
the	O
SIXD	O
challenge	O
)	O
from	O
Vidal	O
et	O
al	O
.	O
performs	O
a	O
time	O
consuming	O
search	O
through	O
pose	O
hypotheses	O
(	O
average	O
of	O
4.9	O
seconds	O
/	O
object	O
)	O
.	O
Our	O
approach	O
yields	O
comparable	O
accuracy	Metric
while	O
being	O
much	O
more	O
efficient	O
.	O
The	O
right	O
part	O
of	O
Table	O
[	O
reference	O
]	O
shows	O
results	O
with	O
ground	O
truth	O
bounding	O
boxes	O
yielding	O
an	O
upper	O
limit	O
on	O
the	O
pose	O
estimation	Task
performance	O
.	O
The	O
appendix	O
shows	O
some	O
failure	O
cases	O
,	O
mostly	O
stemming	O
from	O
missed	O
detections	O
or	O
strong	O
occlusions	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
compare	O
our	O
method	O
against	O
the	O
recently	O
introduced	O
SSD6D	Method
and	O
other	O
methods	O
on	O
the	O
LineMOD	O
dataset	O
.	O
SSD6D	Method
also	O
trains	O
on	O
synthetic	Task
views	Task
of	Task
3D	Task
models	Task
,	O
but	O
their	O
performance	O
seems	O
quite	O
dependent	O
on	O
a	O
sophisticated	O
occlusion	O
-	O
aware	O
,	O
projective	O
ICP	Method
refinement	O
step	O
.	O
Our	O
basic	O
ICP	Method
sometimes	O
converges	O
to	O
similarly	O
shaped	O
objects	O
in	O
the	O
vicinity	O
.	O
In	O
the	O
RGB	O
domain	O
our	O
method	O
outperforms	O
SSD6D.	O
justification	O
=	O
centering	O
,	O
width=0.8	O
max	O
width=	O
Test	O
dataRGB	O
+	O
Depth	O
(	O
ICP	Method
)	O
Train	O
dataRGB	O
w	O
/	O
o	O
Real	O
Pose	O
LabelsRGB	O
with	O
Real	O
Pose	O
Labels	O
+	O
DepthObjectSSD6D	O
[]	O
OURSBrachmann	O
[]	O
BB8	O
[	O
]	O
Tekin	O
[	O
]	O
OURSSSD6D	O
[]	O
Ape0.003.96	O
-	O
27.921.6220.5565Benchvise0.1820.92	O
-	O
62.081.8064.2580Cam0.4130.47	O
-	O
40.136.5763.2078Can1.3535.87	O
-	O
48.168.8076.0986Cat0.5117.90	O
-	O
45.241.8272.0170Driller2.5823.99	O
-	O
58.663.5141.5873Duck0.004.86	O
-	O
32.827.2332.3866Eggbox8.9081.01	O
-	O
40.069.5898.64100Glue0.0045.49	O
-	O
27.080.0296.39100Holepuncher0.3017.60	O
-	O
42.442.6349.8849Iron8.8632.03	O
-	O
67.074.9763.1178Lamp8.260.47	O
-	O
39.971.1191.6973Phone0.1833.79	O
-	O
35.247.7470.9679Mean2.4228.6532.343.655.9564.6779	O
section	O
:	O
Conclusion	O
We	O
have	O
proposed	O
a	O
new	O
self	Method
-	Method
supervised	Method
training	Method
strategy	Method
for	O
Autoencoder	Method
architectures	Method
that	O
enables	O
robust	O
3D	O
object	O
orientation	O
estimation	Task
on	O
various	O
RGB	O
sensors	O
while	O
training	O
only	O
on	O
synthetic	O
views	O
of	O
a	O
3D	Method
model	Method
.	O
By	O
demanding	O
the	O
Autoencoder	Method
to	O
revert	O
geometric	O
and	O
color	O
input	O
augmentations	O
,	O
we	O
learn	O
representations	O
that	O
(	O
1	O
)	O
specifically	O
encode	O
3D	O
object	O
orientations	O
,	O
(	O
2	O
)	O
are	O
invariant	O
to	O
a	O
significant	O
domain	O
gap	O
between	O
synthetic	O
and	O
real	O
RGB	O
images	O
,	O
(	O
3	O
)	O
inherently	O
regard	O
pose	O
ambiguities	O
from	O
symmetric	O
object	O
views	O
.	O
Around	O
this	O
approach	O
,	O
we	O
created	O
a	O
real	O
-	O
time	O
(	O
42	O
fps	O
)	O
,	O
RGB	Method
-	Method
based	Method
pipeline	Method
for	O
6D	Task
object	Task
detection	Task
which	O
is	O
especially	O
suitable	O
when	O
pose	O
-	O
annotated	O
RGB	O
sensor	O
data	O
is	O
not	O
available	O
.	O
section	O
:	O
Acknowledgement	O
We	O
would	O
like	O
to	O
thank	O
Dr.	O
Ingo	O
Kossyk	O
,	O
Dimitri	O
Henkel	O
and	O
Max	O
Denninger	O
for	O
helpful	O
discussions	O
.	O
We	O
also	O
thank	O
the	O
reviewers	O
for	O
their	O
useful	O
comments	O
.	O
bibliography	O
:	O
References	O
