There	O
is	O
intense	O
interest	O
in	O
applying	O
machine	Method
learning	Method
to	O
problems	O
of	O
causal	Task
inference	Task
in	O
fields	O
such	O
as	O
healthcare	Task
,	O
economics	Task
and	O
education	Task
.	O
In	O
particular	O
,	O
individual	O
-	O
level	O
causal	Task
inference	Task
has	O
important	O
applications	O
such	O
as	O
precision	Task
medicine	Task
.	O
We	O
give	O
a	O
new	O
theoretical	Method
analysis	Method
and	O
family	Method
of	Method
algorithms	Method
for	O
predicting	Task
individual	Task
treatment	Task
effect	Task
(	O
ITE	Task
)	O
from	O
observational	O
data	O
,	O
under	O
the	O
assumption	O
known	O
as	O
strong	O
ignorability	O
.	O
The	O
algorithms	O
learn	O
a	O
“	O
balanced	Method
”	Method
representation	Method
such	O
that	O
the	O
induced	O
treated	O
and	O
control	O
distributions	O
look	O
similar	O
.	O
We	O
give	O
a	O
novel	O
,	O
simple	O
and	O
intuitive	O
generalization	Metric
-	Metric
error	Metric
bound	Metric
showing	O
that	O
the	O
expected	O
ITE	Task
estimation	O
error	O
of	O
a	O
representation	O
is	O
bounded	O
by	O
a	O
sum	O
of	O
the	O
standard	O
generalization	Metric
-	Metric
error	Metric
of	O
that	O
representation	O
and	O
the	O
distance	O
between	O
the	O
treated	O
and	O
control	O
distributions	O
induced	O
by	O
the	O
representation	O
.	O
We	O
use	O
Integral	Method
Probability	Method
Metrics	Method
to	O
measure	O
distances	O
between	O
distributions	O
,	O
deriving	O
explicit	O
bounds	O
for	O
the	O
Wasserstein	Metric
and	O
Maximum	Metric
Mean	Metric
Discrepancy	Metric
(	O
MMD	Metric
)	O
distances	O
.	O
Experiments	O
on	O
real	O
and	O
simulated	O
data	O
show	O
the	O
new	O
algorithms	O
match	O
or	O
outperform	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
inline	O
,	O
nomargin	O
,	O
theme	O
=	O
color	O
ufxuanfx	O
blueUri	O
Estimatingindividualtreatmenteffect	O
:	O
generalizationboundsandalgorithms	O
section	O
:	O
Introduction	O
Making	Task
predictions	Task
about	Task
causal	Task
effects	Task
of	Task
actions	Task
is	O
a	O
central	O
problem	O
in	O
many	O
domains	O
.	O
For	O
example	O
,	O
a	O
doctor	O
deciding	O
which	O
medication	O
will	O
cause	O
better	O
outcomes	O
for	O
a	O
patient	O
;	O
a	O
government	O
deciding	O
who	O
would	O
benefit	O
most	O
from	O
subsidized	O
job	O
training	O
;	O
or	O
a	O
teacher	O
deciding	O
which	O
study	O
program	O
would	O
most	O
benefit	O
a	O
specific	O
student	O
.	O
In	O
this	O
paper	O
we	O
focus	O
on	O
the	O
problem	O
of	O
making	O
these	O
predictions	O
based	O
on	O
observational	O
data	O
.	O
Observational	O
data	O
is	O
data	O
which	O
contains	O
past	O
actions	O
,	O
their	O
outcomes	O
,	O
and	O
possibly	O
more	O
context	O
,	O
but	O
without	O
direct	O
access	O
to	O
the	O
mechanism	O
which	O
gave	O
rise	O
to	O
the	O
action	O
.	O
For	O
example	O
we	O
might	O
have	O
access	O
to	O
records	O
of	O
patients	O
(	O
context	O
)	O
,	O
their	O
medications	O
(	O
actions	O
)	O
,	O
and	O
outcomes	O
,	O
but	O
we	O
do	O
not	O
have	O
complete	O
knowledge	O
of	O
why	O
a	O
specific	O
action	O
was	O
applied	O
to	O
a	O
patient	O
.	O
The	O
hallmark	O
of	O
learning	Task
from	O
observational	O
data	O
is	O
that	O
the	O
actions	O
observed	O
in	O
the	O
data	O
depend	O
on	O
variables	O
which	O
might	O
also	O
affect	O
the	O
outcome	O
,	O
resulting	O
in	O
confounding	O
:	O
For	O
example	O
,	O
richer	O
patients	O
might	O
better	O
afford	O
certain	O
medications	O
,	O
and	O
job	Task
training	Task
might	O
only	O
be	O
given	O
to	O
those	O
motivated	O
enough	O
to	O
seek	O
it	O
.	O
The	O
challenge	O
is	O
how	O
to	O
untangle	O
these	O
confounding	O
factors	O
and	O
make	O
valid	O
predictions	O
.	O
Specifically	O
,	O
we	O
work	O
under	O
the	O
common	O
simplifying	O
assumption	O
of	O
“	O
no	O
-	O
hidden	O
confounding	O
”	O
,	O
assuming	O
that	O
all	O
the	O
factors	O
determining	O
which	O
actions	O
were	O
taken	O
are	O
observed	O
.	O
In	O
the	O
examples	O
above	O
,	O
it	O
would	O
mean	O
that	O
we	O
have	O
measured	O
a	O
patient	O
’s	O
wealth	O
or	O
an	O
employee	O
’s	O
motivation	O
.	O
As	O
a	O
learning	Task
problem	Task
,	O
estimating	Task
causal	Task
effects	Task
from	O
observational	O
data	O
is	O
different	O
from	O
classic	Method
learning	Method
in	O
that	O
in	O
our	O
training	O
data	O
we	O
never	O
see	O
the	O
individual	O
-	O
level	O
effect	O
.	O
For	O
each	O
unit	O
,	O
we	O
only	O
see	O
their	O
response	O
to	O
one	O
of	O
the	O
possible	O
actions	O
-	O
the	O
one	O
they	O
had	O
actually	O
received	O
.	O
This	O
is	O
close	O
to	O
what	O
is	O
known	O
in	O
the	O
machine	Task
learning	Task
literature	Task
as	O
“	O
learning	O
from	O
logged	O
bandit	O
feedback	O
”	O
,	O
with	O
the	O
distinction	O
that	O
we	O
do	O
not	O
have	O
access	O
to	O
the	O
model	O
generating	O
the	O
action	O
.	O
Our	O
work	O
differs	O
from	O
much	O
work	O
in	O
causal	Task
inference	Task
in	O
that	O
we	O
focus	O
on	O
the	O
individual	O
-	O
level	O
causal	O
effect	O
(	O
also	O
known	O
as	O
“	O
c	O
-	O
specific	O
treatment	O
effects	O
”	O
)	O
,	O
rather	O
that	O
the	O
average	O
or	O
population	O
level	O
.	O
Our	O
main	O
contribution	O
is	O
to	O
give	O
what	O
is	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
first	O
generalization	Metric
-	Metric
error	Metric
bound	Metric
for	O
estimating	O
individual	Task
-	Task
level	Task
causal	Task
effect	Task
,	O
where	O
each	O
individual	O
is	O
identified	O
by	O
its	O
features	O
.	O
The	O
bound	O
leads	O
naturally	O
to	O
a	O
new	O
family	O
of	O
representation	Method
-	Method
learning	Method
based	Method
algorithms	Method
,	O
which	O
we	O
show	O
to	O
match	O
or	O
outperform	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
several	O
causal	Task
effect	Task
inference	Task
tasks	Task
.	O
We	O
frame	O
our	O
results	O
using	O
the	O
Rubin	Method
-	Method
Neyman	Method
potential	Method
outcomes	Method
framework	Method
,	O
as	O
follows	O
.	O
We	O
assume	O
that	O
for	O
a	O
unit	O
with	O
features	O
,	O
and	O
an	O
action	O
(	O
also	O
known	O
as	O
treatment	O
or	O
intervention	O
)	O
,	O
there	O
are	O
two	O
potential	O
outcomes	O
:	O
and	O
.	O
In	O
our	O
data	O
,	O
for	O
each	O
unit	O
we	O
only	O
see	O
one	O
of	O
the	O
potential	O
outcomes	O
,	O
depending	O
on	O
the	O
treatment	O
assignment	O
:	O
if	O
we	O
observe	O
,	O
if	O
,	O
we	O
observe	O
;	O
this	O
is	O
known	O
as	O
the	O
Consistency	O
assumption	O
.	O
For	O
example	O
,	O
can	O
denote	O
the	O
set	O
of	O
lab	O
tests	O
and	O
demographic	O
factors	O
of	O
a	O
diabetic	O
patient	O
,	O
denote	O
the	O
standard	O
medication	O
for	O
controlling	Task
blood	Task
sugar	Task
,	O
denotes	O
a	O
new	O
medication	O
,	O
and	O
and	O
indicate	O
the	O
patient	O
’s	O
blood	O
sugar	O
level	O
if	O
they	O
were	O
to	O
be	O
given	O
medications	O
and	O
,	O
respectively	O
.	O
We	O
will	O
denote	O
,	O
.	O
We	O
are	O
interested	O
in	O
learning	O
the	O
function	O
.	O
is	O
the	O
expected	O
treatment	O
effect	O
of	O
relative	O
to	O
on	O
an	O
individual	O
unit	O
with	O
characteristics	O
,	O
or	O
the	O
Individual	O
Treatment	O
Effect	O
(	O
ITE	Task
)	O
.	O
For	O
example	O
,	O
for	O
a	O
patient	O
with	O
features	O
,	O
we	O
can	O
use	O
this	O
to	O
predict	O
which	O
of	O
two	O
treatments	O
will	O
have	O
a	O
better	O
outcome	O
.	O
The	O
fundamental	O
problem	O
of	O
causal	Task
inference	Task
is	O
that	O
for	O
any	O
in	O
our	O
data	O
we	O
only	O
observe	O
or	O
,	O
but	O
never	O
both	O
.	O
As	O
mentioned	O
above	O
,	O
we	O
make	O
an	O
important	O
“	O
no	O
-	O
hidden	O
confounders	O
”	O
assumption	O
,	O
in	O
order	O
to	O
make	O
the	O
conditional	O
causal	O
effect	O
identifiable	O
.	O
We	O
formalize	O
this	O
assumption	O
by	O
using	O
the	O
standard	O
strong	O
ignorability	O
condition	O
:	O
,	O
and	O
for	O
all	O
.	O
Strong	O
ignorability	O
is	O
a	O
sufficient	O
condition	O
for	O
the	O
ITE	Task
function	O
to	O
be	O
identifiable	O
:	O
see	O
proof	O
in	O
the	O
supplement	O
.	O
The	O
validity	O
of	O
strong	O
ignorability	O
can	O
not	O
be	O
assessed	O
from	O
data	O
,	O
and	O
must	O
be	O
determined	O
by	O
domain	O
knowledge	O
and	O
understanding	O
of	O
the	O
causal	O
relationships	O
between	O
the	O
variables	O
.	O
One	O
approach	O
to	O
the	O
problem	O
of	O
estimating	O
the	O
function	O
is	O
by	O
learning	O
the	O
two	O
functions	O
and	O
using	O
samples	O
from	O
.	O
This	O
is	O
similar	O
to	O
a	O
standard	O
machine	Task
learning	Task
problem	Task
of	Task
learning	Task
from	O
finite	O
samples	O
.	O
However	O
,	O
there	O
is	O
an	O
additional	O
source	O
of	O
variance	O
at	O
work	O
here	O
:	O
For	O
example	O
,	O
if	O
mostly	O
rich	O
patients	O
received	O
treatment	O
,	O
and	O
mostly	O
poor	O
patients	O
received	O
treatment	O
,	O
we	O
might	O
have	O
an	O
unreliable	O
estimation	O
of	O
for	O
poor	O
patients	O
.	O
In	O
this	O
paper	O
we	O
upper	O
bound	O
this	O
additional	O
source	O
of	O
variance	O
using	O
an	O
Integral	Metric
Probability	Metric
Metric	Metric
(	O
IPM	Metric
)	O
measure	O
of	O
distance	O
between	O
two	O
distributions	O
,	O
and	O
,	O
also	O
known	O
as	O
the	O
control	O
and	O
treated	O
distributions	O
.	O
In	O
practice	O
we	O
use	O
two	O
specific	O
IPMs	Metric
:	O
the	O
Maximum	Metric
Mean	Metric
Discrepancy	Metric
,	O
and	O
the	O
Wasserstein	Metric
distance	Metric
.	O
We	O
show	O
that	O
the	O
expected	Metric
error	Metric
in	O
learning	O
the	O
individual	Metric
treatment	Metric
effect	Metric
function	Metric
is	O
upper	O
bounded	O
by	O
the	O
error	Metric
of	Metric
learning	Metric
and	O
,	O
plus	O
the	O
IPM	Metric
term	Metric
.	O
In	O
the	O
randomized	Task
controlled	Task
trial	Task
setting	Task
,	O
where	O
,	O
the	O
IPM	Metric
term	Metric
is	O
,	O
and	O
our	O
bound	O
naturally	O
reduces	O
to	O
a	O
standard	O
learning	Task
problem	Task
of	Task
learning	Task
two	Task
functions	Task
.	O
The	O
bound	O
we	O
derive	O
points	O
the	O
way	O
to	O
a	O
family	O
of	O
algorithms	O
based	O
on	O
the	O
idea	O
of	O
representation	Method
learning	Method
:	O
Jointly	O
learn	O
hypotheses	O
for	O
both	O
treated	O
and	O
control	O
on	O
top	O
of	O
a	O
representation	O
which	O
minimizes	O
a	O
weighted	O
sum	O
of	O
the	O
factual	O
loss	Metric
(	O
the	O
standard	O
supervised	Metric
machine	Metric
learning	Metric
objective	Metric
)	O
,	O
and	O
the	O
IPM	Metric
distance	Metric
between	O
the	O
control	O
and	O
treated	O
distributions	O
induced	O
by	O
the	O
representation	O
.	O
This	O
can	O
be	O
viewed	O
as	O
learning	O
the	O
functions	O
and	O
under	O
a	O
constraint	O
that	O
encourages	O
better	O
generalization	O
across	O
the	O
treated	O
and	O
control	O
populations	O
.	O
In	O
the	O
Experiments	O
section	O
we	O
apply	O
algorithms	O
based	O
on	O
multi	Method
-	Method
layer	Method
neural	Method
nets	Method
as	O
representations	O
and	O
hypotheses	O
,	O
along	O
with	O
MMD	Metric
or	O
Wasserstein	Metric
distributional	O
distances	O
over	O
the	O
representation	Method
layer	Method
;	O
see	O
Figure	O
[	O
reference	O
]	O
for	O
the	O
basic	O
architecture	O
.	O
In	O
his	O
foundational	O
text	O
about	O
causality	O
,	O
writes	O
:	O
“	O
Whereas	O
in	O
traditional	O
learning	Task
tasks	Task
we	O
attempt	O
to	O
generalize	O
from	O
one	O
set	O
of	O
instances	O
to	O
another	O
,	O
the	O
causal	Task
modeling	Task
task	Task
is	O
to	O
generalize	O
from	O
behavior	O
under	O
one	O
set	O
of	O
conditions	O
to	O
behavior	O
under	O
another	O
set	O
.	O
Causal	Method
models	Method
should	O
therefore	O
be	O
chosen	O
by	O
a	O
criterion	O
that	O
challenges	O
their	O
stability	O
against	O
changing	O
conditions	O
…	O
”	O
[	O
emphasis	O
ours	O
]	O
.	O
We	O
believe	O
our	O
work	O
points	O
the	O
way	O
to	O
one	O
such	O
stability	Metric
criterion	Metric
,	O
for	O
causal	Task
inference	Task
in	O
the	O
strongly	Task
ignorable	Task
case	Task
.	O
section	O
:	O
Related	O
work	O
Much	O
recent	O
work	O
in	O
machine	Task
learning	Task
for	O
causal	Task
inference	Task
focuses	O
on	O
causal	Task
discovery	Task
,	O
with	O
the	O
goal	O
of	O
discovering	O
the	O
underlying	O
causal	O
graph	O
or	O
causal	O
direction	O
from	O
data	O
.	O
We	O
focus	O
on	O
the	O
case	O
when	O
the	O
causal	O
graph	O
is	O
simple	O
and	O
known	O
to	O
be	O
of	O
the	O
form	O
,	O
with	O
no	O
hidden	O
confounders	O
.	O
Under	O
the	O
causal	Method
model	Method
we	O
assume	O
,	O
the	O
most	O
common	O
goal	O
of	O
causal	Task
effect	Task
inference	Task
as	O
used	O
in	O
the	O
applied	Task
sciences	Task
is	O
to	O
obtain	O
the	O
average	Metric
treatment	Metric
effect	Metric
:	O
.	O
We	O
will	O
briefly	O
discuss	O
how	O
some	O
standard	O
statistical	Method
causal	Method
effect	Method
inference	Method
methods	Method
relate	O
to	O
our	O
proposed	O
method	O
.	O
Note	O
that	O
most	O
of	O
these	O
approaches	O
assume	O
some	O
form	O
of	O
ignorability	O
.	O
One	O
of	O
the	O
most	O
widely	O
used	O
approaches	O
to	O
estimating	O
ATE	Task
is	O
covariate	Task
adjustment	Task
,	O
also	O
known	O
as	O
back	Method
-	Method
door	Method
adjustment	Method
or	O
the	O
G	Method
-	Method
computation	Method
formula	Method
.	O
In	O
its	O
basic	O
version	O
,	O
covariate	Task
adjustment	Task
amounts	O
to	O
estimating	O
the	O
functions	O
,	O
.	O
Therefore	O
,	O
covariate	Method
adjustment	Method
methods	Method
are	O
the	O
most	O
natural	O
candidates	O
for	O
estimating	O
ITE	Task
as	O
well	O
as	O
ATE	Task
,	O
using	O
the	O
estimates	O
of	O
.	O
However	O
,	O
most	O
previous	O
work	O
on	O
this	O
subject	O
focused	O
on	O
asymptotic	Task
consistency	Task
,	O
and	O
so	O
far	O
there	O
has	O
not	O
been	O
much	O
work	O
on	O
the	O
generalization	Metric
-	Metric
error	Metric
of	O
such	O
a	O
procedure	O
.	O
One	O
way	O
to	O
view	O
our	O
results	O
is	O
that	O
we	O
point	O
out	O
a	O
previously	O
unaccounted	O
for	O
source	O
of	O
variance	O
when	O
using	O
covariate	Method
adjustment	Method
to	O
estimate	O
ITE	Task
.	O
We	O
suggest	O
a	O
new	O
type	O
of	O
regularization	Method
,	O
by	O
learning	Method
representations	Method
with	O
reduced	O
IPM	Metric
distance	O
between	O
treated	O
and	O
control	O
,	O
enabling	O
a	O
new	O
type	O
of	O
bias	Metric
-	Metric
variance	Metric
trade	Metric
-	Metric
off	Metric
.	O
Another	O
widely	O
used	O
family	O
of	O
statistical	Method
methods	Method
used	O
in	O
causal	Task
effect	Task
inference	Task
are	O
weighting	Method
methods	Method
.	O
Methods	O
such	O
as	O
propensity	Method
score	Method
weighting	Method
re	O
-	O
weight	O
the	O
units	O
in	O
the	O
observational	O
data	O
so	O
as	O
to	O
make	O
the	O
treated	O
and	O
control	O
populations	O
more	O
comparable	O
.	O
These	O
methods	O
do	O
not	O
yield	O
themselves	O
immediately	O
to	O
estimating	O
an	O
individual	O
level	O
effect	O
,	O
and	O
adapting	O
them	O
for	O
that	O
purpose	O
is	O
an	O
interesting	O
research	O
question	O
.	O
Doubly	Method
robust	Method
methods	Method
combine	O
re	O
-	O
weighting	O
the	O
samples	O
and	O
covariate	Method
adjustment	Method
in	O
clever	O
ways	O
to	O
reduce	O
model	O
bias	O
.	O
Again	O
,	O
we	O
believe	O
that	O
finding	O
how	O
to	O
adapt	O
the	O
concept	O
of	O
double	O
robustness	O
to	O
the	O
problem	O
of	O
effectively	O
estimating	Task
ITE	Task
is	O
an	O
interesting	O
open	O
question	O
.	O
Adapting	O
machine	Method
learning	Method
methods	Method
for	O
causal	Task
effect	Task
inference	Task
,	O
and	O
in	O
particular	O
for	O
individual	Task
level	Task
treatment	Task
effect	Task
,	O
has	O
gained	O
much	O
interest	O
recently	O
.	O
For	O
example	O
discuss	O
how	O
tree	Method
-	Method
based	Method
methods	Method
can	O
be	O
adapted	O
to	O
obtain	O
a	O
consistent	Method
estimator	Method
with	O
semi	Metric
-	Metric
parametric	Metric
asymptotic	Metric
convergence	Metric
rate	Metric
.	O
Recent	O
work	O
has	O
also	O
looked	O
into	O
how	O
machine	Method
learning	Method
method	Method
can	O
help	O
detect	O
heterogeneous	O
treatment	O
effects	O
when	O
some	O
data	O
from	O
randomized	O
experiments	O
is	O
available	O
.	O
Neural	Method
nets	Method
have	O
also	O
been	O
used	O
for	O
this	O
purpose	O
,	O
exemplified	O
in	O
early	O
work	O
by	O
,	O
and	O
more	O
recently	O
by	O
’s	O
work	O
on	O
deep	Task
instrumental	Task
variables	Task
.	O
Our	O
work	O
differs	O
from	O
all	O
the	O
above	O
by	O
focusing	O
on	O
the	O
generalization	Metric
-	Metric
error	Metric
aspects	Metric
of	O
estimating	Task
individual	Task
treatment	Task
effect	Task
,	O
as	O
opposed	O
to	O
asymptotic	Task
consistency	Task
,	O
and	O
by	O
focusing	O
solely	O
on	O
the	O
observational	O
study	O
case	O
,	O
with	O
no	O
randomized	O
components	O
or	O
instrumental	O
variables	O
.	O
Another	O
line	O
of	O
work	O
in	O
the	O
causal	Task
inference	Task
community	O
relates	O
to	O
bounding	O
the	O
estimate	O
of	O
the	O
average	Metric
treatment	Metric
effect	Metric
given	O
an	O
instrumental	O
variable	O
,	O
or	O
under	O
hidden	O
confounding	O
,	O
for	O
example	O
when	O
the	O
ignorability	O
assumption	O
does	O
not	O
hold	O
.	O
Our	O
work	O
differs	O
,	O
in	O
that	O
we	O
only	O
deal	O
with	O
the	O
ignorable	O
case	O
,	O
and	O
in	O
that	O
we	O
bound	O
a	O
very	O
different	O
quantity	O
:	O
the	O
generalization	Metric
-	Metric
error	Metric
of	O
estimating	O
individual	O
level	O
treatment	O
effect	O
.	O
Our	O
work	O
has	O
strong	O
connections	O
with	O
work	O
on	O
domain	Task
adaptation	Task
.	O
In	O
particular	O
,	O
estimating	O
ITE	Task
requires	O
prediction	O
of	O
outcomes	O
over	O
a	O
different	O
distribution	O
from	O
the	O
observed	O
one	O
.	O
Our	O
ITE	Task
error	O
upper	O
bound	O
has	O
similarities	O
with	O
generalization	Method
bounds	Method
in	O
domain	Task
adaptation	Task
given	O
by	O
.	O
These	O
bounds	O
employ	O
distribution	Metric
distance	Metric
metrics	Metric
such	O
as	O
the	O
A	Metric
-	Metric
distance	Metric
or	O
the	O
discrepancy	Method
metric	Method
,	O
which	O
are	O
related	O
to	O
the	O
IPM	Metric
distance	O
we	O
use	O
.	O
Our	O
algorithm	O
is	O
similar	O
to	O
a	O
recent	O
algorithm	O
for	O
domain	Task
adaptation	Task
by	O
,	O
and	O
in	O
principle	O
other	O
domain	Method
adaptation	Method
methods	Method
(	O
e.g.	O
)	O
could	O
be	O
adapted	O
for	O
use	O
in	O
ITE	Task
estimation	O
as	O
presented	O
here	O
.	O
Finally	O
,	O
our	O
paper	O
builds	O
on	O
work	O
by	O
,	O
where	O
the	O
authors	O
show	O
a	O
connection	O
between	O
covariate	O
shift	O
and	O
the	O
task	O
of	O
estimating	Task
the	Task
counterfactual	Task
outcome	Task
in	O
a	O
causal	Task
inference	Task
scenario	O
.	O
They	O
proposed	O
learning	O
a	O
representation	O
of	O
the	O
data	O
that	O
makes	O
the	O
treated	O
and	O
control	O
distributions	O
more	O
similar	O
,	O
and	O
fitting	O
a	O
linear	Method
ridge	Method
-	Method
regression	Method
model	Method
on	O
top	O
of	O
it	O
.	O
They	O
then	O
bounded	O
the	O
relative	Metric
error	Metric
of	O
fitting	O
a	O
ridge	Method
-	Method
regression	Method
using	O
the	O
distribution	O
with	O
reverse	Method
treatment	Method
assignment	Method
versus	O
fitting	O
a	O
ridge	Method
-	Method
regression	Method
using	O
the	O
factual	O
distribution	O
.	O
Unfortunately	O
,	O
the	O
relative	Metric
error	Metric
bound	Metric
is	O
not	O
at	O
all	O
informative	O
regarding	O
the	O
absolute	O
quality	O
of	O
the	O
representation	O
.	O
In	O
this	O
paper	O
we	O
focus	O
on	O
a	O
related	O
but	O
more	O
substantive	O
task	O
:	O
estimating	O
the	O
individual	Task
treatment	Task
effect	Task
,	O
building	O
on	O
top	O
of	O
the	O
counterfactual	O
error	O
term	O
.	O
We	O
further	O
provide	O
an	O
informative	O
bound	O
on	O
the	O
absolute	Metric
quality	Metric
of	O
the	O
representation	O
.	O
We	O
also	O
derive	O
a	O
much	O
more	O
flexible	O
family	O
of	O
algorithms	O
,	O
including	O
non	O
-	O
linear	O
hypotheses	O
and	O
much	O
more	O
powerful	O
distribution	O
metrics	O
in	O
the	O
form	O
of	O
IPMs	Metric
such	O
as	O
the	O
Wasserstein	Metric
and	O
MMD	Metric
distances	O
.	O
Finally	O
,	O
we	O
conduct	O
significantly	O
more	O
thorough	O
experiments	O
including	O
a	O
real	O
-	O
world	O
dataset	O
and	O
out	Metric
-	Metric
of	Metric
-	Metric
sample	Metric
performance	Metric
,	O
and	O
show	O
our	O
methods	O
outperform	O
previously	O
proposed	O
ones	O
.	O
section	O
:	O
Estimating	O
ITE	Task
:	O
Error	Metric
bounds	Metric
In	O
this	O
section	O
we	O
prove	O
a	O
bound	O
on	O
the	O
expected	Metric
error	Metric
in	O
estimating	O
the	O
individual	O
treatment	O
effect	O
for	O
a	O
given	O
representation	O
,	O
and	O
a	O
hypothesis	Method
defined	O
over	O
that	O
representation	O
.	O
The	O
bound	O
is	O
expressed	O
in	O
terms	O
of	O
(	O
1	O
)	O
the	O
expected	O
loss	Metric
of	O
the	O
model	O
when	O
learning	O
the	O
observed	O
outcomes	O
as	O
a	O
function	O
of	O
and	O
,	O
denoted	O
,	O
standing	O
for	O
“	O
Factual	O
”	O
;	O
(	O
2	O
)	O
an	O
Integral	Metric
Probability	Metric
Metric	Metric
(	O
IPM	Metric
)	O
distance	O
between	O
the	O
distribution	O
of	O
treated	O
and	O
control	O
units	O
.	O
The	O
term	O
is	O
the	O
classic	O
machine	O
learning	O
generalization	Metric
-	Metric
error	Metric
,	O
and	O
in	O
turn	O
can	O
be	O
upper	O
bounded	O
using	O
the	O
empirical	Metric
error	Metric
and	Metric
model	Metric
complexity	Metric
terms	Metric
,	O
applying	O
standard	O
machine	Method
learning	Method
theory	Method
.	O
subsection	O
:	O
Problem	O
setup	O
We	O
will	O
employ	O
the	O
following	O
assumptions	O
and	O
notations	O
.	O
The	O
most	O
important	O
notations	O
are	O
in	O
the	O
Notation	O
box	O
in	O
the	O
supplement	O
.	O
The	O
space	O
of	O
covariates	O
is	O
a	O
bounded	O
subset	O
.	O
The	O
outcome	O
space	O
is	O
.	O
Treatment	O
is	O
a	O
binary	O
variable	O
.	O
We	O
assume	O
there	O
exists	O
a	O
joint	O
distribution	O
,	O
such	O
that	O
and	O
for	O
all	O
(	O
strong	O
ignorability	O
)	O
.	O
The	O
treated	O
and	O
control	O
distributions	O
are	O
the	O
distribution	O
of	O
the	O
features	O
conditioned	O
on	O
treatment	O
:	O
,	O
and	O
,	O
respectively	O
.	O
Throughout	O
this	O
paper	O
we	O
will	O
discuss	O
representation	O
functions	O
of	O
the	O
form	O
,	O
where	O
is	O
the	O
representation	O
space	O
.	O
We	O
make	O
the	O
following	O
assumption	O
about	O
:	O
theorem	O
:	O
.	O
The	O
representation	Method
Φ	Method
is	O
a	O
twice	O
-	O
differentiable	O
,	O
one	O
-	O
to	O
-	O
one	O
function	O
.	O
Without	O
loss	Metric
of	O
generality	O
we	O
will	O
assume	O
that	O
R	O
is	O
the	O
image	O
of	O
X	O
under	O
Φ.	O
We	O
then	O
have	O
:	O
Ψ→RX	O
as	O
the	O
inverse	O
of	O
Φ	O
,	O
such	O
that	O
=	O
⁢Ψ	O
(	O
⁢Φ	O
(	O
x	O
))	O
x	O
for	O
all	O
∈xX.	O
The	O
representation	O
pushes	O
forward	O
the	O
treated	O
and	O
control	O
distributions	O
into	O
the	O
new	O
space	O
;	O
we	O
denote	O
the	O
induced	O
distribution	O
by	O
.	O
theorem	O
:	O
.	O
Define	O
p	O
=	O
t1Φ	O
(	O
r	O
)	O
:=pΦ	O
(	O
r|t=1	O
)	O
,	O
p	O
=	O
t0Φ	O
(	O
r	O
)	O
:=pΦ	O
(	O
r|t=0	O
)	O
,	O
to	O
be	O
the	O
treated	O
and	O
control	O
distributions	O
induced	O
over	O
R.	O
For	O
a	O
one	O
-	O
to	O
-	O
one	O
Φ	O
,	O
the	O
distributions	O
⁢p	O
=	O
t1Φ	O
(	O
r	O
)	O
and	O
⁢p	O
=	O
t0Φ	O
(	O
r	O
)	O
can	O
be	O
obtained	O
by	O
the	O
standard	O
change	O
of	O
variables	O
formula	O
,	O
using	O
the	O
determinant	O
of	O
the	O
Jacobian	O
of	O
⁢Ψ	O
(	O
r	O
)	O
.	O
Let	O
be	O
a	O
representation	Method
function	Method
,	O
and	O
be	O
an	O
hypothesis	Method
defined	O
over	O
the	O
representation	O
space	O
.	O
Let	O
be	O
a	O
loss	Metric
function	O
.	O
We	O
define	O
two	O
complimentary	O
loss	Metric
functions	O
:	O
one	O
is	O
the	O
standard	O
machine	O
learning	O
loss	Metric
,	O
which	O
we	O
will	O
call	O
the	O
factual	O
loss	Metric
and	O
denote	O
.	O
The	O
other	O
is	O
the	O
expected	O
loss	Metric
with	O
respect	O
to	O
the	O
distribution	O
where	O
the	O
treatment	O
assignment	O
is	O
flipped	O
,	O
which	O
we	O
call	O
the	O
counterfactual	O
loss	Metric
,	O
.	O
theorem	O
:	O
.	O
The	O
expected	O
loss	Metric
for	O
the	O
unit	O
and	O
treatment	O
pair	O
(	O
x	O
,	O
t	O
)	O
is	O
:	O
ℓh	O
,	O
Φ	O
(	O
x	O
,	O
t	O
)	O
=∫YL	O
(	O
Yt	O
,	O
h	O
(	O
Φ	O
(	O
x	O
),	O
t	O
))	O
p	O
(	O
Yt|x	O
)	O
dYt	O
.	O
The	O
expected	O
factual	O
and	O
counterfactual	O
losses	O
of	O
h	O
and	O
Φ	O
are	O
:	O
If	O
denotes	O
patients	O
’	O
features	O
,	O
a	O
treatment	O
,	O
and	O
a	O
potential	O
outcome	O
such	O
as	O
mortality	O
,	O
we	O
think	O
of	O
as	O
measuring	O
how	O
well	O
do	O
and	O
predict	O
mortality	O
for	O
the	O
patients	O
and	O
doctors	O
’	O
actions	O
sampled	O
from	O
the	O
same	O
distribution	O
as	O
our	O
data	O
sample	O
.	O
measures	O
how	O
well	O
our	O
prediction	O
with	O
and	O
would	O
do	O
in	O
a	O
“	O
topsy	O
-	O
turvy	O
”	O
world	O
where	O
the	O
patients	O
are	O
the	O
same	O
but	O
the	O
doctors	O
are	O
inclined	O
to	O
prescribe	O
exactly	O
the	O
opposite	O
treatment	O
than	O
the	O
one	O
the	O
real	O
-	O
world	O
doctors	O
would	O
prescribe	O
.	O
theorem	O
:	O
.	O
The	O
expected	O
factual	O
treated	O
and	O
control	O
losses	O
are	O
:	O
For	O
,	O
it	O
is	O
immediate	O
to	O
show	O
that	O
.	O
theorem	O
:	O
.	O
The	O
treatment	O
effect	O
(	O
ITE	Task
)	O
for	O
unit	O
x	O
is	O
:	O
Let	O
by	O
an	O
hypothesis	Method
.	O
For	O
example	O
,	O
we	O
could	O
have	O
that	O
.	O
theorem	O
:	O
.	O
The	O
treatment	O
effect	O
estimate	O
of	O
the	O
hypothesis	Method
f	O
for	O
unit	O
x	O
is	O
:	O
theorem	O
:	O
.	O
The	O
expected	Metric
Precision	Metric
in	O
Estimation	Task
of	Task
Heterogeneous	Task
Effect	Task
(	O
PEHE	O
,	O
(	O
)	O
)	O
loss	Metric
of	O
f	O
is	O
:	O
When	O
=	O
⁢f	O
(	O
x	O
,	O
t	O
)	O
⁢h	O
(	O
⁢Φ	O
(	O
x	O
),	O
t	O
)	O
,	O
we	O
will	O
also	O
use	O
the	O
notation	O
=	O
⁢ϵPEHE	O
(	O
h	O
,	O
Φ	O
)	O
⁢ϵPEHE	O
(	O
f	O
)	O
.	O
Our	O
proof	O
relies	O
on	O
the	O
notion	O
of	O
an	O
Integral	Metric
Probability	Metric
Metric	Metric
(	O
IPM	Metric
)	Metric
,	O
which	O
is	O
a	O
class	O
of	O
metrics	O
between	O
probability	O
distributions	O
.	O
For	O
two	O
probability	O
density	O
functions	O
,	O
defined	O
over	O
,	O
and	O
for	O
a	O
function	O
family	O
of	O
functions	O
,	O
we	O
have	O
that	O
Integral	O
probability	O
metrics	O
are	O
always	O
symmetric	O
and	O
obey	O
the	O
triangle	O
inequality	O
,	O
and	O
trivially	O
satisfy	O
.	O
For	O
rich	O
enough	O
function	O
families	O
,	O
we	O
also	O
have	O
that	O
and	O
then	O
is	O
a	O
true	O
metric	O
over	O
the	O
corresponding	O
set	O
of	O
probabilities	O
.	O
Examples	O
of	O
function	O
families	O
for	O
which	O
is	O
a	O
true	O
metric	O
are	O
the	O
family	O
of	O
bounded	O
continuous	O
functions	O
,	O
the	O
family	O
of	O
-	O
Lipschitz	O
functions	O
,	O
and	O
the	O
unit	O
-	O
ball	O
of	O
functions	O
in	O
a	O
universal	O
reproducing	O
Hilbert	O
kernel	O
space	O
.	O
theorem	O
:	O
.	O
Recall	O
that	O
mt	O
(	O
x	O
)	O
=E	O
[	O
Yt|x	O
]	O
.	O
The	O
expected	O
variance	O
of	O
Yt	O
with	O
respect	O
to	O
a	O
distribution	O
⁢p	O
(	O
x	O
,	O
t	O
)	O
:	O
We	O
define	O
:	O
subsection	O
:	O
Bounds	O
We	O
first	O
state	O
a	O
Lemma	O
bounding	O
the	O
counterfactual	O
loss	Metric
,	O
a	O
key	O
step	O
in	O
obtaining	O
the	O
bound	O
on	O
the	O
error	O
in	O
estimating	Task
individual	Task
treatment	Task
effect	Task
.	O
We	O
then	O
give	O
the	O
main	O
Thoerem	O
.	O
The	O
proofs	O
and	O
details	O
are	O
in	O
the	O
supplement	O
.	O
Let	O
be	O
the	O
marginal	O
probability	O
of	O
treatment	O
.	O
By	O
the	O
strong	O
ignorability	O
assumption	O
,	O
.	O
theorem	O
:	O
.	O
Let	O
:	O
Φ→XR	O
be	O
a	O
one	O
-	O
to	O
-	O
one	O
representation	Method
function	Method
,	O
with	O
inverse	O
Ψ.	O
Let	O
:	O
h→×R{0	O
,	O
1}Y	O
be	O
an	O
hypothesis	Method
.	O
Let	O
G	O
be	O
a	O
family	O
of	O
functions	O
:	O
g→RY	O
.	O
Assume	O
there	O
exists	O
a	O
constant	O
>	O
BΦ0	O
,	O
such	O
that	O
for	O
fixed	O
∈t{0	O
,	O
1	O
}	O
,	O
the	O
per	O
-	O
unit	O
expected	O
loss	Metric
functions	O
⁢ℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
(	O
Definition	O
)	O
obey	O
∈⁢⋅1BΦℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
G.	O
We	O
have	O
:	O
where	O
ϵ⁢CF	O
,	O
ϵ=t0F	O
and	O
ϵ=t1F	O
are	O
as	O
in	O
Definitions	O
and	O
.	O
theorem	O
:	O
.	O
Under	O
the	O
conditions	O
of	O
Lemma	O
,	O
and	O
assuming	O
the	O
loss	Metric
L	O
used	O
to	O
define	O
ℓh	O
,	O
Φ	O
in	O
Definitions	O
and	O
is	O
the	O
squared	O
loss	Metric
,	O
we	O
have	O
:	O
where	O
ϵF	O
and	O
ϵ⁢CF	O
are	O
defined	O
w.r.t	O
.	O
the	O
squared	O
loss	Metric
.	O
The	O
main	O
idea	O
of	O
the	O
proof	O
is	O
showing	O
that	O
is	O
upper	O
bounded	O
by	O
the	O
sum	O
of	O
the	O
expected	O
factual	O
loss	Metric
and	O
expected	O
counterfactual	O
loss	Metric
.	O
However	O
,	O
we	O
can	O
not	O
estimate	O
,	O
since	O
we	O
only	O
have	O
samples	O
relevant	O
to	O
.	O
We	O
therefore	O
bound	O
the	O
difference	O
using	O
an	O
IPM	Metric
.	O
Choosing	O
a	O
small	O
function	O
family	O
will	O
make	O
the	O
bound	O
tighter	O
.	O
However	O
,	O
choosing	O
too	O
small	O
a	O
family	O
could	O
result	O
in	O
an	O
incomputable	O
bound	O
.	O
For	O
example	O
,	O
for	O
the	O
minimal	O
choice	O
,	O
we	O
will	O
have	O
to	O
evaluate	O
an	O
expectation	O
term	O
of	O
over	O
,	O
and	O
of	O
over	O
.	O
We	O
can	O
not	O
in	O
general	O
evaluate	O
these	O
expectations	O
,	O
since	O
by	O
assumption	O
when	O
we	O
only	O
observe	O
,	O
and	O
the	O
same	O
for	O
and	O
.	O
In	O
addition	O
,	O
for	O
some	O
function	O
families	O
there	O
is	O
no	O
known	O
way	O
to	O
efficiently	O
compute	O
the	O
IPM	Metric
distance	O
or	O
its	O
gradients	O
.	O
In	O
this	O
paper	O
we	O
use	O
two	O
function	Method
families	Method
for	O
which	O
there	O
are	O
available	O
optimization	Method
tools	Method
.	O
The	O
first	O
is	O
the	O
family	O
of	O
-	O
Lipschitz	O
functions	O
,	O
which	O
leads	O
to	O
IPM	Metric
being	O
the	O
Wasserstein	Metric
distance	O
,	O
denoted	O
.	O
The	O
second	O
is	O
the	O
family	O
of	O
norm	Method
-	Method
reproducing	Method
kernel	Method
Hilbert	Method
space	Method
(	O
RKHS	Method
)	O
functions	O
,	O
leading	O
to	O
the	O
MMD	Metric
metric	O
,	O
denoted	O
.	O
Both	O
the	O
Wasserstein	Metric
and	O
MMD	Metric
metrics	Metric
have	O
consistent	Method
estimators	Method
which	O
can	O
be	O
efficiently	O
computed	O
in	O
the	O
finite	Task
sample	Task
case	Task
.	O
Both	O
have	O
been	O
used	O
for	O
various	O
machine	Task
learning	Task
tasks	Task
in	O
recent	O
years	O
.	O
In	O
order	O
to	O
explicitly	O
evaluate	O
the	O
constant	O
in	O
Theorem	O
[	O
reference	O
]	O
,	O
we	O
have	O
to	O
make	O
some	O
assumptions	O
about	O
the	O
elements	O
of	O
the	O
problem	O
.	O
For	O
the	O
Wasserstein	Metric
case	O
these	O
are	O
the	O
loss	Metric
,	O
the	O
Lipschitz	O
constants	O
of	O
and	O
,	O
and	O
the	O
condition	O
number	O
of	O
the	O
Jacobian	O
of	O
.	O
For	O
the	O
MMD	Metric
case	O
,	O
we	O
make	O
assumptions	O
about	O
the	O
RKHS	Method
representability	O
and	O
RKHS	Method
norms	O
of	O
,	O
,	O
and	O
the	O
standard	Metric
deviation	Metric
of	Metric
.	O
The	O
full	O
details	O
are	O
given	O
in	O
the	O
supplement	O
,	O
with	O
the	O
major	O
results	O
stated	O
in	O
Theorems	O
2	O
and	O
3	O
.	O
In	O
all	O
cases	O
we	O
obtain	O
that	O
making	O
smaller	O
increases	O
the	O
constant	O
precluding	O
trivial	O
solutions	O
such	O
as	O
making	O
arbitrarily	O
small	O
.	O
For	O
an	O
empirical	O
sample	O
,	O
and	O
a	O
family	O
of	O
representations	O
and	O
hypotheses	O
,	O
we	O
can	O
further	O
upper	O
bound	O
and	O
by	O
their	O
respective	O
empirical	Metric
losses	Metric
and	O
a	O
model	Metric
complexity	Metric
term	Metric
using	O
standard	O
arguments	O
.	O
The	O
IPMs	Metric
we	O
use	O
can	O
be	O
consistently	O
estimated	O
from	O
finite	O
samples	O
.	O
The	O
negative	O
variance	O
term	O
arises	O
from	O
the	O
fact	O
that	O
,	O
following	O
,	O
we	O
define	O
the	O
error	O
in	O
terms	O
of	O
the	O
conditional	O
mean	O
functions	O
,	O
as	O
opposed	O
to	O
fitting	O
the	O
random	O
variables	O
.	O
Our	O
results	O
hold	O
for	O
any	O
given	O
and	O
obeying	O
the	O
Theorem	O
conditions	O
.	O
This	O
immediately	O
suggest	O
an	O
algorithm	O
in	O
which	O
we	O
minimize	O
the	O
upper	O
bound	O
in	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
with	O
respect	O
to	O
and	O
and	O
either	O
the	O
Wasserstein	Metric
or	O
MMD	Metric
IPM	Metric
,	O
in	O
order	O
to	O
minimize	O
the	O
error	O
in	O
estimating	O
the	O
individual	O
treatment	O
effect	O
.	O
This	O
leads	O
us	O
to	O
Algorithm	O
[	O
reference	O
]	O
below	O
.	O
section	O
:	O
Algorithm	O
for	O
estimating	Task
ITE	Task
We	O
propose	O
a	O
general	O
framework	O
called	O
CFR	Method
(	O
for	O
Counterfactual	Method
Regression	Method
)	O
for	O
ITE	Task
estimation	O
based	O
on	O
the	O
theoretical	O
results	O
above	O
.	O
Our	O
algorithm	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
,	O
regularized	Method
minimization	Method
procedure	Method
which	O
simultaneously	O
fits	O
both	O
a	O
balanced	Method
representation	Method
of	O
the	O
data	O
and	O
a	O
hypothesis	Method
for	O
the	O
outcome	O
.	O
CFR	Method
draws	O
on	O
the	O
same	O
intuition	O
as	O
the	O
approach	O
proposed	O
by	O
,	O
but	O
overcomes	O
the	O
following	O
limitations	O
of	O
their	O
method	O
:	O
a	O
)	O
Their	O
theory	O
requires	O
a	O
two	O
-	O
step	O
optimization	Method
procedure	Method
and	O
is	O
specific	O
to	O
linear	O
hypotheses	O
of	O
the	O
learned	O
representation	O
(	O
and	O
does	O
not	O
support	O
e.g.	O
deep	Method
neural	Method
networks	Method
)	O
,	O
b	O
)	O
The	O
treatment	O
indicator	O
might	O
get	O
lost	O
if	O
the	O
learned	O
representation	O
is	O
high	O
-	O
dimensional	O
(	O
see	O
discussion	O
below	O
)	O
.	O
We	O
assume	O
there	O
exists	O
a	O
distribution	O
over	O
,	O
such	O
that	O
strong	O
ignorability	O
holds	O
.	O
We	O
further	O
assume	O
we	O
have	O
a	O
sample	O
from	O
that	O
distribution	O
,	O
where	O
if	O
,	O
if	O
.	O
This	O
standard	O
assumption	O
means	O
that	O
the	O
treatment	O
assignment	O
determines	O
which	O
potential	O
outcome	O
we	O
see	O
.	O
Our	O
goal	O
is	O
to	O
find	O
a	O
representation	O
and	O
hypothesis	Method
that	O
will	O
minimize	O
for	O
.	O
In	O
this	O
work	O
,	O
we	O
let	O
and	O
be	O
parameterized	O
by	O
deep	Method
neural	Method
networks	Method
trained	O
jointly	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
,	O
see	O
Figure	O
[	O
reference	O
]	O
.	O
This	O
model	O
allows	O
for	O
learning	O
complex	O
non	O
-	O
linear	O
representations	O
and	O
hypotheses	O
with	O
large	O
flexibility	O
.	O
parameterized	O
with	O
a	O
single	O
network	O
using	O
the	O
concatenation	O
of	O
and	O
as	O
input	O
.	O
When	O
the	O
dimension	O
of	O
is	O
high	O
,	O
this	O
risks	O
losing	O
the	O
influence	O
of	O
on	O
during	O
training	Task
.	O
To	O
combat	O
this	O
,	O
our	O
first	O
contribution	O
is	O
to	O
parameterize	O
and	O
as	O
two	O
separate	O
“	O
heads	O
”	O
of	O
the	O
joint	Method
network	Method
,	O
the	O
former	O
used	O
to	O
estimate	O
the	O
outcome	O
under	O
treatment	O
,	O
and	O
the	O
latter	O
under	O
control	O
.	O
This	O
means	O
that	O
statistical	O
power	O
is	O
shared	O
in	O
the	O
representation	O
layers	O
of	O
the	O
network	O
,	O
while	O
the	O
effect	O
of	O
treatment	O
is	O
retained	O
in	O
the	O
separate	O
heads	O
.	O
Note	O
that	O
each	O
sample	O
is	O
used	O
to	O
update	O
only	O
the	O
head	O
corresponding	O
to	O
the	O
observed	O
treatment	O
;	O
for	O
example	O
,	O
an	O
observation	O
is	O
only	O
used	O
to	O
update	O
.	O
Our	O
second	O
contribution	O
is	O
to	O
excplicitly	O
account	O
and	O
adjust	O
for	O
the	O
bias	O
induced	O
by	O
treatment	O
group	O
imbalance	O
.	O
To	O
this	O
end	O
,	O
we	O
seek	O
a	O
representation	O
and	O
hypothesis	Method
that	O
minimizes	O
a	O
trade	O
-	O
off	O
between	O
predictive	Metric
accuracy	Metric
and	O
imbalance	Metric
in	O
the	O
representation	O
space	O
,	O
using	O
the	O
following	O
objective	O
:	O
Note	O
that	O
in	O
the	O
definition	O
of	O
is	O
simply	O
the	O
proportion	O
of	O
treated	O
units	O
in	O
the	O
population	O
.	O
The	O
weights	O
compensate	O
for	O
the	O
difference	O
in	O
treatment	O
group	O
size	O
in	O
our	O
sample	O
,	O
see	O
Theorem	O
[	O
reference	O
]	O
.	O
is	O
the	O
(	O
empirical	O
)	O
integral	O
probability	O
metric	O
defined	O
by	O
the	O
function	Method
family	Method
.	O
For	O
most	O
IPMs	Metric
,	O
we	O
can	O
not	O
compute	O
the	O
factor	O
in	O
Equation	O
[	O
reference	O
]	O
,	O
but	O
treat	O
it	O
as	O
part	O
of	O
the	O
hyperparameter	O
.	O
This	O
makes	O
our	O
objective	O
sensitive	O
to	O
the	O
scaling	O
of	O
,	O
even	O
for	O
a	O
constant	O
.	O
We	O
therefore	O
normalize	O
through	O
either	O
projection	Method
or	O
batch	Method
-	Method
normalization	Method
with	O
fixed	O
scale	O
.	O
We	O
refer	O
to	O
the	O
model	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
with	O
as	O
Counterfactual	Method
Regression	Method
(	O
CFR	Method
)	O
and	O
the	O
variant	O
without	O
balance	Method
regularization	Method
(	O
)	O
as	O
Treatment	Method
-	Method
Agnostic	Method
Representation	Method
Network	Method
(	O
TARNet	Method
)	O
.	O
We	O
train	O
our	O
models	O
by	O
minimizing	O
(	O
[	O
reference	O
]	O
)	O
using	O
stochastic	Method
gradient	Method
descent	Method
,	O
where	O
we	O
backpropagate	O
the	O
error	O
through	O
both	O
the	O
hypothesis	Method
and	O
representation	Method
networks	Method
,	O
as	O
described	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
Both	O
the	O
prediction	O
loss	Metric
and	O
the	O
penalty	Method
term	Method
are	O
computed	O
for	O
one	O
mini	O
-	O
batch	O
at	O
a	O
time	O
.	O
Details	O
of	O
how	O
to	O
obtain	O
the	O
gradient	O
with	O
respect	O
to	O
the	O
empirical	O
IPMs	Metric
are	O
in	O
the	O
supplement	O
.	O
[	O
tbp	O
]	O
CFR	Method
:	O
Counterfactual	Method
regression	Method
with	O
integral	Method
probability	Method
metrics	Method
[	O
1	O
]	O
Input	O
:	O
Factual	O
sample	O
,	O
scaling	O
parameter	O
,	O
loss	Metric
function	O
,	O
representation	Method
network	Method
with	O
initial	O
weights	O
,	O
outcome	Method
network	Method
with	O
initial	O
weights	O
,	O
function	Method
family	Method
for	O
IPM	Metric
.	O
Compute	O
Compute	O
for	O
not	O
converged	O
Sample	O
mini	O
-	O
batch	O
Calculate	O
the	O
gradient	O
of	O
the	O
IPM	Metric
term	Metric
:	O
Calculate	O
the	O
gradients	O
of	O
the	O
empirical	O
loss	Metric
:	O
Obtain	O
step	O
size	O
scalar	O
or	O
matrix	O
with	O
standard	O
neural	Method
net	Method
methods	Method
e.g.	O
Adam	Method
Check	O
convergence	Metric
criterion	Metric
section	O
:	O
Experiments	O
Evaluating	O
causal	Task
inference	Task
algorithms	O
is	O
more	O
difficult	O
than	O
many	O
machine	Task
learning	Task
tasks	Task
,	O
since	O
for	O
real	O
-	O
world	O
data	O
we	O
rarely	O
have	O
access	O
to	O
the	O
ground	O
truth	O
treatment	O
effect	O
.	O
Existing	O
literature	O
mostly	O
deals	O
with	O
this	O
in	O
two	O
ways	O
.	O
One	O
is	O
by	O
using	O
synthetic	O
or	O
semi	O
-	O
synthetic	O
datasets	O
,	O
where	O
the	O
outcome	O
or	O
treatment	O
assignment	O
are	O
fully	O
known	O
;	O
we	O
use	O
the	O
semi	Material
-	Material
synthetic	Material
IHDP	Material
dataset	Material
from	O
.	O
The	O
other	O
is	O
using	O
real	O
-	O
world	O
data	O
from	O
randomized	O
controlled	O
trials	O
(	O
RCT	O
)	O
.	O
The	O
problem	O
in	O
using	O
data	O
from	O
RCTs	O
is	O
that	O
there	O
is	O
no	O
imbalance	O
between	O
the	O
treated	O
and	O
control	O
distributions	O
,	O
making	O
our	O
method	O
redundant	O
.	O
We	O
partially	O
overcome	O
this	O
problem	O
by	O
using	O
the	O
Jobs	O
dataset	O
from	O
,	O
which	O
includes	O
both	O
a	O
randomized	O
and	O
a	O
non	Method
-	Method
randomized	Method
component	Method
.	O
We	O
use	O
both	O
for	O
training	Task
,	O
but	O
can	O
only	O
use	O
the	O
randomized	Method
component	Method
for	O
evaluation	Task
.	O
This	O
alleviates	O
,	O
but	O
does	O
not	O
solve	O
,	O
the	O
issue	O
of	O
a	O
completely	O
balanced	O
dataset	O
being	O
unsuited	O
for	O
our	O
method	O
.	O
We	O
evaluate	O
our	O
framework	O
CFR	Method
,	O
and	O
its	O
variant	O
without	O
balancing	Method
regularization	Method
(	O
TARNet	Method
)	O
,	O
in	O
the	O
task	O
of	O
estimating	Task
ITE	Task
and	O
ATE	Task
.	O
CFR	Method
is	O
implemented	O
as	O
a	O
feed	Method
-	Method
forward	Method
neural	Method
network	Method
with	O
3	O
fully	Method
-	Method
connected	Method
exponential	Method
-	Method
linear	Method
layers	Method
for	O
the	O
representation	O
and	O
3	O
for	O
the	O
hypothesis	Method
.	O
Layer	O
sizes	O
were	O
200	O
for	O
all	O
layers	O
used	O
for	O
Jobs	O
and	O
200	O
and	O
100	O
for	O
the	O
representation	O
and	O
hypothesis	Method
used	O
for	O
IHDP	Material
.	O
The	O
model	O
is	O
trained	O
using	O
Adam	Method
.	O
For	O
an	O
overview	O
,	O
see	O
Figure	O
[	O
reference	O
]	O
.	O
Layers	O
corresponding	O
to	O
the	O
hypothesis	Method
are	O
regularized	O
with	O
a	O
small	O
weight	O
decay	O
.	O
For	O
continuous	O
data	O
we	O
use	O
mean	O
squared	O
loss	Metric
and	O
for	O
binary	O
data	O
,	O
we	O
use	O
log	O
-	O
loss	Metric
.	O
While	O
our	O
theory	O
does	O
not	O
immediately	O
apply	O
to	O
log	O
-	O
loss	Metric
,	O
we	O
were	O
curious	O
to	O
see	O
how	O
our	O
model	O
performs	O
with	O
it	O
.	O
We	O
compare	O
our	O
method	O
to	O
Ordinary	Method
Least	Method
Squares	Method
with	O
treatment	O
as	O
a	O
feature	O
(	O
OLS	Method
-	O
1	O
)	O
,	O
OLS	Method
with	O
separate	O
regressors	O
for	O
each	O
treatment	O
(	O
OLS	Method
-	O
2	O
)	O
,	O
-	O
nearest	O
neighbor	Method
(	O
-	O
NN	O
)	O
,	O
Targeted	Method
Maximum	Method
Likelihood	Method
,	O
which	O
is	O
a	O
doubly	Method
robust	Method
method	Method
(	O
TMLE	Method
)	O
,	O
Bayesian	Method
Additive	Method
Regression	Method
Trees	Method
(	O
BART	Method
)	O
,	O
Random	Method
Forests	Method
(	O
Rand	O
.	O
For	O
.	O
)	O
,	O
Causal	Method
Forests	Method
(	O
Caus	O
.	O
For	O
.	O
)	O
as	O
well	O
as	O
the	O
Balancing	Method
Linear	Method
Regression	Method
(	O
BLR	Method
)	O
and	O
Balancing	Method
Neural	Method
Network	Method
(	O
BNN	Method
)	O
by	O
.	O
For	O
classification	Task
tasks	Task
we	O
substitute	O
Logistic	Method
Regression	Method
(	O
LR	Method
)	Method
for	O
OLS	Method
.	O
Choosing	O
hyperparameters	O
for	O
estimating	O
PEHE	Task
is	O
non	O
-	O
trivial	O
;	O
we	O
detail	O
our	O
selection	O
procedure	O
,	O
applied	O
to	O
all	O
methods	O
,	O
in	O
subsection	O
C.1	O
of	O
the	O
supplement	O
.	O
We	O
evaluate	O
our	O
model	O
in	O
two	O
different	O
settings	O
.	O
One	O
is	O
within	O
-	O
sample	O
,	O
where	O
the	O
task	O
is	O
to	O
estimate	O
ITE	Task
for	O
all	O
units	O
in	O
a	O
sample	O
for	O
which	O
the	O
(	O
factual	O
)	O
outcome	O
of	O
one	O
treatment	O
is	O
observed	O
.	O
This	O
corresponds	O
to	O
the	O
common	O
scenario	O
in	O
which	O
a	O
cohort	O
is	O
selected	O
once	O
and	O
not	O
changed	O
.	O
This	O
task	O
is	O
non	O
-	O
trivial	O
,	O
as	O
we	O
never	O
observe	O
the	O
ITE	Task
for	O
any	O
unit	O
.	O
The	O
other	O
is	O
the	O
out	Task
-	Task
of	Task
-	Task
sample	Task
setting	Task
,	O
where	O
the	O
goal	O
is	O
to	O
estimate	O
ITE	Task
for	O
units	O
with	O
no	O
observed	O
outcomes	O
.	O
This	O
corresponds	O
to	O
the	O
case	O
where	O
a	O
new	O
patient	O
arrives	O
and	O
the	O
goal	O
is	O
to	O
select	O
the	O
best	O
possible	O
treatment	O
.	O
Within	Metric
-	Metric
sample	Metric
error	Metric
is	O
computed	O
over	O
both	O
the	O
training	O
and	O
validation	O
sets	O
,	O
and	O
out	Metric
-	Metric
of	Metric
-	Metric
sample	Metric
error	Metric
over	O
the	O
test	O
set	O
.	O
subsection	O
:	O
Simulated	O
outcome	O
:	O
IHDP	Material
compiled	O
a	O
dataset	O
for	O
causal	Task
effect	Task
estimation	Task
based	O
on	O
the	O
Infant	Material
Health	Material
and	Material
Development	Material
Program	Material
(	O
IHDP	Material
)	Material
,	O
in	O
which	O
the	O
covariates	O
come	O
from	O
a	O
randomized	O
experiment	O
studying	O
the	O
effects	O
of	O
specialist	O
home	O
visits	O
on	O
future	O
cognitive	O
test	O
scores	O
.	O
The	O
treatment	O
groups	O
have	O
been	O
made	O
imbalanced	O
by	O
removing	O
a	O
biased	O
subset	O
of	O
the	O
treated	O
population	O
.	O
The	O
dataset	O
comprises	O
747	O
units	O
(	O
139	O
treated	O
,	O
608	O
control	O
)	O
and	O
25	O
covariates	O
measuring	O
aspects	O
of	O
children	O
and	O
their	O
mothers	O
.	O
We	O
use	O
the	O
simulated	O
outcome	O
implemented	O
as	O
setting	O
“	O
A	O
”	O
in	O
the	O
NPCI	Method
package	Method
.	O
Following	O
,	O
we	O
use	O
the	O
noiseless	O
outcome	O
to	O
compute	O
the	O
true	O
effect	O
.	O
We	O
report	O
the	O
estimated	O
(	O
finite	O
-	O
sample	O
)	O
PEHE	O
loss	Metric
(	O
Eq	O
.	O
[	O
reference	O
]	O
)	O
,	O
and	O
the	O
absolute	O
error	O
in	O
average	Metric
treatment	Metric
effect	Metric
.	O
The	O
results	O
of	O
the	O
experiments	O
on	O
IHDP	Material
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
(	O
left	O
)	O
.	O
We	O
average	O
over	O
1000	O
realizations	O
of	O
the	O
outcomes	O
with	O
63	O
/	O
27	O
/	O
10	O
train	O
/	O
validation	O
/	O
test	O
splits	O
.	O
We	O
investigate	O
the	O
effects	O
of	O
increasing	O
imbalance	O
between	O
the	O
original	O
treatment	O
groups	O
by	O
constructing	O
biased	O
subsamples	O
of	O
the	O
IHDP	Material
dataset	O
.	O
A	O
logistic	Method
-	Method
regression	Method
propensity	Method
score	Method
model	Method
is	O
fit	O
to	O
form	O
estimates	O
of	O
the	O
conditional	O
treatment	O
probability	O
.	O
Then	O
,	O
repeatedly	O
,	O
with	O
probability	O
we	O
remove	O
the	O
remaining	O
control	O
observation	O
that	O
has	O
closest	O
to	O
,	O
and	O
with	O
probability	O
,	O
we	O
remove	O
a	O
random	O
control	O
observation	O
.	O
The	O
higher	O
,	O
the	O
more	O
imbalance	O
.	O
For	O
each	O
value	O
of	O
,	O
we	O
remove	O
observations	O
from	O
each	O
set	O
,	O
leaving	O
.	O
subsection	O
:	O
Real	O
-	O
world	O
outcome	O
:	O
Jobs	O
The	O
study	O
by	O
is	O
a	O
widely	O
used	O
benchmark	O
in	O
the	O
causal	Task
inference	Task
community	O
,	O
where	O
the	O
treatment	O
is	O
job	Task
training	Task
and	O
the	O
outcomes	O
are	O
income	O
and	O
employment	O
status	O
after	O
training	O
.	O
This	O
dataset	O
combines	O
a	O
randomized	O
study	O
based	O
on	O
the	O
National	O
Supported	O
Work	O
program	O
with	O
observational	O
data	O
to	O
form	O
a	O
larger	O
dataset	O
.	O
The	O
presence	O
of	O
the	O
randomized	O
subgroup	O
gives	O
a	O
way	O
to	O
estimate	O
the	O
“	O
ground	O
truth	O
”	O
causal	O
effect	O
.	O
The	O
study	O
includes	O
8	O
covariates	O
such	O
as	O
age	O
and	O
education	O
,	O
as	O
well	O
as	O
previous	O
earnings	O
.	O
We	O
construct	O
a	O
binary	Task
classification	Task
task	Task
,	O
called	O
Jobs	O
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
unemployment	O
,	O
using	O
the	O
feature	O
set	O
of	O
.	O
Following	O
,	O
we	O
use	O
the	O
LaLonde	O
experimental	O
sample	O
(	O
297	O
treated	O
,	O
425	O
control	O
)	O
and	O
the	O
PSID	O
comparison	O
group	O
(	O
2490	O
control	O
)	O
.	O
There	O
were	O
482	O
(	O
15	O
%	O
)	O
subjects	O
unemployed	O
by	O
the	O
end	O
of	O
the	O
study	O
.	O
We	O
average	O
over	O
10	O
train	O
/	O
validation	O
/	O
test	O
splits	O
with	O
ratios	O
56	O
/	O
24	O
/	O
20	O
.	O
Because	O
all	O
the	O
treated	O
subjects	O
were	O
part	O
of	O
the	O
original	O
randomized	O
sample	O
,	O
we	O
can	O
compute	O
the	O
true	O
average	Metric
treatment	Metric
effect	Metric
on	O
the	O
treated	O
by	O
,	O
where	O
is	O
the	O
control	O
group	O
.	O
We	O
report	O
the	O
error	O
.	O
We	O
can	O
not	O
evaluate	O
on	O
this	O
dataset	O
,	O
since	O
there	O
is	O
no	O
ground	O
truth	O
for	O
the	O
ITE	Task
.	O
Instead	O
,	O
in	O
order	O
to	O
evaluate	O
the	O
quality	O
of	O
ITE	Task
estimation	O
,	O
we	O
use	O
a	O
measure	O
we	O
call	O
policy	Metric
risk	Metric
.	O
The	O
policy	Metric
risk	Metric
is	O
defined	O
as	O
the	O
average	Metric
loss	Metric
in	Metric
value	Metric
when	O
treating	O
according	O
to	O
the	O
policy	O
implied	O
by	O
an	O
ITE	Task
estimator	O
.	O
In	O
our	O
case	O
,	O
for	O
a	O
model	O
,	O
we	O
let	O
the	O
policy	O
be	O
to	O
treat	O
,	O
,	O
if	O
,	O
and	O
to	O
not	O
treat	O
,	O
otherwise	O
.	O
The	O
policy	O
risk	O
is	O
which	O
we	O
can	O
estimate	O
for	O
the	O
randomized	O
trial	O
subset	O
of	O
Jobs	O
by	O
.	O
See	O
figure	O
[	O
reference	O
]	O
for	O
risk	O
as	O
a	O
function	O
of	O
treatment	O
threshold	O
,	O
aligned	O
by	O
proportion	O
of	O
treated	O
,	O
and	O
Table	O
[	O
reference	O
]	O
for	O
the	O
risk	O
when	O
.	O
subsection	O
:	O
Results	O
We	O
begin	O
by	O
noting	O
that	O
indeed	O
imbalance	O
confers	O
an	O
advantage	O
to	O
using	O
the	O
IPM	Metric
regularization	O
term	O
,	O
as	O
our	O
theoretical	O
results	O
indicate	O
,	O
see	O
e.g.	O
the	O
results	O
for	O
CFR	Method
Wass	Method
(	O
)	O
and	O
TARNet	Method
(	O
)	O
on	O
IHDP	Material
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
also	O
see	O
in	O
Figure	O
[	O
reference	O
]	O
that	O
even	O
for	O
the	O
harder	O
case	O
of	O
increased	O
imbalance	O
(	O
)	O
between	O
treated	O
and	O
control	O
,	O
the	O
relative	O
gain	O
from	O
using	O
our	O
method	O
remains	O
significant	O
.	O
On	O
Jobs	O
,	O
we	O
see	O
a	O
smaller	O
gain	O
from	O
using	O
IPM	Metric
penalties	Metric
than	O
on	O
IHDP	Material
.	O
We	O
believe	O
this	O
is	O
the	O
case	O
because	O
,	O
while	O
we	O
are	O
minimizing	O
our	O
bound	O
over	O
observational	O
data	O
and	O
accounting	O
for	O
this	O
bias	O
,	O
we	O
are	O
evaluating	O
the	O
predictions	O
only	O
on	O
a	O
randomized	O
subset	O
,	O
where	O
the	O
treatment	O
groups	O
are	O
distributed	O
identically	O
.	O
For	O
both	O
IHDP	Material
,	O
non	Method
-	Method
linear	Method
estimators	Method
do	O
significantly	O
better	O
than	O
linear	O
ones	O
in	O
terms	O
of	O
individual	O
effect	O
(	O
)	O
.	O
On	O
the	O
Jobs	Material
dataset	Material
,	O
straightforward	O
logistic	Method
regression	Method
does	O
remarkably	O
well	O
in	O
estimating	O
the	O
ATT	Task
.	O
However	O
,	O
being	O
a	O
linear	Method
model	Method
,	O
LR	Method
can	O
only	O
ascribe	O
a	O
uniform	Method
policy	Method
-	O
in	O
this	O
case	O
,	O
“	O
treat	O
everyone	O
”	O
.	O
The	O
more	O
nuanced	O
policies	O
offered	O
by	O
non	Method
-	Method
linear	Method
methods	Method
achieve	O
lower	O
policy	Metric
risk	Metric
in	O
the	O
case	O
of	O
Causal	Method
Forests	Method
and	O
CFR	Method
.	O
This	O
emphasizes	O
the	O
fact	O
that	O
estimating	O
average	O
effect	O
and	O
individual	O
effect	O
can	O
require	O
different	O
models	O
.	O
Specifically	O
,	O
while	O
smoothing	O
over	O
many	O
units	O
may	O
yield	O
a	O
good	O
ATE	Task
estimate	O
,	O
this	O
might	O
significantly	O
hurt	O
ITE	Task
estimation	O
.	O
-	O
nearest	O
neighbors	Method
has	O
very	O
good	O
within	O
-	O
sample	O
results	O
on	O
Jobs	Task
,	O
because	O
evaluation	Task
is	O
performed	O
over	O
the	O
randomized	Method
component	Method
,	O
but	O
suffers	O
heavily	O
in	O
generalizing	O
out	O
of	O
sample	O
,	O
as	O
expected	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
we	O
give	O
a	O
meaningful	O
and	O
intuitive	O
error	Metric
bound	Metric
for	O
the	O
problem	O
of	O
estimating	Task
individual	Task
treatment	Task
effect	Task
.	O
Our	O
bound	O
relates	O
ITE	Task
estimation	O
to	O
the	O
classic	O
machine	Task
learning	Task
problem	Task
of	Task
learning	Task
from	O
finite	O
samples	O
,	O
along	O
with	O
methods	O
for	O
measuring	Task
distributional	Task
distances	Task
from	O
finite	O
samples	O
.	O
The	O
bound	O
lends	O
itself	O
naturally	O
to	O
the	O
creation	O
of	O
learning	Method
algorithms	Method
;	O
we	O
focus	O
on	O
using	O
neural	Method
nets	Method
as	O
representations	O
and	O
hypotheses	O
.	O
We	O
apply	O
our	O
theory	Method
-	Method
guided	Method
approach	Method
to	O
both	O
synthetic	Task
and	Task
real	Task
-	Task
world	Task
tasks	Task
,	O
showing	O
that	O
in	O
every	O
case	O
our	O
method	O
matches	O
or	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
Important	O
open	O
questions	O
are	O
theoretical	O
considerations	O
in	O
choosing	O
the	O
IPM	Metric
weight	O
,	O
how	O
to	O
best	O
derive	O
confidence	O
intervals	O
for	O
our	O
model	O
’s	O
predictions	O
,	O
and	O
how	O
to	O
integrate	O
our	O
work	O
with	O
more	O
complicated	O
causal	Method
models	Method
such	O
as	O
those	O
with	O
hidden	O
confounding	O
or	O
instrumental	O
variables	O
.	O
subsubsection	O
:	O
Acknowledgments	O
We	O
wish	O
to	O
thank	O
Aahlad	O
Manas	O
for	O
his	O
assistance	O
with	O
the	O
experiments	O
.	O
We	O
also	O
thank	O
Jennifer	O
Hill	O
,	O
Marco	O
Cuturi	O
,	O
Esteban	O
Tabak	O
and	O
Sanjong	O
Misra	O
for	O
fruitful	O
conversations	O
,	O
and	O
Stefan	O
Wager	O
for	O
his	O
help	O
with	O
the	O
code	O
for	O
Causal	Method
Forests	Method
.	O
DS	Method
and	O
US	O
were	O
supported	O
by	O
NSF	O
CAREER	O
award	O
#	O
1350965	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Proofs	O
subsection	O
:	O
Definitions	O
,	O
assumptions	O
,	O
and	O
auxiliary	O
lemmas	O
Notation:⁢p	O
(	O
x	O
,	O
t	O
)	O
:	O
distribution	O
on	O
×X{0	O
,	O
1}u	O
=	O
p	O
(	O
t=1	O
)	O
:	O
the	O
marginal	O
probability	O
of	O
treatment.p	O
=	O
t1	O
(	O
x	O
)	O
=p	O
(	O
x|t=1	O
)	O
:	O
treated	O
distribution	O
.	O
p	O
=	O
t0	O
(	O
x	O
)	O
=p	O
(	O
x|t=0	O
)	O
:	O
control	O
distribution	O
.	O
Φ	O
:	O
representation	Method
function	Method
mapping	O
from	O
X	O
to	O
R.Ψ	O
:	O
the	O
inverse	O
function	O
of	O
Φ	O
,	O
mapping	O
from	O
R	O
to	O
X.⁢pΦ	O
(	O
r	O
,	O
t	O
)	O
:	O
the	O
distribution	O
induced	O
by	O
Φ	O
on	O
×R{0	O
,	O
1}.⁢p	O
=	O
t1Φ	O
(	O
r	O
)	O
,	O
⁢p	O
=	O
t0Φ	O
(	O
r	O
)	O
:	O
treated	O
and	O
control	O
distributions	O
induced	O
by	O
Φ	O
on	O
R.⁢L	O
(	O
⋅	O
,	O
⋅	O
)	O
:	O
loss	Metric
function	O
,	O
from	O
×YY	O
to	O
R	O
+	O
.⁢ℓh	O
,	O
Φ	O
(	O
x	O
,	O
t	O
)	O
:	O
the	O
expected	O
loss	Metric
of	O
⁢h	O
(	O
⁢Φ	O
(	O
x	O
),	O
t	O
)	O
for	O
the	O
unit	O
x	O
and	O
treatment	O
t.⁢ϵF	O
(	O
h	O
,	O
Φ	O
)	O
,	O
⁢ϵ⁢CF	O
(	O
h	O
,	O
Φ	O
)	O
:	O
expected	O
factual	O
and	O
counterfactual	O
loss	Metric
of	O
⁢h	O
(	O
⁢Φ	O
(	O
x	O
),	O
t	O
)	O
.τ	O
(	O
x	O
)	O
:=E	O
[	O
Y1	O
-	O
Y0|x	O
]	O
,	O
the	O
expected	O
treatment	O
effect	O
for	O
unit	O
x.⁢ϵPEHE	O
(	O
f	O
)	O
:	O
expected	Metric
error	Metric
in	O
estimating	O
the	O
individual	O
treatment	O
effect	O
of	O
a	O
function	O
⁢f	O
(	O
x	O
,	O
t	O
)	O
.⁢IPMG	O
(	O
p	O
,	O
q	O
)	O
:	O
the	O
integral	Metric
probability	Metric
metric	Metric
distance	Metric
induced	O
by	O
function	O
family	O
G	O
between	O
distributions	O
p	O
and	O
q.	O
Notation	O
:	O
:	O
distribution	O
on	O
:	O
the	O
marginal	O
probability	O
of	O
treatment	O
.	O
:	O
treated	O
distribution	O
.	O
:	O
control	Task
distribution	Task
.	O
:	O
representation	Method
function	Method
mapping	O
from	O
to	O
.	O
:	O
the	O
inverse	O
function	O
of	O
,	O
mapping	O
from	O
to	O
.	O
:	O
the	O
distribution	O
induced	O
by	O
on	O
.	O
,	O
:	O
treated	O
and	O
control	O
distributions	O
induced	O
by	O
on	O
.	O
:	O
loss	Metric
function	O
,	O
from	O
to	O
.	O
:	O
the	O
expected	O
loss	Metric
of	O
for	O
the	O
unit	O
and	O
treatment	O
.	O
,	O
:	O
expected	O
factual	O
and	O
counterfactual	O
loss	Metric
of	O
.	O
,	O
the	O
expected	O
treatment	O
effect	O
for	O
unit	O
.	O
:	O
expected	Metric
error	Metric
in	O
estimating	O
the	O
individual	O
treatment	O
effect	O
of	O
a	O
function	O
.	O
:	O
the	O
integral	O
probability	O
metric	O
distance	O
induced	O
by	O
function	O
family	O
between	O
distributions	O
and	O
.	O
We	O
first	O
define	O
the	O
necessary	O
distributions	O
and	O
prove	O
some	O
simple	O
results	O
about	O
them	O
.	O
We	O
assume	O
a	O
joint	O
distribution	O
function	O
,	O
such	O
that	O
,	O
and	O
for	O
all	O
.	O
Recall	O
that	O
we	O
assume	O
Consistency	O
,	O
that	O
is	O
we	O
assume	O
that	O
we	O
observe	O
and	O
.	O
theorem	O
:	O
.	O
The	O
treatment	O
effect	O
for	O
unit	O
x	O
is	O
:	O
We	O
first	O
show	O
that	O
under	O
consistency	O
and	O
strong	O
ignorability	O
,	O
the	O
ITE	Task
function	O
is	O
identifiable	O
:	O
theorem	O
:	O
.	O
We	O
have	O
:	O
Equality	O
(	O
)	O
is	O
because	O
we	O
assume	O
that	O
Yt	O
and	O
t	O
are	O
independent	O
conditioned	O
on	O
x.	O
Equality	O
(	O
)	O
follows	O
from	O
the	O
consistency	O
assumption	O
.	O
Finally	O
,	O
the	O
last	O
equation	O
is	O
composed	O
entirely	O
of	O
observable	O
quantities	O
and	O
can	O
be	O
estimated	O
from	O
data	O
since	O
we	O
assume	O
0<p	O
(	O
t=1|x	O
)	O
<1	O
for	O
all	O
x.	O
theorem	O
:	O
.	O
Let	O
p	O
=	O
t1	O
(	O
x	O
)	O
:=p	O
(	O
x|t=1	O
)	O
,	O
and	O
p	O
=	O
t0	O
(	O
x	O
)	O
:=p	O
(	O
x|t=0	O
)	O
denote	O
respectively	O
the	O
treatment	O
and	O
control	O
distributions	O
.	O
Let	O
be	O
a	O
representation	Method
function	Method
.	O
We	O
will	O
assume	O
that	O
is	O
differentiable	O
.	O
theorem	O
:	O
.	O
The	O
representation	Method
function	Method
Φ	O
is	O
one	O
-	O
to	O
-	O
one	O
.	O
Without	O
loss	Metric
of	O
generality	O
we	O
will	O
assume	O
that	O
R	O
is	O
the	O
image	O
of	O
X	O
under	O
Φ	O
,	O
and	O
define	O
:	O
Ψ→RX	O
to	O
be	O
the	O
inverse	O
of	O
Φ	O
,	O
such	O
that	O
=	O
⁢Ψ	O
(	O
⁢Φ	O
(	O
x	O
))	O
x	O
for	O
all	O
∈xX.	O
theorem	O
:	O
.	O
For	O
a	O
representation	Method
function	Method
:	O
Φ→XR	O
,	O
and	O
for	O
a	O
distribution	O
p	O
defined	O
over	O
X	O
,	O
let	O
pΦ	O
be	O
the	O
distribution	O
induced	O
by	O
Φ	O
over	O
R.	O
Define	O
p	O
=	O
t1Φ	O
(	O
r	O
)	O
:=pΦ	O
(	O
r|t=1	O
)	O
,	O
p	O
=	O
t0Φ	O
(	O
r	O
)	O
:=pΦ	O
(	O
r|t=0	O
)	O
,	O
to	O
be	O
the	O
treatment	O
and	O
control	O
distributions	O
induced	O
over	O
R.	O
For	O
a	O
one	O
-	O
to	O
-	O
one	O
,	O
the	O
distribution	O
over	O
can	O
be	O
obtained	O
by	O
the	O
standard	O
change	O
of	O
variables	O
formula	O
,	O
using	O
the	O
determinant	O
of	O
the	O
Jacobian	O
of	O
.	O
See	O
for	O
the	O
case	O
of	O
a	O
mapping	O
between	O
spaces	O
of	O
different	O
dimensions	O
.	O
theorem	O
:	O
.	O
For	O
all	O
∈rR	O
,	O
∈t{0	O
,	O
1	O
}	O
:	O
proof	O
:	O
Proof	O
.	O
Let	O
be	O
the	O
absolute	O
of	O
the	O
determinant	O
of	O
the	O
Jacobian	O
of	O
.	O
where	O
equality	O
(	O
a	O
)	O
is	O
by	O
the	O
change	O
of	O
variable	O
formula	O
.	O
The	O
proof	O
is	O
identical	O
for	O
.	O
∎	O
Let	O
be	O
a	O
loss	Metric
function	O
,	O
e.g.	O
the	O
absolute	O
loss	Metric
or	O
squared	O
loss	Metric
.	O
theorem	O
:	O
.	O
Let	O
:	O
Φ→XR	O
be	O
a	O
representation	Method
function	Method
.	O
Let	O
:	O
h→×R{0	O
,	O
1}Y	O
be	O
an	O
hypothesis	Method
defined	O
over	O
the	O
representation	O
space	O
R.	O
The	O
expected	O
loss	Metric
for	O
the	O
unit	O
and	O
treatment	O
pair	O
(	O
x	O
,	O
t	O
)	O
is	O
:	O
theorem	O
:	O
.	O
The	O
expected	O
factual	O
loss	Metric
and	O
counterfactual	O
losses	O
of	O
h	O
and	O
Φ	O
are	O
,	O
respectively	O
:	O
When	O
it	O
is	O
clear	O
from	O
the	O
context	O
,	O
we	O
will	O
sometimes	O
use	O
and	O
for	O
the	O
expected	O
factual	O
and	O
counterfactual	O
losses	O
of	O
an	O
arbitrary	O
function	O
.	O
theorem	O
:	O
.	O
The	O
expected	O
treated	O
and	O
control	O
losses	O
are	O
:	O
The	O
four	O
losses	O
above	O
are	O
simply	O
the	O
loss	Metric
conditioned	O
on	O
either	O
the	O
control	O
or	O
treated	O
set	O
.	O
Let	O
be	O
the	O
proportion	O
of	O
treated	O
in	O
the	O
population	O
.	O
We	O
then	O
have	O
the	O
immediate	O
result	O
:	O
theorem	O
:	O
.	O
The	O
proof	O
is	O
immediate	O
,	O
noting	O
that	O
,	O
and	O
from	O
the	O
Definitions	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
of	O
the	O
losses	O
.	O
theorem	O
:	O
.	O
Let	O
G	O
be	O
a	O
function	O
family	O
consisting	O
of	O
functions	O
:	O
g→SR	O
.	O
For	O
a	O
pair	O
of	O
distributions	O
p1	O
,	O
p2	O
over	O
S	O
,	O
define	O
the	O
Integral	Metric
Probability	Metric
Metric	Metric
:	O
defines	O
a	O
pseudo	O
-	O
metric	O
on	O
the	O
space	O
of	O
probability	O
functions	O
over	O
,	O
and	O
for	O
sufficiently	O
large	O
function	O
families	O
,	O
is	O
a	O
proper	O
metric	O
.	O
Examples	O
of	O
sufficiently	O
large	O
functions	O
families	O
includes	O
the	O
set	O
of	O
bounded	O
continuous	O
functions	O
,	O
the	O
set	O
of	O
-	O
Lipschitz	O
functions	O
,	O
and	O
the	O
set	O
of	O
unit	O
norm	O
functions	O
in	O
a	O
universal	O
Reproducing	O
Norm	O
Hilbert	O
Space	O
.	O
The	O
latter	O
two	O
give	O
rise	O
to	O
the	O
Wasserstein	Metric
and	O
Maximum	Metric
Mean	Metric
Discrepancy	Metric
metrics	O
,	O
respectively	O
.	O
We	O
note	O
that	O
for	O
function	O
families	O
such	O
as	O
the	O
three	O
mentioned	O
above	O
,	O
for	O
which	O
,	O
the	O
absolute	O
value	O
can	O
be	O
omitted	O
from	O
definition	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
General	O
IPM	Metric
bound	Metric
We	O
now	O
state	O
and	O
prove	O
the	O
most	O
important	O
technical	O
lemma	O
of	O
this	O
section	O
.	O
theorem	O
:	O
(	O
Lemma	O
1	O
,	O
main	O
text	O
)	O
.	O
Let	O
:	O
Φ→XR	O
be	O
an	O
invertible	Method
representation	Method
with	O
Ψ	O
its	O
inverse	O
.	O
Let	O
p	O
=	O
t1Φ	O
,	O
p	O
=	O
t0Φ	O
be	O
defined	O
as	O
in	O
Definition	O
.	O
Let	O
u	O
=	O
p	O
(	O
t=1	O
)	O
.	O
Let	O
G	O
be	O
a	O
family	O
of	O
functions	O
:	O
g→RR	O
,	O
and	O
denote	O
by	O
⁢IPMG	Method
(	Method
⋅	Method
,	Method
⋅	Method
)	O
the	O
integral	O
probability	O
metric	O
induced	O
by	O
G.	O
Let	O
:	O
h→×R{0	O
,	O
1}Y	O
be	O
an	O
hypothesis	Method
.	O
Assume	O
there	O
exists	O
a	O
constant	O
>	O
BΦ0	O
,	O
such	O
that	O
for	O
=	O
t0	O
,	O
1	O
,	O
the	O
function	O
⁢gΦ	O
,	O
h	O
(	O
r	O
,	O
t	O
)	O
:=⁢⋅1BΦℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
∈G.	O
Then	O
we	O
have	O
:	O
proof	O
:	O
Proof	O
.	O
Equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Definition	O
[	O
reference	O
]	O
of	O
the	O
treated	O
and	O
control	O
loss	Metric
,	O
equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
change	O
of	O
variables	O
formula	O
and	O
Definition	O
[	O
reference	O
]	O
of	O
and	O
,	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
premise	O
that	O
for	O
,	O
and	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Definition	O
[	O
reference	O
]	O
of	O
an	O
IPM	Metric
.	O
∎	O
The	O
essential	O
point	O
in	O
the	O
proof	O
of	O
Lemma	O
[	O
reference	O
]	O
is	O
inequality	O
[	O
reference	O
]	O
.	O
Note	O
that	O
on	O
the	O
l.h.s	O
.	O
of	O
the	O
inequality	O
,	O
we	O
need	O
to	O
evaluate	O
the	O
expectations	O
of	O
over	O
and	O
over	O
.	O
Both	O
of	O
these	O
expectations	O
are	O
in	O
general	O
unavailable	O
,	O
since	O
they	O
require	O
us	O
to	O
evaluate	O
treatment	O
outcomes	O
on	O
the	O
control	O
,	O
and	O
control	O
outcomes	O
on	O
the	O
treated	O
.	O
We	O
therefore	O
upper	O
bound	O
these	O
unknowable	O
quantities	O
by	O
taking	O
a	O
supremum	O
over	O
a	O
function	O
family	O
which	O
includes	O
and	O
.	O
The	O
upper	O
bound	O
ignores	O
most	O
of	O
the	O
details	O
of	O
the	O
outcome	O
,	O
and	O
amounts	O
to	O
measuring	O
a	O
distance	O
between	O
two	O
distributions	O
we	O
have	O
samples	O
from	O
:	O
the	O
control	O
and	O
treated	O
distribution	O
.	O
Note	O
that	O
for	O
a	O
randomized	Task
trial	Task
(	O
i.e.	O
when	O
)	O
with	O
we	O
have	O
that	O
.	O
Indeed	O
,	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
in	O
that	O
case	O
we	O
actually	O
have	O
an	O
equality	O
:	O
.	O
The	O
crucial	O
condition	O
in	O
Lemma	O
[	O
reference	O
]	O
is	O
that	O
the	O
function	O
is	O
in	O
.	O
In	O
subsections	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
below	O
we	O
look	O
into	O
two	O
specific	O
function	O
families	O
,	O
and	O
evaluate	O
what	O
does	O
this	O
inclusion	O
condition	O
entail	O
,	O
and	O
in	O
particular	O
we	O
will	O
derive	O
specific	O
bounds	O
for	O
.	O
theorem	O
:	O
.	O
For	O
=	O
t0	O
,	O
1	O
define	O
:	O
Obviously	O
for	O
the	O
treatment	O
effect	O
we	O
have	O
.	O
Let	O
by	O
an	O
hypothesis	Method
,	O
such	O
that	O
for	O
a	O
representation	O
and	O
hypothesis	Method
defined	O
over	O
the	O
output	O
of	O
.	O
theorem	O
:	O
.	O
The	O
treatment	Task
effect	Task
estimate	Task
for	O
unit	O
x	O
is	O
:	O
theorem	O
:	O
.	O
The	O
expected	Metric
Precision	Metric
in	O
Estimation	O
of	O
Heterogeneous	O
Effect	O
(	O
PEHE	O
)	O
loss	Metric
of	O
g	O
is	O
:	O
theorem	O
:	O
.	O
The	O
expected	O
variance	O
of	O
Yt	O
with	O
respect	O
to	O
a	O
distribution	O
⁢p	O
(	O
x	O
,	O
t	O
)	O
:	O
We	O
define	O
:	O
If	O
are	O
deterministic	O
functions	O
of	O
,	O
then	O
.	O
We	O
now	O
show	O
that	O
is	O
upper	O
bounded	O
by	O
where	O
and	O
are	O
w.r.t	O
.	O
to	O
the	O
squared	O
loss	Metric
.	O
An	O
analogous	O
result	O
can	O
be	O
obtained	O
for	O
the	O
absolute	O
loss	Metric
,	O
using	O
mean	Metric
absolute	Metric
deviation	Metric
.	O
theorem	O
:	O
.	O
For	O
any	O
function	O
:	O
f→×X{0	O
,	O
1}Y	O
,	O
and	O
distribution	O
⁢p	O
(	O
x	O
,	O
t	O
)	O
over	O
×X{0	O
,	O
1	O
}	O
:	O
where	O
⁢ϵF	O
(	O
f	O
)	O
and	O
⁢ϵ⁢CF	O
(	O
f	O
)	O
are	O
w.r.t	O
.	O
to	O
the	O
squared	O
loss	Metric
.	O
proof	O
:	O
Proof	O
.	O
For	O
simplicity	O
we	O
will	O
prove	O
for	O
and	O
.	O
The	O
proof	O
for	O
and	O
is	O
identical	O
.	O
where	O
the	O
equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
Definition	O
[	O
reference	O
]	O
of	O
,	O
and	O
because	O
the	O
integral	O
in	O
(	O
[	O
reference	O
]	O
)	O
evaluates	O
to	O
zero	O
,	O
since	O
.	O
∎	O
theorem	O
:	O
.	O
Let	O
:	O
Φ→XR	O
be	O
a	O
one	O
-	O
to	O
-	O
one	O
representation	Method
function	Method
,	O
with	O
inverse	O
Ψ.	O
Let	O
p	O
=	O
t1Φ	O
,	O
p	O
=	O
t0Φ	O
be	O
defined	O
as	O
in	O
Definition	O
.	O
Let	O
u	O
=	O
p	O
(	O
t=1	O
)	O
.	O
Let	O
G	O
be	O
a	O
family	O
of	O
functions	O
:	O
g→RR	O
,	O
and	O
denote	O
by	O
⁢IPMG	Method
(	Method
⋅	Method
,	Method
⋅	Method
)	O
the	O
integral	O
probability	O
metric	O
induced	O
by	O
G.	O
Let	O
:	O
h→×R{0	O
,	O
1}Y	O
be	O
an	O
hypothesis	Method
.	O
Let	O
the	O
loss	Metric
=	O
⁢L	O
(	O
y1	O
,	O
y2	O
)(-	O
y1y2	O
)	O
2	O
.	O
Assume	O
there	O
exists	O
a	O
constant	O
>	O
BΦ0	O
,	O
such	O
that	O
for	O
∈t{0	O
,	O
1	O
}	O
,	O
the	O
functions	O
⁢gΦ	O
,	O
h	O
(	O
r	O
,	O
t	O
)	O
:=⁢⋅1BΦℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
∈G.	O
We	O
then	O
have	O
:	O
where	O
ϵF	O
and	O
ϵ⁢CF	O
are	O
with	O
respect	O
to	O
the	O
squared	O
loss	Metric
.	O
proof	O
:	O
Proof	O
.	O
We	O
will	O
prove	O
the	O
first	O
inequality	O
,	O
.	O
The	O
second	O
inequality	O
is	O
then	O
immediate	O
by	O
Lemma	O
[	O
reference	O
]	O
.	O
Recall	O
that	O
we	O
denote	O
for	O
.	O
where	O
(	O
[	O
reference	O
]	O
)	O
is	O
because	O
,	O
(	O
[	O
reference	O
]	O
)	O
is	O
because	O
and	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Lemma	O
[	O
reference	O
]	O
and	O
Definition	O
[	O
reference	O
]	O
of	O
the	O
losses	O
,	O
and	O
Definition	O
[	O
reference	O
]	O
of	O
.	O
Having	O
established	O
the	O
first	O
inequality	O
in	O
the	O
Theorem	O
statement	O
,	O
we	O
now	O
show	O
the	O
second	O
.	O
We	O
have	O
by	O
Lemma	O
[	O
reference	O
]	O
that	O
:	O
We	O
further	O
have	O
by	O
Lemma	O
[	O
reference	O
]	O
that	O
:	O
Therefore	O
∎	O
The	O
upper	O
bound	O
is	O
in	O
terms	O
of	O
the	O
standard	O
generalization	Metric
error	Metric
on	O
the	O
treated	O
and	O
control	O
distributions	O
separately	O
.	O
Note	O
that	O
in	O
some	O
cases	O
we	O
might	O
have	O
very	O
different	O
sample	O
sizes	O
for	O
treated	O
and	O
control	O
,	O
and	O
that	O
will	O
show	O
up	O
in	O
the	O
finite	Metric
sample	Metric
bounds	Metric
of	O
these	O
generalization	Metric
errors	Metric
.	O
We	O
also	O
note	O
that	O
the	O
upper	O
bound	O
can	O
be	O
easily	O
adapted	O
to	O
the	O
case	O
of	O
the	O
absolute	O
loss	Metric
PEHE	O
.	O
In	O
that	O
case	O
the	O
upper	O
bound	O
in	O
the	O
Theorem	O
will	O
have	O
a	O
factor	O
instead	O
of	O
the	O
stated	O
above	O
,	O
and	O
the	O
standard	O
deviation	O
replaced	O
by	O
mean	Metric
absolute	Metric
deviation	Metric
.	O
The	O
proof	O
is	O
straightforward	O
where	O
one	O
simply	O
applies	O
the	O
triangle	O
inequality	O
in	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
.	O
We	O
will	O
now	O
give	O
specific	O
upper	O
bounds	O
for	O
the	O
constant	O
in	O
Theorem	O
[	O
reference	O
]	O
,	O
using	O
two	O
function	Method
families	Method
in	O
the	O
IPM	Metric
:	O
the	O
family	Method
of	Method
-	Method
Lipschitz	Method
functions	Method
,	O
and	O
the	O
family	Method
of	Method
-	Method
norm	Method
reproducing	Method
kernel	Method
Hilbert	Method
space	Method
functions	Method
.	O
Each	O
one	O
will	O
have	O
different	O
assumptions	O
about	O
the	O
distribution	O
and	O
about	O
the	O
representation	O
and	O
hypothesis	Method
.	O
subsection	O
:	O
The	O
family	Method
of	Method
-	Method
Lipschitz	Method
functions	Method
For	O
,	O
a	O
function	O
has	O
Lipschitz	O
constant	O
if	O
for	O
all	O
,	O
.	O
If	O
is	O
differentiable	O
,	O
then	O
a	O
sufficient	O
condition	O
for	O
-	O
Lipschitz	O
constant	O
is	O
if	O
for	O
all	O
.	O
For	O
simplicity	O
’s	O
sake	O
we	O
assume	O
throughout	O
this	O
subsection	O
that	O
the	O
true	O
labeling	O
functions	O
the	O
densities	O
and	O
the	O
loss	Metric
are	O
differentiable	O
.	O
However	O
,	O
this	O
assumption	O
could	O
be	O
relaxed	O
to	O
a	O
mere	O
Lipschitzness	O
assumption	O
.	O
theorem	O
:	O
.	O
There	O
exists	O
a	O
constant	O
>	O
K0	O
such	O
that	O
for	O
all	O
∈xX	O
,	O
∈t{0	O
,	O
1	O
}	O
,	O
≤∥p	O
(	O
Yt|x	O
)	O
∂x∥K.	O
Assumption	O
[	O
reference	O
]	O
entails	O
that	O
each	O
of	O
the	O
potential	O
outcomes	O
change	O
smoothly	O
as	O
a	O
function	O
of	O
the	O
covariates	O
(	O
context	O
)	O
.	O
theorem	O
:	O
.	O
The	O
loss	Metric
function	O
L	O
is	O
differentiable	O
,	O
and	O
there	O
exists	O
a	O
constant	O
>	O
KL0	O
such	O
that	O
≤|⁢dL	O
(	O
y1	O
,	O
y2	O
)	O
⁢dyi|KL	O
for	O
=	O
i1	O
,	O
2	O
.	O
Additionally	O
,	O
there	O
exists	O
a	O
constant	O
M	O
such	O
that	O
for	O
all	O
∈y2Y	O
,	O
≥M∫Y⁢L	O
(	O
y1	O
,	O
y2	O
)	O
dy1	O
.	O
Assuming	O
is	O
compact	O
,	O
loss	Metric
functions	O
which	O
obey	O
Assumption	O
[	O
reference	O
]	O
include	O
the	O
log	O
-	O
loss	Metric
,	O
hinge	O
-	O
loss	Metric
,	O
absolute	O
loss	Metric
,	O
and	O
the	O
squared	O
loss	Metric
.	O
When	O
we	O
let	O
in	O
Definition	O
[	O
reference	O
]	O
be	O
the	O
family	O
of	O
-	O
Lipschitz	O
functions	O
,	O
we	O
obtain	O
the	O
so	O
-	O
called	O
1	O
-	O
Wasserstein	Metric
distance	O
between	O
distributions	O
,	O
which	O
we	O
denote	O
.	O
It	O
is	O
well	O
known	O
that	O
is	O
indeed	O
a	O
metric	Metric
between	Metric
distributions	Metric
.	O
theorem	O
:	O
.	O
Let	O
⁢∂Φ	O
(	O
x	O
)	O
∂x	O
be	O
the	O
Jacobian	O
matrix	O
of	O
Φ	O
at	O
point	O
x	O
,	O
i.e.	O
the	O
matrix	O
of	O
the	O
partial	O
derivatives	O
of	O
Φ.	O
Let	O
⁢σ⁢max	O
(	O
A	O
)	O
and	O
⁢σ⁢min	O
(	O
A	O
)	O
denote	O
respectively	O
the	O
largest	O
and	O
smallest	O
singular	O
values	O
of	O
a	O
matrix	O
A.	O
Define	O
=	O
⁢ρ	O
(	O
Φ	O
)	O
sup∈xX⁢	O
/	O
⁢σ⁢max	O
(	O
⁢∂Φ	O
(	O
x	O
)	O
∂x	O
)	O
σ⁢min	O
(	O
⁢∂Φ	O
(	O
x	O
)	O
∂x	O
)	O
.	O
It	O
is	O
an	O
immediate	O
result	O
that	O
.	O
theorem	O
:	O
.	O
We	O
will	O
call	O
a	O
representation	Method
function	Method
:	O
Φ→XR	O
Jacobian	O
-	O
normalized	O
if	O
=	O
sup∈xX⁢σ⁢max	O
(	O
⁢∂Φ	O
(	O
x	O
)	O
∂x	O
)	O
1	O
.	O
Note	O
that	O
any	O
non	O
-	O
constant	O
representation	Method
function	Method
can	O
be	O
Jacobian	O
-	O
normalized	O
by	O
a	O
simple	O
scalar	Method
multiplication	Method
.	O
theorem	O
:	O
.	O
Assume	O
that	O
Φ	O
is	O
a	O
Jacobian	Method
-	Method
normalized	Method
representation	Method
,	O
and	O
let	O
Ψ	O
be	O
its	O
inverse	O
.	O
For	O
=	O
t0	O
,	O
1	O
,	O
the	O
Lipschitz	O
constant	O
of	O
p	O
(	O
Yt|Ψ	O
(	O
r	O
)	O
)	O
is	O
bounded	O
by	O
⁢ρ	O
(	O
Φ	O
)	O
K	O
,	O
where	O
K	O
is	O
from	O
Assumption	O
,	O
and	O
⁢ρ	O
(	O
Φ	O
)	O
as	O
in	O
Definition	O
.	O
proof	O
:	O
Proof	O
.	O
Let	O
be	O
the	O
inverse	O
of	O
,	O
which	O
exists	O
by	O
the	O
assumption	O
that	O
is	O
one	O
-	O
to	O
-	O
one	O
.	O
Let	O
be	O
the	O
Jacobian	O
matrix	O
of	O
evaluated	O
at	O
,	O
and	O
similarly	O
let	O
be	O
the	O
Jacobian	O
matrix	O
of	O
evaluated	O
at	O
.	O
Note	O
that	O
for	O
,	O
since	O
is	O
the	O
identity	O
function	O
on	O
.	O
Therefore	O
for	O
any	O
and	O
:	O
where	O
and	O
are	O
respectively	O
the	O
largest	O
and	O
smallest	O
singular	O
values	O
of	O
the	O
matrix	O
,	O
i.e.	O
is	O
the	O
spectral	O
norm	O
of	O
.	O
For	O
and	O
,	O
we	O
have	O
by	O
the	O
chain	Method
rule	Method
:	O
where	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
matrix	O
norm	O
inequality	O
,	O
equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
(	O
[	O
reference	O
]	O
)	O
,	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
assumption	O
[	O
reference	O
]	O
on	O
the	O
norms	O
of	O
the	O
gradient	O
of	O
w.r.t	O
,	O
and	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Definition	O
[	O
reference	O
]	O
of	O
,	O
the	O
assumption	O
that	O
is	O
Jacobian	O
-	O
normalized	O
,	O
and	O
noting	O
that	O
singular	O
values	O
are	O
necessarily	O
non	O
-	O
negative	O
.	O
∎	O
theorem	O
:	O
.	O
Under	O
the	O
conditions	O
of	O
Lemma	O
,	O
further	O
assume	O
that	O
for	O
=	O
t0	O
,	O
1	O
,	O
p	O
(	O
Yt|x	O
)	O
has	O
gradients	O
bounded	O
by	O
K	O
as	O
in	O
,	O
that	O
h	O
has	O
bounded	O
gradient	O
norm	O
⁢bK	O
,	O
that	O
the	O
loss	Metric
L	O
has	O
bounded	O
gradient	O
norm	O
KL	O
,	O
and	O
that	O
Φ	O
is	O
Jacobian	O
-	O
normalized	O
.	O
Then	O
the	O
Lipschitz	O
constant	O
of	O
⁢ℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
is	O
upper	O
bounded	O
by	O
⁢⋅KLK	O
(+	O
⁢Mρ	O
(	O
Φ	O
)	O
b	O
)	O
for	O
=	O
t0	O
,	O
1	O
.	O
proof	O
:	O
Proof	O
.	O
Using	O
the	O
chain	Method
rule	Method
,	O
we	O
have	O
that	O
:	O
where	O
inequality	O
[	O
reference	O
]	O
is	O
due	O
to	O
Assumption	O
[	O
reference	O
]	O
and	O
inequality	O
[	O
reference	O
]	O
is	O
due	O
to	O
Lemma	O
[	O
reference	O
]	O
.	O
∎	O
theorem	O
:	O
.	O
Let	O
u	O
=	O
p	O
(	O
t=1	O
)	O
be	O
the	O
marginal	O
probability	O
of	O
treatment	O
,	O
and	O
assume	O
0<u<1	O
.	O
Let	O
:	O
Φ→XR	O
be	O
a	O
one	O
-	O
to	O
-	O
one	O
,	O
Jacobian	Method
-	Method
normalized	Method
representation	Method
function	Method
.	O
Let	O
K	O
be	O
the	O
Lipschitz	O
constant	O
of	O
the	O
functions	O
p	O
(	O
Yt|x	O
)	O
on	O
X.	O
Let	O
KL	O
be	O
the	O
Lipschitz	O
constant	O
of	O
the	O
loss	Metric
function	O
L	O
,	O
and	O
M	O
be	O
as	O
in	O
Assumption	O
.	O
Let	O
:	O
h→×R{0	O
,	O
1}R	O
be	O
an	O
hypothesis	Method
with	O
Lipschitz	O
constant	O
⁢bK.	O
Then	O
:	O
proof	O
:	O
Proof	O
.	O
We	O
will	O
apply	O
Lemma	O
[	O
reference	O
]	O
with	O
.	O
By	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
have	O
that	O
for	O
,	O
the	O
function	O
.	O
Inequality	O
(	O
[	O
reference	O
]	O
)	O
then	O
holds	O
as	O
a	O
special	O
case	O
of	O
Lemma	O
[	O
reference	O
]	O
.	O
∎	O
theorem	O
:	O
.	O
Under	O
the	O
assumptions	O
of	O
Lemma	O
,	O
using	O
the	O
squared	O
loss	Metric
for	O
ϵF	O
,	O
we	O
have	O
:	O
proof	O
:	O
Proof	O
.	O
Plug	O
in	O
the	O
upper	O
bound	O
of	O
Lemma	O
[	O
reference	O
]	O
into	O
the	O
upper	O
bound	O
of	O
Theorem	O
[	O
reference	O
]	O
.	O
∎	O
We	O
examine	O
the	O
constant	O
in	O
Theorem	O
[	O
reference	O
]	O
.	O
,	O
the	O
Lipschitz	O
constant	O
of	O
and	O
,	O
is	O
not	O
under	O
our	O
control	O
and	O
measures	O
an	O
aspect	O
of	O
the	O
complexity	O
of	O
the	O
true	O
underlying	O
functions	O
we	O
wish	O
to	O
approximate	O
.	O
The	O
terms	O
and	O
depend	O
on	O
our	O
choice	O
of	O
loss	Metric
function	O
and	O
the	O
size	O
of	O
the	O
space	O
.	O
The	O
term	O
comes	O
from	O
our	O
assumption	O
that	O
the	O
hypothesis	Method
has	O
norm	O
.	O
Note	O
that	O
smaller	O
,	O
while	O
reducing	O
the	O
bound	O
,	O
might	O
force	O
the	O
factual	O
loss	Metric
term	O
to	O
be	O
larger	O
since	O
a	O
small	O
implies	O
a	O
less	O
flexible	O
.	O
Finally	O
,	O
consider	O
the	O
term	O
.	O
The	O
assumption	O
that	O
is	O
normalized	O
is	O
rather	O
natural	O
,	O
as	O
we	O
do	O
not	O
expect	O
a	O
certain	O
scale	O
from	O
a	O
representation	O
.	O
Furthermore	O
,	O
below	O
we	O
show	O
that	O
in	O
fact	O
the	O
Wasserstein	Metric
distance	O
is	O
positively	O
homogeneous	O
with	O
respect	O
to	O
the	O
representation	O
.	O
Therefore	O
,	O
in	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
can	O
indeed	O
assume	O
that	O
is	O
normalized	O
.	O
The	O
specific	O
choice	O
of	O
Jacobian	Method
-	Method
normalized	Method
scaling	Method
yields	O
what	O
is	O
in	O
our	O
opinion	O
a	O
more	O
interpretable	O
result	O
in	O
terms	O
of	O
the	O
inverse	Metric
condition	Metric
number	Metric
.	O
For	O
twice	Task
-	Task
differentiable	Task
,	O
is	O
minimized	O
if	O
and	O
only	O
if	O
is	O
a	O
linear	Method
orthogonal	Method
transformation	Method
.	O
theorem	O
:	O
.	O
The	O
Wasserstein	Metric
distance	O
is	O
positive	O
homogeneous	O
for	O
scalar	O
transformations	O
of	O
the	O
underlying	O
space	O
.	O
Let	O
p	O
,	O
q	O
be	O
probability	O
density	O
functions	O
defined	O
over	O
X.	O
For	O
>	O
α0	O
and	O
the	O
mapping	O
=	O
⁢Φ	O
(	O
x	O
)	O
⁢αx	O
,	O
let	O
pα	O
and	O
qα	O
be	O
the	O
distributions	O
on	O
⁢αX	O
induced	O
by	O
Φ.	O
Then	O
:	O
proof	O
:	O
Proof	O
.	O
Following	O
,	O
we	O
use	O
another	O
characterization	O
of	O
the	O
Wasserstein	Metric
distance	Metric
.	O
Let	O
be	O
the	O
set	O
of	O
mass	O
preserving	O
maps	O
from	O
to	O
itself	O
which	O
map	O
the	O
distribution	O
to	O
the	O
distribution	O
.	O
That	O
is	O
,	O
.	O
We	O
then	O
have	O
that	O
:	O
It	O
is	O
known	O
that	O
the	O
infimum	O
in	O
(	O
[	O
reference	O
]	O
)	O
is	O
actually	O
achievable	O
.	O
Denote	O
by	O
the	O
map	O
achieving	O
the	O
infimum	O
for	O
.	O
Define	O
,	O
by	O
,	O
where	O
.	O
maps	O
to	O
,	O
and	O
we	O
have	O
that	O
.	O
Therefore	O
achieves	O
the	O
infimum	O
for	O
the	O
pair	O
,	O
and	O
we	O
have	O
that	O
.	O
∎	O
subsection	O
:	O
Functions	O
in	O
the	O
unit	O
ball	O
of	O
a	O
RKHS	Method
Let	O
be	O
a	O
reproducing	Method
kernel	Method
Hilbert	Method
space	Method
,	O
with	O
corresponding	O
kernels	O
,	O
.	O
We	O
have	O
for	O
all	O
that	O
is	O
its	O
Hilbert	Method
space	Method
mapping	Method
,	O
and	O
similarly	O
for	O
all	O
.	O
Recall	O
that	O
the	O
major	O
condition	O
in	O
Lemma	O
[	O
reference	O
]	O
is	O
that	O
.	O
The	O
function	O
space	O
we	O
use	O
here	O
is	O
.	O
We	O
will	O
focus	O
on	O
the	O
case	O
where	O
is	O
the	O
squared	O
loss	Metric
,	O
and	O
we	O
will	O
make	O
the	O
following	O
two	O
assumptions	O
:	O
theorem	O
:	O
.	O
There	O
exist	O
∈fY0	O
,	O
fY1Hx	O
such	O
that	O
=	O
⁢mt	O
(	O
x	O
)	O
⟨fYt	O
,	O
⁢kx	O
(	O
x	O
,	O
⋅	O
)	O
⟩Hx	O
,	O
i.e.	O
the	O
mean	O
potential	O
outcome	O
functions	O
m0	O
,	O
m1	O
are	O
in	O
Hx	O
.	O
Further	O
assume	O
that	O
≤∥fYt∥HxK.	O
theorem	O
:	O
.	O
Define	O
:	O
=	O
⁢ηYt	O
(	O
x	O
)	O
∫Y	O
(	O
Yt	O
-	O
mt	O
(	O
x	O
))	O
2p	O
(	O
Yt|x	O
)	O
.	O
⁢ηYt	O
(	O
x	O
)	O
is	O
the	O
standard	O
deviation	O
of	O
Yt|x	O
.	O
theorem	O
:	O
.	O
There	O
exists	O
∈fη0	O
,	O
fη1Hx	O
such	O
that	O
=	O
⁢ηYt	O
(	O
x	O
)	O
⟨fηt	O
,	O
⁢kx	O
(	O
x	O
,	O
⋅	O
)	O
⟩Hx	O
,	O
i.e.	O
the	O
conditional	Method
standard	Method
deviation	Method
functions	Method
of	O
Yt|x	O
are	O
in	O
Hx	O
.	O
Further	O
assume	O
that	O
≤∥fηt∥HxM.	O
theorem	O
:	O
.	O
Let	O
:	O
Φ→XY	O
be	O
an	O
invertible	O
representation	Method
function	Method
,	O
and	O
let	O
Ψ	O
be	O
its	O
inverse	O
.	O
We	O
assume	O
there	O
exists	O
a	O
bounded	Method
linear	Method
operator	Method
:	O
ΓΦ→HrHx	O
such	O
that	O
=	O
⟨fYt	O
,	O
⁢kx	O
(	O
⁢Ψ	O
(	O
r	O
),	O
⋅	O
)	O
⟩Hx⟨fYt	O
,	O
⁢ΓΦkr	O
(	O
r	O
,	O
⋅	O
)	O
⟩Hx	O
.	O
We	O
further	O
assume	O
that	O
the	O
Hilbert	O
-	O
Schmidt	O
norm	O
(	O
operator	O
norm	O
)	O
∥ΓΦ∥⁢HS	O
of	O
ΓΦ	O
is	O
bounded	O
by	O
KΦ	O
.	O
The	O
two	O
assumptions	O
above	O
amount	O
to	O
assuming	O
that	O
can	O
be	O
represented	O
as	O
one	O
-	O
to	O
-	O
one	O
linear	Method
map	Method
between	O
the	O
two	O
Hilbert	O
spaces	O
and	O
.	O
Under	O
Assumptions	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
about	O
,	O
and	O
,	O
we	O
have	O
that	O
,	O
where	O
is	O
the	O
adjoint	O
operator	O
of	O
.	O
theorem	O
:	O
.	O
Let	O
:	O
h→×R{0	O
,	O
1}R	O
be	O
an	O
hypothesis	Method
,	O
and	O
assume	O
that	O
there	O
exist	O
∈fhtHr	O
such	O
that	O
=	O
⁢h	O
(	O
r	O
,	O
t	O
)	O
⟨fht	O
,	O
⁢kr	O
(	O
r	O
,	O
⋅	O
)	O
⟩Hr	O
,	O
and	O
such	O
that	O
≤∥fht∥Hrb	O
.	O
Under	O
Assumption	O
about	O
m0	O
,	O
m1	O
,	O
we	O
have	O
that	O
ℓh	O
,	O
Φ	O
(	O
Ψ	O
(	O
r	O
),	O
t	O
)	O
=∫Y	O
(	O
Yt	O
-	O
h	O
(	O
r	O
,	O
t	O
))	O
2p	O
(	O
Yt|r	O
)	O
dYt	O
is	O
in	O
the	O
tensor	O
Hilbert	O
space	O
⊗HrHr	O
.	O
Moreover	O
,	O
the	O
norm	O
of	O
⁢ℓh	O
,	O
Φ	O
(	O
⁢Ψ	O
(	O
r	O
),	O
t	O
)	O
in	O
⊗HrHr	O
is	O
upper	O
bounded	O
by	O
⁢4	O
(+	O
⁢KΦ2K2b2	O
)	O
.	O
proof	O
:	O
Proof	O
.	O
We	O
first	O
decompose	O
into	O
a	O
noise	Method
and	O
mean	Method
fitting	Method
term	Method
,	O
using	O
:	O
where	O
equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Definition	O
[	O
reference	O
]	O
of	O
,	O
and	O
because	O
by	O
definition	O
of	O
.	O
Moving	O
to	O
,	O
recall	O
that	O
,	O
.	O
By	O
linearity	O
of	O
the	O
Hilbert	O
space	O
,	O
we	O
have	O
that	O
.	O
By	O
a	O
well	O
known	O
result	O
,	O
the	O
product	O
lies	O
in	O
the	O
tensor	O
product	O
space	O
,	O
and	O
is	O
equal	O
to	O
.	O
The	O
norm	O
of	O
this	O
function	O
in	O
is	O
.	O
This	O
is	O
the	O
general	O
Hilbert	O
space	O
version	O
of	O
the	O
fact	O
that	O
for	O
a	O
vector	O
one	O
has	O
that	O
,	O
where	O
is	O
the	O
matrix	O
Frobenius	O
norm	O
,	O
and	O
is	O
the	O
square	O
of	O
the	O
standard	O
Euclidean	O
norm	O
.	O
We	O
therefore	O
have	O
a	O
similar	O
result	O
for	O
,	O
using	O
Assumption	O
[	O
reference	O
]	O
:	O
.	O
The	O
norm	O
of	O
this	O
function	O
in	O
is	O
.	O
Overall	O
this	O
leads	O
us	O
to	O
conclude	O
,	O
using	O
Equation	O
(	O
[	O
reference	O
]	O
)	O
that	O
.	O
Now	O
we	O
have	O
,	O
using	O
(	O
[	O
reference	O
]	O
)	O
:	O
Inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
norms	O
given	O
above	O
and	O
the	O
triangle	O
inequality	O
.	O
Inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
because	O
for	O
any	O
Hilbert	O
space	O
,	O
.	O
Inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
the	O
definition	O
of	O
the	O
operator	O
norm	O
.	O
Equality	O
(	O
[	O
reference	O
]	O
)	O
is	O
because	O
the	O
norm	O
of	O
the	O
adjoint	O
operator	O
is	O
equal	O
to	O
the	O
norm	O
of	O
the	O
original	O
operator	O
,	O
where	O
we	O
abused	O
the	O
notation	O
to	O
mean	O
both	O
the	O
norm	O
of	O
operators	O
from	O
to	O
and	O
vice	O
-	O
versa	O
.	O
Finally	O
,	O
inequality	O
(	O
[	O
reference	O
]	O
)	O
is	O
by	O
Assumptions	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
,	O
and	O
by	O
the	O
Lemma	O
’s	O
premise	O
on	O
the	O
norm	O
of	O
.	O
∎	O
theorem	O
:	O
.	O
Let	O
u	O
=	O
p	O
(	O
t=1	O
)	O
be	O
the	O
marginal	O
probability	O
of	O
treatment	O
,	O
and	O
assume	O
0<u<1	O
.	O
Assume	O
the	O
distribution	O
of	O
Yt	O
conditioned	O
on	O
x	O
follows	O
Assumptions	O
with	O
constant	O
M.	O
Let	O
:	O
Φ→XR	O
be	O
a	O
one	O
-	O
to	O
-	O
one	O
representation	Method
function	Method
which	O
obeys	O
Assumption	O
with	O
corresponding	O
operator	O
ΓΦ	O
with	O
operator	O
norm	O
KΦ	O
.	O
Let	O
the	O
functions	O
Y0	O
,	O
Y1	O
obey	O
Assumption	O
,	O
with	O
bounded	O
Hilbert	O
space	O
norm	O
K	O
.	O
Let	O
:	O
h→×R{0	O
,	O
1}R	O
be	O
an	O
hypothesis	Method
,	O
and	O
assume	O
that	O
there	O
exist	O
∈fhtHr	O
such	O
that	O
=	O
⁢h	O
(	O
r	O
,	O
t	O
)	O
⟨fht	O
,	O
⁢kr	O
(	O
r	O
,	O
⋅	O
)	O
⟩Hr	O
,	O
such	O
that	O
≤∥fht∥Hrb	O
.	O
Assume	O
that	O
ϵF	O
and	O
ϵ⁢CF	O
are	O
defined	O
with	O
respect	O
to	O
L	O
being	O
the	O
squared	O
loss	Metric
.	O
Then	O
:	O
where	O
ϵ⁢CF	O
and	O
ϵF	O
use	O
the	O
squared	O
loss	Metric
.	O
proof	O
:	O
Proof	O
.	O
We	O
will	O
apply	O
Lemma	O
[	O
reference	O
]	O
with	O
.	O
By	O
Lemma	O
[	O
reference	O
]	O
,	O
we	O
have	O
that	O
for	O
and	O
being	O
the	O
squared	O
loss	Metric
,	O
.	O
Inequality	O
(	O
[	O
reference	O
]	O
)	O
then	O
holds	O
as	O
a	O
special	O
case	O
of	O
Lemma	O
[	O
reference	O
]	O
.	O
∎	O
theorem	O
:	O
.	O
Under	O
the	O
assumptions	O
of	O
Lemma	O
,	O
using	O
the	O
squared	O
loss	Metric
for	O
ϵF	O
,	O
we	O
have	O
:	O
proof	O
:	O
Proof	O
.	O
Plug	O
in	O
the	O
upper	O
bound	O
of	O
Lemma	O
[	O
reference	O
]	O
into	O
the	O
upper	O
bound	O
of	O
Theorem	O
[	O
reference	O
]	O
.	O
∎	O
appendix	O
:	O
Algorithmic	O
details	O
We	O
give	O
details	O
about	O
the	O
algorithms	O
used	O
in	O
our	O
framework	O
.	O
subsection	O
:	O
Minimizing	O
the	O
Wasserstein	Metric
distance	Metric
In	O
general	O
,	O
computing	O
(	O
and	O
minimizing	Task
)	O
the	O
Wasserstein	Metric
distance	Metric
involves	O
solving	O
a	O
linear	Method
program	Method
,	O
which	O
may	O
be	O
prohibitively	O
expensive	O
for	O
many	O
practical	O
applications	O
.	O
showed	O
that	O
an	O
approximation	O
based	O
on	O
entropic	Method
regularization	Method
can	O
be	O
obtained	O
through	O
the	O
Sinkhorn	Method
-	Method
Knopp	Method
matrix	Method
scaling	Method
algorithm	Method
,	O
at	O
orders	O
of	O
magnitude	O
faster	O
speed	O
.	O
Dubbed	O
Sinkhorn	O
distances	O
,	O
the	O
approximation	O
is	O
computed	O
using	O
a	O
fixed	Method
-	Method
point	Method
iteration	Method
involving	O
repeated	Method
multiplication	Method
with	O
a	O
kernel	O
matrix	O
.	O
We	O
can	O
use	O
the	O
algorithm	O
of	O
in	O
our	O
framework	O
.	O
See	O
Algorithm	O
[	O
reference	O
]	O
for	O
an	O
overview	O
of	O
how	O
to	O
compute	O
the	O
gradient	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
When	O
computing	O
,	O
disregarding	O
the	O
gradient	O
amounts	O
to	O
minimizing	O
an	O
upper	O
bound	O
on	O
the	O
Sinkhorn	O
transport	O
.	O
More	O
advanced	O
ideas	O
for	O
stochastic	Task
optimization	Task
of	O
this	O
distance	O
have	O
recently	O
proposed	O
by	O
,	O
and	O
might	O
be	O
used	O
in	O
future	O
work	O
.	O
[	O
tbp	O
]	O
Computing	O
the	O
stochastic	Method
gradient	Method
of	Method
the	Method
Wasserstein	Method
distance	Method
[	O
1	O
]	O
Input	O
:	O
Factual	O
,	O
representation	Method
network	Method
with	O
current	O
weights	O
by	O
Randomly	O
sample	O
a	O
mini	O
-	O
batch	O
with	O
treated	O
and	O
control	O
units	O
Calculate	O
the	O
pairwise	O
distance	O
matrix	O
between	O
all	O
treatment	O
and	O
control	O
pairs	O
:	O
Calculate	O
the	O
approximate	Method
optimal	Method
transport	Method
matrix	Method
using	O
Algorithm	O
3	O
of	O
,	O
with	O
input	O
Calculate	O
the	O
gradient	O
:	O
While	O
our	O
framework	O
is	O
agnostic	O
to	O
the	O
parameterization	O
of	O
,	O
our	O
experiments	O
focus	O
on	O
the	O
case	O
where	O
is	O
a	O
neural	Method
network	Method
.	O
For	O
convenience	O
of	O
implementation	O
,	O
we	O
may	O
represent	O
the	O
fixed	Method
-	Method
point	Method
iterations	Method
of	O
the	O
Sinkhorn	Method
algorithm	Method
as	O
a	O
recurrent	Method
neural	Method
network	Method
,	O
where	O
the	O
states	O
evolve	O
according	O
to	O
Here	O
,	O
is	O
a	O
kernel	O
matrix	O
corresponding	O
to	O
a	O
metric	O
such	O
as	O
the	O
euclidean	O
distance	O
,	O
,	O
and	O
are	O
the	O
sizes	O
of	O
the	O
control	O
and	O
treatment	O
groups	O
.	O
In	O
this	O
way	O
,	O
we	O
can	O
minimize	O
our	O
entire	O
objective	O
with	O
most	O
of	O
the	O
frameworks	O
commonly	O
used	O
for	O
training	O
neural	Method
networks	Method
,	O
out	O
of	O
the	O
box	O
.	O
subsection	O
:	O
Minimizing	O
the	O
maximum	Metric
mean	Metric
discrepancy	Metric
The	O
MMD	Metric
of	O
treatment	O
populations	O
in	O
the	O
representation	O
,	O
for	O
a	O
kernel	Method
can	O
be	O
written	O
as	O
,	O
The	O
linear	Metric
maximum	Metric
-	Metric
mean	Metric
discrepancy	Metric
can	O
be	O
written	O
as	O
a	O
distance	O
between	O
means	O
.	O
In	O
the	O
notation	O
of	O
Algorithm	O
[	O
reference	O
]	O
,	O
Let	O
Then	O
the	O
gradient	O
of	O
the	O
MMD	Metric
with	O
respect	O
to	O
is	O
,	O
appendix	O
:	O
Experimental	O
details	O
subsection	O
:	O
Hyperparameter	Method
selection	Method
Standard	O
methods	O
for	O
hyperparameter	Method
selection	Method
,	O
such	O
as	O
cross	Method
-	Method
validation	Method
,	O
are	O
not	O
generally	O
applicable	O
for	O
estimating	O
the	O
PEHE	O
loss	Metric
since	O
only	O
one	O
potential	O
outcome	O
is	O
observed	O
(	O
unless	O
the	O
outcome	O
is	O
simulated	O
)	O
.	O
For	O
real	O
-	O
world	O
data	O
,	O
we	O
may	O
use	O
the	O
observed	O
outcome	O
of	O
the	O
nearest	O
neighbor	O
to	O
in	O
the	O
opposite	O
treatment	O
group	O
,	O
as	O
surrogate	O
for	O
the	O
counterfactual	O
outcome	O
.	O
We	O
use	O
this	O
to	O
define	O
a	O
nearest	Method
-	Method
neighbor	Method
approximation	Method
of	Method
the	Method
PEHE	Method
loss	Method
,	O
.	O
On	O
IHDP	Material
,	O
we	O
use	O
the	O
objective	Metric
value	Metric
on	O
the	O
validation	O
set	O
for	O
early	Task
stopping	Task
in	O
CFR	Method
,	O
and	O
for	O
hyperparameter	Method
selection	Method
.	O
On	O
the	O
Jobs	Material
dataset	Material
,	O
we	O
use	O
the	O
policy	O
risk	O
on	O
the	O
validation	O
set	O
.	O
See	O
Table	O
[	O
reference	O
]	O
for	O
a	O
description	O
of	O
hyperparameters	O
and	O
search	O
ranges	O
.	O
subsection	O
:	O
Learned	Method
representations	Method
Figure	O
[	O
reference	O
]	O
show	O
the	O
representations	O
learned	O
by	O
our	O
CFR	Method
algorithm	O
.	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
subsection	O
:	O
Absolute	Metric
error	Metric
for	O
increasingly	O
imbalanced	O
data	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
same	O
experiment	O
as	O
Figure	O
2	O
of	O
the	O
main	O
paper	O
,	O
but	O
in	O
absolute	O
terms	O
.	O
