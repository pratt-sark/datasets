Convolutional	Method
Sequence	Method
to	Method
Sequence	Method
Learning	Method
section	O
:	O
Abstract	O
The	O
prevalent	O
approach	O
to	O
sequence	Task
to	Task
sequence	Task
learning	Task
maps	O
an	O
input	O
sequence	O
to	O
a	O
variable	O
length	O
output	O
sequence	O
via	O
recurrent	Method
neural	Method
networks	Method
.	O
We	O
introduce	O
an	O
architecture	O
based	O
entirely	O
on	O
convolutional	Method
neural	Method
networks	Method
.	O
Compared	O
to	O
recurrent	Method
models	Method
,	O
computations	O
over	O
all	O
elements	O
can	O
be	O
fully	O
parallelized	O
during	O
training	Task
to	O
better	O
exploit	O
the	O
GPU	O
hardware	O
and	O
optimization	Task
is	O
easier	O
since	O
the	O
number	O
of	O
non	O
-	O
linearities	O
is	O
fixed	O
and	O
independent	O
of	O
the	O
input	O
length	O
.	O
Our	O
use	O
of	O
gated	Method
linear	Method
units	Method
eases	O
gradient	Method
propagation	Method
and	O
we	O
equip	O
each	O
decoder	Method
layer	Method
with	O
a	O
separate	O
attention	Method
module	Method
.	O
We	O
outperform	O
the	O
accuracy	Metric
of	O
the	O
deep	Method
LSTM	Method
setup	Method
of	O
[	O
reference	O
]	O
on	O
both	O
WMT'14	Material
English	Material
-	Material
German	Material
and	O
WMT'14	Material
English	Material
-	Material
French	Material
translation	Material
at	O
an	O
order	O
of	O
magnitude	O
faster	O
speed	O
,	O
both	O
on	O
GPU	O
and	O
CPU	Method
.	O
section	O
:	O
Introduction	O
Sequence	Method
to	Method
sequence	Method
learning	Method
has	O
been	O
successful	O
in	O
many	O
tasks	O
such	O
as	O
machine	Task
translation	Task
,	O
speech	Task
recognition	Task
[	O
reference	O
]	O
and	O
text	Task
summarization	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
amongst	O
others	O
.	O
The	O
dominant	O
approach	O
to	O
date	O
encodes	O
the	O
input	O
sequence	O
with	O
a	O
series	O
of	O
bi	Method
-	Method
directional	Method
recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
and	O
generates	O
a	O
variable	O
length	O
output	O
with	O
another	O
set	O
of	O
decoder	Method
RNNs	Method
,	O
both	O
of	O
which	O
interface	O
via	O
a	O
soft	Method
-	Method
attention	Method
mechanism	Method
[	O
reference	O
]	O
.	O
In	O
machine	Task
translation	Task
,	O
this	O
architecture	O
has	O
been	O
demonstrated	O
to	O
outperform	O
traditional	O
phrase	Method
-	Method
based	Method
models	Method
by	O
large	O
margins	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Convolutional	Method
neural	Method
networks	Method
are	O
less	O
common	O
for	O
sequence	Task
modeling	Task
,	O
despite	O
several	O
advantages	O
[	O
reference	O
][	O
reference	O
]	O
.	O
Compared	O
to	O
recurrent	Method
layers	Method
,	O
convolutions	Method
create	O
representations	Method
for	O
fixed	O
size	O
contexts	O
,	O
however	O
,	O
the	O
effective	O
context	Metric
size	Metric
of	O
the	O
network	O
can	O
easily	O
be	O
made	O
larger	O
by	O
stacking	O
several	O
layers	O
on	O
top	O
of	O
each	O
other	O
.	O
This	O
allows	O
to	O
precisely	O
control	O
the	O
maximum	O
length	O
of	O
dependencies	O
to	O
be	O
modeled	O
.	O
Convolutional	Method
networks	Method
do	O
not	O
depend	O
on	O
the	O
computations	O
of	O
the	O
previous	O
time	O
step	O
and	O
therefore	O
allow	O
parallelization	O
over	O
every	O
element	O
in	O
a	O
sequence	O
.	O
This	O
contrasts	O
with	O
RNNs	Method
which	O
maintain	O
a	O
hidden	O
state	O
of	O
the	O
entire	O
past	O
that	O
prevents	O
parallel	O
computation	O
within	O
a	O
sequence	O
.	O
Multi	Method
-	Method
layer	Method
convolutional	Method
neural	Method
networks	Method
create	O
hierarchical	Method
representations	Method
over	O
the	O
input	O
sequence	O
in	O
which	O
nearby	O
input	O
elements	O
interact	O
at	O
lower	O
layers	O
while	O
distant	O
elements	O
interact	O
at	O
higher	O
layers	O
.	O
Hierarchical	O
structure	O
provides	O
a	O
shorter	O
path	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
compared	O
to	O
the	O
chain	O
structure	O
modeled	O
by	O
recurrent	Method
networks	Method
,	O
e.g.	O
we	O
can	O
obtain	O
a	O
feature	Method
representation	Method
capturing	O
relationships	O
within	O
a	O
window	O
of	O
n	O
words	O
by	O
applying	O
only	O
O	O
(	O
n	O
k	O
)	O
convolutional	Method
operations	Method
for	O
kernels	O
of	O
width	O
k	O
,	O
compared	O
to	O
a	O
linear	O
number	O
O	O
(	O
n	O
)	O
for	O
recurrent	Method
neural	Method
networks	Method
.	O
Inputs	O
to	O
a	O
convolutional	Method
network	Method
are	O
fed	O
through	O
a	O
constant	O
number	O
of	O
kernels	O
and	O
non	O
-	O
linearities	O
,	O
whereas	O
recurrent	Method
networks	Method
apply	O
up	O
to	O
n	O
operations	O
and	O
non	O
-	O
linearities	O
to	O
the	O
first	O
word	O
and	O
only	O
a	O
single	O
set	O
of	O
operations	O
to	O
the	O
last	O
word	O
.	O
Fixing	O
the	O
number	O
of	O
nonlinearities	O
applied	O
to	O
the	O
inputs	O
also	O
eases	O
learning	Task
.	O
Recent	O
work	O
has	O
applied	O
convolutional	Method
neural	Method
networks	Method
to	O
sequence	Task
modeling	Task
such	O
as	O
[	O
reference	O
]	O
who	O
introduce	O
recurrent	Method
pooling	Method
between	O
a	O
succession	O
of	O
convolutional	Method
layers	Method
or	O
[	O
reference	O
]	O
who	O
tackle	O
neural	Task
translation	Task
without	Task
attention	Task
.	O
However	O
,	O
none	O
of	O
these	O
approaches	O
has	O
been	O
demonstrated	O
improvements	O
over	O
state	O
of	O
the	O
art	O
results	O
on	O
large	O
benchmark	O
datasets	O
.	O
Gated	Method
convolutions	Method
have	O
been	O
previously	O
explored	O
for	O
machine	Task
translation	Task
by	O
[	O
reference	O
]	O
but	O
their	O
evaluation	O
was	O
restricted	O
to	O
a	O
small	O
dataset	O
and	O
the	O
model	O
was	O
used	O
in	O
tandem	O
with	O
a	O
traditional	O
count	Method
-	Method
based	Method
model	Method
.	O
Architec	O
-	O
section	O
:	O
arXiv:1705.03122v3	O
[	O
cs	O
.	O
CL	O
]	O
25	O
Jul	O
2017	O
tures	Method
which	O
are	O
partially	Method
convolutional	Method
have	O
shown	O
strong	O
performance	O
on	O
larger	O
tasks	O
but	O
their	O
decoder	O
is	O
still	O
recurrent	O
[	O
reference	O
]	O
.	O
In	O
this	O
paper	O
we	O
propose	O
an	O
architecture	O
for	O
sequence	Task
to	Task
sequence	Task
modeling	Task
that	O
is	O
entirely	O
convolutional	Method
.	O
Our	O
model	O
is	O
equipped	O
with	O
gated	Method
linear	Method
units	Method
and	O
residual	Method
connections	Method
[	O
reference	O
]	O
.	O
We	O
also	O
use	O
attention	O
in	O
every	O
decoder	Method
layer	Method
and	O
demonstrate	O
that	O
each	O
attention	Method
layer	Method
only	O
adds	O
a	O
negligible	O
amount	O
of	O
overhead	O
.	O
The	O
combination	O
of	O
these	O
choices	O
enables	O
us	O
to	O
tackle	O
large	Task
scale	Task
problems	Task
(	O
§	O
3	O
)	O
.	O
We	O
evaluate	O
our	O
approach	O
on	O
several	O
large	O
datasets	O
for	O
machine	Task
translation	Task
as	O
well	O
as	O
summarization	Task
and	O
compare	O
to	O
the	O
current	O
best	O
architectures	O
reported	O
in	O
the	O
literature	O
.	O
On	O
WMT'16	Material
English	O
-	O
Romanian	O
translation	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
,	O
outperforming	O
the	O
previous	O
best	O
result	O
by	O
1.9	O
BLEU	Metric
.	O
On	O
WMT'14	Material
English	Material
-	Material
German	Material
we	O
outperform	O
the	O
strong	O
LSTM	Method
setup	Method
of	O
[	O
reference	O
]	O
by	O
0.5	O
BLEU	Metric
and	O
on	O
WMT'14	Material
English	Material
-	Material
French	Material
we	O
outperform	O
the	O
likelihood	Method
trained	Method
system	Method
of	O
[	O
reference	O
]	O
by	O
1.6	O
BLEU	Metric
.	O
Furthermore	O
,	O
our	O
model	O
can	O
translate	O
unseen	O
sentences	O
at	O
an	O
order	O
of	O
magnitude	O
faster	O
speed	O
than	O
[	O
reference	O
]	O
on	O
GPU	O
and	O
CPU	O
hardware	O
(	O
§	O
4	O
,	O
§	O
5	O
)	O
.	O
section	O
:	O
Recurrent	Task
Sequence	Task
to	Task
Sequence	Task
Learning	Task
Sequence	Task
to	Task
sequence	Task
modeling	Task
has	O
been	O
synonymous	O
with	O
recurrent	Method
neural	Method
network	Method
based	Method
encoder	Method
-	Method
decoder	Method
architectures	Method
[	O
reference	O
]	O
.	O
The	O
encoder	Method
RNN	Method
processes	O
an	O
input	O
sequence	O
x	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
m	O
)	O
of	O
m	O
elements	O
and	O
returns	O
state	Method
representations	Method
z	O
=	O
(	O
z	O
1	O
.	O
.	O
.	O
.	O
,	O
z	O
m	O
)	O
.	O
The	O
decoder	Method
RNN	Method
takes	O
z	O
and	O
generates	O
the	O
output	O
sequence	O
y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
n	O
)	O
left	O
to	O
right	O
,	O
one	O
element	O
at	O
a	O
time	O
.	O
To	O
generate	O
output	O
y	O
i	O
+	O
1	O
,	O
the	O
decoder	O
computes	O
a	O
new	O
hidden	O
state	O
h	O
i	O
+	O
1	O
based	O
on	O
the	O
previous	O
state	O
h	O
i	O
,	O
an	O
embedding	O
g	O
i	O
of	O
the	O
previous	O
target	O
language	O
word	O
y	O
i	O
,	O
as	O
well	O
as	O
a	O
conditional	O
input	O
c	O
i	O
derived	O
from	O
the	O
encoder	O
output	O
z.	O
Based	O
on	O
this	O
generic	O
formulation	O
,	O
various	O
encoder	Method
-	Method
decoder	Method
architectures	Method
have	O
been	O
proposed	O
,	O
which	O
differ	O
mainly	O
in	O
the	O
conditional	O
input	O
and	O
the	O
type	O
of	O
RNN	Method
.	O
Models	O
without	O
attention	O
consider	O
only	O
the	O
final	O
encoder	O
state	O
z	O
m	O
by	O
setting	O
c	O
i	O
=	O
z	O
m	O
for	O
all	O
i	O
,	O
or	O
simply	O
initialize	O
the	O
first	O
decoder	O
state	O
with	O
z	O
m	O
,	O
in	O
which	O
case	O
c	O
i	O
is	O
not	O
used	O
.	O
Architectures	O
with	O
attention	O
[	O
reference	O
]	O
compute	O
c	O
i	O
as	O
a	O
weighted	O
sum	O
of	O
(	O
z	O
1	O
.	O
.	O
.	O
.	O
,	O
z	O
m	O
)	O
at	O
each	O
time	O
step	O
.	O
The	O
weights	O
of	O
the	O
sum	O
are	O
referred	O
to	O
as	O
attention	Metric
scores	Metric
and	O
allow	O
the	O
network	O
to	O
focus	O
on	O
different	O
parts	O
of	O
the	O
input	O
sequence	O
as	O
it	O
generates	O
the	O
output	O
sequences	O
.	O
Attention	O
scores	O
are	O
computed	O
by	O
essentially	O
comparing	O
each	O
encoder	Method
state	O
z	O
j	O
to	O
a	O
combination	O
of	O
the	O
previous	O
decoder	O
state	O
h	O
i	O
and	O
the	O
last	O
prediction	O
y	O
i	O
;	O
the	O
result	O
is	O
normalized	O
to	O
be	O
a	O
distribution	O
over	O
input	O
elements	O
.	O
Popular	O
choices	O
for	O
recurrent	Method
networks	Method
in	O
encoder	Method
-	Method
decoder	Method
models	Method
are	O
long	Method
short	Method
term	Method
memory	Method
networks	Method
(	O
LSTM	Method
;	O
[	O
reference	O
]	O
and	O
gated	Method
recurrent	Method
units	Method
(	O
GRU	Method
;	O
[	O
reference	O
]	O
.	O
Both	O
extend	O
Elman	Method
RNNs	Method
[	O
reference	O
]	O
)	O
with	O
a	O
gating	Method
mechanism	Method
that	O
allows	O
the	O
memorization	O
of	O
information	O
from	O
previous	O
time	O
steps	O
in	O
order	O
to	O
model	O
long	O
-	O
term	O
dependencies	O
.	O
Most	O
recent	O
approaches	O
also	O
rely	O
on	O
bi	Method
-	Method
directional	Method
encoders	Method
to	O
build	O
representations	O
of	O
both	O
past	O
and	O
future	O
contexts	O
[	O
reference	O
][	O
reference	O
]	O
.	O
Models	O
with	O
many	O
layers	O
often	O
rely	O
on	O
shortcut	O
or	O
residual	O
connections	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
section	O
:	O
A	O
Convolutional	Method
Architecture	Method
Next	O
we	O
introduce	O
a	O
fully	Method
convolutional	Method
architecture	Method
for	O
sequence	Task
to	Task
sequence	Task
modeling	Task
.	O
Instead	O
of	O
relying	O
on	O
RNNs	Method
to	O
compute	O
intermediate	O
encoder	O
states	O
z	O
and	O
decoder	O
states	O
h	O
we	O
use	O
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
.	O
section	O
:	O
Position	Task
Embeddings	Task
First	O
,	O
we	O
embed	O
input	O
elements	O
x	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
m	O
)	O
in	O
distributional	O
space	O
as	O
w	O
=	O
(	O
w	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
m	O
)	O
,	O
where	O
w	O
j	O
∈	O
R	O
f	O
is	O
a	O
column	O
in	O
an	O
embedding	Method
matrix	Method
D	O
∈	O
R	O
V	O
×f	O
.	O
We	O
also	O
equip	O
our	O
model	O
with	O
a	O
sense	O
of	O
order	O
by	O
embedding	O
the	O
absolute	O
position	O
of	O
input	O
elements	O
p	O
=	O
(	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
m	O
)	O
where	O
p	O
j	O
∈	O
R	O
f	O
.	O
Both	O
are	O
combined	O
to	O
obtain	O
input	O
element	Method
representations	Method
e	O
=	O
(	O
w	O
1	O
+	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
m	O
+	O
p	O
m	O
)	O
.	O
We	O
proceed	O
similarly	O
for	O
output	O
elements	O
that	O
were	O
already	O
generated	O
by	O
the	O
decoder	Method
network	Method
to	O
yield	O
output	Method
element	Method
representations	Method
that	O
are	O
being	O
fed	O
back	O
into	O
the	O
decoder	Method
network	Method
g	O
=	O
(	O
g	O
1	O
,	O
.	O
.	O
.	O
,	O
g	O
n	O
)	O
.	O
Position	Method
embeddings	Method
are	O
useful	O
in	O
our	O
architecture	O
since	O
they	O
give	O
our	O
model	O
a	O
sense	O
of	O
which	O
portion	O
of	O
the	O
sequence	O
in	O
the	O
input	O
or	O
output	O
it	O
is	O
currently	O
dealing	O
with	O
(	O
§	O
5.4	O
)	O
.	O
section	O
:	O
Convolutional	Method
Block	Method
Structure	Method
Both	O
encoder	Method
and	Method
decoder	Method
networks	Method
share	O
a	O
simple	O
block	Method
structure	Method
that	O
computes	O
intermediate	O
states	O
based	O
on	O
a	O
fixed	O
number	O
of	O
input	O
elements	O
.	O
We	O
denote	O
the	O
output	O
of	O
the	O
lth	O
block	O
as	O
h	O
l	O
=	O
(	O
h	O
l	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
l	O
n	O
)	O
for	O
the	O
decoder	Method
network	Method
,	O
and	O
z	O
l	O
=	O
(	O
z	O
l	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
l	O
m	O
)	O
for	O
the	O
encoder	Method
network	Method
;	O
we	O
refer	O
to	O
blocks	O
and	O
layers	O
interchangeably	O
.	O
Each	O
block	O
contains	O
a	O
one	Method
dimensional	Method
convolution	Method
followed	O
by	O
a	O
non	Method
-	Method
linearity	Method
.	O
For	O
a	O
decoder	Method
network	Method
with	O
a	O
single	O
block	O
and	O
kernel	O
width	O
k	O
,	O
each	O
resulting	O
state	O
h	O
1	O
i	O
contains	O
information	O
over	O
k	O
input	O
elements	O
.	O
Stacking	O
several	O
blocks	O
on	O
top	O
of	O
each	O
other	O
increases	O
the	O
number	O
of	O
input	O
elements	O
represented	O
in	O
a	O
state	O
.	O
For	O
instance	O
,	O
stacking	O
6	O
blocks	O
with	O
k	O
=	O
5	O
results	O
in	O
an	O
input	O
field	O
of	O
25	O
elements	O
,	O
i.e.	O
each	O
output	O
depends	O
on	O
25	O
inputs	O
.	O
Non	O
-	O
linearities	O
allow	O
the	O
networks	O
to	O
exploit	O
the	O
full	O
input	O
field	O
,	O
or	O
to	O
focus	O
on	O
fewer	O
elements	O
if	O
needed	O
.	O
section	O
:	O
Each	O
convolution	Method
kernel	Method
is	O
parameterized	O
as	O
and	O
takes	O
as	O
input	O
X	O
∈	O
R	O
k×d	O
which	O
is	O
a	O
concatenation	O
of	O
k	O
input	O
elements	O
embedded	O
in	O
d	O
dimensions	O
and	O
maps	O
them	O
to	O
a	O
single	O
output	O
element	O
Y	O
∈	O
R	O
2d	O
that	O
has	O
twice	O
the	O
dimensionality	O
of	O
the	O
input	O
elements	O
;	O
subsequent	O
layers	O
operate	O
over	O
the	O
k	O
output	O
elements	O
of	O
the	O
previous	O
layer	O
.	O
We	O
choose	O
gated	Method
linear	Method
units	Method
(	O
GLU	Method
;	O
[	O
reference	O
]	O
as	O
non	O
-	O
linearity	O
which	O
implement	O
a	O
simple	O
gating	Method
mechanism	Method
over	O
the	O
output	O
of	O
the	O
convolu	Method
-	Method
where	O
A	O
,	O
B	O
∈	O
R	O
d	O
are	O
the	O
inputs	O
to	O
the	O
non	O
-	O
linearity	O
,	O
⊗	O
is	O
the	O
point	O
-	O
wise	O
multiplication	O
and	O
the	O
output	O
v	O
(	O
is	O
half	O
the	O
size	O
of	O
Y	O
.	O
The	O
gates	O
σ	O
(	O
B	O
)	O
control	O
which	O
inputs	O
A	O
of	O
the	O
current	O
context	O
are	O
relevant	O
.	O
A	O
similar	O
nonlinearity	O
has	O
been	O
introduced	O
in	O
[	O
reference	O
]	O
who	O
apply	O
tanh	Method
to	O
A	O
but	O
[	O
reference	O
]	O
shows	O
that	O
GLUs	Method
perform	O
better	O
in	O
the	O
context	O
of	O
language	Task
modelling	Task
.	O
To	O
enable	O
deep	Method
convolutional	Method
networks	Method
,	O
we	O
add	O
residual	O
connections	O
from	O
the	O
input	O
of	O
each	O
convolution	O
to	O
the	O
output	O
of	O
the	O
block	O
[	O
reference	O
]	O
.	O
For	O
encoder	Method
networks	Method
we	O
ensure	O
that	O
the	O
output	O
of	O
the	O
convolutional	O
layers	O
matches	O
the	O
input	O
length	O
by	O
padding	O
the	O
input	O
at	O
each	O
layer	O
.	O
However	O
,	O
for	O
decoder	Task
networks	Task
we	O
have	O
to	O
take	O
care	O
that	O
no	O
future	O
information	O
is	O
available	O
to	O
the	O
decoder	O
[	O
reference	O
]	O
.	O
Specifically	O
,	O
we	O
pad	O
the	O
input	O
by	O
k	O
−	O
1	O
elements	O
on	O
both	O
the	O
left	O
and	O
right	O
side	O
by	O
zero	O
vectors	O
,	O
and	O
then	O
remove	O
k	O
elements	O
from	O
the	O
end	O
of	O
the	O
convolution	O
output	O
.	O
We	O
also	O
add	O
linear	O
mappings	O
to	O
project	O
between	O
the	O
embedding	O
size	O
f	O
and	O
the	O
convolution	O
outputs	O
that	O
are	O
of	O
size	O
2d	O
.	O
We	O
apply	O
such	O
a	O
transform	O
to	O
w	O
when	O
feeding	O
embeddings	O
to	O
the	O
encoder	Method
network	Method
,	O
to	O
the	O
encoder	O
output	O
z	O
We	O
introduce	O
a	O
separate	O
attention	Method
mechanism	Method
for	O
each	O
decoder	Method
layer	Method
.	O
To	O
compute	O
the	O
attention	O
,	O
we	O
combine	O
the	O
current	O
decoder	O
state	O
h	O
l	O
i	O
with	O
an	O
embedding	O
of	O
the	O
previous	O
Figure	O
1	O
.	O
Illustration	O
of	O
batching	Method
during	O
training	O
.	O
The	O
English	Material
source	Material
sentence	Material
is	O
encoded	O
(	O
top	O
)	O
and	O
we	O
compute	O
all	O
attention	O
values	O
for	O
the	O
four	O
German	O
target	O
words	O
(	O
center	O
)	O
simultaneously	O
.	O
Our	O
attentions	O
are	O
just	O
dot	O
products	O
between	O
decoder	Method
context	Method
representations	Method
(	O
bottom	O
left	O
)	O
and	O
encoder	Method
representations	Method
.	O
We	O
add	O
the	O
conditional	O
inputs	O
computed	O
by	O
the	O
attention	O
(	O
center	O
right	O
)	O
to	O
the	O
decoder	O
states	O
which	O
then	O
predict	O
the	O
target	O
words	O
(	O
bottom	O
right	O
)	O
.	O
The	O
sigmoid	O
and	O
multiplicative	O
boxes	O
illustrate	O
Gated	Method
Linear	Method
Units	Method
.	O
target	O
element	O
g	O
i	O
:	O
For	O
decoder	O
layer	O
l	O
the	O
attention	O
a	O
l	O
ij	O
of	O
state	O
i	O
and	O
source	O
element	O
j	O
is	O
computed	O
as	O
a	O
dot	O
-	O
product	O
between	O
the	O
decoder	O
state	O
summary	O
d	O
The	O
conditional	O
input	O
c	O
l	O
i	O
to	O
the	O
current	O
decoder	Method
layer	Method
is	O
a	O
weighted	O
sum	O
of	O
the	O
encoder	O
outputs	O
as	O
well	O
as	O
the	O
input	O
element	O
embeddings	O
e	O
j	O
(	O
Figure	O
1	O
,	O
center	O
right	O
)	O
:	O
This	O
is	O
slightly	O
different	O
to	O
recurrent	Method
approaches	Method
which	O
compute	O
both	O
the	O
attention	O
and	O
the	O
weighted	Method
sum	Method
over	O
z	O
u	O
j	O
only	O
.	O
We	O
found	O
adding	O
e	O
j	O
to	O
be	O
beneficial	O
and	O
it	O
resembles	O
key	Method
-	Method
value	Method
memory	Method
networks	Method
where	O
the	O
keys	O
are	O
the	O
z	O
u	O
j	O
and	O
the	O
values	O
are	O
the	O
z	O
u	O
j	O
+	O
e	O
j	O
[	O
reference	O
]	O
.	O
Encoder	O
outputs	O
z	O
u	O
j	O
represent	O
potentially	O
large	O
input	O
contexts	O
and	O
e	O
j	O
provides	O
point	O
information	O
about	O
a	O
specific	O
input	O
element	O
that	O
is	O
useful	O
when	O
making	O
a	O
prediction	Task
.	O
Once	O
c	O
l	O
i	O
has	O
been	O
computed	O
,	O
it	O
is	O
simply	O
added	O
to	O
the	O
output	O
of	O
the	O
corresponding	O
decoder	O
layer	O
h	O
l	O
i	O
.	O
This	O
can	O
be	O
seen	O
as	O
attention	O
with	O
multiple	O
'	O
hops	O
'	O
[	O
reference	O
]	O
compared	O
to	O
single	O
step	O
attention	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
In	O
particular	O
,	O
the	O
attention	O
of	O
the	O
first	O
layer	O
determines	O
a	O
useful	O
source	O
context	O
which	O
is	O
then	O
fed	O
to	O
the	O
second	O
layer	O
that	O
takes	O
this	O
information	O
into	O
account	O
when	O
computing	O
attention	O
etc	O
.	O
The	O
decoder	O
also	O
has	O
immediate	O
access	O
to	O
the	O
attention	O
history	O
of	O
the	O
k	O
−	O
1	O
previous	O
time	O
steps	O
because	O
the	O
conditional	O
inputs	O
c	O
which	O
are	O
input	O
to	O
h	O
l	O
i	O
.	O
This	O
makes	O
it	O
easier	O
for	O
the	O
model	O
to	O
take	O
into	O
account	O
which	O
previous	O
inputs	O
have	O
been	O
attended	O
to	O
already	O
compared	O
to	O
recurrent	Method
nets	Method
where	O
this	O
information	O
is	O
in	O
the	O
recurrent	O
state	O
and	O
needs	O
to	O
survive	O
several	O
non	O
-	O
linearities	O
.	O
Overall	O
,	O
our	O
attention	Method
mechanism	Method
considers	O
which	O
words	O
we	O
previously	O
attended	O
to	O
[	O
reference	O
]	O
and	O
performs	O
multiple	O
attention	O
'	O
hops	O
'	O
per	O
time	O
step	O
.	O
In	O
Appendix	O
§	O
C	O
,	O
we	O
plot	O
attention	Metric
scores	Metric
for	O
a	O
deep	Method
decoder	Method
and	O
show	O
that	O
at	O
different	O
layers	O
,	O
different	O
portions	O
of	O
the	O
source	O
are	O
attended	O
to	O
.	O
Our	O
convolutional	Method
architecture	Method
also	O
allows	O
to	O
batch	O
the	O
attention	Task
computation	Task
across	O
all	O
elements	O
of	O
a	O
sequence	O
compared	O
to	O
RNNs	Method
(	O
Figure	O
1	O
,	O
middle	O
)	O
.	O
We	O
batch	O
the	O
computations	O
of	O
each	O
decoder	Method
layer	Method
individually	O
.	O
section	O
:	O
Normalization	Method
Strategy	Method
We	O
stabilize	O
learning	Task
through	O
careful	O
weight	Method
initialization	Method
(	O
§	O
3.5	O
)	O
and	O
by	O
scaling	O
parts	O
of	O
the	O
network	O
to	O
ensure	O
that	O
the	O
variance	O
throughout	O
the	O
network	O
does	O
not	O
change	O
dramatically	O
.	O
In	O
particular	O
,	O
we	O
scale	O
the	O
output	O
of	O
residual	O
blocks	O
as	O
well	O
as	O
the	O
attention	O
to	O
preserve	O
the	O
variance	O
of	O
activations	O
.	O
We	O
multiply	O
the	O
sum	O
of	O
the	O
input	O
and	O
output	O
of	O
a	O
residual	O
block	O
by	O
√	O
0.5	O
to	O
halve	O
the	O
variance	O
of	O
the	O
sum	O
.	O
This	O
assumes	O
that	O
both	O
summands	O
have	O
the	O
same	O
variance	O
which	O
is	O
not	O
always	O
true	O
but	O
effective	O
in	O
practice	O
.	O
The	O
conditional	O
input	O
c	O
l	O
i	O
generated	O
by	O
the	O
attention	Method
is	O
a	O
weighted	O
sum	O
of	O
m	O
vectors	O
(	O
2	O
)	O
and	O
we	O
counteract	O
a	O
change	O
in	O
variance	O
through	O
scaling	O
by	O
m	O
1	O
/	O
m	O
;	O
we	O
multiply	O
by	O
m	O
to	O
scale	O
up	O
the	O
inputs	O
to	O
their	O
original	O
size	O
,	O
assuming	O
the	O
attention	Metric
scores	Metric
are	O
uniformly	O
distributed	O
.	O
This	O
is	O
generally	O
not	O
the	O
case	O
but	O
we	O
found	O
it	O
to	O
work	O
well	O
in	O
practice	O
.	O
For	O
convolutional	Method
decoders	Method
with	O
multiple	O
attention	O
,	O
we	O
scale	O
the	O
gradients	O
for	O
the	O
encoder	O
layers	O
by	O
the	O
number	O
of	O
attention	O
mechanisms	O
we	O
use	O
;	O
we	O
exclude	O
source	O
word	O
embeddings	O
.	O
We	O
found	O
this	O
to	O
stabilize	O
learning	Task
since	O
the	O
encoder	Method
received	O
too	O
much	O
gradient	O
otherwise	O
.	O
section	O
:	O
Initialization	O
Normalizing	O
activations	O
when	O
adding	O
the	O
output	O
of	O
different	O
layers	O
,	O
e.g.	O
residual	O
connections	O
,	O
requires	O
careful	O
weight	Method
initialization	Method
.	O
The	O
motivation	O
for	O
our	O
initialization	O
is	O
the	O
same	O
as	O
for	O
the	O
normalization	Task
:	O
maintain	O
the	O
variance	O
of	O
activations	O
throughout	O
the	O
forward	O
and	O
backward	O
passes	O
.	O
All	O
embeddings	O
are	O
initialized	O
from	O
a	O
normal	Method
distribution	Method
with	O
mean	O
0	O
and	O
standard	O
deviation	O
0.1	O
.	O
For	O
layers	O
whose	O
output	O
is	O
not	O
directly	O
fed	O
to	O
a	O
gated	Method
linear	Method
unit	Method
,	O
we	O
initialize	O
weights	O
from	O
N	O
(	O
0	O
,	O
1	O
/	O
n	O
l	O
)	O
where	O
n	O
l	O
is	O
the	O
number	O
of	O
input	O
connections	O
to	O
each	O
neuron	O
.	O
This	O
ensures	O
that	O
the	O
variance	O
of	O
a	O
normally	O
distributed	O
input	O
is	O
retained	O
.	O
For	O
layers	O
which	O
are	O
followed	O
by	O
a	O
GLU	Method
activation	Method
,	O
we	O
propose	O
a	O
weight	Method
initialization	Method
scheme	Method
by	O
adapting	O
the	O
derivations	O
in	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
If	O
the	O
GLU	O
inputs	O
are	O
distributed	O
with	O
mean	O
0	O
and	O
have	O
sufficiently	O
small	O
variance	O
,	O
then	O
we	O
can	O
approximate	O
the	O
output	O
variance	O
with	O
1	O
/	O
4	O
of	O
the	O
input	O
variance	O
(	O
Appendix	O
A.1	O
)	O
.	O
Hence	O
,	O
we	O
initialize	O
the	O
weights	O
so	O
that	O
the	O
input	O
to	O
the	O
GLU	O
activations	O
have	O
4	O
times	O
the	O
variance	O
of	O
the	O
layer	O
input	O
.	O
This	O
is	O
achieved	O
by	O
drawing	O
their	O
initial	O
values	O
from	O
N	O
(	O
0	O
,	O
4	O
/	O
n	O
l	O
)	O
.	O
Biases	O
are	O
uniformly	O
set	O
to	O
zero	O
when	O
the	O
network	O
is	O
constructed	O
.	O
We	O
apply	O
dropout	Method
to	O
the	O
input	O
of	O
some	O
layers	O
so	O
that	O
inputs	O
are	O
retained	O
with	O
a	O
probability	O
of	O
p.	O
This	O
can	O
be	O
seen	O
as	O
multiplication	O
with	O
a	O
Bernoulli	O
random	O
variable	O
taking	O
value	O
1	O
/	O
p	O
with	O
probability	O
p	O
and	O
0	O
otherwise	O
[	O
reference	O
]	O
.	O
The	O
application	O
of	O
dropout	O
will	O
then	O
cause	O
the	O
variance	O
to	O
be	O
scaled	O
by	O
1	O
/	O
p	O
.	O
We	O
aim	O
to	O
restore	O
the	O
incoming	O
variance	O
by	O
initializing	O
the	O
respective	O
layers	O
with	O
larger	O
weights	O
.	O
Specifically	O
,	O
we	O
use	O
N	O
(	O
0	O
,	O
4p	O
/	O
n	O
l	O
)	O
for	O
layers	O
whose	O
output	O
is	O
subject	O
to	O
a	O
GLU	O
and	O
N	O
(	O
0	O
,	O
p	O
/	O
n	O
l	O
)	O
otherwise	O
(	O
Appendix	O
A.3	O
)	O
.	O
section	O
:	O
Experimental	O
Setup	O
section	O
:	O
Datasets	O
We	O
consider	O
three	O
major	O
WMT	Task
translation	Task
tasks	Task
as	O
well	O
as	O
a	O
text	Task
summarization	Task
task	Task
.	O
WMT'16	Material
English	O
-	O
Romanian	O
.	O
We	O
use	O
the	O
same	O
data	O
and	O
pre	O
-	O
processing	O
as	O
[	O
reference	O
]	O
but	O
remove	O
sentences	O
with	O
more	O
than	O
175	O
words	O
.	O
This	O
results	O
in	O
2.8	O
M	O
sentence	O
pairs	O
for	O
training	O
and	O
we	O
evaluate	O
on	O
newstest2016	O
.	O
We	O
experiment	O
with	O
word	Method
-	Method
based	Method
models	Method
using	O
a	O
source	O
vocabulary	O
of	O
200	O
K	O
types	O
and	O
a	O
target	O
vocabulary	O
of	O
80	O
K	O
types	O
.	O
We	O
also	O
consider	O
a	O
joint	O
source	Method
and	Method
target	Method
byte	Method
-	Method
pair	Method
encoding	Method
(	O
BPE	Method
)	O
with	O
40	O
K	O
types	O
[	O
reference	O
][	O
reference	O
]	O
.	O
WMT'14	Material
English	Material
-	Material
German	Material
.	O
We	O
use	O
the	O
same	O
setup	O
as	O
[	O
reference	O
]	O
which	O
comprises	O
4.5	O
M	O
sentence	O
pairs	O
for	O
training	O
and	O
we	O
test	O
on	O
newstest2014	O
.	O
section	O
:	O
3	O
As	O
vocabulary	O
we	O
use	O
40	O
K	O
sub	O
-	O
word	O
types	O
based	O
on	O
BPE	Method
.	O
WMT'14	Material
English	Material
-	Material
French	Material
.	O
We	O
use	O
the	O
full	O
training	O
set	O
of	O
36	O
M	O
sentence	O
pairs	O
,	O
and	O
remove	O
sentences	O
longer	O
than	O
175	O
words	O
as	O
well	O
as	O
pairs	O
with	O
a	O
source	O
/	O
target	O
length	O
ratio	O
exceeding	O
1.5	O
.	O
This	O
results	O
in	O
35.5	O
M	O
sentence	O
-	O
pairs	O
for	O
training	O
.	O
Results	O
are	O
reported	O
on	O
newstest2014	O
.	O
We	O
use	O
a	O
source	O
and	O
target	O
vocabulary	O
with	O
40	O
K	O
BPE	O
types	O
.	O
In	O
all	O
setups	O
a	O
small	O
subset	O
of	O
the	O
training	O
data	O
serves	O
as	O
validation	O
set	O
(	O
about	O
0.5	O
-	O
1	O
%	O
for	O
each	O
dataset	O
)	O
for	O
early	Task
stopping	Task
and	O
learning	Method
rate	Method
annealing	Method
.	O
Abstractive	Task
summarization	Task
.	O
We	O
train	O
on	O
the	O
Gigaword	Material
corpus	Material
[	O
reference	O
]	O
and	O
pre	O
-	O
process	O
it	O
identically	O
to	O
[	O
reference	O
]	O
resulting	O
in	O
3.8	O
M	O
training	O
examples	O
and	O
190	O
K	O
for	O
validation	O
.	O
We	O
evaluate	O
on	O
the	O
DUC	Material
-	Material
2004	Material
test	Material
data	Material
comprising	O
500	O
article	O
-	O
title	O
pairs	O
[	O
reference	O
]	O
and	O
report	O
three	O
variants	O
of	O
recall	Method
-	Method
based	Method
ROUGE	Method
[	O
reference	O
]	O
,	O
namely	O
,	O
ROUGE	Method
-	Method
1	Method
(	Method
unigrams	Method
)	O
,	O
ROUGE	Method
-	Method
2	Method
(	O
bigrams	Method
)	O
,	O
and	O
ROUGE	Method
-	Method
L	Method
(	O
longest	Method
-	Method
common	Method
substring	Method
)	O
.	O
We	O
also	O
evaluate	O
on	O
a	O
Gigaword	Material
test	Material
set	Material
of	O
2000	O
pairs	O
which	O
is	O
identical	O
to	O
the	O
one	O
used	O
by	O
[	O
reference	O
]	O
and	O
we	O
report	O
F1	Metric
ROUGE	Metric
similar	O
to	O
prior	O
work	O
.	O
Similar	O
to	O
[	O
reference	O
]	O
we	O
use	O
a	O
source	O
and	O
target	O
vocabulary	O
of	O
30	O
K	O
words	O
and	O
require	O
outputs	O
to	O
be	O
at	O
least	O
14	O
words	O
long	O
.	O
section	O
:	O
Model	O
Parameters	O
and	O
Optimization	Task
We	O
use	O
512	O
hidden	O
units	O
for	O
both	O
encoders	Method
and	O
decoders	Method
,	O
unless	O
otherwise	O
stated	O
.	O
All	O
embeddings	O
,	O
including	O
the	O
output	O
produced	O
by	O
the	O
decoder	Method
before	O
the	O
final	O
linear	Method
layer	Method
,	O
have	O
dimensionality	O
512	O
;	O
we	O
use	O
the	O
same	O
dimensionalities	O
for	O
linear	Method
layers	Method
mapping	O
between	O
the	O
hidden	O
and	O
embedding	O
sizes	O
(	O
§	O
3.2	O
)	O
.	O
We	O
train	O
our	O
convolutional	Method
models	Method
with	O
Nesterov	Method
's	Method
accelerated	Method
gradient	Method
method	Method
[	O
reference	O
]	O
)	O
using	O
a	O
momentum	O
value	O
of	O
0.99	O
and	O
renormalize	O
gradients	O
if	O
their	O
norm	O
exceeds	O
0.1	O
[	O
reference	O
]	O
.	O
We	O
use	O
a	O
learning	Metric
rate	Metric
of	O
0.25	O
and	O
once	O
the	O
validation	Metric
perplexity	Metric
stops	O
improving	O
,	O
we	O
reduce	O
the	O
learning	Metric
rate	Metric
by	O
an	O
order	O
of	O
magnitude	O
after	O
each	O
epoch	O
until	O
it	O
falls	O
below	O
10	O
−4	O
.	O
Unless	O
otherwise	O
stated	O
,	O
we	O
use	O
mini	O
-	O
batches	O
of	O
64	O
sentences	O
.	O
We	O
restrict	O
the	O
maximum	O
number	O
of	O
words	O
in	O
a	O
mini	O
-	O
batch	O
to	O
make	O
sure	O
that	O
batches	O
with	O
long	O
sentences	O
backtranslations	O
/	O
en	O
-	O
ro	O
.	O
3	O
http:	O
//	O
nlp.stanford.edu	O
/	O
projects	O
/	O
nmt	O
still	O
fit	O
in	O
GPU	O
memory	O
.	O
If	O
the	O
threshold	O
is	O
exceeded	O
,	O
we	O
simply	O
split	O
the	O
batch	O
until	O
the	O
threshold	O
is	O
met	O
and	O
process	O
the	O
parts	O
separatedly	O
.	O
Gradients	O
are	O
normalized	O
by	O
the	O
number	O
of	O
non	O
-	O
padding	O
tokens	O
per	O
mini	O
-	O
batch	O
.	O
We	O
also	O
use	O
weight	Method
normalization	Method
for	O
all	O
layers	O
except	O
for	O
lookup	O
tables	O
[	O
reference	O
]	O
.	O
Besides	O
dropout	O
on	O
the	O
embeddings	O
and	O
the	O
decoder	O
output	O
,	O
we	O
also	O
apply	O
dropout	Method
to	O
the	O
input	O
of	O
the	O
convolutional	Method
blocks	Method
[	O
reference	O
]	O
.	O
All	O
models	O
are	O
implemented	O
in	O
Torch	Method
[	O
reference	O
]	O
and	O
trained	O
on	O
a	O
single	O
Nvidia	Method
M40	Method
GPU	Method
except	O
for	O
WMT'14	Material
EnglishFrench	Material
for	O
which	O
we	O
use	O
a	O
multi	O
-	O
GPU	O
setup	O
on	O
a	O
single	O
machine	O
.	O
We	O
train	O
on	O
up	O
to	O
eight	O
GPUs	O
synchronously	O
by	O
maintaining	O
copies	O
of	O
the	O
model	O
on	O
each	O
card	O
and	O
split	O
the	O
batch	O
so	O
that	O
each	O
worker	O
computes	O
1	O
/	O
8	O
-	O
th	O
of	O
the	O
gradients	O
;	O
at	O
the	O
end	O
we	O
sum	O
the	O
gradients	O
via	O
Nvidia	Method
NCCL	Method
.	O
section	O
:	O
Evaluation	O
We	O
report	O
average	O
results	O
over	O
three	O
runs	O
of	O
each	O
model	O
,	O
where	O
each	O
differs	O
only	O
in	O
the	O
initial	O
random	O
seed	O
.	O
Translations	O
are	O
generated	O
by	O
a	O
beam	Method
search	Method
and	O
we	O
normalize	O
log	O
-	O
likelihood	O
scores	O
by	O
sentence	O
length	O
.	O
We	O
use	O
a	O
beam	O
of	O
width	O
5	O
.	O
We	O
divide	O
the	O
log	O
-	O
likelihoods	O
of	O
the	O
final	O
hypothesis	O
in	O
beam	Task
search	Task
by	O
their	O
length	O
|y|.	O
For	O
WMT'14	Material
English	Material
-	Material
German	Material
we	O
tune	O
a	O
length	O
normalization	O
constant	O
on	O
a	O
separate	O
development	O
set	O
(	O
newstest2015	O
)	O
and	O
we	O
normalize	O
log	O
-	O
likelihoods	O
by	O
|y|	O
α	O
[	O
reference	O
]	O
.	O
On	O
other	O
datasets	O
we	O
did	O
not	O
find	O
any	O
benefit	O
with	O
length	Method
normalization	Method
.	O
For	O
word	Method
-	Method
based	Method
models	Method
,	O
we	O
perform	O
unknown	Task
word	Task
replacement	Task
based	O
on	O
attention	Metric
scores	Metric
after	O
generation	O
[	O
reference	O
]	O
.	O
Unknown	O
words	O
are	O
replaced	O
by	O
looking	O
up	O
the	O
source	O
word	O
with	O
the	O
maximum	O
attention	O
score	O
in	O
a	O
precomputed	O
dictionary	O
.	O
If	O
the	O
dictionary	O
contains	O
no	O
translation	O
,	O
then	O
we	O
simply	O
copy	O
the	O
source	O
word	O
.	O
Dictionaries	O
were	O
extracted	O
from	O
the	O
word	O
aligned	O
training	O
data	O
that	O
we	O
obtained	O
with	O
fast	O
align	O
[	O
reference	O
]	O
.	O
Each	O
source	O
word	O
is	O
mapped	O
to	O
the	O
target	O
word	O
it	O
is	O
most	O
frequently	O
aligned	O
to	O
.	O
In	O
our	O
multi	Method
-	Method
step	Method
attention	Method
(	O
§	O
3.3	O
)	O
we	O
simply	O
average	O
the	O
attention	Metric
scores	Metric
over	O
all	O
layers	O
.	O
Finally	O
,	O
we	O
compute	O
case	O
-	O
sensitive	O
tokenized	O
BLEU	Metric
,	O
except	O
for	O
WMT'16	Material
English	O
-	O
Romanian	O
where	O
we	O
use	O
detokenized	O
BLEU	Metric
to	O
be	O
comparable	O
with	O
[	O
reference	O
]	O
5	O
.	O
Results	O
section	O
:	O
Recurrent	Method
vs.	Method
Convolutional	Method
Models	Method
We	O
first	O
evaluate	O
our	O
convolutional	Method
model	Method
on	O
three	O
translation	Task
tasks	Task
.	O
On	O
WMT'16	Material
English	O
-	O
Romanian	O
translation	O
we	O
compare	O
to	O
[	O
reference	O
]	O
which	O
is	O
the	O
winning	O
entry	O
on	O
this	O
language	O
pair	O
at	O
WMT'16	Material
[	O
reference	O
]	O
.	O
Their	O
model	O
implements	O
the	O
attention	Method
-	Method
based	Method
sequence	Method
to	Method
sequence	Method
architecture	Method
of	O
[	O
reference	O
]	O
and	O
uses	O
GRU	Method
cells	Method
both	O
in	O
the	O
encoder	Method
and	Method
decoder	Method
.	O
We	O
test	O
both	O
word	Method
-	Method
based	Method
and	Method
BPE	Method
vocabularies	Method
(	O
§	O
4	O
)	O
.	O
Table	O
1	O
shows	O
that	O
our	O
fully	Method
convolutional	Method
sequence	Method
to	Method
sequence	Method
model	Method
(	O
ConvS2S	Method
)	O
outperforms	O
the	O
WMT'16	Material
winning	Material
entry	Material
for	O
English	Material
-	Material
Romanian	Material
by	O
1.9	O
BLEU	Metric
with	O
a	O
BPE	Method
encoding	Method
and	O
by	O
1.3	O
BLEU	Metric
with	O
a	O
word	O
factored	O
vocabulary	O
.	O
This	O
instance	O
of	O
our	O
architecture	O
has	O
20	O
layes	O
in	O
the	O
encoder	Method
and	O
20	O
layers	O
in	O
the	O
decoder	Method
,	O
both	O
using	O
kernels	O
of	O
width	O
3	O
and	O
hidden	O
size	O
512	O
throughout	O
.	O
Training	Task
took	O
between	O
6	O
and	O
7.5	O
days	O
on	O
a	O
single	O
GPU	O
.	O
On	O
WMT'14	Task
English	Task
to	Task
German	Task
translation	Task
we	O
compare	O
to	O
the	O
following	O
prior	O
work	O
:	O
[	O
reference	O
]	O
is	O
based	O
on	O
a	O
four	Method
layer	Method
LSTM	Method
attention	Method
model	Method
,	O
ByteNet	Method
[	O
reference	O
]	O
)	O
propose	O
a	O
convolutional	Method
model	Method
based	O
on	O
characters	Method
without	Method
attention	Method
,	O
with	O
30	O
layers	O
in	O
the	O
encoder	Method
and	O
30	O
layers	O
in	O
the	O
decoder	O
,	O
GNMT	Method
[	O
reference	O
]	O
represents	O
the	O
state	O
of	O
the	O
art	O
on	O
this	O
dataset	O
and	O
they	O
use	O
eight	O
encoder	Method
LSTMs	Method
as	O
well	O
as	O
eight	O
decoder	Method
LSTMs	Method
,	O
we	O
quote	O
their	O
result	O
for	O
a	O
word	Method
-	Method
based	Method
model	Method
,	O
such	O
as	O
ours	O
,	O
as	O
well	O
as	O
a	O
word	Method
-	Method
piece	Method
model	Method
[	O
reference	O
]	O
.	O
section	O
:	O
5	O
The	O
results	O
(	O
Table	O
1	O
)	O
show	O
that	O
our	O
convolutional	Method
model	Method
outpeforms	O
GNMT	Method
by	O
0.5	O
BLEU	Metric
.	O
Our	O
encoder	Method
has	O
15	O
layers	O
and	O
the	O
decoder	Method
has	O
15	O
layers	O
,	O
both	O
with	O
512	O
hidden	O
units	O
in	O
the	O
first	O
ten	O
layers	O
and	O
768	O
units	O
in	O
the	O
subsequent	O
three	O
layers	O
,	O
all	O
using	O
kernel	O
width	O
3	O
.	O
The	O
final	O
two	O
layers	O
have	O
2048	O
units	O
which	O
are	O
just	O
linear	Method
mappings	Method
with	O
a	O
single	O
input	O
.	O
We	O
trained	O
this	O
model	O
on	O
a	O
single	O
GPU	Method
over	O
a	O
period	O
of	O
18.5	O
days	O
with	O
a	O
batch	O
size	O
of	O
48	O
.	O
LSTM	Method
sparse	Method
mixtures	Method
have	O
shown	O
strong	O
accuracy	Metric
at	O
26.03	O
BLEU	Metric
for	O
a	O
single	O
run	O
[	O
reference	O
]	O
)	O
which	O
compares	O
to	O
25.39	O
BLEU	Metric
for	O
our	O
best	O
run	O
.	O
This	O
mixture	O
sums	O
the	O
output	O
of	O
four	O
experts	O
,	O
not	O
unlike	O
an	O
ensemble	Method
which	O
sums	O
the	O
output	O
of	O
multiple	O
networks	O
.	O
ConvS2S	Method
also	O
benefits	O
from	O
ensembling	Method
(	O
§	O
5.2	O
)	O
,	O
therefore	O
mixtures	O
are	O
a	O
promising	O
direction	O
.	O
Finally	O
,	O
we	O
train	O
on	O
the	O
much	O
larger	O
WMT'14	Material
EnglishFrench	Material
task	Material
where	O
we	O
compare	O
to	O
the	O
state	O
of	O
the	O
art	O
result	O
of	O
GNMT	Method
[	O
reference	O
]	O
.	O
Our	O
model	O
is	O
trained	O
with	O
a	O
simple	O
token	Metric
-	Metric
level	Metric
likelihood	Metric
objective	Metric
and	O
we	O
improve	O
over	O
GNMT	Method
in	O
the	O
same	O
setting	O
by	O
1.6	O
BLEU	Metric
on	O
average	O
.	O
We	O
also	O
outperform	O
their	O
reinforcement	Method
(	Method
RL	Method
)	Method
models	Method
by	O
0.5	O
[	O
reference	O
]	O
We	O
did	O
not	O
use	O
the	O
exact	O
same	O
vocabulary	O
size	O
because	O
word	O
pieces	O
and	O
BPE	Method
estimate	O
the	O
vocabulary	O
differently	O
.	O
The	O
translations	O
produced	O
by	O
our	O
models	O
often	O
match	O
the	O
length	O
of	O
the	O
references	O
,	O
particularly	O
for	O
the	O
large	O
WMT'14	Material
English	Material
-	Material
French	Material
task	Material
,	O
or	O
are	O
very	O
close	O
for	O
small	O
to	O
medium	O
data	O
sets	O
such	O
as	O
WMT'14	Material
English	Material
-	Material
German	Material
or	O
WMT'16	Material
English	O
-	O
Romanian	O
.	O
section	O
:	O
WMT'16	Metric
English	Metric
-	Metric
Romanian	Metric
BLEU	Metric
section	O
:	O
Ensemble	O
Results	O
Next	O
,	O
we	O
ensemble	O
eight	O
likelihood	Method
-	Method
trained	Method
models	Method
for	O
both	O
WMT'14	Material
English	Material
-	Material
German	Material
and	O
WMT'14	Material
English	Material
-	Material
French	Material
and	O
compare	O
to	O
previous	O
work	O
which	O
also	O
reported	O
ensemble	O
results	O
.	O
For	O
the	O
former	O
,	O
we	O
also	O
show	O
the	O
result	O
when	O
ensembling	O
10	O
models	O
.	O
Table	O
2	O
shows	O
that	O
we	O
outperform	O
the	O
best	O
current	O
ensembles	O
on	O
both	O
datasets	O
.	O
section	O
:	O
Generation	Metric
Speed	Metric
Next	O
,	O
we	O
evaluate	O
the	O
inference	Metric
speed	Metric
of	O
our	O
architecture	O
on	O
the	O
development	O
set	O
of	O
the	O
WMT'14	Material
English	Material
-	Material
French	Material
task	Material
which	O
is	O
the	O
concatenation	O
of	O
newstest2012	O
and	O
newstest2013	O
;	O
it	O
comprises	O
6003	O
sentences	O
.	O
We	O
measure	O
generation	Metric
speed	Metric
both	O
on	O
GPU	O
and	O
CPU	O
hardware	O
.	O
Specifically	O
,	O
we	O
measure	O
GPU	Metric
speed	Metric
on	O
three	O
generations	O
of	O
Nvidia	Method
cards	Method
:	O
a	O
GTX	Method
-	Method
1080ti	Method
,	O
an	O
M40	Method
as	O
well	O
as	O
an	O
older	O
K40	Method
card	Method
.	O
CPU	Metric
timings	Metric
are	O
measured	O
on	O
one	O
host	O
with	O
48	O
hyperthreaded	O
cores	O
(	O
Intel	O
Xeon	O
E5	O
-	O
2680	O
@	O
2.50GHz	O
)	O
with	O
40	O
workers	O
.	O
In	O
all	O
settings	O
,	O
we	O
batch	O
up	O
to	O
128	O
sentences	O
,	O
composing	O
batches	O
with	O
sentences	O
of	O
equal	O
length	O
.	O
Note	O
that	O
the	O
majority	O
of	O
batches	O
is	O
smaller	O
because	O
of	O
the	O
small	O
size	O
of	O
the	O
development	O
set	O
.	O
We	O
experiment	O
with	O
beams	O
of	O
size	O
5	O
as	O
well	O
as	O
greedy	Method
search	Method
,	O
i.e	O
beam	O
of	O
size	O
1	O
.	O
To	O
make	O
generation	Task
fast	O
,	O
we	O
do	O
not	O
recompute	O
convolution	O
states	O
that	O
have	O
not	O
changed	O
compared	O
to	O
the	O
previous	O
time	O
step	O
but	O
rather	O
copy	O
(	O
shift	O
)	O
these	O
activations	O
.	O
We	O
compare	O
to	O
results	O
reported	O
in	O
[	O
reference	O
]	O
use	O
Nvidia	Method
K80	Method
GPUs	Method
which	O
are	O
essentially	O
two	O
K40s	O
.	O
We	O
did	O
not	O
have	O
such	O
a	O
GPU	Method
available	O
and	O
therefore	O
run	O
experiments	O
on	O
an	O
older	O
K40	Method
card	Method
which	O
is	O
inferior	O
to	O
a	O
K80	Method
,	O
in	O
addition	O
to	O
the	O
newer	O
M40	Method
and	Method
GTX	Method
-	Method
1080ti	Method
cards	Method
.	O
The	O
results	O
(	O
Table	O
3	O
)	O
show	O
that	O
our	O
model	O
can	O
generate	O
translations	O
on	O
a	O
K40	O
GPU	O
at	O
9.3	O
times	O
the	O
speed	O
and	O
2.25	O
higher	O
BLEU	Metric
;	O
on	O
an	O
M40	O
the	O
speed	O
-	O
up	O
is	O
up	O
to	O
13.7	O
times	O
and	O
on	O
a	O
GTX	O
-	O
1080ti	O
card	O
the	O
speed	O
is	O
21.3	O
times	O
faster	O
.	O
A	O
larger	O
beam	O
of	O
size	O
5	O
decreases	O
speed	O
but	O
gives	O
better	O
BLEU	Metric
.	O
On	O
CPU	Method
,	O
our	O
model	O
is	O
up	O
to	O
9.3	O
times	O
faster	O
,	O
however	O
,	O
the	O
GNMT	Material
CPU	Material
results	O
were	O
obtained	O
with	O
an	O
88	O
core	O
machine	O
whereas	O
our	O
results	O
were	O
obtained	O
with	O
just	O
over	O
half	O
the	O
number	O
of	O
cores	O
.	O
On	O
a	O
per	O
CPU	O
core	O
basis	O
,	O
our	O
model	O
is	O
17	O
times	O
faster	O
at	O
a	O
better	O
BLEU	Metric
.	O
Finally	O
,	O
our	O
CPU	Metric
speed	Metric
is	O
2.7	O
times	O
higher	O
than	O
GNMT	Method
on	O
a	O
custom	O
TPU	Method
chip	Method
which	O
shows	O
that	O
high	O
speed	O
can	O
be	O
achieved	O
on	O
commodity	O
hardware	O
.	O
We	O
do	O
no	O
report	O
TPU	O
figures	O
as	O
we	O
do	O
not	O
have	O
access	O
to	O
this	O
hardware	O
.	O
section	O
:	O
Position	Task
Embeddings	Task
In	O
the	O
following	O
sections	O
,	O
we	O
analyze	O
the	O
design	O
choices	O
in	O
our	O
architecture	O
.	O
The	O
remaining	O
results	O
in	O
this	O
paper	O
are	O
based	O
on	O
the	O
WMT'14	Material
English	Material
-	Material
German	Material
task	O
with	O
13	O
encoder	O
layers	O
at	O
kernel	O
size	O
3	O
and	O
5	O
decoder	Method
layers	Method
at	O
kernel	O
size	O
5	O
.	O
We	O
use	O
a	O
target	O
vocabulary	O
of	O
160	O
K	O
words	O
as	O
well	O
as	O
vocabulary	O
selection	O
[	O
reference	O
][	O
reference	O
]	O
to	O
decrease	O
the	O
size	O
of	O
the	O
output	Method
layer	Method
which	O
speeds	O
up	O
training	Task
and	O
testing	Task
.	O
The	O
average	O
vocabulary	O
size	O
for	O
each	O
training	O
batch	O
is	O
about	O
20	O
K	O
target	O
words	O
.	O
All	O
figures	O
are	O
averaged	O
over	O
three	O
runs	O
(	O
§	O
4	O
)	O
and	O
BLEU	Metric
is	O
reported	O
on	O
newstest2014	O
before	O
unknown	Task
word	Task
replacement	Task
.	O
We	O
start	O
with	O
an	O
experiment	O
that	O
removes	O
the	O
position	O
em	O
-	O
beddings	O
from	O
the	O
encoder	Method
and	Method
decoder	Method
(	O
§	O
3.1	O
)	O
.	O
These	O
embeddings	O
allow	O
our	O
model	O
to	O
identify	O
which	O
portion	O
of	O
the	O
source	O
and	O
target	O
sequence	O
it	O
is	O
dealing	O
with	O
but	O
also	O
impose	O
a	O
restriction	O
on	O
the	O
maximum	O
sentence	O
length	O
.	O
Table	O
4	O
shows	O
that	O
position	O
embeddings	O
are	O
helpful	O
but	O
that	O
our	O
model	O
still	O
performs	O
well	O
without	O
them	O
.	O
Removing	O
the	O
source	O
position	O
embeddings	O
results	O
in	O
a	O
larger	O
accuracy	Metric
decrease	O
than	O
target	O
position	O
embeddings	O
.	O
However	O
,	O
removing	O
both	O
source	O
and	O
target	O
positions	O
decreases	O
accuracy	Metric
only	O
by	O
0.5	O
BLEU	Metric
.	O
We	O
had	O
assumed	O
that	O
the	O
model	O
would	O
not	O
be	O
able	O
to	O
calibrate	O
the	O
length	O
of	O
the	O
output	O
sequences	O
very	O
well	O
without	O
explicit	O
position	O
information	O
,	O
however	O
,	O
the	O
output	O
lengths	O
of	O
models	O
without	O
position	O
embeddings	O
closely	O
matches	O
models	O
with	O
position	O
information	O
.	O
This	O
indicates	O
that	O
the	O
models	O
can	O
learn	O
relative	O
position	O
information	O
within	O
the	O
contexts	O
visible	O
to	O
the	O
encoder	Method
and	Method
decoder	Method
networks	Method
which	O
can	O
observe	O
up	O
to	O
27	O
and	O
25	O
words	O
respectively	O
.	O
Recurrent	Method
models	Method
typically	O
do	O
not	O
use	O
explicit	O
position	O
embeddings	O
since	O
they	O
can	O
learn	O
where	O
they	O
are	O
in	O
the	O
sequence	O
through	O
the	O
recurrent	Method
hidden	Method
state	Method
computation	Method
.	O
In	O
our	O
setting	O
,	O
the	O
use	O
of	O
position	O
embeddings	O
requires	O
only	O
a	O
simple	O
addition	O
to	O
the	O
input	O
word	O
embeddings	O
which	O
is	O
a	O
negligible	O
overhead	O
.	O
section	O
:	O
Multi	O
-	O
step	O
Attention	O
The	O
multiple	Method
attention	Method
mechanism	Method
(	O
§	O
3.3	O
)	O
computes	O
a	O
separate	O
source	O
context	O
vector	O
for	O
each	O
decoder	Method
layer	Method
.	O
The	O
computation	O
also	O
takes	O
into	O
account	O
contexts	O
computed	O
for	O
preceding	O
decoder	O
layers	O
of	O
the	O
current	O
time	O
step	O
as	O
well	O
as	O
previous	O
time	O
steps	O
that	O
are	O
within	O
the	O
receptive	O
field	O
of	O
the	O
decoder	O
.	O
How	O
does	O
multiple	O
attention	O
compare	O
to	O
attention	O
in	O
fewer	O
layers	O
or	O
even	O
only	O
in	O
a	O
single	O
layer	O
as	O
is	O
usual	O
?	O
Table	O
5	O
shows	O
that	O
attention	O
in	O
all	O
decoder	O
layers	O
achieves	O
the	O
best	O
validation	Metric
perplexity	Metric
(	O
PPL	Metric
)	O
.	O
Furthermore	O
,	O
removing	O
more	O
and	O
more	O
attention	O
layers	O
decreases	O
accuracy	Metric
,	O
both	O
in	O
terms	O
of	O
BLEU	Metric
as	O
well	O
as	O
PPL	Method
.	O
The	O
computational	Metric
overhead	Metric
for	O
attention	Task
is	O
very	O
small	O
compared	O
to	O
the	O
rest	O
of	O
the	O
network	O
.	O
Training	O
with	O
attention	O
in	O
all	O
five	O
decoder	Method
layers	Method
processes	O
3624	O
target	O
words	O
per	O
second	O
on	O
average	O
on	O
a	O
single	O
GPU	O
,	O
compared	O
to	O
3772	O
words	O
per	O
second	O
for	O
attention	O
in	O
a	O
single	O
layer	O
.	O
This	O
is	O
only	O
Table	O
5	O
.	O
Multi	O
-	O
step	O
attention	O
in	O
all	O
five	O
decoder	O
layers	O
or	O
fewer	O
layers	O
in	O
terms	O
of	O
validation	Metric
perplexity	Metric
(	O
PPL	Task
)	O
and	O
test	O
BLEU	Metric
.	O
a	O
4	O
%	O
slow	O
down	O
when	O
adding	O
4	O
attention	Method
modules	Method
.	O
Most	O
neural	O
machine	Task
translation	Task
systems	O
only	O
use	O
a	O
single	O
module	O
.	O
This	O
demonstrates	O
that	O
attention	Task
is	O
not	O
the	O
bottleneck	O
in	O
neural	O
machine	Task
translation	Task
,	O
even	O
though	O
it	O
is	O
quadratic	O
in	O
the	O
sequence	O
length	O
(	O
cf	O
.	O
[	O
reference	O
]	O
.	O
Part	O
of	O
the	O
reason	O
for	O
the	O
low	O
impact	O
on	O
speed	Metric
is	O
that	O
we	O
batch	O
the	O
computation	O
of	O
an	O
attention	Method
module	Method
over	O
all	O
target	O
words	O
,	O
similar	O
to	O
[	O
reference	O
]	O
.	O
However	O
,	O
for	O
RNNs	Task
batching	Task
of	Task
the	Task
attention	Task
may	O
be	O
less	O
effective	O
because	O
of	O
the	O
dependence	O
on	O
the	O
previous	O
time	O
step	O
.	O
section	O
:	O
Kernel	O
size	O
and	O
Depth	O
Figure	O
2	O
shows	O
accuracy	Metric
when	O
we	O
change	O
the	O
number	O
of	O
layers	O
in	O
the	O
encoder	O
or	O
decoder	O
.	O
The	O
kernel	O
width	O
for	O
layers	O
in	O
the	O
encoder	O
is	O
3	O
and	O
for	O
the	O
decoder	O
it	O
is	O
5	O
.	O
Deeper	Method
architectures	Method
are	O
particularly	O
beneficial	O
for	O
the	O
encoder	Method
but	O
less	O
so	O
for	O
the	O
decoder	Task
.	O
Decoder	Method
setups	Method
with	O
two	O
layers	O
already	O
perform	O
well	O
whereas	O
for	O
the	O
encoder	Metric
accuracy	Metric
keeps	O
increasing	O
steadily	O
with	O
more	O
layers	O
until	O
up	O
to	O
9	O
layers	O
when	O
accuracy	Metric
starts	O
to	O
plateau	O
.	O
RNN	Method
MLE	Method
[	O
reference	O
]	O
24	O
Table	O
6	O
.	O
Accuracy	Metric
on	O
two	O
summarization	Task
tasks	Task
in	O
terms	O
of	O
Rouge	Task
-	Task
1	Task
(	Task
RG	Task
-	Task
1	Task
)	O
,	O
Rouge	Task
-	Task
2	Task
(	Task
RG	Task
-	Task
2	Task
)	O
,	O
and	O
Rouge	Task
-	Task
L	Task
(	O
RG	Task
-	Task
L	Task
)	O
.	O
Aside	O
from	O
increasing	O
the	O
depth	O
of	O
the	O
networks	O
,	O
we	O
can	O
also	O
change	O
the	O
kernel	O
width	O
.	O
Table	O
7	O
shows	O
that	O
encoders	Method
with	O
narrow	Method
kernels	Method
and	O
many	O
layers	O
perform	O
better	O
than	O
wider	Method
kernels	Method
.	O
These	O
networks	O
can	O
also	O
be	O
faster	O
since	O
the	O
amount	O
of	O
work	O
to	O
compute	O
a	O
kernel	O
operating	O
over	O
3	O
input	O
elements	O
is	O
less	O
than	O
half	O
compared	O
to	O
kernels	O
over	O
7	O
elements	O
.	O
We	O
see	O
a	O
similar	O
picture	O
for	O
decoder	Method
networks	Method
with	O
large	O
kernel	O
sizes	O
(	O
Table	O
8	O
)	O
.	O
[	O
reference	O
]	O
shows	O
that	O
context	O
sizes	O
of	O
20	O
words	O
are	O
often	O
sufficient	O
to	O
achieve	O
very	O
good	O
accuracy	Metric
on	O
language	Task
modeling	Task
for	O
English	Material
.	O
section	O
:	O
Kernel	Method
width	Method
Encoder	Method
layers	Method
section	O
:	O
Summarization	Task
Finally	O
,	O
we	O
evaluate	O
our	O
model	O
on	O
abstractive	Task
sentence	Task
summarization	Task
which	O
takes	O
a	O
long	O
sentence	O
as	O
input	O
and	O
outputs	O
a	O
shortened	O
version	O
.	O
The	O
current	O
best	O
models	O
on	O
this	O
task	O
are	O
recurrent	Method
neural	Method
networks	Method
which	O
either	O
optimize	O
the	O
evaluation	Metric
metric	Metric
[	O
reference	O
]	O
or	O
address	O
specific	O
problems	O
of	O
summarization	Task
such	O
as	O
avoiding	O
repeated	O
generations	O
[	O
reference	O
]	O
.	O
We	O
use	O
standard	O
likelhood	Method
training	Method
for	O
our	O
model	O
and	O
a	O
simple	O
model	O
with	O
six	O
layers	O
in	O
the	O
encoder	O
and	O
decoder	O
each	O
,	O
hidden	O
size	O
256	O
,	O
batch	O
size	O
128	O
,	O
and	O
we	O
trained	O
on	O
a	O
single	O
GPU	O
in	O
one	O
night	O
.	O
Table	O
6	O
shows	O
that	O
our	O
likelhood	Method
trained	Method
model	Method
outperforms	O
the	O
likelihood	Method
trained	Method
model	Method
(	O
RNN	Method
MLE	Method
)	O
of	O
[	O
reference	O
]	O
and	O
is	O
not	O
far	O
behind	O
the	O
best	O
models	O
on	O
this	O
task	O
which	O
benefit	O
from	O
task	O
-	O
specific	O
optimization	Method
and	O
model	Method
structure	Method
.	O
We	O
expect	O
our	O
model	O
to	O
benefit	O
from	O
these	O
improvements	O
as	O
well	O
.	O
section	O
:	O
Conclusion	O
and	O
Future	O
Work	O
We	O
introduce	O
the	O
first	O
fully	Method
convolutional	Method
model	Method
for	O
sequence	Task
to	Task
sequence	Task
learning	Task
that	O
outperforms	O
strong	O
recurrent	Method
models	Method
on	O
very	O
large	O
benchmark	O
datasets	O
at	O
an	O
order	O
of	O
magnitude	O
faster	O
speed	O
.	O
Compared	O
to	O
recurrent	Method
networks	Method
,	O
our	O
convolutional	Method
approach	Method
allows	O
to	O
discover	O
compositional	O
structure	O
in	O
the	O
sequences	O
more	O
easily	O
since	O
representations	O
are	O
built	O
hierarchically	O
.	O
Our	O
model	O
relies	O
on	O
gating	Method
and	O
performs	O
multiple	O
attention	Method
steps	Method
.	O
We	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
several	O
public	O
translation	O
benchmark	O
data	O
sets	O
.	O
On	O
the	O
WMT'16	Material
EnglishRomanian	O
task	O
we	O
outperform	O
the	O
previous	O
best	O
result	O
by	O
1.9	O
BLEU	Metric
,	O
on	O
WMT'14	Material
English	Material
-	Material
French	Material
translation	Material
we	O
improve	O
over	O
the	O
LSTM	Method
model	Method
of	O
[	O
reference	O
]	O
by	O
1.6	O
BLEU	Metric
in	O
a	O
comparable	O
setting	O
,	O
and	O
on	O
WMT'14	Task
EnglishGerman	Task
translation	Task
we	O
ouperform	O
the	O
same	O
model	O
by	O
0.5	O
BLEU	Metric
.	O
In	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
apply	O
convolutional	Method
architectures	Method
to	O
other	O
sequence	Task
to	Task
sequence	Task
learning	Task
problems	Task
which	O
may	O
benefit	O
from	O
learning	O
hierarchical	Method
representations	Method
as	O
well	O
.	O
section	O
:	O
A.	O
Weight	Method
Initialization	Method
We	O
derive	O
a	O
weight	Method
initialization	Method
scheme	Method
tailored	O
to	O
the	O
GLU	O
activation	O
function	O
similar	O
to	O
[	O
reference	O
]	O
;	O
[	O
reference	O
]	O
by	O
focusing	O
on	O
the	O
variance	O
of	O
activations	O
within	O
the	O
network	O
for	O
both	O
forward	O
and	O
backward	O
passes	O
.	O
We	O
also	O
detail	O
how	O
we	O
modify	O
the	O
weight	Method
initialization	Method
for	O
dropout	Task
.	O
section	O
:	O
A.1	O
.	O
Forward	Method
Pass	Method
Assuming	O
that	O
the	O
inputs	O
x	O
l	O
of	O
a	O
convolutional	Method
layer	Method
l	O
and	O
its	O
weights	O
W	O
l	O
are	O
independent	O
and	O
identically	O
distributed	O
(	O
i.i.d	O
.	O
)	O
,	O
the	O
variance	O
of	O
its	O
output	O
,	O
computed	O
as	O
where	O
n	O
l	O
is	O
the	O
number	O
inputs	O
to	O
the	O
layer	O
.	O
For	O
onedimensional	Method
convolutional	Method
layers	Method
with	O
kernel	O
width	O
k	O
and	O
input	O
dimension	O
c	O
,	O
this	O
is	O
kc	O
.	O
We	O
adopt	O
the	O
notation	O
in	O
[	O
reference	O
]	O
,	O
i.e.	O
y	O
l	O
,	O
w	O
l	O
and	O
x	O
l	O
represent	O
the	O
random	O
variables	O
in	O
y	O
l	O
,	O
W	O
l	O
and	O
x	O
l	O
.	O
With	O
w	O
l	O
and	O
x	O
l	O
independent	O
from	O
each	O
other	O
and	O
normally	O
distributed	O
with	O
zero	O
mean	O
,	O
this	O
amounts	O
to	O
x	O
l	O
is	O
the	O
result	O
of	O
the	O
GLU	O
activation	O
function	O
y	O
A	O
lower	O
bound	O
is	O
given	O
by	O
(	O
1	O
/	O
4	O
)	O
V	O
ar	O
[	O
y	O
With	O
x	O
∼	O
N	O
(	O
0	O
,	O
std	O
(	O
x	O
)	O
)	O
,	O
this	O
yields	O
With	O
(	O
7	O
)	O
We	O
initialize	O
the	O
embedding	O
matrices	O
in	O
our	O
network	O
with	O
small	O
variances	O
(	O
around	O
0.01	O
)	O
,	O
which	O
allows	O
us	O
to	O
dismiss	O
the	O
quadratic	O
term	O
and	O
approximate	O
the	O
GLU	O
output	O
variance	O
with	O
If	O
L	O
network	Method
layers	Method
of	O
equal	O
size	O
and	O
with	O
GLU	O
activations	O
are	O
combined	O
,	O
the	O
variance	O
of	O
the	O
final	O
output	O
y	O
L	O
is	O
given	O
by	O
Following	O
[	O
reference	O
]	O
,	O
we	O
aim	O
to	O
satisfy	O
the	O
condition	O
so	O
that	O
the	O
activations	O
in	O
a	O
network	O
are	O
neither	O
exponentially	O
magnified	O
nor	O
reduced	O
.	O
This	O
is	O
achieved	O
by	O
initializing	O
W	O
l	O
from	O
N	O
(	O
0	O
,	O
4	O
/	O
n	O
l	O
)	O
.	O
section	O
:	O
A.2	O
.	O
Backward	Method
Pass	Method
The	O
gradient	O
of	O
a	O
convolutional	Method
layer	Method
is	O
computed	O
via	O
backpropagation	Method
as	O
∆x	O
l	O
=	O
Ŵ	O
l	O
y	O
l	O
.	O
Considering	O
separate	O
gradients	O
∆y	O
a	O
l	O
and	O
∆y	O
b	O
l	O
for	O
GLU	O
,	O
the	O
gradient	O
of	O
x	O
is	O
given	O
by	O
W	O
corresponds	O
to	O
W	O
with	O
re	O
-	O
arranged	O
weights	O
to	O
enable	O
back	Method
-	Method
propagation	Method
.	O
Analogously	O
to	O
the	O
forward	Method
pass	Method
,	O
∆x	O
l	O
,	O
w	O
l	O
and	O
∆y	O
l	O
represent	O
the	O
random	O
variables	O
for	O
the	O
values	O
in	O
∆x	O
l	O
,	O
Ŵ	O
l	O
and	O
∆y	O
l	O
,	O
respectively	O
.	O
Note	O
that	O
W	O
andŴ	O
contain	O
the	O
same	O
values	O
,	O
i.e.ŵ	O
=	O
w.	O
Similar	O
to	O
(	O
3	O
)	O
,	O
the	O
variance	O
of	O
∆x	O
l	O
is	O
Here	O
,	O
n	O
l	O
is	O
the	O
number	O
of	O
inputs	O
to	O
layer	O
l	O
+	O
1	O
.	O
The	O
gradients	O
for	O
the	O
GLU	O
inputs	O
are	O
:	O
The	O
approximation	O
for	O
the	O
forward	O
pass	O
can	O
be	O
used	O
for	O
V	O
ar	O
[	O
∆y	O
V	O
ar	O
[	O
∆y	O
We	O
observe	O
relatively	O
small	O
gradients	O
in	O
our	O
network	O
,	O
typically	O
around	O
0.001	O
at	O
the	O
start	O
of	O
training	O
.	O
Therefore	O
,	O
we	O
approximate	O
by	O
discarding	O
the	O
quadratic	O
terms	O
above	O
,	O
i.e.	O
V	O
ar	O
[	O
∆y	O
V	O
ar	O
[	O
∆y	O
As	O
for	O
the	O
forward	O
pass	O
,	O
the	O
above	O
result	O
can	O
be	O
generalized	O
to	O
backpropagation	O
through	O
many	O
successive	O
layers	O
,	O
resulting	O
in	O
For	O
arbitrarily	O
large	O
variances	O
of	O
network	O
inputs	O
and	O
activations	O
,	O
our	O
approximations	O
are	O
invalid	O
;	O
in	O
that	O
case	O
,	O
the	O
initial	O
values	O
for	O
W	O
a	O
l	O
and	O
W	O
b	O
l	O
would	O
have	O
to	O
be	O
balanced	O
for	O
the	O
input	O
distribution	O
to	O
be	O
retained	O
.	O
Alternatively	O
,	O
methods	O
that	O
explicitly	O
control	O
the	O
variance	O
in	O
the	O
network	O
,	O
e.g.	O
batch	Method
normalization	Method
[	O
reference	O
]	O
or	O
layer	Method
normalization	Method
[	O
reference	O
]	O
could	O
be	O
employed	O
.	O
section	O
:	O
A.3	O
.	O
Dropout	O
Dropout	Method
retains	O
activations	O
in	O
a	O
neural	Method
network	Method
with	O
a	O
probability	O
p	O
and	O
sets	O
them	O
to	O
zero	O
otherwise	O
[	O
reference	O
]	O
.	O
It	O
is	O
common	O
practice	O
to	O
scale	O
the	O
retained	O
activations	O
by	O
1	O
/	O
p	O
during	O
training	O
so	O
that	O
the	O
weights	O
of	O
the	O
network	O
do	O
not	O
have	O
to	O
be	O
modified	O
at	O
test	O
time	O
when	O
p	O
is	O
set	O
to	O
1	O
.	O
In	O
this	O
case	O
,	O
dropout	Task
amounts	O
to	O
multiplying	O
activations	O
Assuming	O
that	O
a	O
the	O
input	O
of	O
a	O
convolutional	Method
layer	Method
has	O
been	O
subject	O
to	O
dropout	O
with	O
a	O
retain	O
probability	O
p	O
,	O
the	O
variations	O
of	O
the	O
forward	O
and	O
backward	O
activations	O
from	O
§	O
A.1	O
and	O
§	O
A.2	O
can	O
now	O
be	O
approximated	O
with	O
This	O
amounts	O
to	O
a	O
modified	O
initialization	Method
of	Method
W	Method
l	Method
from	O
a	O
normal	Method
distribution	Method
with	O
zero	O
mean	O
and	O
a	O
standard	O
deviation	O
of	O
4p	O
/	O
n	O
.	O
For	O
layers	O
without	O
a	O
succeeding	O
GLU	O
activation	O
function	O
,	O
we	O
initialize	O
weights	O
from	O
N	O
(	O
0	O
,	O
p	O
/	O
n	O
)	O
to	O
calibrate	O
for	O
any	O
immediately	O
preceding	O
dropout	Task
application	Task
.	O
section	O
:	O
B.	O
Upper	O
Bound	O
on	O
Squared	O
Sigmoid	O
The	O
sigmoid	O
function	O
σ	O
(	O
x	O
)	O
can	O
be	O
expressed	O
as	O
a	O
hyperbolic	O
tangent	O
by	O
using	O
the	O
identity	O
tanh	O
(	O
x	O
)	O
=	O
2	O
σ	O
(	O
2x	O
)	O
−	O
1	O
.	O
The	O
derivative	O
of	O
tanh	O
is	O
tanh	O
(	O
x	O
)	O
=	O
1	O
−	O
tanh	O
2	O
(	O
x	O
)	O
,	O
and	O
with	O
tanh	O
(	O
x	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
x	O
≥	O
0	O
it	O
holds	O
that	O
tanh	O
(	O
x	O
)	O
≤	O
1	O
,	O
x	O
≥	O
0	O
(	O
34	O
)	O
We	O
can	O
express	O
this	O
relation	O
with	O
σ	O
(	O
x	O
)	O
as	O
follows	O
:	O
Both	O
terms	O
of	O
this	O
inequality	O
have	O
rotational	O
symmetry	O
w.r.t	O
0	O
,	O
and	O
thus	O
C.	O
Attention	Task
Visualization	Task
Figure	O
3	O
shows	O
attention	Metric
scores	Metric
for	O
a	O
generated	O
sentence	O
from	O
the	O
WMT'14	Material
English	Material
-	Material
German	Material
task	O
.	O
The	O
model	O
used	O
for	O
this	O
plot	O
has	O
8	O
decoder	Method
layers	Method
and	O
a	O
80	O
K	O
BPE	O
vocabulary	O
.	O
The	O
attention	O
passes	O
in	O
different	O
decoder	Method
layers	Method
capture	O
different	O
portions	O
of	O
the	O
source	O
sentence	O
.	O
Layer	O
1	O
,	O
3	O
and	O
6	O
exhibit	O
a	O
linear	O
alignment	O
.	O
The	O
first	O
layer	O
shows	O
the	O
clearest	O
alignment	O
,	O
although	O
it	O
is	O
slightly	O
off	O
and	O
frequently	O
attends	O
to	O
the	O
corresponding	O
source	O
word	O
of	O
the	O
previously	O
generated	O
target	O
word	O
.	O
Layer	O
2	O
and	O
8	O
lack	O
a	O
clear	O
structure	O
and	O
are	O
presumably	O
collecting	O
information	O
about	O
the	O
whole	O
source	O
sentence	O
.	O
The	O
fourth	O
layer	O
shows	O
high	O
alignment	Metric
scores	Metric
on	O
nouns	O
such	O
as	O
"	O
festival	O
"	O
,	O
"	O
way	O
"	O
and	O
"	O
work	O
"	O
for	O
both	O
the	O
generated	O
target	O
nouns	O
as	O
well	O
as	O
their	O
preceding	O
words	O
.	O
Note	O
that	O
in	O
German	O
,	O
those	O
preceding	O
words	O
depend	O
on	O
gender	O
and	O
object	O
relationship	O
of	O
the	O
respective	O
noun	O
.	O
Finally	O
,	O
the	O
attention	Metric
scores	Metric
in	O
layer	O
5	O
and	O
7	O
focus	O
on	O
"	O
built	O
"	O
,	O
which	O
is	O
reordered	O
in	O
the	O
German	Material
translation	Material
and	O
is	O
moved	O
from	O
the	O
beginning	O
to	O
the	O
very	O
end	O
of	O
the	O
sentence	O
.	O
One	O
interpretation	O
for	O
this	O
is	O
that	O
as	O
generation	Task
progresses	O
,	O
the	O
model	O
repeatedly	O
tries	O
to	O
perform	O
the	O
re	O
-	O
ordering	O
.	O
"	O
aufgebaut	O
"	O
can	O
be	O
generated	O
after	O
a	O
noun	O
or	O
pronoun	O
only	O
,	O
which	O
is	O
reflected	O
in	O
the	O
higher	O
scores	O
at	O
positions	O
2	O
,	O
5	O
,	O
8	O
,	O
11	O
and	O
13	O
.	O
Layer	O
7	O
Layer	O
8	O
Figure	O
3	O
.	O
Attention	Metric
scores	Metric
for	O
different	O
decoder	Method
layers	Method
for	O
a	O
sentence	O
translated	O
from	O
English	O
(	O
y	O
-	O
axis	O
)	O
to	O
German	O
(	O
x	O
-	O
axis	O
)	O
.	O
This	O
model	O
uses	O
8	O
decoder	Method
layers	Method
and	O
a	O
80k	O
BPE	O
vocabulary	O
.	O
section	O
:	O
section	O
:	O
Acknowledgements	O
We	O
thank	O
Benjamin	O
Graham	O
for	O
providing	O
a	O
fast	O
1	Method
-	Method
D	Method
convolution	Method
,	O
and	O
Ronan	O
Collobert	O
as	O
well	O
as	O
Yann	O
LeCun	O
for	O
helpful	O
discussions	O
related	O
to	O
this	O
work	O
.	O
section	O
:	O
