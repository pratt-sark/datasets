document	O
:	O
Quasi	Method
-	Method
Recurrent	Method
Neural	Method
Networks	Method
Recurrent	Method
neural	Method
networks	Method
are	O
a	O
powerful	O
tool	O
for	O
modeling	O
sequential	O
data	O
,	O
but	O
the	O
dependence	O
of	O
each	O
timestep	O
’s	O
computation	O
on	O
the	O
previous	O
timestep	O
’s	O
output	O
limits	O
parallelism	O
and	O
makes	O
RNNs	Method
unwieldy	O
for	O
very	O
long	O
sequences	O
.	O
We	O
introduce	O
quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
(	O
QRNNs	Method
)	O
,	O
an	O
approach	O
to	O
neural	Task
sequence	Task
modeling	Task
that	O
alternates	O
convolutional	Method
layers	Method
,	O
which	O
apply	O
in	O
parallel	O
across	O
timesteps	O
,	O
and	O
a	O
minimalist	Method
recurrent	Method
pooling	Method
function	Method
that	O
applies	O
in	O
parallel	O
across	O
channels	O
.	O
Despite	O
lacking	O
trainable	O
recurrent	Method
layers	Method
,	O
stacked	O
QRNNs	Method
have	O
better	O
predictive	Metric
accuracy	Metric
than	O
stacked	Method
LSTMs	Method
of	O
the	O
same	O
hidden	O
size	O
.	O
Due	O
to	O
their	O
increased	O
parallelism	O
,	O
they	O
are	O
up	O
to	O
16	O
times	O
faster	O
at	O
train	Metric
and	Metric
test	Metric
time	Metric
.	O
Experiments	O
on	O
language	Task
modeling	Task
,	O
sentiment	Task
classification	Task
,	O
and	O
character	O
-	O
level	O
neural	O
machine	Task
translation	Task
demonstrate	O
these	O
advantages	O
and	O
underline	O
the	O
viability	O
of	O
QRNNs	Method
as	O
a	O
basic	O
building	O
block	O
for	O
a	O
variety	O
of	O
sequence	Task
tasks	Task
.	O
section	O
:	O
Introduction	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
,	O
including	O
gated	Method
variants	Method
such	O
as	O
the	O
long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
Hochreiter1997	O
have	O
become	O
the	O
standard	O
model	Method
architecture	Method
for	O
deep	Method
learning	Method
approaches	Method
to	O
sequence	Task
modeling	Task
tasks	Task
.	O
RNNs	Method
repeatedly	O
apply	O
a	O
function	O
with	O
trainable	O
parameters	O
to	O
a	O
hidden	O
state	O
.	O
Recurrent	Method
layers	Method
can	O
also	O
be	O
stacked	O
,	O
increasing	O
network	O
depth	O
,	O
representational	O
power	O
and	O
often	O
accuracy	Metric
.	O
RNN	Method
applications	Method
in	O
the	O
natural	Task
language	Task
domain	Task
range	O
from	O
sentence	Task
classification	Task
Wang2015	O
to	O
word	Task
-	Task
and	Task
character	Task
-	Task
level	Task
language	Task
modeling	Task
Zaremba2014	O
.	O
RNNs	Method
are	O
also	O
commonly	O
the	O
basic	O
building	O
block	O
for	O
more	O
complex	O
models	O
for	O
tasks	O
such	O
as	O
machine	Task
translation	Task
Bahdanau2015	O
,	O
Luong2015	O
,	O
Bradbury2016	O
or	O
question	Task
answering	Task
Kumar2016	O
,	O
Xiong2016	O
.	O
Unfortunately	O
standard	O
RNNs	Method
,	O
including	O
LSTMs	Method
,	O
are	O
limited	O
in	O
their	O
capability	O
to	O
handle	O
tasks	O
involving	O
very	O
long	O
sequences	O
,	O
such	O
as	O
document	Task
classification	Task
or	O
character	O
-	O
level	O
machine	Task
translation	Task
,	O
as	O
the	O
computation	O
of	O
features	O
or	O
states	O
for	O
different	O
parts	O
of	O
the	O
document	O
can	O
not	O
occur	O
in	O
parallel	O
.	O
Convolutional	Method
neural	Method
networks	Method
(	O
CNNs	Method
)	O
Krizhevsky2012	O
,	O
though	O
more	O
popular	O
on	O
tasks	O
involving	O
image	O
data	O
,	O
have	O
also	O
been	O
applied	O
to	O
sequence	Task
encoding	Task
tasks	Task
Zhang2015	O
.	O
Such	O
models	O
apply	O
time	Method
-	Method
invariant	Method
filter	Method
functions	Method
in	O
parallel	O
to	O
windows	O
along	O
the	O
input	O
sequence	O
.	O
CNNs	Method
possess	O
several	O
advantages	O
over	O
recurrent	Method
models	Method
,	O
including	O
increased	O
parallelism	O
and	O
better	O
scaling	O
to	O
long	O
sequences	O
such	O
as	O
those	O
often	O
seen	O
with	O
character	O
-	O
level	O
language	O
data	O
.	O
Convolutional	Method
models	Method
for	O
sequence	Task
processing	Task
have	O
been	O
more	O
successful	O
when	O
combined	O
with	O
RNN	Method
layers	Method
in	O
a	O
hybrid	Method
architecture	Method
Lee2016	O
,	O
because	O
traditional	O
max	Method
-	Method
and	Method
average	Method
-	Method
pooling	Method
approaches	Method
to	O
combining	O
convolutional	O
features	O
across	O
timesteps	O
assume	O
time	O
invariance	O
and	O
hence	O
can	O
not	O
make	O
full	O
use	O
of	O
large	O
-	O
scale	O
sequence	O
order	O
information	O
.	O
We	O
present	O
quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
for	O
neural	Task
sequence	Task
modeling	Task
.	O
QRNNs	Method
address	O
both	O
drawbacks	O
of	O
standard	O
models	O
:	O
like	O
CNNs	Method
,	O
QRNNs	Method
allow	O
for	O
parallel	Task
computation	Task
across	O
both	O
timestep	O
and	O
minibatch	O
dimensions	O
,	O
enabling	O
high	O
throughput	Metric
and	O
good	O
scaling	O
to	O
long	O
sequences	O
.	O
Like	O
RNNs	Method
,	O
QRNNs	Method
allow	O
the	O
output	O
to	O
depend	O
on	O
the	O
overall	O
order	O
of	O
elements	O
in	O
the	O
sequence	O
.	O
We	O
describe	O
QRNN	Method
variants	Method
tailored	O
to	O
several	O
natural	Task
language	Task
tasks	Task
,	O
including	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
,	O
language	Task
modeling	Task
,	O
and	O
character	O
-	O
level	O
machine	Task
translation	Task
.	O
These	O
models	O
outperform	O
strong	O
LSTM	Method
baselines	O
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
reducing	O
computation	Metric
time	Metric
.	O
section	O
:	O
Model	O
Each	O
layer	O
of	O
a	O
quasi	Method
-	Method
recurrent	Method
neural	Method
network	Method
consists	O
of	O
two	O
kinds	O
of	O
subcomponents	O
,	O
analogous	O
to	O
convolution	Method
and	Method
pooling	Method
layers	Method
in	O
CNNs	Method
.	O
The	O
convolutional	Method
component	Method
,	O
like	O
convolutional	Method
layers	Method
in	O
CNNs	Method
,	O
allows	O
fully	O
parallel	Task
computation	Task
across	O
both	O
minibatches	O
and	O
spatial	O
dimensions	O
,	O
in	O
this	O
case	O
the	O
sequence	O
dimension	O
.	O
The	O
pooling	Method
component	Method
,	O
like	O
pooling	Method
layers	Method
in	O
CNNs	Method
,	O
lacks	O
trainable	O
parameters	O
and	O
allows	O
fully	O
parallel	O
computation	O
across	O
minibatch	O
and	O
feature	O
dimensions	O
.	O
Given	O
an	O
input	O
sequence	O
of	O
-	O
dimensional	O
vectors	O
,	O
the	O
convolutional	Method
subcomponent	Method
of	O
a	O
QRNN	Method
performs	O
convolutions	Method
in	O
the	O
timestep	O
dimension	O
with	O
a	O
bank	Method
of	Method
filters	Method
,	O
producing	O
a	O
sequence	O
of	O
-	O
dimensional	O
candidate	O
vectors	O
.	O
In	O
order	O
to	O
be	O
useful	O
for	O
tasks	O
that	O
include	O
prediction	Task
of	Task
the	Task
next	Task
token	Task
,	O
the	O
filters	Method
must	O
not	O
allow	O
the	O
computation	O
for	O
any	O
given	O
timestep	O
to	O
access	O
information	O
from	O
future	O
timesteps	O
.	O
That	O
is	O
,	O
with	O
filters	O
of	O
width	O
,	O
each	O
depends	O
only	O
on	O
through	O
.	O
This	O
concept	O
,	O
known	O
as	O
a	O
masked	Method
convolution	Method
vandenOord2016	O
,	O
is	O
implemented	O
by	O
padding	O
the	O
input	O
to	O
the	O
left	O
by	O
the	O
convolution	O
’s	O
filter	O
size	O
minus	O
one	O
.	O
We	O
apply	O
additional	O
convolutions	Method
with	O
separate	O
filter	Method
banks	Method
to	O
obtain	O
sequences	O
of	O
vectors	O
for	O
the	O
elementwise	O
gates	O
that	O
are	O
needed	O
for	O
the	O
pooling	O
function	O
.	O
While	O
the	O
candidate	O
vectors	O
are	O
passed	O
through	O
a	O
nonlinearity	Method
,	O
the	O
gates	O
use	O
an	O
elementwise	Method
sigmoid	Method
.	O
If	O
the	O
pooling	Method
function	Method
requires	O
a	O
forget	O
gate	O
and	O
an	O
output	O
gate	O
at	O
each	O
timestep	O
,	O
the	O
full	O
set	O
of	O
computations	O
in	O
the	O
convolutional	Method
component	Method
is	O
then	O
:	O
where	O
,	O
,	O
and	O
,	O
each	O
in	O
,	O
are	O
the	O
convolutional	Method
filter	Method
banks	Method
and	O
denotes	O
a	O
masked	Method
convolution	Method
along	O
the	O
timestep	O
dimension	O
.	O
Note	O
that	O
if	O
the	O
filter	O
width	O
is	O
2	O
,	O
these	O
equations	O
reduce	O
to	O
the	O
LSTM	Method
-	O
like	O
Convolution	O
filters	O
of	O
larger	O
width	O
effectively	O
compute	O
higher	O
-	O
gram	O
features	O
at	O
each	O
timestep	O
;	O
thus	O
larger	O
widths	O
are	O
especially	O
important	O
for	O
character	Task
-	Task
level	Task
tasks	Task
.	O
Suitable	O
functions	O
for	O
the	O
pooling	O
subcomponent	O
can	O
be	O
constructed	O
from	O
the	O
familiar	O
elementwise	O
gates	O
of	O
the	O
traditional	O
LSTM	Method
cell	O
.	O
We	O
seek	O
a	O
function	O
controlled	O
by	O
gates	O
that	O
can	O
mix	O
states	O
across	O
timesteps	O
,	O
but	O
which	O
acts	O
independently	O
on	O
each	O
channel	O
of	O
the	O
state	O
vector	O
.	O
The	O
simplest	O
option	O
,	O
which	O
term	O
“	O
dynamic	Method
average	Method
pooling	Method
”	O
,	O
uses	O
only	O
a	O
forget	O
gate	O
:	O
where	O
denotes	O
elementwise	O
multiplication	O
.	O
The	O
function	O
may	O
also	O
include	O
an	O
output	O
gate	O
:	O
Or	O
the	O
recurrence	O
relation	O
may	O
include	O
an	O
independent	O
input	O
and	O
forget	O
gate	O
:	O
We	O
term	O
these	O
three	O
options	O
f	O
-	O
pooling	O
,	O
fo	Method
-	Method
pooling	Method
,	O
and	O
ifo	O
-	O
pooling	O
respectively	O
;	O
in	O
each	O
case	O
we	O
initialize	O
or	O
to	O
zero	O
.	O
Although	O
the	O
recurrent	O
parts	O
of	O
these	O
functions	O
must	O
be	O
calculated	O
for	O
each	O
timestep	O
in	O
sequence	O
,	O
their	O
simplicity	O
and	O
parallelism	O
along	O
feature	O
dimensions	O
means	O
that	O
,	O
in	O
practice	O
,	O
evaluating	O
them	O
over	O
even	O
long	O
sequences	O
requires	O
a	O
negligible	O
amount	O
of	O
computation	Metric
time	Metric
.	O
A	O
single	O
QRNN	Method
layer	Method
thus	O
performs	O
an	O
input	Method
-	Method
dependent	Method
pooling	Method
,	O
followed	O
by	O
a	O
gated	Method
linear	Method
combination	Method
of	Method
convolutional	Method
features	Method
.	O
As	O
with	O
convolutional	Method
neural	Method
networks	Method
,	O
two	O
or	O
more	O
QRNN	Method
layers	Method
should	O
be	O
stacked	O
to	O
create	O
a	O
model	O
with	O
the	O
capacity	O
to	O
approximate	O
more	O
complex	O
functions	O
.	O
subsection	O
:	O
Variants	O
Motivated	O
by	O
several	O
common	O
natural	Task
language	Task
tasks	Task
,	O
and	O
the	O
long	O
history	O
of	O
work	O
on	O
related	O
architectures	O
,	O
we	O
introduce	O
several	O
extensions	O
to	O
the	O
stacked	Method
QRNN	Method
described	O
above	O
.	O
Notably	O
,	O
many	O
extensions	O
to	O
both	O
recurrent	Method
and	Method
convolutional	Method
models	Method
can	O
be	O
applied	O
directly	O
to	O
the	O
QRNN	Method
as	O
it	O
combines	O
elements	O
of	O
both	O
model	O
types	O
.	O
Regularization	Task
An	O
important	O
extension	O
to	O
the	O
stacked	Method
QRNN	Method
is	O
a	O
robust	Method
regularization	Method
scheme	Method
inspired	O
by	O
recent	O
work	O
in	O
regularizing	Method
LSTMs	Method
.	O
The	O
need	O
for	O
an	O
effective	O
regularization	Method
method	Method
for	O
LSTMs	Method
,	O
and	O
dropout	Method
’s	O
relative	O
lack	O
of	O
efficacy	O
when	O
applied	O
to	O
recurrent	O
connections	O
,	O
led	O
to	O
the	O
development	O
of	O
recurrent	Method
dropout	Method
schemes	Method
,	O
including	O
variational	Method
inference	Method
–	O
based	O
dropout	O
Gal2015	Method
and	O
zoneout	Method
Krueger2016	O
.	O
These	O
schemes	O
extend	O
dropout	Method
to	O
the	O
recurrent	Task
setting	Task
by	O
taking	O
advantage	O
of	O
the	O
repeating	Method
structure	Method
of	Method
recurrent	Method
networks	Method
,	O
providing	O
more	O
powerful	O
and	O
less	O
destructive	Method
regularization	Method
.	O
Variational	Method
inference	Method
–	Method
based	Method
dropout	Method
locks	O
the	O
dropout	O
mask	O
used	O
for	O
the	O
recurrent	O
connections	O
across	O
timesteps	O
,	O
so	O
a	O
single	O
RNN	Method
pass	Method
uses	O
a	O
single	O
stochastic	O
subset	O
of	O
the	O
recurrent	O
weights	O
.	O
Zoneout	Method
stochastically	O
chooses	O
a	O
new	O
subset	O
of	O
channels	O
to	O
“	O
zone	O
out	O
”	O
at	O
each	O
timestep	O
;	O
for	O
these	O
channels	O
the	O
network	O
copies	O
states	O
from	O
one	O
timestep	O
to	O
the	O
next	O
without	O
modification	O
.	O
As	O
QRNNs	Method
lack	O
recurrent	O
weights	O
,	O
the	O
variational	Method
inference	Method
approach	Method
does	O
not	O
apply	O
.	O
Thus	O
we	O
extended	O
zoneout	Method
to	O
the	O
QRNN	Method
architecture	Method
by	O
modifying	O
the	O
pooling	Method
function	Method
to	O
keep	O
the	O
previous	O
pooling	O
state	O
for	O
a	O
stochastic	O
subset	O
of	O
channels	O
.	O
Conveniently	O
,	O
this	O
is	O
equivalent	O
to	O
stochastically	O
setting	O
a	O
subset	O
of	O
the	O
QRNN	O
’s	O
gate	O
channels	O
to	O
1	O
,	O
or	O
applying	O
dropout	Method
on	O
:	O
Thus	O
the	O
pooling	Method
function	Method
itself	O
need	O
not	O
be	O
modified	O
at	O
all	O
.	O
We	O
note	O
that	O
when	O
using	O
an	O
off	O
-	O
the	O
-	O
shelf	O
dropout	Method
layer	Method
in	O
this	O
context	O
,	O
it	O
is	O
important	O
to	O
remove	O
automatic	O
rescaling	O
functionality	O
from	O
the	O
implementation	O
if	O
it	O
is	O
present	O
.	O
In	O
many	O
experiments	O
,	O
we	O
also	O
apply	O
ordinary	Method
dropout	Method
between	Method
layers	Method
,	O
including	O
between	O
word	Method
embeddings	Method
and	O
the	O
first	Method
QRNN	Method
layer	Method
.	O
Densely	Method
-	Method
Connected	Method
Layers	Method
We	O
can	O
also	O
extend	O
the	O
QRNN	Method
architecture	Method
using	O
techniques	O
introduced	O
for	O
convolutional	Method
networks	Method
.	O
For	O
sequence	Task
classification	Task
tasks	Task
,	O
we	O
found	O
it	O
helpful	O
to	O
use	O
skip	O
-	O
connections	O
between	O
every	O
QRNN	Method
layer	Method
,	O
a	O
technique	O
termed	O
“	O
dense	Method
convolution	Method
”	O
by	O
.	O
Where	O
traditional	O
feed	Method
-	Method
forward	Method
or	Method
convolutional	Method
networks	Method
have	O
connections	O
only	O
between	O
subsequent	O
layers	O
,	O
a	O
“	O
DenseNet	Method
”	Method
with	O
layers	O
has	O
feed	O
-	O
forward	O
or	O
convolutional	O
connections	O
between	O
every	O
pair	O
of	O
layers	O
,	O
for	O
a	O
total	O
of	O
.	O
This	O
can	O
improve	O
gradient	Metric
flow	Metric
and	O
convergence	Metric
properties	Metric
,	O
especially	O
in	O
deeper	Task
networks	Task
,	O
although	O
it	O
requires	O
a	O
parameter	O
count	O
that	O
is	O
quadratic	O
in	O
the	O
number	O
of	O
layers	O
.	O
When	O
applying	O
this	O
technique	O
to	O
the	O
QRNN	Method
,	O
we	O
include	O
connections	O
between	O
the	O
input	O
embeddings	O
and	O
every	O
QRNN	Method
layer	Method
and	O
between	O
every	O
pair	O
of	O
QRNN	Method
layers	Method
.	O
This	O
is	O
equivalent	O
to	O
concatenating	O
each	O
QRNN	Method
layer	Method
’s	O
input	O
to	O
its	O
output	O
along	O
the	O
channel	O
dimension	O
before	O
feeding	O
the	O
state	O
into	O
the	O
next	O
layer	O
.	O
The	O
output	O
of	O
the	O
last	O
layer	O
alone	O
is	O
then	O
used	O
as	O
the	O
overall	O
encoding	O
result	O
.	O
Encoder	Method
–	Method
Decoder	Method
Models	Method
To	O
demonstrate	O
the	O
generality	O
of	O
QRNNs	Method
,	O
we	O
extend	O
the	O
model	Method
architecture	Method
to	O
sequence	Task
-	Task
to	Task
-	Task
sequence	Task
tasks	Task
,	O
such	O
as	O
machine	Task
translation	Task
,	O
by	O
using	O
a	O
QRNN	Method
as	O
encoder	Method
and	O
a	O
modified	O
QRNN	Method
,	O
enhanced	O
with	O
attention	Method
,	O
as	O
decoder	Method
.	O
The	O
motivation	O
for	O
modifying	O
the	O
decoder	O
is	O
that	O
simply	O
feeding	O
the	O
last	O
encoder	O
hidden	O
state	O
(	O
the	O
output	O
of	O
the	O
encoder	Method
’s	Method
pooling	Method
layer	Method
)	O
into	O
the	O
decoder	Method
’s	Method
recurrent	Method
pooling	Method
layer	Method
,	O
analogously	O
to	O
conventional	O
recurrent	Method
encoder	Method
–	Method
decoder	Method
architectures	Method
,	O
would	O
not	O
allow	O
the	O
encoder	O
state	O
to	O
affect	O
the	O
gate	O
or	O
update	O
values	O
that	O
are	O
provided	O
to	O
the	O
decoder	Method
’s	Method
pooling	Method
layer	Method
.	O
This	O
would	O
substantially	O
limit	O
the	O
representational	O
power	O
of	O
the	O
decoder	Method
.	O
Instead	O
,	O
the	O
output	O
of	O
each	O
decoder	Method
QRNN	Method
layer	Method
’s	Method
convolution	Method
functions	Method
is	O
supplemented	O
at	O
every	O
timestep	O
with	O
the	O
final	O
encoder	O
hidden	O
state	O
.	O
This	O
is	O
accomplished	O
by	O
adding	O
the	O
result	O
of	O
the	O
convolution	O
for	O
layer	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
with	O
broadcasting	O
to	O
a	O
linearly	O
projected	O
copy	O
of	O
layer	O
’s	O
last	O
encoder	O
state	O
(	O
e.g.	O
,	O
,	O
in	O
)	O
:	O
where	O
the	O
tilde	O
denotes	O
that	O
is	O
an	O
encoder	O
variable	O
.	O
Encoder	Method
–	Method
decoder	Method
models	Method
which	O
operate	O
on	O
long	O
sequences	O
are	O
made	O
significantly	O
more	O
powerful	O
with	O
the	O
addition	O
of	O
soft	O
attention	O
Bahdanau2015	O
,	O
which	O
removes	O
the	O
need	O
for	O
the	O
entire	O
input	O
representation	O
to	O
fit	O
into	O
a	O
fixed	O
-	O
length	O
encoding	O
vector	O
.	O
In	O
our	O
experiments	O
,	O
we	O
computed	O
an	O
attentional	O
sum	O
of	O
the	O
encoder	Method
’s	O
last	O
layer	O
’s	O
hidden	O
states	O
.	O
We	O
used	O
the	O
dot	O
products	O
of	O
these	O
encoder	O
hidden	O
states	O
with	O
the	O
decoder	O
’s	O
last	O
layer	O
’s	O
un	O
-	O
gated	O
hidden	O
states	O
,	O
applying	O
a	O
along	O
the	O
encoder	O
timesteps	O
,	O
to	O
weight	O
the	O
encoder	O
states	O
into	O
an	O
attentional	O
sum	O
for	O
each	O
decoder	O
timestep	O
.	O
This	O
context	O
,	O
and	O
the	O
decoder	O
state	O
,	O
are	O
then	O
fed	O
into	O
a	O
linear	Method
layer	Method
followed	O
by	O
the	O
output	O
gate	O
:	O
where	O
is	O
the	O
last	O
layer	O
.	O
While	O
the	O
first	O
step	O
of	O
this	O
attention	Method
procedure	Method
is	O
quadratic	O
in	O
the	O
sequence	O
length	O
,	O
in	O
practice	O
it	O
takes	O
significantly	O
less	O
computation	Metric
time	Metric
than	O
the	O
model	O
’s	O
linear	Method
and	Method
convolutional	Method
layers	Method
due	O
to	O
the	O
simple	O
and	O
highly	O
parallel	O
dot	O
-	O
product	O
scoring	O
function	O
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
performance	O
of	O
the	O
QRNN	Method
on	O
three	O
different	O
natural	Task
language	Task
tasks	Task
:	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
,	O
language	Task
modeling	Task
,	O
and	O
character	O
-	O
based	O
neural	O
machine	Task
translation	Task
.	O
Our	O
QRNN	Method
models	Method
outperform	O
LSTM	Method
-	O
based	O
models	O
of	O
equal	Method
hidden	Method
size	Method
on	O
all	O
three	O
tasks	O
while	O
dramatically	O
improving	O
computation	Metric
speed	Metric
.	O
Experiments	O
were	O
implemented	O
in	O
Chainer	O
Tokui2015	O
.	O
subsection	O
:	O
Sentiment	Task
Classification	Task
We	O
evaluate	O
the	O
QRNN	Method
architecture	Method
on	O
a	O
popular	O
document	Task
-	Task
level	Task
sentiment	Task
classification	Task
benchmark	Task
,	O
the	O
IMDb	Material
movie	Material
review	Material
dataset	Material
Maas2011	O
.	O
The	O
dataset	O
consists	O
of	O
a	O
balanced	O
sample	O
of	O
25	O
,	O
000	O
positive	O
and	O
25	O
,	O
000	O
negative	O
reviews	O
,	O
divided	O
into	O
equal	O
-	O
size	O
train	O
and	O
test	O
sets	O
,	O
with	O
an	O
average	O
document	O
length	O
of	O
231	O
words	O
Wang2012	O
.	O
We	O
compare	O
only	O
to	O
other	O
results	O
that	O
do	O
not	O
make	O
use	O
of	O
additional	O
unlabeled	O
data	O
(	O
thus	O
excluding	O
e.g.	O
,	O
Miyato2016	O
)	O
.	O
Our	O
best	O
performance	O
on	O
a	O
held	O
-	O
out	O
development	O
set	O
was	O
achieved	O
using	O
a	O
four	Method
-	Method
layer	Method
densely	Method
-	Method
connected	Method
QRNN	Method
with	O
256	O
units	O
per	O
layer	O
and	O
word	O
vectors	O
initialized	O
using	O
300	Method
-	Method
dimensional	Method
cased	Method
GloVe	Method
embeddings	Method
Pennington2014	O
.	O
Dropout	O
of	O
0.3	O
was	O
applied	O
between	O
layers	O
,	O
and	O
we	O
used	O
regularization	Method
of	Method
.	O
Optimization	Task
was	O
performed	O
on	O
minibatches	O
of	O
24	O
examples	O
using	O
RMSprop	Method
Tieleman2012	O
with	O
learning	Metric
rate	Metric
of	O
,	O
,	O
and	O
.	O
Small	O
batch	O
sizes	O
and	O
long	O
sequence	O
lengths	O
provide	O
an	O
ideal	O
situation	O
for	O
demonstrating	O
the	O
QRNN	Method
’s	Method
performance	O
advantages	O
over	O
traditional	O
recurrent	Method
architectures	Method
.	O
We	O
observed	O
a	O
speedup	O
of	O
3.2x	O
on	O
IMDb	Metric
train	Metric
time	Metric
per	O
epoch	O
compared	O
to	O
the	O
optimized	O
LSTM	Method
implementation	O
provided	O
in	O
NVIDIA	Method
’s	Method
cuDNN	Method
library	Method
.	O
For	O
specific	O
batch	O
sizes	O
and	O
sequence	O
lengths	O
,	O
a	O
16x	O
speed	O
gain	O
is	O
possible	O
.	O
Figure	O
[	O
reference	O
]	O
provides	O
extensive	O
speed	O
comparisons	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
visualize	O
the	O
hidden	O
state	O
vectors	O
of	O
the	O
final	O
QRNN	Method
layer	Method
on	O
part	O
of	O
an	O
example	O
from	O
the	O
IMDb	Material
dataset	Material
.	O
Even	O
without	O
any	O
post	Method
-	Method
processing	Method
,	O
changes	O
in	O
the	O
hidden	O
state	O
are	O
visible	O
and	O
interpretable	O
in	O
regards	O
to	O
the	O
input	O
.	O
This	O
is	O
a	O
consequence	O
of	O
the	O
elementwise	O
nature	O
of	O
the	O
recurrent	Method
pooling	Method
function	Method
,	O
which	O
delays	O
direct	O
interaction	O
between	O
different	O
channels	O
of	O
the	O
hidden	O
state	O
until	O
the	O
computation	O
of	O
the	O
next	O
QRNN	Method
layer	Method
.	O
subsection	O
:	O
Language	Method
Modeling	Method
We	O
replicate	O
the	O
language	Method
modeling	Method
experiment	O
of	O
Zaremba2014	O
and	O
Gal2015	O
to	O
benchmark	O
the	O
QRNN	Method
architecture	Method
for	O
natural	Task
language	Task
sequence	Task
prediction	Task
.	O
The	O
experiment	O
uses	O
a	O
standard	O
preprocessed	Method
version	Method
of	O
the	O
Penn	Material
Treebank	Material
(	O
PTB	Material
)	O
by	O
Mikolov2010	O
.	O
We	O
implemented	O
a	O
gated	Method
QRNN	Method
model	Method
with	O
medium	O
hidden	O
size	O
:	O
2	O
layers	O
with	O
640	O
units	O
in	O
each	O
layer	O
.	O
Both	O
QRNN	Method
layers	Method
use	O
a	O
convolutional	Method
filter	Method
width	Method
of	O
two	O
timesteps	O
.	O
While	O
the	O
“	O
medium	Method
”	Method
models	Method
used	O
in	O
other	O
work	O
Zaremba2014	O
,	O
Gal2015	O
consist	O
of	O
650	O
units	O
in	O
each	O
layer	O
,	O
it	O
was	O
more	O
computationally	O
convenient	O
to	O
use	O
a	O
multiple	O
of	O
32	O
.	O
As	O
the	O
Penn	Material
Treebank	Material
is	O
a	O
relatively	O
small	O
dataset	O
,	O
preventing	O
overfitting	Task
is	O
of	O
considerable	O
importance	O
and	O
a	O
major	O
focus	O
of	O
recent	O
research	O
.	O
It	O
is	O
not	O
obvious	O
in	O
advance	O
which	O
of	O
the	O
many	O
RNN	Method
regularization	Method
schemes	Method
would	O
perform	O
well	O
when	O
applied	O
to	O
the	O
QRNN	Method
.	O
Our	O
tests	O
showed	O
encouraging	O
results	O
from	O
zoneout	Method
applied	O
to	O
the	O
QRNN	Method
’s	Method
recurrent	Method
pooling	Method
layer	Method
,	O
implemented	O
as	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
The	O
experimental	O
settings	O
largely	O
followed	O
the	O
“	O
medium	O
”	O
setup	O
of	O
Zaremba2014	O
.	O
Optimization	Task
was	O
performed	O
by	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
without	O
momentum	Method
.	O
The	O
learning	Metric
rate	Metric
was	O
set	O
at	O
1	O
for	O
six	O
epochs	O
,	O
then	O
decayed	O
by	O
0.95	O
for	O
each	O
subsequent	O
epoch	O
,	O
for	O
a	O
total	O
of	O
72	O
epochs	O
.	O
We	O
additionally	O
used	O
regularization	O
of	O
and	O
rescaled	O
gradients	O
with	O
norm	O
above	O
10	O
.	O
Zoneout	Method
was	O
applied	O
by	O
performing	O
dropout	Method
with	O
ratio	O
0.1	O
on	O
the	O
forget	O
gates	O
of	O
the	O
QRNN	Method
,	O
without	O
rescaling	O
the	O
output	O
of	O
the	O
dropout	O
function	O
.	O
Batches	O
consist	O
of	O
20	O
examples	O
,	O
each	O
105	O
timesteps	O
.	O
Comparing	O
our	O
results	O
on	O
the	O
gated	Method
QRNN	Method
with	O
zoneout	Method
to	O
the	O
results	O
of	O
LSTMs	Method
with	O
both	O
ordinary	Method
and	Method
variational	Method
dropout	Method
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
see	O
that	O
the	O
QRNN	Method
is	O
highly	O
competitive	O
.	O
The	O
QRNN	O
without	O
zoneout	Method
strongly	O
outperforms	O
both	O
our	O
medium	O
LSTM	Method
and	O
the	O
medium	O
LSTM	Method
of	O
Zaremba2014	Method
which	O
do	O
not	O
use	O
recurrent	Method
dropout	Method
and	O
is	O
even	O
competitive	O
with	O
variational	Method
LSTMs	Method
.	O
This	O
may	O
be	O
due	O
to	O
the	O
limited	O
computational	O
capacity	O
that	O
the	O
QRNN	Method
’s	Method
pooling	Method
layer	Method
has	O
relative	O
to	O
the	O
LSTM	Method
’s	O
recurrent	O
weights	O
,	O
providing	O
structural	O
regularization	O
over	O
the	O
recurrence	O
.	O
Without	O
zoneout	Method
,	O
early	O
stopping	O
based	O
upon	O
validation	Metric
loss	Metric
was	O
required	O
as	O
the	O
QRNN	Method
would	O
begin	O
overfitting	O
.	O
By	O
applying	O
a	O
small	O
amount	O
of	O
zoneout	Method
(	O
)	O
,	O
no	O
early	O
stopping	O
is	O
required	O
and	O
the	O
QRNN	Method
achieves	O
competitive	O
levels	O
of	O
perplexity	Metric
to	O
the	O
variational	O
LSTM	Method
of	O
Gal2015	O
,	O
which	O
had	O
variational	Method
inference	Method
based	Method
dropout	Method
of	Method
0.2	Method
applied	O
recurrently	O
.	O
Their	O
best	O
performing	O
variation	O
also	O
used	O
Monte	Method
Carlo	Method
(	Method
MC	Method
)	Method
dropout	Method
averaging	Method
at	O
test	O
time	O
of	O
1000	O
different	O
masks	O
,	O
making	O
it	O
computationally	O
more	O
expensive	O
to	O
run	O
.	O
When	O
training	O
on	O
the	O
PTB	Material
dataset	O
with	O
an	O
NVIDIA	Method
K40	Method
GPU	Method
,	O
we	O
found	O
that	O
the	O
QRNN	Method
is	O
substantially	O
faster	O
than	O
a	O
standard	O
LSTM	Method
,	O
even	O
when	O
comparing	O
against	O
the	O
optimized	O
cuDNN	O
LSTM	Method
.	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
provide	O
a	O
breakdown	O
of	O
the	O
time	O
taken	O
for	O
Chainer	O
’s	O
default	O
LSTM	Method
,	O
the	O
cuDNN	O
LSTM	Method
,	O
and	O
QRNN	Method
to	O
perform	O
a	O
full	O
forward	Method
and	Method
backward	Method
pass	Method
on	O
a	O
single	O
batch	O
during	O
training	O
of	O
the	O
RNN	Method
LM	Method
on	O
PTB	Material
.	O
For	O
both	O
LSTM	Method
implementations	O
,	O
running	Metric
time	Metric
was	O
dominated	O
by	O
the	O
RNN	Method
computations	Method
,	O
even	O
with	O
the	O
highly	O
optimized	O
cuDNN	Method
implementation	Method
.	O
For	O
the	O
QRNN	Method
implementation	Method
,	O
however	O
,	O
the	O
“	O
RNN	Method
”	Method
layers	Method
are	O
no	O
longer	O
the	O
bottleneck	O
.	O
Indeed	O
,	O
there	O
are	O
diminishing	O
returns	O
from	O
further	O
optimization	Task
of	O
the	O
QRNN	Method
itself	Method
as	O
the	O
softmax	O
and	O
optimization	O
overhead	O
take	O
equal	O
or	O
greater	O
time	O
.	O
Note	O
that	O
the	O
softmax	Method
,	O
over	O
a	O
vocabulary	O
size	O
of	O
only	O
10	O
,	O
000	O
words	O
,	O
is	O
relatively	O
small	O
;	O
for	O
tasks	O
with	O
larger	O
vocabularies	O
,	O
the	O
softmax	Method
would	O
likely	O
dominate	O
computation	Metric
time	Metric
.	O
It	O
is	O
also	O
important	O
to	O
note	O
that	O
the	O
cuDNN	Method
library	Method
’s	Method
RNN	Method
primitives	Method
do	O
not	O
natively	O
support	O
any	O
form	O
of	O
recurrent	Method
dropout	Method
.	O
That	O
is	O
,	O
running	O
an	O
LSTM	Method
that	O
uses	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
regularization	Method
scheme	Method
at	O
cuDNN	Method
-	O
like	O
speeds	O
would	O
likely	O
require	O
an	O
entirely	O
custom	O
kernel	O
.	O
Batch	O
size	O
subsection	O
:	O
Character	Task
-	Task
level	Task
Neural	Task
Machine	Task
Translation	Task
We	O
evaluate	O
the	O
sequence	Method
-	Method
to	Method
-	Method
sequence	Method
QRNN	Method
architecture	Method
described	O
in	O
[	O
reference	O
]	O
on	O
a	O
challenging	O
neural	O
machine	Task
translation	Task
task	O
,	O
IWSLT	Task
German	Task
–	Task
English	Task
spoken	Task
-	Task
domain	Task
translation	Task
,	O
applying	O
fully	Task
character	Task
-	Task
level	Task
segmentation	Task
.	O
This	O
dataset	O
consists	O
of	O
209	O
,	O
772	O
sentence	O
pairs	O
of	O
parallel	O
training	O
data	O
from	O
transcribed	O
TED	O
and	O
TEDx	O
presentations	O
,	O
with	O
a	O
mean	O
sentence	O
length	O
of	O
103	O
characters	O
for	O
German	Material
and	O
93	O
for	O
English	Material
.	O
We	O
remove	O
training	O
sentences	O
with	O
more	O
than	O
300	O
characters	O
in	O
English	O
or	O
German	O
,	O
and	O
use	O
a	O
unified	O
vocabulary	O
of	O
187	O
Unicode	O
code	O
points	O
.	O
Our	O
best	O
performance	O
on	O
a	O
development	O
set	O
(	O
TED.tst2013	Material
)	O
was	O
achieved	O
using	O
a	O
four	Method
-	Method
layer	Method
encoder	Method
–	Method
decoder	Method
QRNN	Method
with	O
320	O
units	O
per	O
layer	O
,	O
no	O
dropout	O
or	O
regularization	O
,	O
and	O
gradient	Method
rescaling	Method
to	O
a	O
maximum	O
magnitude	O
of	O
5	O
.	O
Inputs	O
were	O
supplied	O
to	O
the	O
encoder	O
reversed	O
,	O
while	O
the	O
encoder	Method
convolutions	Method
were	O
not	O
masked	O
.	O
The	O
first	O
encoder	Method
layer	Method
used	O
convolutional	O
filter	O
width	O
,	O
while	O
the	O
other	O
encoder	Method
layers	Method
used	O
.	O
Optimization	Task
was	O
performed	O
for	O
10	O
epochs	O
on	O
minibatches	O
of	O
16	O
examples	O
using	O
Adam	Method
kingma2014adam	Method
with	O
,	O
,	O
,	O
and	O
.	O
Decoding	Task
was	O
performed	O
using	O
beam	Method
search	Method
with	O
beam	O
width	O
8	O
and	O
length	Method
normalization	Method
.	O
The	O
modified	O
log	Metric
-	Metric
probability	Metric
ranking	Metric
criterion	Metric
is	O
provided	O
in	O
the	O
appendix	O
.	O
Results	O
using	O
this	O
architecture	O
were	O
compared	O
to	O
an	O
equal	O
-	O
sized	O
four	O
-	O
layer	O
encoder	O
–	O
decoder	O
LSTM	Method
with	O
attention	Method
,	O
applying	O
dropout	Method
of	Method
0.2	Method
.	O
We	O
again	O
optimized	O
using	O
Adam	Method
;	O
other	O
hyperparameters	O
were	O
equal	O
to	O
their	O
values	O
for	O
the	O
QRNN	Method
and	O
the	O
same	O
beam	Method
search	Method
procedure	Method
was	O
applied	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
QRNN	Method
outperformed	O
the	O
character	O
-	O
level	O
LSTM	Method
,	O
almost	O
matching	O
the	O
performance	O
of	O
a	O
word	Method
-	Method
level	Method
attentional	Method
baseline	Method
.	O
section	O
:	O
Related	O
Work	O
Exploring	O
alternatives	O
to	O
traditional	O
RNNs	Method
for	O
sequence	Task
tasks	Task
is	O
a	O
major	O
area	O
of	O
current	O
research	O
.	O
Quasi	Method
-	Method
recurrent	Method
neural	Method
networks	Method
are	O
related	O
to	O
several	O
such	O
recently	O
described	O
models	O
,	O
especially	O
the	O
strongly	Method
-	Method
typed	Method
recurrent	Method
neural	Method
networks	Method
(	O
T	Method
-	Method
RNN	Method
)	O
introduced	O
by	O
.	O
While	O
the	O
motivation	O
and	O
constraints	O
described	O
in	O
that	O
work	O
are	O
different	O
,	O
’s	O
concepts	O
of	O
“	O
learnware	O
”	O
and	O
“	O
firmware	Method
”	O
parallel	O
our	O
discussion	O
of	O
convolution	Method
-	Method
like	Method
and	Method
pooling	Method
-	Method
like	Method
subcomponents	Method
.	O
As	O
the	O
use	O
of	O
a	O
fully	O
connected	O
layer	O
for	O
recurrent	O
connections	O
violates	O
the	O
constraint	O
of	O
“	O
strong	O
typing	O
”	O
,	O
all	O
strongly	Method
-	Method
typed	Method
RNN	Method
architectures	Method
(	O
including	O
the	O
T	Method
-	Method
RNN	Method
,	O
T	Method
-	Method
GRU	Method
,	O
and	O
T	O
-	O
LSTM	Method
)	O
are	O
also	O
quasi	Method
-	Method
recurrent	Method
.	O
However	O
,	O
some	O
QRNN	Method
models	Method
(	O
including	O
those	O
with	O
attention	O
or	O
skip	O
-	O
connections	O
)	O
are	O
not	O
“	O
strongly	O
typed	O
”	O
.	O
In	O
particular	O
,	O
a	O
T	Method
-	Method
RNN	Method
differs	O
from	O
a	O
QRNN	Method
as	O
described	O
in	O
this	O
paper	O
with	O
filter	O
size	O
1	O
and	O
f	Method
-	Method
pooling	Method
only	O
in	O
the	O
absence	O
of	O
an	O
activation	O
function	O
on	O
.	O
Similarly	O
,	O
T	Method
-	Method
GRUs	Method
and	O
T	Method
-	Method
LSTMs	Method
differ	O
from	O
QRNNs	Method
with	O
filter	O
size	O
2	O
and	O
fo	Method
-	Method
or	Method
ifo	Method
-	Method
pooling	Method
respectively	O
in	O
that	O
they	O
lack	O
on	O
and	O
use	O
rather	O
than	O
sigmoid	O
on	O
.	O
The	O
QRNN	Method
is	O
also	O
related	O
to	O
work	O
in	O
hybrid	Method
convolutional	Method
–	Method
recurrent	Method
models	Method
.	O
Zhou2015b	O
apply	O
CNNs	Method
at	O
the	O
word	O
level	O
to	O
generate	O
-	O
gram	O
features	O
used	O
by	O
an	O
LSTM	Method
for	O
text	Task
classification	Task
.	O
Xiao2016	O
also	O
tackle	O
text	Task
classification	Task
by	O
applying	O
convolutions	Method
at	O
the	O
character	O
level	O
,	O
with	O
a	O
stride	O
to	O
reduce	O
sequence	O
length	O
,	O
then	O
feeding	O
these	O
features	O
into	O
a	O
bidirectional	O
LSTM	Method
.	O
A	O
similar	O
approach	O
was	O
taken	O
by	O
Lee2016	O
for	O
character	O
-	O
level	O
machine	Task
translation	Task
.	O
Their	O
model	O
’s	O
encoder	Method
uses	O
a	O
convolutional	Method
layer	Method
followed	O
by	O
max	Method
-	Method
pooling	Method
to	O
reduce	O
sequence	O
length	O
,	O
a	O
four	Method
-	Method
layer	Method
highway	Method
network	Method
,	O
and	O
a	O
bidirectional	Method
GRU	Method
.	O
The	O
parallelism	O
of	O
the	O
convolutional	Method
,	Method
pooling	Method
,	Method
and	Method
highway	Method
layers	Method
allows	O
training	O
speed	O
comparable	O
to	O
subword	Method
-	Method
level	Method
models	Method
without	O
hard	Task
-	Task
coded	Task
text	Task
segmentation	Task
.	O
The	O
QRNN	Method
encoder	Method
–	Method
decoder	Method
model	Method
shares	O
the	O
favorable	O
parallelism	O
and	O
path	O
-	O
length	O
properties	O
exhibited	O
by	O
the	O
ByteNet	Method
Kalchbrenner2016	O
,	O
an	O
architecture	O
for	O
character	O
-	O
level	O
machine	Task
translation	Task
based	O
on	O
residual	Method
convolutions	Method
over	O
binary	O
trees	O
.	O
Their	O
model	O
was	O
constructed	O
to	O
achieve	O
three	O
desired	O
properties	O
:	O
parallelism	O
,	O
linear	Metric
-	Metric
time	Metric
computational	Metric
complexity	Metric
,	O
and	O
short	O
paths	O
between	O
any	O
pair	O
of	O
words	O
in	O
order	O
to	O
better	O
propagate	O
gradient	O
signals	O
.	O
section	O
:	O
Conclusion	O
Intuitively	O
,	O
many	O
aspects	O
of	O
the	O
semantics	O
of	O
long	O
sequences	O
are	O
context	O
-	O
invariant	O
and	O
can	O
be	O
computed	O
in	O
parallel	O
(	O
e.g.	O
,	O
convolutionally	O
)	O
,	O
but	O
some	O
aspects	O
require	O
long	O
-	O
distance	O
context	O
and	O
must	O
be	O
computed	O
recurrently	O
.	O
Many	O
existing	O
neural	Method
network	Method
architectures	Method
either	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
contextual	O
information	O
or	O
fail	O
to	O
take	O
advantage	O
of	O
the	O
parallelism	O
.	O
QRNNs	Method
exploit	O
both	O
parallelism	O
and	O
context	O
,	O
exhibiting	O
advantages	O
from	O
both	O
convolutional	Method
and	Method
recurrent	Method
neural	Method
networks	Method
.	O
QRNNs	Method
have	O
better	O
predictive	Metric
accuracy	Metric
than	O
LSTM	Method
-	O
based	O
models	O
of	O
equal	Method
hidden	Method
size	Method
,	O
even	O
though	O
they	O
use	O
fewer	O
parameters	O
and	O
run	O
substantially	O
faster	O
.	O
Our	O
experiments	O
show	O
that	O
the	O
speed	Metric
and	Metric
accuracy	Metric
advantages	Metric
remain	O
consistent	O
across	O
tasks	O
and	O
at	O
both	O
word	O
and	O
character	O
levels	O
.	O
Extensions	O
to	O
both	O
CNNs	Method
and	O
RNNs	Method
are	O
often	O
directly	O
applicable	O
to	O
the	O
QRNN	Method
,	O
while	O
the	O
model	O
’s	O
hidden	O
states	O
are	O
more	O
interpretable	O
than	O
those	O
of	O
other	O
recurrent	Method
architectures	Method
as	O
its	O
channels	O
maintain	O
their	O
independence	O
across	O
timesteps	O
.	O
We	O
believe	O
that	O
QRNNs	Method
can	O
serve	O
as	O
a	O
building	O
block	O
for	O
long	Task
-	Task
sequence	Task
tasks	Task
that	O
were	O
previously	O
impractical	O
with	O
traditional	O
RNNs	Method
.	O
bibliography	O
:	O
References	O
section	O
:	O
Appendix	O
subsection	O
:	O
Beam	Metric
search	Metric
ranking	Metric
criterion	Metric
The	O
modified	O
log	Metric
-	Metric
probability	Metric
ranking	Metric
criterion	Metric
we	O
used	O
in	O
beam	Task
search	Task
for	O
translation	Task
experiments	Task
is	O
:	O
where	O
is	O
a	O
length	O
normalization	O
parameter	O
Wu2016	O
,	O
is	O
the	O
th	O
output	O
character	O
,	O
and	O
is	O
a	O
“	O
target	O
length	O
”	O
equal	O
to	O
the	O
source	O
sentence	O
length	O
plus	O
five	O
characters	O
.	O
This	O
reduces	O
at	O
to	O
ordinary	O
beam	Method
search	Method
with	O
probabilities	O
:	O
and	O
at	O
to	O
beam	Method
search	Method
with	O
probabilities	O
normalized	O
by	O
length	O
(	O
up	O
to	O
the	O
target	O
length	O
)	O
:	O
Conveniently	O
,	O
this	O
ranking	Metric
criterion	Metric
can	O
be	O
computed	O
at	O
intermediate	O
beam	O
-	O
search	O
timesteps	O
,	O
obviating	O
the	O
need	O
to	O
apply	O
a	O
separate	O
reranking	Method
on	O
complete	O
hypotheses	O
.	O
