document	O
:	O
A	O
Thorough	O
Examination	O
of	O
the	O
CNN	Material
/	O
Daily	Material
Mail	Material
Reading	O
Comprehension	O
Task	O
Enabling	O
a	O
computer	O
to	O
understand	O
a	O
document	O
so	O
that	O
it	O
can	O
answer	O
comprehension	Task
questions	Task
is	O
a	O
central	O
,	O
yet	O
unsolved	O
goal	O
of	O
NLP	Task
.	O
A	O
key	O
factor	O
impeding	O
its	O
solution	O
by	O
machine	Method
learned	Method
systems	Method
is	O
the	O
limited	O
availability	O
of	O
human	O
-	O
annotated	O
data	O
.	O
hermann2015teaching	O
seek	O
to	O
solve	O
this	O
problem	O
by	O
creating	O
over	O
a	O
million	O
training	O
examples	O
by	O
pairing	O
CNN	Material
and	Material
Daily	Material
Mail	Material
news	Material
articles	Material
with	O
their	O
summarized	O
bullet	O
points	O
,	O
and	O
show	O
that	O
a	O
neural	Method
network	Method
can	O
then	O
be	O
trained	O
to	O
give	O
good	O
performance	O
on	O
this	O
task	O
.	O
In	O
this	O
paper	O
,	O
we	O
conduct	O
a	O
thorough	O
examination	O
of	O
this	O
new	O
reading	Task
comprehension	Task
task	Task
.	O
Our	O
primary	O
aim	O
is	O
to	O
understand	O
what	O
depth	O
of	O
language	Task
understanding	Task
is	O
required	O
to	O
do	O
well	O
on	O
this	O
task	O
.	O
We	O
approach	O
this	O
from	O
one	O
side	O
by	O
doing	O
a	O
careful	O
hand	O
-	O
analysis	O
of	O
a	O
small	O
subset	O
of	O
the	O
problems	O
and	O
from	O
the	O
other	O
by	O
showing	O
that	O
simple	O
,	O
carefully	O
designed	O
systems	O
can	O
obtain	O
accuracies	Metric
of	O
73.6	O
%	O
and	O
76.6	O
%	O
on	O
these	O
two	O
datasets	O
,	O
exceeding	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
by	O
7–10	O
%	O
and	O
approaching	O
what	O
we	O
believe	O
is	O
the	O
ceiling	O
for	O
performance	O
on	O
this	O
task	O
.	O
section	O
:	O
Introduction	O
Reading	Task
comprehension	Task
(	O
RC	Task
)	O
is	O
the	O
ability	O
to	O
read	O
text	O
,	O
process	O
it	O
,	O
and	O
understand	O
its	O
meaning	O
.	O
How	O
to	O
endow	O
computers	O
with	O
this	O
capacity	O
has	O
been	O
an	O
elusive	O
challenge	O
and	O
a	O
long	O
-	O
standing	O
goal	O
of	O
Artificial	Task
Intelligence	Task
(	O
e.g.	O
,	O
)	O
.	O
Genuine	Task
reading	Task
comprehension	Task
involves	O
interpretation	Task
of	Task
the	Task
text	Task
and	O
making	O
complex	O
inferences	O
.	O
Human	Task
reading	Task
comprehension	Task
is	O
often	O
tested	O
by	O
asking	O
questions	O
that	O
require	O
interpretive	O
understanding	O
of	O
a	O
passage	O
,	O
and	O
the	O
same	O
approach	O
has	O
been	O
suggested	O
for	O
testing	O
computers	O
.	O
In	O
recent	O
years	O
,	O
there	O
have	O
been	O
several	O
strands	O
of	O
work	O
which	O
attempt	O
to	O
collect	O
human	O
-	O
labeled	O
data	O
for	O
this	O
task	O
–	O
in	O
the	O
form	O
of	O
document	Task
,	Task
question	Task
and	Task
answer	Task
triples	Task
–	O
and	O
to	O
learn	O
machine	Method
learning	Method
models	Method
directly	O
from	O
it	O
.	O
However	O
,	O
these	O
datasets	O
consist	O
of	O
only	O
hundreds	O
of	O
documents	O
,	O
as	O
the	O
labeled	O
examples	O
usually	O
require	O
considerable	O
expertise	O
and	O
neat	O
design	O
,	O
making	O
the	O
annotation	Task
process	Task
quite	O
expensive	O
.	O
The	O
subsequent	O
scarcity	O
of	O
labeled	O
examples	O
prevents	O
us	O
from	O
training	O
powerful	O
statistical	Method
models	Method
,	O
such	O
as	O
deep	Method
learning	Method
models	Method
,	O
and	O
would	O
seem	O
to	O
prevent	O
a	O
system	O
from	O
learning	O
complex	O
textual	Task
reasoning	Task
capacities	Task
.	O
Recently	O
,	O
researchers	O
at	O
DeepMind	Method
had	O
the	O
appealing	O
,	O
original	O
idea	O
of	O
exploiting	O
the	O
fact	O
that	O
the	O
abundant	O
news	Material
articles	Material
of	O
CNN	Material
and	O
Daily	Material
Mail	Material
are	O
accompanied	O
by	O
bullet	O
point	O
summaries	O
in	O
order	O
to	O
heuristically	O
create	O
large	O
-	O
scale	O
supervised	O
training	O
data	O
for	O
the	O
reading	Task
comprehension	Task
task	Task
.	O
Figure	O
[	O
reference	O
]	O
gives	O
an	O
example	O
.	O
Their	O
idea	O
is	O
that	O
a	O
bullet	O
point	O
usually	O
summarizes	O
one	O
or	O
several	O
aspects	O
of	O
the	O
article	O
.	O
If	O
the	O
computer	O
understands	O
the	O
content	O
of	O
the	O
article	O
,	O
it	O
should	O
be	O
able	O
to	O
infer	O
the	O
missing	O
entity	O
in	O
the	O
bullet	O
point	O
.	O
[	O
scale=0.38	O
]	O
figures	O
/	O
fig_example.pdf	O
This	O
is	O
a	O
clever	O
way	O
of	O
creating	O
supervised	O
data	O
cheaply	O
and	O
holds	O
promise	O
for	O
making	O
progress	O
on	O
training	O
RC	Method
models	Method
;	O
however	O
,	O
it	O
is	O
unclear	O
what	O
level	O
of	O
reading	Task
comprehension	Task
is	O
actually	O
needed	O
to	O
solve	O
this	O
somewhat	O
artificial	O
task	O
and	O
,	O
indeed	O
,	O
what	O
statistical	Method
models	Method
that	O
do	O
reasonably	O
well	O
on	O
this	O
task	O
have	O
actually	O
learned	O
.	O
In	O
this	O
paper	O
,	O
our	O
aim	O
is	O
to	O
provide	O
an	O
in	O
-	O
depth	O
and	O
thoughtful	O
analysis	O
of	O
this	O
dataset	O
and	O
what	O
level	O
of	O
natural	Task
language	Task
understanding	Task
is	O
needed	O
to	O
do	O
well	O
on	O
it	O
.	O
We	O
demonstrate	O
that	O
simple	O
,	O
carefully	O
designed	O
systems	O
can	O
obtain	O
high	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	Metric
of	O
73.6	O
%	O
and	O
76.6	O
%	O
on	O
CNN	Material
and	O
Daily	Material
Mail	Material
respectively	O
.	O
We	O
do	O
a	O
careful	O
hand	O
-	O
analysis	O
of	O
a	O
small	O
subset	O
of	O
the	O
problems	O
to	O
provide	O
data	O
on	O
their	O
difficulty	O
and	O
what	O
kinds	O
of	O
language	Task
understanding	Task
are	O
needed	O
to	O
be	O
successful	O
and	O
we	O
try	O
to	O
diagnose	O
what	O
is	O
learned	O
by	O
the	O
systems	O
that	O
we	O
have	O
built	O
.	O
We	O
conclude	O
that	O
:	O
(	O
i	O
)	O
this	O
dataset	O
is	O
easier	O
than	O
previously	O
realized	O
,	O
(	O
ii	O
)	O
straightforward	O
,	O
conventional	O
NLP	Method
systems	Method
can	O
do	O
much	O
better	O
on	O
it	O
than	O
previously	O
suggested	O
,	O
(	O
iii	O
)	O
the	O
distributed	Method
representations	Method
of	O
deep	Method
learning	Method
systems	Method
are	O
very	O
effective	O
at	O
recognizing	Task
paraphrases	Task
,	O
(	O
iv	O
)	O
partly	O
because	O
of	O
the	O
nature	O
of	O
the	O
questions	O
,	O
current	O
systems	O
much	O
more	O
have	O
the	O
nature	O
of	O
single	Method
-	Method
sentence	Method
relation	Method
extraction	Method
systems	Method
than	O
larger	Task
-	Task
discourse	Task
-	Task
context	Task
text	Task
understanding	Task
systems	Task
,	O
(	O
v	O
)	O
the	O
systems	O
that	O
we	O
present	O
here	O
are	O
close	O
to	O
the	O
ceiling	O
of	O
performance	O
for	O
single	O
-	O
sentence	O
and	O
unambiguous	O
cases	O
of	O
this	O
dataset	O
,	O
and	O
(	O
vi	O
)	O
the	O
prospects	O
for	O
getting	O
the	O
final	O
20	O
%	O
of	O
questions	O
correct	O
appear	O
poor	O
,	O
since	O
most	O
of	O
them	O
involve	O
issues	O
in	O
the	O
data	Task
preparation	Task
which	O
undermine	O
the	O
chances	O
of	O
answering	O
the	O
question	O
(	O
coreference	O
errors	O
or	O
anonymization	O
of	O
entities	O
making	O
understanding	Task
too	O
difficult	O
)	O
.	O
section	O
:	O
The	O
Reading	Task
Comprehension	Task
Task	Task
The	O
RC	O
datasets	O
introduced	O
in	O
are	O
made	O
from	O
articles	O
on	O
the	O
news	Material
websites	Material
CNN	Material
and	O
Daily	Material
Mail	Material
,	O
utilizing	O
articles	O
and	O
their	O
bullet	O
point	O
summaries	O
.	O
Figure	O
[	O
reference	O
]	O
demonstrates	O
an	O
example	O
:	O
it	O
consists	O
of	O
a	O
passage	O
,	O
a	O
question	O
and	O
an	O
answer	O
,	O
where	O
the	O
passage	O
is	O
a	O
news	Material
article	Material
,	O
the	O
question	O
is	O
a	O
cloze	Task
-	Task
style	Task
task	Task
,	O
in	O
which	O
one	O
of	O
the	O
article	O
’s	O
bullet	O
points	O
has	O
had	O
one	O
entity	O
replaced	O
by	O
a	O
placeholder	O
,	O
and	O
the	O
answer	O
is	O
this	O
questioned	O
entity	O
.	O
The	O
goal	O
is	O
to	O
infer	O
the	O
missing	O
entity	O
(	O
answer	O
)	O
from	O
all	O
the	O
possible	O
entities	O
which	O
appear	O
in	O
the	O
passage	O
.	O
A	O
news	Material
article	Material
is	O
usually	O
associated	O
with	O
a	O
few	O
(	O
e.g.	O
,	O
3–5	O
)	O
bullet	O
points	O
and	O
each	O
of	O
them	O
highlights	O
one	O
aspect	O
of	O
its	O
content	O
.	O
The	O
text	O
has	O
been	O
run	O
through	O
a	O
Google	Method
NLP	Method
pipeline	Method
.	O
It	O
it	O
tokenized	O
,	O
lowercased	O
,	O
and	O
named	Task
entity	Task
recognition	Task
and	O
coreference	Task
resolution	Task
have	O
been	O
run	O
.	O
For	O
each	O
coreference	O
chain	O
containing	O
at	O
least	O
one	O
named	O
entity	O
,	O
all	O
items	O
in	O
the	O
chain	O
are	O
replaced	O
by	O
an	O
@entity	O
marker	O
,	O
for	O
a	O
distinct	O
index	O
.	O
hermann2015teaching	Method
argue	O
convincingly	O
that	O
such	O
a	O
strategy	O
is	O
necessary	O
to	O
ensure	O
that	O
systems	O
approach	O
this	O
task	O
by	O
understanding	O
the	O
passage	O
in	O
front	O
of	O
them	O
,	O
rather	O
than	O
by	O
using	O
world	O
knowledge	O
or	O
a	O
language	Method
model	Method
to	O
answer	O
questions	O
without	O
needing	O
to	O
understand	O
the	O
passage	O
.	O
However	O
,	O
this	O
also	O
gives	O
the	O
task	O
a	O
somewhat	O
artificial	O
character	O
.	O
On	O
the	O
one	O
hand	O
,	O
systems	O
are	O
greatly	O
helped	O
by	O
entity	Task
recognition	Task
and	O
coreference	Task
having	O
already	O
been	O
performed	O
;	O
on	O
the	O
other	O
,	O
they	O
suffer	O
when	O
either	O
of	O
these	O
modules	O
fail	O
,	O
as	O
they	O
do	O
(	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
“	O
the	O
character	O
”	O
should	O
probably	O
be	O
coreferent	O
with	O
@entity14	O
;	O
clearer	O
examples	O
of	O
failure	O
appear	O
later	O
on	O
in	O
our	O
data	Task
analysis	Task
)	O
.	O
Moreover	O
,	O
this	O
inability	O
to	O
use	O
world	O
knowledge	O
also	O
makes	O
it	O
much	O
more	O
difficult	O
for	O
a	O
human	O
to	O
do	O
this	O
task	O
–	O
occasionally	O
it	O
is	O
very	O
difficult	O
or	O
impossible	O
for	O
a	O
human	O
to	O
determine	O
the	O
correct	O
answer	O
when	O
presented	O
with	O
an	O
item	O
anonymized	O
in	O
this	O
way	O
.	O
The	O
creation	O
of	O
the	O
datasets	O
benefits	O
from	O
the	O
sheer	O
volume	O
of	O
news	Material
articles	Material
available	O
online	O
,	O
so	O
they	O
offer	O
a	O
large	O
and	O
realistic	O
testing	O
ground	O
for	O
statistical	Method
models	Method
.	O
Table	O
[	O
reference	O
]	O
provides	O
some	O
statistics	O
on	O
the	O
two	O
datasets	O
:	O
there	O
are	O
380k	O
and	O
879k	O
training	O
examples	O
for	O
CNN	Material
and	O
Daily	Material
Mail	Material
respectively	O
.	O
The	O
passages	O
are	O
around	O
30	O
sentences	O
and	O
800	O
tokens	O
on	O
average	O
,	O
while	O
each	O
question	O
contains	O
around	O
12–14	O
tokens	O
.	O
In	O
the	O
following	O
sections	O
,	O
we	O
seek	O
to	O
more	O
deeply	O
understand	O
the	O
nature	O
of	O
this	O
dataset	O
.	O
We	O
first	O
build	O
some	O
straightforward	O
systems	O
in	O
order	O
to	O
get	O
a	O
better	O
idea	O
of	O
a	O
lower	O
-	O
bound	O
for	O
the	O
performance	O
of	O
current	O
NLP	Method
systems	Method
.	O
Then	O
we	O
turn	O
to	O
data	Task
analysis	Task
of	O
a	O
sample	O
of	O
the	O
items	O
to	O
examine	O
their	O
nature	O
and	O
an	O
upper	O
bound	O
on	O
performance	O
.	O
section	O
:	O
Our	O
Systems	O
In	O
this	O
section	O
,	O
we	O
describe	O
two	O
systems	O
we	O
implemented	O
–	O
a	O
conventional	O
entity	Method
-	Method
centric	Method
classifier	Method
and	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
.	O
While	O
hermann2015teaching	O
do	O
provide	O
several	O
baselines	O
for	O
performance	O
on	O
the	O
RC	Task
task	Task
,	O
we	O
suspect	O
that	O
their	O
baselines	O
are	O
not	O
that	O
strong	O
.	O
They	O
attempt	O
to	O
use	O
a	O
frame	Method
-	Method
semantic	Method
parser	Method
,	O
and	O
we	O
feel	O
that	O
the	O
poor	O
coverage	O
of	O
that	O
parser	O
undermines	O
the	O
results	O
,	O
and	O
is	O
not	O
representative	O
of	O
what	O
a	O
straightforward	O
NLP	Method
system	Method
–	O
based	O
on	O
standard	O
approaches	O
to	O
factoid	Task
question	Task
answering	Task
and	O
relation	Task
extraction	Task
developed	O
over	O
the	O
last	O
15	O
years	O
–	O
can	O
achieve	O
.	O
Indeed	O
,	O
their	O
frame	Method
-	Method
semantic	Method
model	Method
is	O
markedly	O
inferior	O
to	O
another	O
baseline	O
they	O
provide	O
,	O
a	O
heuristic	Method
word	Method
distance	Method
model	Method
.	O
At	O
present	O
just	O
two	O
papers	O
are	O
available	O
presenting	O
results	O
on	O
this	O
RC	Task
task	Task
,	O
both	O
presenting	O
neural	Method
network	Method
approaches	Method
:	O
and	O
.	O
While	O
the	O
latter	O
is	O
wrapped	O
in	O
the	O
language	O
of	O
end	Method
-	Method
to	Method
-	Method
end	Method
memory	Method
networks	Method
,	O
it	O
actually	O
presents	O
a	O
fairly	O
simple	O
window	Method
-	Method
based	Method
neural	Method
network	Method
classifier	Method
running	O
on	O
the	O
CNN	Material
data	O
.	O
Its	O
success	O
again	O
raises	O
questions	O
about	O
the	O
true	O
nature	O
and	O
complexity	O
of	O
the	O
RC	Task
task	Task
provided	O
by	O
this	O
dataset	O
,	O
which	O
we	O
seek	O
to	O
clarify	O
by	O
building	O
a	O
simple	O
attention	Method
-	Method
based	Method
neural	Method
net	Method
classifier	Method
.	O
Given	O
the	O
(	O
passage	O
,	O
question	O
,	O
answer	O
)	O
triple	O
,	O
and	O
are	O
sequences	O
of	O
tokens	O
for	O
the	O
passage	O
and	O
question	O
sentence	O
,	O
with	O
containing	O
exactly	O
one	O
“	O
@placeholder	O
”	O
token	O
.	O
The	O
goal	O
is	O
to	O
infer	O
the	O
correct	O
entity	O
that	O
the	O
placeholder	O
corresponds	O
to	O
,	O
where	O
is	O
the	O
set	O
of	O
all	O
abstract	O
entity	O
markers	O
.	O
Note	O
that	O
the	O
correct	O
answer	O
entity	O
must	O
appear	O
in	O
the	O
passage	O
.	O
subsection	O
:	O
Entity	Method
-	Method
Centric	Method
Classifier	Method
We	O
first	O
build	O
a	O
conventional	O
feature	Method
-	Method
based	Method
classifier	Method
,	O
aiming	O
to	O
explore	O
what	O
features	O
are	O
effective	O
for	O
this	O
task	O
.	O
This	O
is	O
similar	O
in	O
spirit	O
to	O
,	O
which	O
at	O
present	O
has	O
very	O
competitive	O
performance	O
on	O
the	O
MCTest	Task
RC	O
dataset	O
.	O
The	O
setup	O
of	O
this	O
system	O
is	O
to	O
design	O
a	O
feature	O
vector	O
for	O
each	O
candidate	O
entity	O
,	O
and	O
to	O
learn	O
a	O
weight	O
vector	O
such	O
that	O
the	O
correct	O
answer	O
is	O
expected	O
to	O
rank	O
higher	O
than	O
all	O
other	O
candidate	O
entities	O
:	O
We	O
employ	O
the	O
following	O
feature	Method
templates	Method
:	O
Whether	O
entity	O
occurs	O
in	O
the	O
passage	O
.	O
Whether	O
entity	O
occurs	O
in	O
the	O
question	O
.	O
The	O
frequency	O
of	O
entity	O
in	O
the	O
passage	O
.	O
The	O
first	O
position	O
of	O
occurence	O
of	O
entity	O
in	O
the	O
passage	O
.	O
-	O
gram	O
exact	O
match	O
:	O
whether	O
there	O
is	O
an	O
exact	O
match	O
between	O
the	O
text	O
surrounding	O
the	O
placeholder	O
and	O
the	O
text	O
surrounding	O
entity	O
.	O
We	O
have	O
features	O
for	O
all	O
combinations	O
of	O
matching	O
left	O
and	O
/	O
or	O
right	O
one	O
or	O
two	O
words	O
.	O
Word	O
distance	O
:	O
we	O
align	O
the	O
placeholder	O
with	O
each	O
occurrence	O
of	O
entity	O
,	O
and	O
compute	O
the	O
average	Metric
minimum	Metric
distance	Metric
of	O
each	O
non	O
-	O
stop	O
question	O
word	O
from	O
the	O
entity	O
in	O
the	O
passage	O
.	O
Sentence	Task
co	Task
-	Task
occurrence	Task
:	O
whether	O
entity	O
co	O
-	O
occurs	O
with	O
another	O
entity	O
or	O
verb	O
that	O
appears	O
in	O
the	O
question	O
,	O
in	O
some	O
sentence	O
of	O
the	O
passage	O
.	O
Dependency	Task
parse	Task
match	Task
:	O
we	O
dependency	O
parse	O
both	O
the	O
question	O
and	O
all	O
the	O
sentences	O
in	O
the	O
passage	O
,	O
and	O
extract	O
an	O
indicator	O
feature	O
of	O
whether	O
and	O
are	O
both	O
found	O
;	O
similar	O
features	O
are	O
constructed	O
for	O
and	O
.	O
subsection	O
:	O
End	O
-	O
to	O
-	O
end	Method
Neural	Method
Network	Method
Our	O
neural	Method
network	Method
system	Method
is	O
based	O
on	O
the	O
AttentiveReader	Method
model	Method
proposed	O
by	O
.	O
The	O
framework	O
can	O
be	O
described	O
in	O
the	O
following	O
three	O
steps	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
:	O
[	O
scale=0.37	O
]	O
figures	O
/	O
fig_model.pdf	O
First	O
,	O
all	O
the	O
words	O
are	O
mapped	O
to	O
-	O
dimensional	O
vectors	O
via	O
an	O
embedding	Method
matrix	Method
;	O
therefore	O
we	O
have	O
:	O
and	O
.	O
Next	O
we	O
use	O
a	O
shallow	Method
bi	Method
-	Method
directional	Method
recurrent	Method
neural	Method
network	Method
(	O
RNN	Method
)	O
with	O
hidden	O
size	O
to	O
encode	O
contextual	O
embeddings	O
of	O
each	O
word	O
in	O
the	O
passage	O
,	O
and	O
,	O
where	O
.	O
Meanwhile	O
,	O
we	O
use	O
another	O
bi	Method
-	Method
directional	Method
RNN	Method
to	O
map	O
the	O
question	O
to	O
an	O
embedding	O
.	O
We	O
choose	O
to	O
use	O
Gated	Method
Recurrent	Method
Unit	Method
(	O
GRU	Method
)	O
in	O
our	O
experiments	O
because	O
it	O
performs	O
similarly	O
but	O
is	O
computationally	O
cheaper	O
than	O
LSTM	Method
.	O
In	O
this	O
step	O
,	O
the	O
goal	O
is	O
to	O
compare	O
the	O
question	Method
embedding	Method
and	O
all	O
the	O
contextual	Method
embeddings	Method
,	O
and	O
select	O
the	O
pieces	O
of	O
information	O
that	O
are	O
relevant	O
to	O
the	O
question	O
.	O
We	O
compute	O
a	O
probability	O
distribution	O
depending	O
on	O
the	O
degree	O
of	O
relevance	O
between	O
word	O
(	O
in	O
its	O
context	O
)	O
and	O
the	O
question	O
and	O
then	O
produce	O
an	O
output	O
vector	O
which	O
is	O
a	O
weighted	O
combination	O
of	O
all	O
contextual	O
embeddings	O
:	O
is	O
used	O
in	O
a	O
bilinear	O
term	O
,	O
which	O
allows	O
us	O
to	O
compute	O
a	O
similarity	O
between	O
and	O
more	O
flexibly	O
than	O
with	O
just	O
a	O
dot	Method
product	Method
.	O
Using	O
the	O
output	O
vector	O
,	O
the	O
system	O
outputs	O
the	O
most	O
likely	O
answer	O
using	O
:	O
Finally	O
,	O
the	O
system	O
adds	O
a	O
softmax	O
function	O
on	O
top	O
of	O
and	O
adopts	O
a	O
negative	O
log	O
-	O
likelihood	O
objective	O
for	O
training	O
.	O
paragraph	O
:	O
Differences	O
from	O
.	O
Our	O
model	O
basically	O
follows	O
the	O
AttentiveReader	Method
.	O
However	O
,	O
to	O
our	O
surprise	O
,	O
our	O
experiments	O
observed	O
nearly	O
7	O
–	O
10	O
%	O
improvement	O
over	O
the	O
original	O
AttentiveReader	O
results	O
on	O
CNN	Material
and	O
Daily	Material
Mail	Material
datasets	O
(	O
discussed	O
in	O
Sec	O
.	O
[	O
reference	O
]	O
)	O
.	O
Concretely	O
,	O
our	O
model	O
has	O
the	O
following	O
differences	O
:	O
We	O
use	O
a	O
bilinear	O
term	O
,	O
instead	O
of	O
a	O
layer	O
to	O
compute	O
the	O
relevance	O
(	O
attention	O
)	O
between	O
question	O
and	O
contextual	O
embeddings	O
.	O
The	O
effectiveness	O
of	O
the	O
simple	O
bilinear	Method
attention	Method
function	Method
has	O
been	O
shown	O
previously	O
for	O
neural	Task
machine	Task
translation	Task
by	O
.	O
After	O
obtaining	O
the	O
weighted	O
contextual	O
embeddings	O
,	O
we	O
use	O
for	O
direct	Task
prediction	Task
.	O
In	O
contrast	O
,	O
the	O
original	O
model	O
in	O
combined	O
and	O
the	O
question	Method
embedding	Method
via	O
another	O
non	Method
-	Method
linear	Method
layer	Method
before	O
making	O
final	O
predictions	Task
.	O
We	O
found	O
that	O
we	O
could	O
remove	O
this	O
layer	O
without	O
harming	O
performance	O
.	O
We	O
believe	O
it	O
is	O
sufficient	O
for	O
the	O
model	O
to	O
learn	O
to	O
return	O
the	O
entity	O
to	O
which	O
it	O
maximally	O
gives	O
attention	O
.	O
The	O
original	O
model	O
considers	O
all	O
the	O
words	O
from	O
the	O
vocabulary	O
in	O
making	O
predictions	Task
.	O
We	O
think	O
this	O
is	O
unnecessary	O
,	O
and	O
only	O
predict	O
among	O
entities	O
which	O
appear	O
in	O
the	O
passage	O
.	O
Of	O
these	O
changes	O
,	O
only	O
the	O
first	O
seems	O
important	O
;	O
the	O
other	O
two	O
just	O
aim	O
at	O
keeping	O
the	O
model	O
simple	O
.	O
paragraph	O
:	O
Window	Method
-	Method
based	Method
MemN2Ns	Method
.	O
Another	O
recent	O
neural	Method
network	Method
approach	Method
proposed	O
by	O
is	O
based	O
on	O
a	O
memory	Method
network	Method
architecture	Method
.	O
We	O
think	O
it	O
is	O
highly	O
similar	O
in	O
spirit	O
.	O
The	O
biggest	O
difference	O
is	O
their	O
way	O
of	O
encoding	O
passages	O
:	O
they	O
demonstrate	O
that	O
it	O
is	O
most	O
effective	O
to	O
only	O
use	O
a	O
5	O
-	O
word	O
context	O
window	O
when	O
evaluating	O
a	O
candidate	O
entity	O
and	O
they	O
use	O
a	O
positional	Method
unigram	Method
approach	Method
to	O
encode	O
the	O
contextual	O
embeddings	O
:	O
if	O
a	O
window	O
consists	O
of	O
5	O
words	O
,	O
then	O
it	O
is	O
encoded	O
as	O
,	O
resulting	O
in	O
separate	O
embedding	O
matrices	O
to	O
learn	O
.	O
They	O
encode	O
the	O
5	O
-	O
word	O
window	O
surrounding	O
the	O
placeholder	O
in	O
a	O
similar	O
way	O
and	O
all	O
other	O
words	O
in	O
the	O
question	O
text	O
are	O
ignored	O
.	O
In	O
addition	O
,	O
they	O
simply	O
use	O
a	O
dot	Method
product	Method
to	O
compute	O
the	O
“	O
relevance	O
”	O
between	O
the	O
question	O
and	O
a	O
contextual	Method
embedding	Method
.	O
This	O
simple	O
model	O
nevertheless	O
works	O
well	O
,	O
showing	O
the	O
extent	O
to	O
which	O
this	O
RC	Task
task	Task
can	O
be	O
done	O
by	O
very	O
local	Method
context	Method
matching	Method
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Training	O
Details	O
For	O
training	O
our	O
conventional	O
classifier	Method
,	O
we	O
use	O
the	O
implementation	O
of	O
LambdaMART	Method
in	O
the	O
RankLib	Method
package	Method
.	O
We	O
use	O
this	O
ranking	Method
algorithm	Method
since	O
our	O
problem	O
is	O
naturally	O
a	O
ranking	Task
problem	Task
and	O
forests	Method
of	Method
boosted	Method
decision	Method
trees	Method
have	O
been	O
very	O
successful	O
lately	O
(	O
as	O
seen	O
,	O
e.g.	O
,	O
in	O
many	O
recent	O
Kaggle	Material
competitions	Material
)	O
.	O
We	O
do	O
not	O
use	O
all	O
the	O
features	O
of	O
LambdaMART	Method
since	O
we	O
are	O
only	O
scoring	O
1	Metric
/	Metric
0	Metric
loss	Metric
on	O
the	O
first	O
ranked	O
proposal	O
,	O
rather	O
than	O
using	O
an	O
IR	Method
-	Method
style	Method
metric	Method
to	O
score	O
ranked	O
results	O
.	O
We	O
use	O
Stanford	Method
’s	Method
neural	Method
network	Method
dependency	Method
parser	Method
to	O
parse	O
all	O
our	O
document	O
and	O
question	O
text	O
,	O
and	O
all	O
other	O
features	O
can	O
be	O
extracted	O
without	O
additional	O
tools	O
.	O
For	O
training	O
our	O
neural	Method
networks	Method
,	O
we	O
only	O
keep	O
the	O
most	O
frequent	O
words	O
(	O
including	O
entity	O
and	O
placeholder	O
markers	O
)	O
,	O
and	O
map	O
all	O
other	O
words	O
to	O
an	O
¡	O
unk	O
¿	O
token	O
.	O
We	O
choose	O
word	O
embedding	O
size	O
,	O
and	O
use	O
the	O
-	O
dimensional	O
pre	O
-	O
trained	O
GloVe	Method
word	Method
embeddings	Method
for	O
initialization	Task
.	O
The	O
attention	O
and	O
output	O
parameters	O
are	O
initialized	O
from	O
a	O
uniform	O
distribution	O
between	O
,	O
and	O
the	O
GRU	O
weights	O
are	O
initialized	O
from	O
a	O
Gaussian	Method
distribution	Method
.	O
We	O
use	O
hidden	O
size	O
for	O
CNN	Material
and	O
256	O
for	O
Daily	Material
Mail	Material
.	O
Optimization	Task
is	O
carried	O
out	O
using	O
vanilla	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
,	O
with	O
a	O
fixed	O
learning	Metric
rate	Metric
of	O
.	O
We	O
sort	O
all	O
the	O
examples	O
by	O
the	O
length	O
of	O
its	O
passage	O
,	O
and	O
randomly	O
sample	O
a	O
mini	O
-	O
batch	O
of	O
size	O
32	O
for	O
each	O
update	O
.	O
We	O
also	O
apply	O
dropout	O
with	O
probability	O
to	O
the	O
embedding	Method
layer	Method
and	O
gradient	Method
clipping	Method
when	O
the	O
norm	O
of	O
gradients	O
exceeds	O
.	O
Additionally	O
,	O
we	O
think	O
the	O
original	O
indices	O
of	O
entity	O
markers	O
are	O
generated	O
arbitrarily	O
.	O
We	O
attempt	O
to	O
relabel	O
the	O
entity	O
markers	O
based	O
on	O
their	O
first	O
occurrence	O
in	O
the	O
passage	O
and	O
question	O
and	O
find	O
that	O
this	O
step	O
can	O
make	O
training	Task
converge	O
faster	O
as	O
well	O
bring	O
slight	O
gains	O
.	O
We	O
report	O
both	O
results	O
(	O
with	O
and	O
without	O
relabeling	Method
)	O
for	O
future	O
reference	O
.	O
All	O
of	O
our	O
models	O
are	O
run	O
on	O
a	O
single	O
GPU	Method
(	O
GeForce	O
GTX	O
TITAN	O
X	O
)	O
,	O
with	O
roughly	O
a	O
runtime	O
of	O
3	O
hours	O
per	O
epoch	O
for	O
CNN	Material
,	O
and	O
12	O
hours	O
per	O
epoch	O
for	O
Daily	Material
Mail	Material
.	O
We	O
run	O
all	O
the	O
models	O
up	O
to	O
epochs	O
and	O
select	O
the	O
model	O
that	O
achieves	O
the	O
best	O
accuracy	Metric
on	O
the	O
development	O
set	O
.	O
We	O
run	O
our	O
models	O
5	O
times	O
independently	O
with	O
different	O
random	O
seeds	O
and	O
report	O
average	O
performance	O
across	O
the	O
runs	O
.	O
We	O
also	O
report	O
ensemble	O
results	O
which	O
average	O
the	O
prediction	Metric
probabilities	Metric
of	O
the	O
5	O
models	O
.	O
subsection	O
:	O
Main	O
Results	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
main	O
results	O
.	O
The	O
conventional	O
feature	Method
-	Method
based	Method
classifier	Method
obtains	O
accuracy	Metric
on	O
the	O
CNN	Material
test	Material
set	Material
.	O
Not	O
only	O
does	O
this	O
significantly	O
outperform	O
any	O
of	O
the	O
symbolic	Method
approaches	Method
reported	O
in	O
,	O
it	O
also	O
outperforms	O
all	O
the	O
neural	Method
network	Method
systems	Method
from	O
their	O
paper	O
and	O
the	O
best	O
single	O
-	O
system	O
result	O
reported	O
so	O
far	O
from	O
.	O
This	O
suggests	O
that	O
the	O
task	O
might	O
not	O
be	O
as	O
difficult	O
as	O
suggested	O
,	O
and	O
a	O
simple	O
feature	Method
set	Method
can	O
cover	O
many	O
of	O
the	O
cases	O
.	O
Table	O
[	O
reference	O
]	O
presents	O
a	O
feature	Method
ablation	Method
analysis	Method
of	O
our	O
entity	Method
-	Method
centric	Method
classifier	Method
on	O
the	O
development	O
portion	O
of	O
the	O
CNN	Material
dataset	Material
.	O
It	O
shows	O
that	O
-	O
gram	O
match	O
and	O
frequency	O
of	O
entities	O
are	O
the	O
two	O
most	O
important	O
classes	O
of	O
features	O
.	O
More	O
dramatically	O
,	O
our	O
single	Method
-	Method
model	Method
neural	Method
network	Method
surpasses	O
the	O
previous	O
results	O
by	O
a	O
large	O
margin	O
(	O
over	O
5	O
%	O
)	O
.	O
The	O
relabeling	Method
process	Method
further	O
improves	O
the	O
results	O
by	O
and	O
,	O
pushing	O
up	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	Metric
to	O
73.6	O
%	O
and	O
76.6	O
%	O
on	O
the	O
two	O
datasets	O
respectively	O
.	O
The	O
ensembles	Method
of	Method
models	Method
consistently	O
bring	O
further	O
gains	O
.	O
Concurrently	O
with	O
our	O
paper	O
,	O
kadlec2016text	O
and	O
kobayashi2016dynamic	O
also	O
experiment	O
on	O
these	O
two	O
datasets	O
and	O
report	O
competitive	O
results	O
.	O
However	O
,	O
our	O
model	O
not	O
only	O
still	O
outperforms	O
theirs	O
,	O
but	O
also	O
appears	O
to	O
be	O
structurally	O
simpler	O
.	O
All	O
these	O
recent	O
efforts	O
converge	O
to	O
similar	O
numbers	O
,	O
and	O
we	O
believe	O
that	O
they	O
are	O
approaching	O
the	O
ceiling	O
performance	O
of	O
this	O
task	O
,	O
as	O
we	O
will	O
indicate	O
in	O
the	O
next	O
section	O
.	O
section	O
:	O
Data	Task
Analysis	Task
So	O
far	O
,	O
we	O
have	O
good	O
results	O
via	O
either	O
of	O
our	O
systems	O
.	O
In	O
this	O
section	O
,	O
we	O
aim	O
to	O
conduct	O
an	O
in	O
-	O
depth	O
analysis	O
and	O
answer	O
the	O
following	O
questions	O
:	O
(	O
i	O
)	O
Since	O
the	O
dataset	O
was	O
created	O
in	O
an	O
automatic	O
and	O
heuristic	O
way	O
,	O
how	O
many	O
of	O
the	O
questions	O
are	O
trivial	O
to	O
answer	O
,	O
and	O
how	O
many	O
are	O
noisy	O
and	O
not	O
answerable	O
?	O
(	O
ii	O
)	O
What	O
have	O
these	O
models	O
learned	O
?	O
What	O
are	O
the	O
prospects	O
for	O
further	O
improving	O
them	O
?	O
To	O
study	O
this	O
,	O
we	O
randomly	O
sampled	O
100	O
examples	O
from	O
the	O
dev	O
portion	O
of	O
the	O
CNN	Material
dataset	Material
for	O
analysis	O
(	O
see	O
more	O
details	O
in	O
Appendix	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Breakdown	O
of	O
the	O
Examples	O
After	O
carefully	O
analyzing	O
these	O
100	O
examples	O
,	O
we	O
roughly	O
classify	O
them	O
into	O
the	O
following	O
categories	O
(	O
if	O
an	O
example	O
satisfies	O
more	O
than	O
one	O
category	O
,	O
we	O
classify	O
it	O
into	O
the	O
earliest	O
one	O
)	O
:	O
The	O
nearest	O
words	O
around	O
the	O
placeholder	O
are	O
also	O
found	O
in	O
the	O
passage	O
surrounding	O
an	O
entity	O
marker	O
;	O
the	O
answer	O
is	O
self	O
-	O
evident	O
.	O
The	O
question	O
text	O
is	O
entailed	O
/	O
rephrased	O
by	O
exactly	O
one	O
sentence	O
in	O
the	O
passage	O
,	O
so	O
the	O
answer	O
can	O
definitely	O
be	O
identified	O
from	O
that	O
sentence	O
.	O
In	O
many	O
cases	O
,	O
even	O
though	O
we	O
can	O
not	O
find	O
a	O
complete	O
semantic	O
match	O
between	O
the	O
question	O
text	O
and	O
some	O
sentence	O
,	O
we	O
are	O
still	O
able	O
to	O
infer	O
the	O
answer	O
through	O
partial	O
clues	O
,	O
such	O
as	O
some	O
word	O
/	O
concept	O
overlap	O
.	O
It	O
requires	O
processing	O
multiple	O
sentences	O
to	O
infer	O
the	O
correct	O
answer	O
.	O
It	O
is	O
unavoidable	O
that	O
there	O
are	O
many	O
coreference	O
errors	O
in	O
the	O
dataset	O
.	O
This	O
category	O
includes	O
those	O
examples	O
with	O
critical	O
coreference	O
errors	O
for	O
the	O
answer	O
entity	O
or	O
key	O
entities	O
appearing	O
in	O
the	O
question	O
.	O
Basically	O
we	O
treat	O
this	O
category	O
as	O
“	O
not	O
answerable	O
”	O
.	O
This	O
category	O
includes	O
examples	O
for	O
which	O
we	O
think	O
humans	O
are	O
not	O
able	O
to	O
obtain	O
the	O
correct	O
answer	O
(	O
confidently	O
)	O
.	O
Table	O
[	O
reference	O
]	O
provides	O
our	O
estimate	O
of	O
the	O
percentage	O
for	O
each	O
category	O
,	O
and	O
Table	O
[	O
reference	O
]	O
presents	O
one	O
representative	O
example	O
from	O
each	O
category	O
.	O
To	O
our	O
surprise	O
,	O
“	O
coreference	O
errors	O
”	O
and	O
“	O
ambiguous	O
/	O
hard	O
”	O
cases	O
account	O
for	O
of	O
this	O
sample	O
set	O
,	O
based	O
on	O
our	O
manual	Task
analysis	Task
,	O
and	O
this	O
certainly	O
will	O
be	O
a	O
barrier	O
for	O
training	O
models	O
with	O
an	O
accuracy	Metric
much	O
above	O
75	O
%	O
(	O
although	O
,	O
of	O
course	O
,	O
a	O
model	O
can	O
sometimes	O
make	O
a	O
lucky	O
guess	O
)	O
.	O
Additionally	O
,	O
only	O
2	O
examples	O
require	O
multiple	O
sentences	O
for	O
inference	Task
–	O
this	O
is	O
a	O
lower	O
rate	O
than	O
we	O
expected	O
and	O
hermann2015teaching	O
suggest	O
.	O
Therefore	O
,	O
we	O
hypothesize	O
that	O
in	O
most	O
of	O
the	O
“	O
answerable	O
”	O
cases	O
,	O
the	O
goal	O
is	O
to	O
identify	O
the	O
most	O
relevant	O
(	O
single	O
)	O
sentence	O
,	O
and	O
then	O
to	O
infer	O
the	O
answer	O
based	O
upon	O
it	O
.	O
subsection	O
:	O
Per	Metric
-	Metric
category	Metric
Performance	Metric
Now	O
,	O
we	O
further	O
analyze	O
the	O
predictions	O
of	O
our	O
two	O
systems	O
,	O
based	O
on	O
the	O
above	O
categorization	O
.	O
As	O
seen	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
i	O
)	O
The	O
exact	O
-	O
match	O
cases	O
are	O
quite	O
simple	O
and	O
both	O
systems	O
get	O
100	O
%	O
correct	O
.	O
(	O
ii	O
)	O
For	O
the	O
ambiguous	Task
/	Task
hard	Task
and	Task
entity	Task
-	Task
linking	Task
-	Task
error	Task
cases	Task
,	O
meeting	O
our	O
expectations	O
,	O
both	O
of	O
the	O
systems	O
perform	O
poorly	O
.	O
(	O
iii	O
)	O
The	O
two	O
systems	O
mainly	O
differ	O
in	O
paraphrasing	O
cases	O
,	O
and	O
some	O
of	O
the	O
“	O
partial	O
clue	O
”	O
cases	O
.	O
This	O
clearly	O
shows	O
how	O
neural	Method
networks	Method
are	O
better	O
capable	O
of	O
learning	O
semantic	O
matches	O
involving	O
paraphrasing	O
or	O
lexical	O
variation	O
between	O
the	O
two	O
sentences	O
.	O
(	O
iv	O
)	O
We	O
believe	O
that	O
the	O
neural	Method
-	Method
net	Method
system	Method
already	O
achieves	O
near	O
-	O
optimal	O
performance	O
on	O
all	O
the	O
single	O
-	O
sentence	O
and	O
unambiguous	O
cases	O
.	O
There	O
does	O
not	O
seem	O
to	O
be	O
much	O
useful	O
headroom	O
for	O
exploring	O
more	O
sophisticated	O
natural	Method
language	Method
understanding	Method
approaches	Method
on	O
this	O
dataset	O
.	O
section	O
:	O
Related	O
Tasks	O
We	O
briefly	O
survey	O
other	O
tasks	O
related	O
to	O
reading	Task
comprehension	Task
.	O
MCTest	Task
is	O
an	O
open	Task
-	Task
domain	Task
reading	Task
comprehension	Task
task	Task
,	O
in	O
the	O
form	O
of	O
fictional	O
short	O
stories	O
,	O
accompanied	O
by	O
multiple	O
-	O
choice	O
questions	O
.	O
It	O
was	O
carefully	O
created	O
using	O
crowd	Method
sourcing	Method
,	O
and	O
aims	O
at	O
a	O
7	O
-	O
year	O
-	O
old	O
reading	Metric
comprehension	Metric
level	Metric
.	O
On	O
the	O
one	O
hand	O
,	O
this	O
dataset	O
has	O
a	O
high	O
demand	O
on	O
various	O
reasoning	O
capacities	O
:	O
over	O
of	O
the	O
questions	O
require	O
multiple	O
sentences	O
to	O
answer	O
and	O
also	O
the	O
questions	O
come	O
in	O
assorted	O
categories	O
(	O
what	O
,	O
why	O
,	O
how	O
,	O
whose	O
,	O
which	O
,	O
etc	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
full	O
dataset	O
has	O
only	O
660	O
paragraphs	O
in	O
total	O
(	O
each	O
paragraph	O
is	O
associated	O
with	O
4	O
questions	O
)	O
,	O
which	O
renders	O
training	O
statistical	Method
models	Method
(	O
especially	O
complex	O
ones	O
)	O
very	O
difficult	O
.	O
Up	O
to	O
now	O
,	O
the	O
best	O
solutions	O
are	O
still	O
heavily	O
relying	O
on	O
manually	O
curated	O
syntactic	O
/	O
semantic	O
features	O
,	O
with	O
the	O
aid	O
of	O
additional	O
knowledge	O
(	O
e.g.	O
,	O
word	O
embeddings	O
,	O
lexical	O
/	O
paragraph	O
databases	O
)	O
.	O
Children	Material
Book	Material
Test	Material
was	O
developed	O
in	O
a	O
similar	O
spirit	O
to	O
the	O
CNN	Material
/	O
Daily	Material
Mail	Material
datasets	O
.	O
It	O
takes	O
any	O
consecutive	O
21	O
sentences	O
from	O
a	O
children	O
’s	O
book	O
–	O
the	O
first	O
20	O
sentences	O
are	O
used	O
as	O
the	O
passage	O
,	O
and	O
the	O
goal	O
is	O
to	O
infer	O
a	O
missing	O
word	O
in	O
the	O
21st	O
sentence	O
(	O
question	O
and	O
answer	O
)	O
.	O
The	O
questions	O
are	O
also	O
categorized	O
by	O
the	O
type	O
of	O
the	O
missing	O
word	O
:	O
named	O
entity	O
,	O
common	O
noun	O
,	O
preposition	O
or	O
verb	O
.	O
According	O
to	O
the	O
first	O
study	O
on	O
this	O
dataset	O
,	O
a	O
language	Method
model	Method
(	O
an	Method
-	Method
gram	Method
model	Method
or	O
a	O
recurrent	Method
neural	Method
network	Method
)	O
with	O
local	O
context	O
is	O
sufficient	O
for	O
predicting	Task
verbs	Task
or	O
prepositions	O
;	O
however	O
,	O
for	O
named	O
entities	O
or	O
common	O
nouns	O
,	O
it	O
improves	O
performance	O
to	O
scan	O
through	O
the	O
whole	O
paragraph	O
to	O
make	O
predictions	O
.	O
So	O
far	O
,	O
the	O
best	O
published	O
results	O
are	O
reported	O
by	O
window	Method
-	Method
based	Method
memory	Method
networks	Method
.	O
bAbI	Material
is	O
a	O
collection	O
of	O
artificial	O
datasets	O
,	O
consisting	O
of	O
20	O
different	O
reasoning	O
types	O
.	O
It	O
encourages	O
the	O
development	O
of	O
models	O
with	O
the	O
ability	O
to	O
chain	O
reasoning	Task
,	O
induction	Task
/	Task
deduction	Task
,	O
etc	O
.	O
,	O
so	O
that	O
they	O
can	O
answer	O
a	O
question	O
like	O
“	O
The	O
football	O
is	O
in	O
the	O
playground	O
”	O
after	O
reading	O
a	O
sequence	O
of	O
sentences	O
“	O
John	O
is	O
in	O
the	O
playground	O
;	O
Bob	O
is	O
in	O
the	O
office	O
;	O
John	O
picked	O
up	O
the	O
football	O
;	O
Bob	O
went	O
to	O
the	O
kitchen	O
.	O
”	O
Various	O
types	O
of	O
memory	Method
networks	Method
have	O
been	O
shown	O
effective	O
on	O
these	O
tasks	O
,	O
and	O
lee2016reasoning	O
show	O
that	O
vector	Method
space	Method
models	Method
based	O
on	O
extensive	O
problem	Method
analysis	Method
can	O
obtain	O
near	O
-	O
perfect	O
accuracies	Metric
on	O
all	O
the	O
categories	O
.	O
Despite	O
these	O
promising	O
results	O
,	O
this	O
dataset	O
is	O
limited	O
to	O
a	O
small	O
vocabulary	O
(	O
only	O
100–200	O
words	O
)	O
and	O
simple	O
language	O
variations	O
,	O
so	O
there	O
is	O
still	O
a	O
huge	O
gap	O
from	O
real	O
-	O
world	O
datasets	O
that	O
we	O
need	O
to	O
fill	O
in	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
carefully	O
examined	O
the	O
recent	O
CNN	Material
/	O
Daily	Material
Mail	Material
reading	O
comprehension	O
task	O
.	O
Our	O
systems	O
demonstrated	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
but	O
more	O
importantly	O
,	O
we	O
performed	O
a	O
careful	O
analysis	O
of	O
the	O
dataset	O
by	O
hand	O
.	O
Overall	O
,	O
we	O
think	O
the	O
CNN	Material
/	O
Daily	Material
Mail	Material
datasets	O
are	O
valuable	O
datasets	O
,	O
which	O
provide	O
a	O
promising	O
avenue	O
for	O
training	O
effective	O
statistical	Method
models	Method
for	O
reading	Task
comprehension	Task
tasks	Task
.	O
Nevertheless	O
,	O
we	O
argue	O
that	O
:	O
(	O
i	O
)	O
this	O
dataset	O
is	O
still	O
quite	O
noisy	O
due	O
to	O
its	O
method	O
of	O
data	Task
creation	Task
and	O
coreference	Task
errors	Task
;	O
(	O
ii	O
)	O
current	O
neural	Method
networks	Method
have	O
almost	O
reached	O
a	O
performance	O
ceiling	O
on	O
this	O
dataset	O
;	O
and	O
(	O
iii	O
)	O
the	O
required	O
reasoning	O
and	O
inference	Metric
level	Metric
of	O
this	O
dataset	O
is	O
still	O
quite	O
simple	O
.	O
As	O
future	O
work	O
,	O
we	O
need	O
to	O
consider	O
how	O
we	O
can	O
utilize	O
these	O
datasets	O
(	O
and	O
the	O
models	O
trained	O
upon	O
them	O
)	O
to	O
help	O
solve	O
more	O
complex	O
RC	Task
reasoning	Task
tasks	Task
(	O
with	O
less	O
annotated	O
data	O
)	O
.	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
thoughtful	O
feedback	O
.	O
Stanford	O
University	O
gratefully	O
acknowledges	O
the	O
support	O
of	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
(	O
DARPA	O
)	O
Deep	Task
Exploration	Task
and	Task
Filtering	Task
of	Task
Text	Task
(	O
DEFT	Task
)	O
Program	O
under	O
Air	O
Force	O
Research	O
Laboratory	O
(	O
AFRL	O
)	O
contract	O
no	O
.	O
FA8750	O
-	O
13	O
-	O
2	O
-	O
0040	O
.	O
Any	O
opinions	O
,	O
findings	O
,	O
and	O
conclusion	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
view	O
of	O
the	O
DARPA	O
,	O
AFRL	O
,	O
or	O
the	O
US	O
government	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Samples	O
and	O
Labeled	O
Categories	O
from	O
the	O
CNN	Material
Dataset	Material
For	O
the	O
analysis	O
in	O
Section	O
[	O
reference	O
]	O
,	O
we	O
uniformly	O
sampled	O
100	O
examples	O
from	O
the	O
development	O
set	O
of	O
the	O
CNN	Material
dataset	Material
.	O
Table	O
[	O
reference	O
]	O
provides	O
a	O
full	O
index	O
list	O
of	O
our	O
samples	O
and	O
Table	O
[	O
reference	O
]	O
presents	O
our	O
labeled	O
categories	O
.	O
1.3	O
