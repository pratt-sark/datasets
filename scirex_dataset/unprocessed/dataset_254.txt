document	O
:	O
Sequence	Task
to	Task
Sequence	Task
Learning	Task
with	O
Neural	Method
Networks	Method
Deep	Method
Neural	Method
Networks	Method
(	O
DNNs	Method
)	O
are	O
powerful	O
models	O
that	O
have	O
achieved	O
excellent	O
performance	O
on	O
difficult	Task
learning	Task
tasks	Task
.	O
Although	O
DNNs	Method
work	O
well	O
whenever	O
large	O
labeled	O
training	O
sets	O
are	O
available	O
,	O
they	O
can	O
not	O
be	O
used	O
to	O
map	O
sequences	O
to	O
sequences	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
general	O
end	Method
-	Method
to	Method
-	Method
end	Method
approach	Method
to	O
sequence	Task
learning	Task
that	O
makes	O
minimal	O
assumptions	O
on	O
the	O
sequence	O
structure	O
.	O
Our	O
method	O
uses	O
a	O
multilayered	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
to	O
map	O
the	O
input	O
sequence	O
to	O
a	O
vector	O
of	O
a	O
fixed	O
dimensionality	O
,	O
and	O
then	O
another	O
deep	O
LSTM	Method
to	O
decode	O
the	O
target	O
sequence	O
from	O
the	O
vector	O
.	O
Our	O
main	O
result	O
is	O
that	O
on	O
an	O
English	Task
to	Task
French	Task
translation	Task
task	Task
from	O
the	O
WMT’14	Material
dataset	Material
,	O
the	O
translations	O
produced	O
by	O
the	O
LSTM	Method
achieve	O
a	O
BLEU	Metric
score	Metric
of	O
34.8	O
on	O
the	O
entire	O
test	O
set	O
,	O
where	O
the	O
LSTM	Method
’s	O
BLEU	Metric
score	Metric
was	O
penalized	O
on	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O
Additionally	O
,	O
the	O
LSTM	Method
did	O
not	O
have	O
difficulty	O
on	O
long	O
sentences	O
.	O
For	O
comparison	O
,	O
a	O
phrase	O
-	O
based	O
SMT	Method
system	Method
achieves	O
a	O
BLEU	Metric
score	Metric
of	O
33.3	O
on	O
the	O
same	O
dataset	O
.	O
When	O
we	O
used	O
the	O
LSTM	Method
to	O
rerank	O
the	O
1000	O
hypotheses	O
produced	O
by	O
the	O
aforementioned	O
SMT	Method
system	Method
,	O
its	O
BLEU	Metric
score	Metric
increases	O
to	O
36.5	O
,	O
which	O
is	O
close	O
to	O
the	O
previous	O
best	O
result	O
on	O
this	O
task	O
.	O
The	O
LSTM	Method
also	O
learned	O
sensible	O
phrase	Method
and	Method
sentence	Method
representations	Method
that	O
are	O
sensitive	O
to	O
word	O
order	O
and	O
are	O
relatively	O
invariant	O
to	O
the	O
active	O
and	O
the	O
passive	O
voice	O
.	O
Finally	O
,	O
we	O
found	O
that	O
reversing	O
the	O
order	O
of	O
the	O
words	O
in	O
all	O
source	O
sentences	O
(	O
but	O
not	O
target	O
sentences	O
)	O
improved	O
the	O
LSTM	Method
’s	O
performance	O
markedly	O
,	O
because	O
doing	O
so	O
introduced	O
many	O
short	O
term	O
dependencies	O
between	O
the	O
source	O
and	O
the	O
target	O
sentence	O
which	O
made	O
the	O
optimization	Task
problem	Task
easier	O
.	O
section	O
:	O
Introduction	O
Deep	Method
Neural	Method
Networks	Method
(	O
DNNs	Method
)	O
are	O
extremely	O
powerful	O
machine	Method
learning	Method
models	Method
that	O
achieve	O
excellent	O
performance	O
on	O
difficult	O
problems	O
such	O
as	O
speech	Task
recognition	Task
and	O
visual	Task
object	Task
recognition	Task
.	O
DNNs	Method
are	O
powerful	O
because	O
they	O
can	O
perform	O
arbitrary	O
parallel	Task
computation	Task
for	O
a	O
modest	O
number	O
of	O
steps	O
.	O
A	O
surprising	O
example	O
of	O
the	O
power	O
of	O
DNNs	Method
is	O
their	O
ability	O
to	O
sort	O
-	O
bit	O
numbers	O
using	O
only	O
2	O
hidden	O
layers	O
of	O
quadratic	O
size	O
.	O
So	O
,	O
while	O
neural	Method
networks	Method
are	O
related	O
to	O
conventional	O
statistical	Method
models	Method
,	O
they	O
learn	O
an	O
intricate	O
computation	O
.	O
Furthermore	O
,	O
large	O
DNNs	Method
can	O
be	O
trained	O
with	O
supervised	Method
backpropagation	Method
whenever	O
the	O
labeled	O
training	O
set	O
has	O
enough	O
information	O
to	O
specify	O
the	O
network	O
’s	O
parameters	O
.	O
Thus	O
,	O
if	O
there	O
exists	O
a	O
parameter	O
setting	O
of	O
a	O
large	O
DNN	Method
that	O
achieves	O
good	O
results	O
(	O
for	O
example	O
,	O
because	O
humans	O
can	O
solve	O
the	O
task	O
very	O
rapidly	O
)	O
,	O
supervised	Method
backpropagation	Method
will	O
find	O
these	O
parameters	O
and	O
solve	O
the	O
problem	O
.	O
Despite	O
their	O
flexibility	O
and	O
power	O
,	O
DNNs	Method
can	O
only	O
be	O
applied	O
to	O
problems	O
whose	O
inputs	O
and	O
targets	O
can	O
be	O
sensibly	O
encoded	O
with	O
vectors	O
of	O
fixed	O
dimensionality	O
.	O
It	O
is	O
a	O
significant	O
limitation	O
,	O
since	O
many	O
important	O
problems	O
are	O
best	O
expressed	O
with	O
sequences	O
whose	O
lengths	O
are	O
not	O
known	O
a	O
-	O
priori	O
.	O
For	O
example	O
,	O
speech	Task
recognition	Task
and	O
machine	Task
translation	Task
are	O
sequential	Task
problems	Task
.	O
Likewise	O
,	O
question	Task
answering	Task
can	O
also	O
be	O
seen	O
as	O
mapping	O
a	O
sequence	O
of	O
words	O
representing	O
the	O
question	O
to	O
a	O
sequence	O
of	O
words	O
representing	O
the	O
answer	O
.	O
It	O
is	O
therefore	O
clear	O
that	O
a	O
domain	Method
-	Method
independent	Method
method	Method
that	O
learns	O
to	O
map	O
sequences	O
to	O
sequences	O
would	O
be	O
useful	O
.	O
Sequences	O
pose	O
a	O
challenge	O
for	O
DNNs	Method
because	O
they	O
require	O
that	O
the	O
dimensionality	O
of	O
the	O
inputs	O
and	O
outputs	O
is	O
known	O
and	O
fixed	O
.	O
In	O
this	O
paper	O
,	O
we	O
show	O
that	O
a	O
straightforward	O
application	O
of	O
the	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
architecture	O
can	O
solve	O
general	Task
sequence	Task
to	Task
sequence	Task
problems	Task
.	O
The	O
idea	O
is	O
to	O
use	O
one	O
LSTM	Method
to	O
read	O
the	O
input	O
sequence	O
,	O
one	O
timestep	O
at	O
a	O
time	O
,	O
to	O
obtain	O
large	O
fixed	Method
-	Method
dimensional	Method
vector	Method
representation	Method
,	O
and	O
then	O
to	O
use	O
another	O
LSTM	Method
to	O
extract	O
the	O
output	O
sequence	O
from	O
that	O
vector	O
(	O
fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
second	O
LSTM	Method
is	O
essentially	O
a	O
recurrent	Method
neural	Method
network	Method
language	Method
model	Method
except	O
that	O
it	O
is	O
conditioned	O
on	O
the	O
input	O
sequence	O
.	O
The	O
LSTM	Method
’s	O
ability	O
to	O
successfully	O
learn	O
on	O
data	O
with	O
long	O
range	O
temporal	O
dependencies	O
makes	O
it	O
a	O
natural	O
choice	O
for	O
this	O
application	O
due	O
to	O
the	O
considerable	O
time	O
lag	O
between	O
the	O
inputs	O
and	O
their	O
corresponding	O
outputs	O
(	O
fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
There	O
have	O
been	O
a	O
number	O
of	O
related	O
attempts	O
to	O
address	O
the	O
general	O
sequence	Task
to	Task
sequence	Task
learning	Task
problem	Task
with	O
neural	Method
networks	Method
.	O
Our	O
approach	O
is	O
closely	O
related	O
to	O
Kalchbrenner	O
and	O
Blunsom	O
who	O
were	O
the	O
first	O
to	O
map	O
the	O
entire	O
input	O
sentence	O
to	O
vector	O
,	O
and	O
is	O
related	O
to	O
Cho	O
et	O
al	O
.	O
although	O
the	O
latter	O
was	O
used	O
only	O
for	O
rescoring	Task
hypotheses	Task
produced	O
by	O
a	O
phrase	Method
-	Method
based	Method
system	Method
.	O
Graves	O
introduced	O
a	O
novel	O
differentiable	Method
attention	Method
mechanism	Method
that	O
allows	O
neural	Method
networks	Method
to	O
focus	O
on	O
different	O
parts	O
of	O
their	O
input	O
,	O
and	O
an	O
elegant	O
variant	O
of	O
this	O
idea	O
was	O
successfully	O
applied	O
to	O
machine	Task
translation	Task
by	O
Bahdanau	O
et	O
al	O
.	O
.	O
The	O
Connectionist	Method
Sequence	Method
Classification	Method
is	O
another	O
popular	O
technique	O
for	O
mapping	Task
sequences	Task
to	O
sequences	O
with	O
neural	Method
networks	Method
,	O
but	O
it	O
assumes	O
a	O
monotonic	O
alignment	O
between	O
the	O
inputs	O
and	O
the	O
outputs	O
.	O
The	O
main	O
result	O
of	O
this	O
work	O
is	O
the	O
following	O
.	O
On	O
the	O
WMT’14	Task
English	Task
to	Task
French	Task
translation	Task
task	Task
,	O
we	O
obtained	O
a	O
BLEU	Metric
score	Metric
of	O
34.81	O
by	O
directly	O
extracting	O
translations	O
from	O
an	O
ensemble	O
of	O
5	O
deep	Method
LSTMs	Method
(	O
with	O
384	O
M	O
parameters	O
and	O
8	O
,	O
000	O
dimensional	O
state	O
each	O
)	O
using	O
a	O
simple	O
left	Method
-	Method
to	Method
-	Method
right	Method
beam	Method
-	Method
search	Method
decoder	Method
.	O
This	O
is	O
by	O
far	O
the	O
best	O
result	O
achieved	O
by	O
direct	Task
translation	Task
with	O
large	Method
neural	Method
networks	Method
.	O
For	O
comparison	O
,	O
the	O
BLEU	Metric
score	Metric
of	O
an	O
SMT	Method
baseline	Method
on	O
this	O
dataset	O
is	O
33.30	O
.	O
The	O
34.81	O
BLEU	Metric
score	Metric
was	O
achieved	O
by	O
an	O
LSTM	Method
with	O
a	O
vocabulary	O
of	O
80k	O
words	O
,	O
so	O
the	O
score	O
was	O
penalized	O
whenever	O
the	O
reference	O
translation	O
contained	O
a	O
word	O
not	O
covered	O
by	O
these	O
80k	O
.	O
This	O
result	O
shows	O
that	O
a	O
relatively	O
unoptimized	Method
small	Method
-	Method
vocabulary	Method
neural	Method
network	Method
architecture	Method
which	O
has	O
much	O
room	O
for	O
improvement	O
outperforms	O
a	O
phrase	O
-	O
based	O
SMT	Method
system	Method
.	O
Finally	O
,	O
we	O
used	O
the	O
LSTM	Method
to	O
rescore	O
the	O
publicly	O
available	O
1000	O
-	O
best	O
lists	O
of	O
the	O
SMT	Method
baseline	Method
on	O
the	O
same	O
task	O
.	O
By	O
doing	O
so	O
,	O
we	O
obtained	O
a	O
BLEU	Metric
score	Metric
of	O
36.5	O
,	O
which	O
improves	O
the	O
baseline	O
by	O
3.2	O
BLEU	Metric
points	O
and	O
is	O
close	O
to	O
the	O
previous	O
best	O
published	O
result	O
on	O
this	O
task	O
(	O
which	O
is	O
37.0	O
)	O
.	O
Surprisingly	O
,	O
the	O
LSTM	Method
did	O
not	O
suffer	O
on	O
very	O
long	O
sentences	O
,	O
despite	O
the	O
recent	O
experience	O
of	O
other	O
researchers	O
with	O
related	O
architectures	O
.	O
We	O
were	O
able	O
to	O
do	O
well	O
on	O
long	O
sentences	O
because	O
we	O
reversed	O
the	O
order	O
of	O
words	O
in	O
the	O
source	O
sentence	O
but	O
not	O
the	O
target	O
sentences	O
in	O
the	O
training	O
and	O
test	O
set	O
.	O
By	O
doing	O
so	O
,	O
we	O
introduced	O
many	O
short	O
term	O
dependencies	O
that	O
made	O
the	O
optimization	Task
problem	Task
much	O
simpler	O
(	O
see	O
sec	O
.	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
)	O
.	O
As	O
a	O
result	O
,	O
SGD	Method
could	O
learn	O
LSTMs	Method
that	O
had	O
no	O
trouble	O
with	O
long	O
sentences	O
.	O
The	O
simple	O
trick	O
of	O
reversing	O
the	O
words	O
in	O
the	O
source	O
sentence	O
is	O
one	O
of	O
the	O
key	O
technical	O
contributions	O
of	O
this	O
work	O
.	O
A	O
useful	O
property	O
of	O
the	O
LSTM	Method
is	O
that	O
it	O
learns	O
to	O
map	O
an	O
input	O
sentence	O
of	O
variable	O
length	O
into	O
a	O
fixed	Method
-	Method
dimensional	Method
vector	Method
representation	Method
.	O
Given	O
that	O
translations	O
tend	O
to	O
be	O
paraphrases	O
of	O
the	O
source	O
sentences	O
,	O
the	O
translation	O
objective	O
encourages	O
the	O
LSTM	Method
to	O
find	O
sentence	Method
representations	Method
that	O
capture	O
their	O
meaning	O
,	O
as	O
sentences	O
with	O
similar	O
meanings	O
are	O
close	O
to	O
each	O
other	O
while	O
different	O
sentences	O
meanings	O
will	O
be	O
far	O
.	O
A	O
qualitative	O
evaluation	O
supports	O
this	O
claim	O
,	O
showing	O
that	O
our	O
model	O
is	O
aware	O
of	O
word	O
order	O
and	O
is	O
fairly	O
invariant	O
to	O
the	O
active	O
and	O
passive	O
voice	O
.	O
section	O
:	O
The	O
model	O
The	O
Recurrent	Method
Neural	Method
Network	Method
(	Method
RNN	Method
)	Method
is	O
a	O
natural	O
generalization	Method
of	Method
feedforward	Method
neural	Method
networks	Method
to	O
sequences	O
.	O
Given	O
a	O
sequence	O
of	O
inputs	O
,	O
a	O
standard	O
RNN	Method
computes	O
a	O
sequence	O
of	O
outputs	O
by	O
iterating	O
the	O
following	O
equation	O
:	O
The	O
RNN	Method
can	O
easily	O
map	O
sequences	O
to	O
sequences	O
whenever	O
the	O
alignment	O
between	O
the	O
inputs	O
the	O
outputs	O
is	O
known	O
ahead	O
of	O
time	O
.	O
However	O
,	O
it	O
is	O
not	O
clear	O
how	O
to	O
apply	O
an	O
RNN	Method
to	O
problems	O
whose	O
input	O
and	O
the	O
output	O
sequences	O
have	O
different	O
lengths	O
with	O
complicated	O
and	O
non	O
-	O
monotonic	O
relationships	O
.	O
The	O
simplest	O
strategy	O
for	O
general	Task
sequence	Task
learning	Task
is	O
to	O
map	O
the	O
input	O
sequence	O
to	O
a	O
fixed	O
-	O
sized	O
vector	O
using	O
one	O
RNN	Method
,	O
and	O
then	O
to	O
map	O
the	O
vector	O
to	O
the	O
target	O
sequence	O
with	O
another	O
RNN	Method
(	O
this	O
approach	O
has	O
also	O
been	O
taken	O
by	O
Cho	O
et	O
al	O
.	O
)	O
.	O
While	O
it	O
could	O
work	O
in	O
principle	O
since	O
the	O
RNN	Method
is	O
provided	O
with	O
all	O
the	O
relevant	O
information	O
,	O
it	O
would	O
be	O
difficult	O
to	O
train	O
the	O
RNNs	Method
due	O
to	O
the	O
resulting	O
long	O
term	O
dependencies	O
(	O
figure	O
[	O
reference	O
]	O
)	O
.	O
However	O
,	O
the	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
is	O
known	O
to	O
learn	O
problems	O
with	O
long	O
range	O
temporal	O
dependencies	O
,	O
so	O
an	O
LSTM	Method
may	O
succeed	O
in	O
this	O
setting	O
.	O
The	O
goal	O
of	O
the	O
LSTM	Method
is	O
to	O
estimate	O
the	O
conditional	O
probability	O
where	O
is	O
an	O
input	O
sequence	O
and	O
is	O
its	O
corresponding	O
output	O
sequence	O
whose	O
length	O
may	O
differ	O
from	O
.	O
The	O
LSTM	Method
computes	O
this	O
conditional	O
probability	O
by	O
first	O
obtaining	O
the	O
fixed	Method
-	Method
dimensional	Method
representation	Method
of	O
the	O
input	O
sequence	O
given	O
by	O
the	O
last	O
hidden	O
state	O
of	O
the	O
LSTM	Method
,	O
and	O
then	O
computing	O
the	O
probability	O
of	O
with	O
a	O
standard	O
LSTM	Method
-	O
LM	O
formulation	O
whose	O
initial	O
hidden	O
state	O
is	O
set	O
to	O
the	O
representation	O
of	O
:	O
In	O
this	O
equation	O
,	O
each	O
distribution	O
is	O
represented	O
with	O
a	O
softmax	O
over	O
all	O
the	O
words	O
in	O
the	O
vocabulary	O
.	O
We	O
use	O
the	O
LSTM	Method
formulation	O
from	O
Graves	O
.	O
Note	O
that	O
we	O
require	O
that	O
each	O
sentence	O
ends	O
with	O
a	O
special	O
end	O
-	O
of	O
-	O
sentence	O
symbol	O
“	O
EOS	O
”	O
,	O
which	O
enables	O
the	O
model	O
to	O
define	O
a	O
distribution	O
over	O
sequences	O
of	O
all	O
possible	O
lengths	O
.	O
The	O
overall	O
scheme	O
is	O
outlined	O
in	O
figure	O
[	O
reference	O
]	O
,	O
where	O
the	O
shown	O
LSTM	Method
computes	O
the	O
representation	O
of	O
“	O
A	O
”	O
,	O
“	O
B	O
”	O
,	O
“	O
C	O
”	O
,	O
“	O
EOS	O
”	O
and	O
then	O
uses	O
this	O
representation	O
to	O
compute	O
the	O
probability	O
of	O
“	O
W	O
”	O
,	O
“	O
X	O
”	O
,	O
“	O
Y	O
”	O
,	O
“	O
Z	O
”	O
,	O
“	O
EOS	O
”	O
.	O
Our	O
actual	O
models	O
differ	O
from	O
the	O
above	O
description	O
in	O
three	O
important	O
ways	O
.	O
First	O
,	O
we	O
used	O
two	O
different	O
LSTMs	Method
:	O
one	O
for	O
the	O
input	O
sequence	O
and	O
another	O
for	O
the	O
output	O
sequence	O
,	O
because	O
doing	O
so	O
increases	O
the	O
number	O
model	O
parameters	O
at	O
negligible	O
computational	Metric
cost	Metric
and	O
makes	O
it	O
natural	O
to	O
train	O
the	O
LSTM	Method
on	O
multiple	O
language	O
pairs	O
simultaneously	O
.	O
Second	O
,	O
we	O
found	O
that	O
deep	Method
LSTMs	Method
significantly	O
outperformed	O
shallow	Method
LSTMs	Method
,	O
so	O
we	O
chose	O
an	O
LSTM	Method
with	O
four	O
layers	O
.	O
Third	O
,	O
we	O
found	O
it	O
extremely	O
valuable	O
to	O
reverse	O
the	O
order	O
of	O
the	O
words	O
of	O
the	O
input	O
sentence	O
.	O
So	O
for	O
example	O
,	O
instead	O
of	O
mapping	O
the	O
sentence	O
to	O
the	O
sentence	O
,	O
the	O
LSTM	Method
is	O
asked	O
to	O
map	O
to	O
,	O
where	O
is	O
the	O
translation	O
of	O
.	O
This	O
way	O
,	O
is	O
in	O
close	O
proximity	O
to	O
,	O
is	O
fairly	O
close	O
to	O
,	O
and	O
so	O
on	O
,	O
a	O
fact	O
that	O
makes	O
it	O
easy	O
for	O
SGD	Method
to	O
“	O
establish	O
communication	O
”	O
between	O
the	O
input	O
and	O
the	O
output	O
.	O
We	O
found	O
this	O
simple	O
data	Method
transformation	Method
to	O
greatly	O
improve	O
the	O
performance	O
of	O
the	O
LSTM	Method
.	O
section	O
:	O
Experiments	O
We	O
applied	O
our	O
method	O
to	O
the	O
WMT’14	O
English	O
to	O
French	O
MT	Task
task	Task
in	O
two	O
ways	O
.	O
We	O
used	O
it	O
to	O
directly	O
translate	O
the	O
input	O
sentence	O
without	O
using	O
a	O
reference	O
SMT	Method
system	Method
and	O
we	O
it	O
to	O
rescore	O
the	O
n	O
-	O
best	O
lists	O
of	O
an	O
SMT	Method
baseline	Method
.	O
We	O
report	O
the	O
accuracy	Metric
of	O
these	O
translation	Method
methods	Method
,	O
present	O
sample	O
translations	O
,	O
and	O
visualize	O
the	O
resulting	O
sentence	Method
representation	Method
.	O
subsection	O
:	O
Dataset	O
details	O
We	O
used	O
the	O
WMT’14	Material
English	Material
to	Material
French	Material
dataset	Material
.	O
We	O
trained	O
our	O
models	O
on	O
a	O
subset	O
of	O
12	O
M	O
sentences	O
consisting	O
of	O
348	O
M	O
French	O
words	O
and	O
304	O
M	O
English	Material
words	Material
,	O
which	O
is	O
a	O
clean	O
“	O
selected	O
”	O
subset	O
from	O
.	O
We	O
chose	O
this	O
translation	Task
task	Task
and	O
this	O
specific	O
training	O
set	O
subset	O
because	O
of	O
the	O
public	O
availability	O
of	O
a	O
tokenized	O
training	O
and	O
test	O
set	O
together	O
with	O
1000	O
-	O
best	O
lists	O
from	O
the	O
baseline	O
SMT	Method
.	O
As	O
typical	O
neural	Method
language	Method
models	Method
rely	O
on	O
a	O
vector	Method
representation	Method
for	O
each	O
word	O
,	O
we	O
used	O
a	O
fixed	O
vocabulary	O
for	O
both	O
languages	O
.	O
We	O
used	O
160	O
,	O
000	O
of	O
the	O
most	O
frequent	O
words	O
for	O
the	O
source	O
language	O
and	O
80	O
,	O
000	O
of	O
the	O
most	O
frequent	O
words	O
for	O
the	O
target	O
language	O
.	O
Every	O
out	O
-	O
of	O
-	O
vocabulary	O
word	O
was	O
replaced	O
with	O
a	O
special	O
“	O
UNK	O
”	O
token	O
.	O
subsection	O
:	O
Decoding	Task
and	O
Rescoring	Task
The	O
core	O
of	O
our	O
experiments	O
involved	O
training	O
a	O
large	O
deep	O
LSTM	Method
on	O
many	O
sentence	O
pairs	O
.	O
We	O
trained	O
it	O
by	O
maximizing	O
the	O
log	O
probability	O
of	O
a	O
correct	O
translation	O
given	O
the	O
source	O
sentence	O
,	O
so	O
the	O
training	Metric
objective	Metric
is	O
where	O
is	O
the	O
training	O
set	O
.	O
Once	O
training	O
is	O
complete	O
,	O
we	O
produce	O
translations	O
by	O
finding	O
the	O
most	O
likely	O
translation	O
according	O
to	O
the	O
LSTM	Method
:	O
We	O
search	O
for	O
the	O
most	O
likely	O
translation	O
using	O
a	O
simple	O
left	Method
-	Method
to	Method
-	Method
right	Method
beam	Method
search	Method
decoder	Method
which	O
maintains	O
a	O
small	O
number	O
of	O
partial	O
hypotheses	O
,	O
where	O
a	O
partial	O
hypothesis	O
is	O
a	O
prefix	O
of	O
some	O
translation	O
.	O
At	O
each	O
timestep	O
we	O
extend	O
each	O
partial	O
hypothesis	O
in	O
the	O
beam	O
with	O
every	O
possible	O
word	O
in	O
the	O
vocabulary	O
.	O
This	O
greatly	O
increases	O
the	O
number	O
of	O
the	O
hypotheses	O
so	O
we	O
discard	O
all	O
but	O
the	O
most	O
likely	O
hypotheses	O
according	O
to	O
the	O
model	O
’s	O
log	O
probability	O
.	O
As	O
soon	O
as	O
the	O
“	O
EOS	O
”	O
symbol	O
is	O
appended	O
to	O
a	O
hypothesis	O
,	O
it	O
is	O
removed	O
from	O
the	O
beam	O
and	O
is	O
added	O
to	O
the	O
set	O
of	O
complete	O
hypotheses	O
.	O
While	O
this	O
decoder	O
is	O
approximate	O
,	O
it	O
is	O
simple	O
to	O
implement	O
.	O
Interestingly	O
,	O
our	O
system	O
performs	O
well	O
even	O
with	O
a	O
beam	O
size	O
of	O
1	O
,	O
and	O
a	O
beam	O
of	O
size	O
2	O
provides	O
most	O
of	O
the	O
benefits	O
of	O
beam	Method
search	Method
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
We	O
also	O
used	O
the	O
LSTM	Method
to	O
rescore	O
the	O
1000	O
-	O
best	O
lists	O
produced	O
by	O
the	O
baseline	O
system	O
.	O
To	O
rescore	O
an	O
n	O
-	O
best	O
list	O
,	O
we	O
computed	O
the	O
log	O
probability	O
of	O
every	O
hypothesis	O
with	O
our	O
LSTM	Method
and	O
took	O
an	O
even	O
average	O
with	O
their	O
score	O
and	O
the	O
LSTM	Metric
’s	Metric
score	Metric
.	O
subsection	O
:	O
Reversing	O
the	O
Source	O
Sentences	O
While	O
the	O
LSTM	Method
is	O
capable	O
of	O
solving	O
problems	O
with	O
long	O
term	O
dependencies	O
,	O
we	O
discovered	O
that	O
the	O
LSTM	Method
learns	O
much	O
better	O
when	O
the	O
source	O
sentences	O
are	O
reversed	O
(	O
the	O
target	O
sentences	O
are	O
not	O
reversed	O
)	O
.	O
By	O
doing	O
so	O
,	O
the	O
LSTM	Method
’s	O
test	O
perplexity	O
dropped	O
from	O
5.8	O
to	O
4.7	O
,	O
and	O
the	O
test	Metric
BLEU	Metric
scores	Metric
of	O
its	O
decoded	O
translations	O
increased	O
from	O
25.9	O
to	O
30.6	O
.	O
While	O
we	O
do	O
not	O
have	O
a	O
complete	O
explanation	O
to	O
this	O
phenomenon	O
,	O
we	O
believe	O
that	O
it	O
is	O
caused	O
by	O
the	O
introduction	O
of	O
many	O
short	O
term	O
dependencies	O
to	O
the	O
dataset	O
.	O
Normally	O
,	O
when	O
we	O
concatenate	O
a	O
source	O
sentence	O
with	O
a	O
target	O
sentence	O
,	O
each	O
word	O
in	O
the	O
source	O
sentence	O
is	O
far	O
from	O
its	O
corresponding	O
word	O
in	O
the	O
target	O
sentence	O
.	O
As	O
a	O
result	O
,	O
the	O
problem	O
has	O
a	O
large	O
“	O
minimal	O
time	O
lag	O
”	O
.	O
By	O
reversing	O
the	O
words	O
in	O
the	O
source	O
sentence	O
,	O
the	O
average	O
distance	O
between	O
corresponding	O
words	O
in	O
the	O
source	O
and	O
target	O
language	O
is	O
unchanged	O
.	O
However	O
,	O
the	O
first	O
few	O
words	O
in	O
the	O
source	O
language	O
are	O
now	O
very	O
close	O
to	O
the	O
first	O
few	O
words	O
in	O
the	O
target	O
language	O
,	O
so	O
the	O
problem	O
’s	O
minimal	O
time	O
lag	O
is	O
greatly	O
reduced	O
.	O
Thus	O
,	O
backpropagation	Method
has	O
an	O
easier	O
time	O
“	O
establishing	O
communication	O
”	O
between	O
the	O
source	O
sentence	O
and	O
the	O
target	O
sentence	O
,	O
which	O
in	O
turn	O
results	O
in	O
substantially	O
improved	O
overall	O
performance	O
.	O
Initially	O
,	O
we	O
believed	O
that	O
reversing	O
the	O
input	O
sentences	O
would	O
only	O
lead	O
to	O
more	O
confident	O
predictions	O
in	O
the	O
early	O
parts	O
of	O
the	O
target	O
sentence	O
and	O
to	O
less	O
confident	O
predictions	O
in	O
the	O
later	O
parts	O
.	O
However	O
,	O
LSTMs	Method
trained	O
on	O
reversed	O
source	O
sentences	O
did	O
much	O
better	O
on	O
long	O
sentences	O
than	O
LSTMs	Method
trained	O
on	O
the	O
raw	O
source	O
sentences	O
(	O
see	O
sec	O
.	O
[	O
reference	O
]	O
)	O
,	O
which	O
suggests	O
that	O
reversing	O
the	O
input	O
sentences	O
results	O
in	O
LSTMs	Method
with	O
better	O
memory	O
utilization	O
.	O
subsection	O
:	O
Training	O
details	O
We	O
found	O
that	O
the	O
LSTM	Method
models	O
are	O
fairly	O
easy	O
to	O
train	O
.	O
We	O
used	O
deep	Method
LSTMs	Method
with	O
4	O
layers	O
,	O
with	O
1000	O
cells	O
at	O
each	O
layer	O
and	O
1000	O
dimensional	O
word	O
embeddings	O
,	O
with	O
an	O
input	O
vocabulary	O
of	O
160	O
,	O
000	O
and	O
an	O
output	O
vocabulary	O
of	O
80	O
,	O
000	O
.	O
Thus	O
the	O
deep	O
LSTM	Method
uses	O
8000	O
real	O
numbers	O
to	O
represent	O
a	O
sentence	O
.	O
We	O
found	O
deep	Method
LSTMs	Method
to	O
significantly	O
outperform	O
shallow	Method
LSTMs	Method
,	O
where	O
each	O
additional	O
layer	O
reduced	O
perplexity	Metric
by	O
nearly	O
10	O
%	O
,	O
possibly	O
due	O
to	O
their	O
much	O
larger	O
hidden	O
state	O
.	O
We	O
used	O
a	O
naive	O
softmax	Method
over	O
80	O
,	O
000	O
words	O
at	O
each	O
output	O
.	O
The	O
resulting	O
LSTM	Method
has	O
384	O
M	O
parameters	O
of	O
which	O
64	O
M	O
are	O
pure	O
recurrent	O
connections	O
(	O
32	O
M	O
for	O
the	O
“	O
encoder	O
”	O
LSTM	Method
and	O
32	O
M	O
for	O
the	O
“	O
decoder	O
”	O
LSTM	Method
)	O
.	O
The	O
complete	O
training	O
details	O
are	O
given	O
below	O
:	O
We	O
initialized	O
all	O
of	O
the	O
LSTM	Method
’s	O
parameters	O
with	O
the	O
uniform	O
distribution	O
between	O
-	O
0.08	O
and	O
0.08	O
We	O
used	O
stochastic	Method
gradient	Method
descent	Method
without	O
momentum	Method
,	O
with	O
a	O
fixed	O
learning	Metric
rate	Metric
of	O
0.7	O
.	O
After	O
5	O
epochs	O
,	O
we	O
begun	O
halving	O
the	O
learning	Metric
rate	Metric
every	O
half	O
epoch	O
.	O
We	O
trained	O
our	O
models	O
for	O
a	O
total	O
of	O
7.5	O
epochs	O
.	O
We	O
used	O
batches	O
of	O
128	O
sequences	O
for	O
the	O
gradient	O
and	O
divided	O
it	O
the	O
size	O
of	O
the	O
batch	O
(	O
namely	O
,	O
128	O
)	O
.	O
Although	O
LSTMs	Method
tend	O
to	O
not	O
suffer	O
from	O
the	O
vanishing	Task
gradient	Task
problem	Task
,	O
they	O
can	O
have	O
exploding	O
gradients	O
.	O
Thus	O
we	O
enforced	O
a	O
hard	O
constraint	O
on	O
the	O
norm	O
of	O
the	O
gradient	O
by	O
scaling	O
it	O
when	O
its	O
norm	O
exceeded	O
a	O
threshold	O
.	O
For	O
each	O
training	O
batch	O
,	O
we	O
compute	O
,	O
where	O
is	O
the	O
gradient	O
divided	O
by	O
128	O
.	O
If	O
,	O
we	O
set	O
.	O
Different	O
sentences	O
have	O
different	O
lengths	O
.	O
Most	O
sentences	O
are	O
short	O
(	O
e.g.	O
,	O
length	O
20	O
-	O
30	O
)	O
but	O
some	O
sentences	O
are	O
long	O
(	O
e.g.	O
,	O
length	O
100	O
)	O
,	O
so	O
a	O
minibatch	O
of	O
128	O
randomly	O
chosen	O
training	O
sentences	O
will	O
have	O
many	O
short	O
sentences	O
and	O
few	O
long	O
sentences	O
,	O
and	O
as	O
a	O
result	O
,	O
much	O
of	O
the	O
computation	O
in	O
the	O
minibatch	Method
is	O
wasted	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
made	O
sure	O
that	O
all	O
sentences	O
in	O
a	O
minibatch	O
are	O
roughly	O
of	O
the	O
same	O
length	O
,	O
yielding	O
a	O
2x	O
speedup	O
.	O
subsection	O
:	O
Parallelization	O
A	O
C	Method
++	Method
implementation	Method
of	O
deep	O
LSTM	Method
with	O
the	O
configuration	O
from	O
the	O
previous	O
section	O
on	O
a	O
single	O
GPU	Method
processes	O
a	O
speed	O
of	O
approximately	O
1	O
,	O
700	O
words	O
per	O
second	O
.	O
This	O
was	O
too	O
slow	O
for	O
our	O
purposes	O
,	O
so	O
we	O
parallelized	O
our	O
model	O
using	O
an	O
8	Method
-	Method
GPU	Method
machine	Method
.	O
Each	O
layer	O
of	O
the	O
LSTM	Method
was	O
executed	O
on	O
a	O
different	O
GPU	O
and	O
communicated	O
its	O
activations	O
to	O
the	O
next	O
GPU	O
/	O
layer	O
as	O
soon	O
as	O
they	O
were	O
computed	O
.	O
Our	O
models	O
have	O
4	O
layers	O
of	O
LSTMs	Method
,	O
each	O
of	O
which	O
resides	O
on	O
a	O
separate	O
GPU	O
.	O
The	O
remaining	O
4	O
GPUs	O
were	O
used	O
to	O
parallelize	O
the	O
softmax	O
,	O
so	O
each	O
GPU	O
was	O
responsible	O
for	O
multiplying	O
by	O
a	O
matrix	O
.	O
The	O
resulting	O
implementation	O
achieved	O
a	O
speed	O
of	O
6	O
,	O
300	O
(	O
both	O
English	O
and	O
French	O
)	O
words	O
per	O
second	O
with	O
a	O
minibatch	O
size	O
of	O
128	O
.	O
Training	Task
took	O
about	O
a	O
ten	O
days	O
with	O
this	O
implementation	O
.	O
subsection	O
:	O
Experimental	O
Results	O
We	O
used	O
the	O
cased	O
BLEU	Metric
score	Metric
to	O
evaluate	O
the	O
quality	Metric
of	O
our	O
translations	O
.	O
We	O
computed	O
our	O
BLEU	Metric
scores	Metric
using	O
multi	O
-	O
bleu.pl	O
There	O
several	O
variants	O
of	O
the	O
BLEU	Metric
score	Metric
,	O
and	O
each	O
variant	O
is	O
defined	O
with	O
a	O
perl	Method
script	Method
.	O
on	O
the	O
tokenized	Task
predictions	Task
and	O
ground	Metric
truth	Metric
.	O
This	O
way	O
of	O
evaluating	O
the	O
BELU	Metric
score	Metric
is	O
consistent	O
with	O
and	O
,	O
and	O
reproduces	O
the	O
33.3	O
score	O
of	O
.	O
However	O
,	O
if	O
we	O
evaluate	O
the	O
best	O
WMT’14	Method
system	Method
(	O
whose	O
predictions	O
can	O
be	O
downloaded	O
from	O
)	O
in	O
this	O
manner	O
,	O
we	O
get	O
37.0	O
,	O
which	O
is	O
greater	O
than	O
the	O
35.8	O
reported	O
by	O
.	O
The	O
results	O
are	O
presented	O
in	O
tables	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
Our	O
best	O
results	O
are	O
obtained	O
with	O
an	O
ensemble	Method
of	Method
LSTMs	Method
that	O
differ	O
in	O
their	O
random	Method
initializations	Method
and	O
in	O
the	O
random	O
order	O
of	O
minibatches	O
.	O
While	O
the	O
decoded	O
translations	O
of	O
the	O
LSTM	Method
ensemble	O
do	O
not	O
outperform	O
the	O
best	O
WMT’14	Method
system	Method
,	O
it	O
is	O
the	O
first	O
time	O
that	O
a	O
pure	O
neural	Method
translation	Method
system	Method
outperforms	O
a	O
phrase	Method
-	Method
based	Method
SMT	Method
baseline	Method
on	O
a	O
large	O
scale	O
MT	Task
task	Task
by	O
a	O
sizeable	O
margin	O
,	O
despite	O
its	O
inability	O
to	O
handle	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O
The	O
LSTM	Method
is	O
within	O
0.5	O
BLEU	Metric
points	Metric
of	O
the	O
best	O
WMT’14	O
result	O
if	O
it	O
is	O
used	O
to	O
rescore	O
the	O
1000	O
-	O
best	O
list	O
of	O
the	O
baseline	O
system	O
.	O
subsection	O
:	O
Performance	O
on	O
long	O
sentences	O
We	O
were	O
surprised	O
to	O
discover	O
that	O
the	O
LSTM	Method
did	O
well	O
on	O
long	O
sentences	O
,	O
which	O
is	O
shown	O
quantitatively	O
in	O
figure	O
[	O
reference	O
]	O
.	O
Table	O
[	O
reference	O
]	O
presents	O
several	O
examples	O
of	O
long	O
sentences	O
and	O
their	O
translations	O
.	O
subsection	O
:	O
Model	O
Analysis	O
One	O
of	O
the	O
attractive	O
features	O
of	O
our	O
model	O
is	O
its	O
ability	O
to	O
turn	O
a	O
sequence	O
of	O
words	O
into	O
a	O
vector	O
of	O
fixed	O
dimensionality	O
.	O
Figure	O
[	O
reference	O
]	O
visualizes	O
some	O
of	O
the	O
learned	O
representations	O
.	O
The	O
figure	O
clearly	O
shows	O
that	O
the	O
representations	O
are	O
sensitive	O
to	O
the	O
order	O
of	O
words	O
,	O
while	O
being	O
fairly	O
insensitive	O
to	O
the	O
replacement	O
of	O
an	O
active	O
voice	O
with	O
a	O
passive	O
voice	O
.	O
The	O
two	O
-	O
dimensional	O
projections	O
are	O
obtained	O
using	O
PCA	Method
.	O
section	O
:	O
Related	O
work	O
There	O
is	O
a	O
large	O
body	O
of	O
work	O
on	O
applications	O
of	O
neural	Method
networks	Method
to	O
machine	Task
translation	Task
.	O
So	O
far	O
,	O
the	O
simplest	O
and	O
most	O
effective	O
way	O
of	O
applying	O
an	O
RNN	Method
-	Method
Language	Method
Model	Method
(	O
RNNLM	Method
)	O
or	O
a	O
Feedforward	Method
Neural	Method
Network	Method
Language	Method
Model	Method
(	O
NNLM	Method
)	O
to	O
an	O
MT	Task
task	Task
is	O
by	O
rescoring	O
the	O
n	O
-	O
best	O
lists	O
of	O
a	O
strong	O
MT	Method
baseline	Method
,	O
which	O
reliably	O
improves	O
translation	Metric
quality	Metric
.	O
More	O
recently	O
,	O
researchers	O
have	O
begun	O
to	O
look	O
into	O
ways	O
of	O
including	O
information	O
about	O
the	O
source	O
language	O
into	O
the	O
NNLM	Method
.	O
Examples	O
of	O
this	O
work	O
include	O
Auli	O
et	O
al	O
.	O
,	O
who	O
combine	O
an	O
NNLM	Method
with	O
a	O
topic	Method
model	Method
of	O
the	O
input	O
sentence	O
,	O
which	O
improves	O
rescoring	Task
performance	O
.	O
Devlin	O
et	O
al	O
.	O
followed	O
a	O
similar	O
approach	O
,	O
but	O
they	O
incorporated	O
their	O
NNLM	Method
into	O
the	O
decoder	Method
of	O
an	O
MT	Method
system	Method
and	O
used	O
the	O
decoder	O
’s	O
alignment	O
information	O
to	O
provide	O
the	O
NNLM	Method
with	O
the	O
most	O
useful	O
words	O
in	O
the	O
input	O
sentence	O
.	O
Their	O
approach	O
was	O
highly	O
successful	O
and	O
it	O
achieved	O
large	O
improvements	O
over	O
their	O
baseline	O
.	O
Our	O
work	O
is	O
closely	O
related	O
to	O
Kalchbrenner	O
and	O
Blunsom	O
,	O
who	O
were	O
the	O
first	O
to	O
map	O
the	O
input	O
sentence	O
into	O
a	O
vector	O
and	O
then	O
back	O
to	O
a	O
sentence	O
,	O
although	O
they	O
map	O
sentences	O
to	O
vectors	O
using	O
convolutional	Method
neural	Method
networks	Method
,	O
which	O
lose	O
the	O
ordering	O
of	O
the	O
words	O
.	O
Similarly	O
to	O
this	O
work	O
,	O
Cho	O
et	O
al	O
.	O
used	O
an	O
LSTM	Method
-	O
like	O
RNN	O
architecture	O
to	O
map	O
sentences	O
into	O
vectors	O
and	O
back	O
,	O
although	O
their	O
primary	O
focus	O
was	O
on	O
integrating	O
their	O
neural	Method
network	Method
into	O
an	O
SMT	Method
system	Method
.	O
Bahdanau	O
et	O
al	O
.	O
also	O
attempted	O
direct	Task
translations	Task
with	O
a	O
neural	Method
network	Method
that	O
used	O
an	O
attention	Method
mechanism	Method
to	O
overcome	O
the	O
poor	O
performance	O
on	O
long	O
sentences	O
experienced	O
by	O
Cho	O
et	O
al	O
.	O
and	O
achieved	O
encouraging	O
results	O
.	O
Likewise	O
,	O
Pouget	O
-	O
Abadie	O
et	O
al	O
.	O
attempted	O
to	O
address	O
the	O
memory	Task
problem	Task
of	O
Cho	O
et	O
al	O
.	O
by	O
translating	O
pieces	O
of	O
the	O
source	O
sentence	O
in	O
way	O
that	O
produces	O
smooth	O
translations	O
,	O
which	O
is	O
similar	O
to	O
a	O
phrase	Method
-	Method
based	Method
approach	Method
.	O
We	O
suspect	O
that	O
they	O
could	O
achieve	O
similar	O
improvements	O
by	O
simply	O
training	O
their	O
networks	O
on	O
reversed	O
source	O
sentences	O
.	O
End	Task
-	Task
to	Task
-	Task
end	Task
training	Task
is	O
also	O
the	O
focus	O
of	O
Hermann	O
et	O
al	O
.	O
,	O
whose	O
model	O
represents	O
the	O
inputs	O
and	O
outputs	O
by	O
feedforward	Method
networks	Method
,	O
and	O
map	O
them	O
to	O
similar	O
points	O
in	O
space	O
.	O
However	O
,	O
their	O
approach	O
can	O
not	O
generate	O
translations	O
directly	O
:	O
to	O
get	O
a	O
translation	O
,	O
they	O
need	O
to	O
do	O
a	O
look	O
up	O
for	O
closest	O
vector	O
in	O
the	O
pre	O
-	O
computed	O
database	O
of	O
sentences	O
,	O
or	O
to	O
rescore	O
a	O
sentence	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
showed	O
that	O
a	O
large	O
deep	O
LSTM	Method
,	O
that	O
has	O
a	O
limited	O
vocabulary	O
and	O
that	O
makes	O
almost	O
no	O
assumption	O
about	O
problem	O
structure	O
can	O
outperform	O
a	O
standard	O
SMT	Method
-	Method
based	Method
system	Method
whose	O
vocabulary	O
is	O
unlimited	O
on	O
a	O
large	Task
-	Task
scale	Task
MT	Task
task	Task
.	O
The	O
success	O
of	O
our	O
simple	O
LSTM	Method
-	O
based	O
approach	O
on	O
MT	Task
suggests	O
that	O
it	O
should	O
do	O
well	O
on	O
many	O
other	O
sequence	Task
learning	Task
problems	Task
,	O
provided	O
they	O
have	O
enough	O
training	O
data	O
.	O
We	O
were	O
surprised	O
by	O
the	O
extent	O
of	O
the	O
improvement	O
obtained	O
by	O
reversing	O
the	O
words	O
in	O
the	O
source	O
sentences	O
.	O
We	O
conclude	O
that	O
it	O
is	O
important	O
to	O
find	O
a	O
problem	Method
encoding	Method
that	O
has	O
the	O
greatest	O
number	O
of	O
short	O
term	O
dependencies	O
,	O
as	O
they	O
make	O
the	O
learning	Task
problem	Task
much	O
simpler	O
.	O
In	O
particular	O
,	O
while	O
we	O
were	O
unable	O
to	O
train	O
a	O
standard	O
RNN	Method
on	O
the	O
non	Task
-	Task
reversed	Task
translation	Task
problem	Task
(	O
shown	O
in	O
fig	O
.	O
[	O
reference	O
]	O
)	O
,	O
we	O
believe	O
that	O
a	O
standard	O
RNN	Method
should	O
be	O
easily	O
trainable	O
when	O
the	O
source	O
sentences	O
are	O
reversed	O
(	O
although	O
we	O
did	O
not	O
verify	O
it	O
experimentally	O
)	O
.	O
We	O
were	O
also	O
surprised	O
by	O
the	O
ability	O
of	O
the	O
LSTM	Method
to	O
correctly	O
translate	O
very	O
long	O
sentences	O
.	O
We	O
were	O
initially	O
convinced	O
that	O
the	O
LSTM	Method
would	O
fail	O
on	O
long	O
sentences	O
due	O
to	O
its	O
limited	O
memory	O
,	O
and	O
other	O
researchers	O
reported	O
poor	O
performance	O
on	O
long	O
sentences	O
with	O
a	O
model	O
similar	O
to	O
ours	O
.	O
And	O
yet	O
,	O
LSTMs	Method
trained	O
on	O
the	O
reversed	O
dataset	O
had	O
little	O
difficulty	O
translating	O
long	O
sentences	O
.	O
Most	O
importantly	O
,	O
we	O
demonstrated	O
that	O
a	O
simple	O
,	O
straightforward	O
and	O
a	O
relatively	O
unoptimized	O
approach	O
can	O
outperform	O
an	O
SMT	Method
system	Method
,	O
so	O
further	O
work	O
will	O
likely	O
lead	O
to	O
even	O
greater	O
translation	Metric
accuracies	Metric
.	O
These	O
results	O
suggest	O
that	O
our	O
approach	O
will	O
likely	O
do	O
well	O
on	O
other	O
challenging	O
sequence	Task
to	Task
sequence	Task
problems	Task
.	O
section	O
:	O
Acknowledgments	O
We	O
thank	O
Samy	O
Bengio	O
,	O
Jeff	O
Dean	O
,	O
Matthieu	O
Devin	O
,	O
Geoffrey	O
Hinton	O
,	O
Nal	O
Kalchbrenner	O
,	O
Thang	O
Luong	O
,	O
Wolfgang	O
Macherey	O
,	O
Rajat	O
Monga	O
,	O
Vincent	O
Vanhoucke	O
,	O
Peng	O
Xu	O
,	O
Wojciech	O
Zaremba	O
,	O
and	O
the	O
Google	O
Brain	O
team	O
for	O
useful	O
comments	O
and	O
discussions	O
.	O
bibliography	O
:	O
References	O
