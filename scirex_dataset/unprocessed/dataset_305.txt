document	O
:	O
Evaluating	O
the	O
Utility	O
of	O
Hand	Method
-	Method
crafted	Method
Features	Method
in	O
Sequence	Task
Labelling	Task
Conventional	O
wisdom	O
is	O
that	O
hand	O
-	O
crafted	O
features	O
are	O
redundant	O
for	O
deep	Method
learning	Method
models	Method
,	O
as	O
they	O
already	O
learn	O
adequate	O
representations	O
of	O
text	O
automatically	O
from	O
corpora	O
.	O
In	O
this	O
work	O
,	O
we	O
test	O
this	O
claim	O
by	O
proposing	O
a	O
new	O
method	O
for	O
exploiting	O
handcrafted	O
features	O
as	O
part	O
of	O
a	O
novel	O
hybrid	Method
learning	Method
approach	Method
,	O
incorporating	O
a	O
feature	Method
auto	Method
-	Method
encoder	Method
loss	Method
component	Method
.	O
We	O
evaluate	O
on	O
the	O
task	O
of	O
named	Task
entity	Task
recognition	Task
(	O
NER	Task
)	O
,	O
where	O
we	O
show	O
that	O
including	O
manual	O
features	O
for	O
part	O
-	O
of	O
-	O
speech	O
,	O
word	O
shapes	O
and	O
gazetteers	O
can	O
improve	O
the	O
performance	O
of	O
a	O
neural	Method
CRF	Method
model	Method
.	O
We	O
obtain	O
a	O
of	O
91.89	O
for	O
the	O
CoNLL	Task
-	Task
2003	Task
English	Task
shared	Task
task	Task
,	O
which	O
significantly	O
outperforms	O
a	O
collection	O
of	O
highly	O
competitive	Method
baseline	Method
models	Method
.	O
We	O
also	O
present	O
an	O
ablation	O
study	O
showing	O
the	O
importance	O
of	O
auto	Method
-	Method
encoding	Method
,	O
over	O
using	O
features	O
as	O
either	O
inputs	O
or	O
outputs	O
alone	O
,	O
and	O
moreover	O
,	O
show	O
including	O
the	O
autoencoder	Method
components	Method
reduces	O
training	O
requirements	O
to	O
60	O
%	O
,	O
while	O
retaining	O
the	O
same	O
predictive	Metric
accuracy	Metric
.	O
section	O
:	O
Introduction	O
Deep	Method
neural	Method
networks	Method
have	O
been	O
proven	O
to	O
be	O
a	O
powerful	O
framework	O
for	O
natural	Task
language	Task
processing	Task
,	O
and	O
have	O
demonstrated	O
strong	O
performance	O
on	O
a	O
number	O
of	O
challenging	O
tasks	O
,	O
ranging	O
from	O
machine	Task
translation	Task
,	O
to	O
text	Task
categorisation	Task
.	O
Not	O
only	O
do	O
such	O
deep	Method
models	Method
outperform	O
traditional	O
machine	Method
learning	Method
methods	Method
,	O
they	O
also	O
come	O
with	O
the	O
benefit	O
of	O
not	O
requiring	O
difficult	O
feature	Method
engineering	Method
.	O
For	O
instance	O
,	O
both	O
lample2016neural	O
and	O
ma2016end	O
propose	O
end	Method
-	Method
to	Method
-	Method
end	Method
models	Method
for	O
sequence	Task
labelling	Task
task	Task
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
Orthogonal	O
to	O
the	O
advances	O
in	O
deep	Method
learning	Method
is	O
the	O
effort	O
spent	O
on	O
feature	Method
engineering	Method
.	O
A	O
representative	O
example	O
is	O
the	O
task	O
of	O
named	Task
entity	Task
recognition	Task
(	O
NER	Task
)	O
,	O
one	O
that	O
requires	O
both	O
lexical	O
and	O
syntactic	O
knowledge	O
,	O
where	O
,	O
until	O
recently	O
,	O
most	O
models	O
heavily	O
rely	O
on	O
statistical	Method
sequential	Method
labelling	Method
models	Method
taking	O
in	O
manually	O
engineered	O
features	O
.	O
Typical	O
features	O
include	O
POS	O
and	O
chunk	O
tags	O
,	O
prefixes	O
and	O
suffixes	O
,	O
and	O
external	O
gazetteers	O
,	O
all	O
of	O
which	O
represent	O
years	O
of	O
accumulated	O
knowledge	O
in	O
the	O
field	O
of	O
computational	Task
linguistics	Task
.	O
The	O
work	O
of	O
collobert2011natural	O
started	O
the	O
trend	O
of	O
feature	Method
engineering	Method
-	Method
free	Method
modelling	Method
by	O
learning	O
internal	Method
representations	Method
of	Method
compositional	Method
components	Method
of	Method
text	Method
(	O
e.g.	O
,	O
word	O
embeddings	O
)	O
.	O
Subsequent	O
work	O
has	O
shown	O
impressive	O
progress	O
through	O
capturing	O
syntactic	O
and	O
semantic	O
knowledge	O
with	O
dense	O
real	Method
-	Method
valued	Method
vectors	Method
trained	O
on	O
large	O
unannotated	O
corpora	O
.	O
Enabled	O
by	O
the	O
powerful	O
representational	Method
capacity	Method
of	O
such	O
embeddings	Method
and	O
neural	Method
networks	Method
,	O
feature	Method
engineering	Method
has	O
largely	O
been	O
replaced	O
with	O
taking	O
off	O
-	O
the	O
-	O
shelf	O
pre	O
-	O
trained	O
word	O
embeddings	O
as	O
input	O
,	O
thereby	O
making	O
models	O
fully	O
end	O
-	O
to	O
-	O
end	O
and	O
the	O
research	O
focus	O
has	O
shifted	O
to	O
neural	Method
network	Method
architecture	Method
engineering	Method
.	O
More	O
recently	O
,	O
there	O
has	O
been	O
increasing	O
recognition	O
of	O
the	O
utility	O
of	O
linguistic	O
features	O
where	O
such	O
features	O
are	O
integrated	O
to	O
improve	O
model	O
performance	O
.	O
Inspired	O
by	O
this	O
,	O
taking	O
NER	Task
as	O
a	O
case	O
study	O
,	O
we	O
investigate	O
the	O
utility	O
of	O
hand	O
-	O
crafted	O
features	O
in	O
deep	Method
learning	Method
models	Method
,	O
challenging	O
conventional	O
wisdom	O
in	O
an	O
attempt	O
to	O
refute	O
the	O
utility	O
of	O
manually	O
-	O
engineered	O
features	O
.	O
Of	O
particular	O
interest	O
to	O
this	O
paper	O
is	O
the	O
work	O
by	O
ma2016end	O
where	O
they	O
introduce	O
a	O
strong	O
end	Method
-	Method
to	Method
-	Method
end	Method
model	Method
combining	O
a	O
bi	Method
-	Method
directional	Method
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	Method
Bi	Method
-	Method
LSTM	Method
)	Method
network	Method
with	O
Convolutional	Method
Neural	Method
Network	Method
(	O
CNN	Method
)	O
character	O
encoding	O
in	O
a	O
Conditional	Method
Random	Method
Field	Method
(	O
CRF	Method
)	O
.	O
Their	O
model	O
is	O
highly	O
capable	O
of	O
capturing	O
not	O
only	O
word	O
-	O
but	O
also	O
character	O
-	O
level	O
features	O
.	O
We	O
extend	O
this	O
model	O
by	O
integrating	O
an	O
auto	Method
-	Method
encoder	Method
loss	Method
,	O
allowing	O
the	O
model	O
to	O
take	O
hand	O
-	O
crafted	O
features	O
as	O
input	O
and	O
re	O
-	O
construct	O
them	O
as	O
output	O
,	O
and	O
show	O
that	O
,	O
even	O
with	O
such	O
a	O
highly	O
competitive	O
model	O
,	O
incorporating	O
linguistic	O
features	O
is	O
still	O
beneficial	O
.	O
Perhaps	O
the	O
closest	O
to	O
this	O
study	O
is	O
the	O
works	O
by	O
Ammar	O
+	O
:2014	O
and	O
Zhang	O
+	O
:2017	O
,	O
who	O
show	O
how	O
CRFs	Method
can	O
be	O
framed	O
as	O
auto	Method
-	Method
encoders	Method
in	O
unsupervised	Task
or	Task
semi	Task
-	Task
supervised	Task
settings	Task
.	O
With	O
our	O
proposed	O
model	O
,	O
we	O
achieve	O
strong	O
performance	O
on	O
the	O
CoNLL	Task
2003	Task
English	Task
NER	Task
shared	Task
task	Task
with	O
an	O
of	O
,	O
significantly	O
outperforming	O
an	O
array	O
of	O
competitive	O
baselines	O
.	O
We	O
conduct	O
an	O
ablation	O
study	O
to	O
better	O
understand	O
the	O
impacts	O
of	O
each	O
manually	O
-	O
crafted	O
feature	O
.	O
Finally	O
,	O
we	O
further	O
provide	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
model	O
performance	O
when	O
trained	O
with	O
varying	O
amount	O
of	O
data	O
and	O
show	O
that	O
the	O
proposed	O
model	O
is	O
highly	O
competent	O
with	O
only	O
60	O
%	O
of	O
the	O
training	O
set	O
.	O
section	O
:	O
Methodology	O
In	O
this	O
section	O
,	O
we	O
first	O
outline	O
the	O
model	Method
architecture	Method
,	O
then	O
the	O
manually	O
crafted	O
features	O
,	O
and	O
finally	O
how	O
they	O
are	O
incorporated	O
into	O
the	O
model	O
.	O
subsection	O
:	O
Model	O
Architecture	O
We	O
build	O
on	O
a	O
highly	O
competitive	Method
sequence	Method
labelling	Method
model	Method
,	O
namely	O
Bi	Method
-	Method
LSTM	Method
-	Method
CNN	Method
-	Method
CRF	Method
,	O
first	O
introduced	O
by	O
ma2016end	O
.	O
Given	O
an	O
input	O
sequence	O
of	O
of	O
length	O
,	O
the	O
model	O
is	O
capable	O
of	O
tagging	O
each	O
input	O
with	O
a	O
predicted	O
label	O
,	O
resulting	O
in	O
a	O
sequence	O
of	O
closely	O
matching	O
the	O
gold	O
label	O
sequence	O
.	O
Here	O
,	O
we	O
extend	O
the	O
model	O
by	O
incorporating	O
an	O
auto	Method
-	Method
encoder	Method
loss	Method
taking	O
hand	O
-	O
crafted	O
features	O
as	O
in	O
/	O
output	O
,	O
thereby	O
forcing	O
the	O
model	O
to	O
preserve	O
crucial	O
information	O
stored	O
in	O
such	O
features	O
and	O
allowing	O
us	O
to	O
evaluate	O
the	O
impacts	O
of	O
each	O
feature	O
on	O
model	O
performance	O
.	O
Specifically	O
,	O
our	O
model	O
,	O
referred	O
to	O
as	O
Neural	Method
-	Method
CRF	Method
+	Method
AE	Method
,	O
consists	O
of	O
four	O
major	O
components	O
:	O
(	O
1	O
)	O
a	O
character	Method
-	Method
level	Method
CNN	Method
(	O
char	Method
-	Method
CNN	Method
)	O
;	O
(	O
2	O
)	O
a	O
word	Method
-	Method
level	Method
bi	Method
-	Method
directional	Method
LSTM	Method
(	O
Bi	Method
-	Method
LSTM	Method
)	O
;	O
(	O
3	O
)	O
a	O
conditional	Method
random	Method
field	Method
(	O
CRF	Method
)	O
;	O
and	O
(	O
4	O
)	O
an	O
auto	Method
-	Method
encoder	Method
auxiliary	Method
loss	Method
.	O
An	O
illustration	O
of	O
the	O
model	Method
architecture	Method
is	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Char	O
-	O
CNN	Method
.	O
Previous	O
studies	O
have	O
demonstrated	O
that	O
CNNs	Method
are	O
highly	O
capable	O
of	O
capturing	O
character	O
-	O
level	O
features	O
.	O
Here	O
,	O
our	O
character	Method
-	Method
level	Method
CNN	Method
is	O
similar	O
to	O
that	O
used	O
in	O
ma2016end	Method
but	O
differs	O
in	O
that	O
we	O
use	O
a	O
ReLU	Method
activation	Method
.	O
paragraph	O
:	O
Bi	Method
-	Method
LSTM	Method
.	O
We	O
use	O
a	O
Bi	Method
-	Method
LSTM	Method
to	O
learn	O
contextual	O
information	O
of	O
a	O
sequence	O
of	O
words	O
.	O
As	O
inputs	O
to	O
the	O
Bi	Method
-	Method
LSTM	Method
,	O
we	O
first	O
concatenate	O
the	O
pre	O
-	O
trained	O
embedding	O
of	O
each	O
word	O
with	O
its	O
character	Method
-	Method
level	Method
representation	Method
(	O
the	O
output	O
of	O
the	O
char	Method
-	Method
CNN	Method
)	O
and	O
a	O
vector	O
of	O
manually	O
crafted	O
features	O
(	O
described	O
in	O
Section	O
[	O
reference	O
]	O
)	O
:	O
where	O
denotes	O
concatenation	O
.	O
The	O
outputs	O
of	O
the	O
forward	Method
and	Method
backward	Method
pass	Method
of	O
the	O
Bi	Method
-	Method
LSTM	Method
is	O
then	O
concatenated	O
to	O
form	O
the	O
output	O
of	O
the	O
Bi	Method
-	Method
LSTM	Method
,	O
where	O
dropout	Method
is	O
also	O
applied	O
.	O
paragraph	O
:	O
CRF	Method
.	O
For	O
sequence	Task
labelling	Task
tasks	Task
,	O
it	O
is	O
intuitive	O
and	O
beneficial	O
to	O
utilise	O
information	O
carried	O
between	O
neighbouring	O
labels	O
to	O
predict	O
the	O
best	O
sequence	O
of	O
labels	O
for	O
a	O
given	O
sentence	O
.	O
Therefore	O
,	O
we	O
employ	O
a	O
conditional	Method
random	Method
field	Method
layer	Method
taking	O
as	O
input	O
the	O
output	O
of	O
the	O
Bi	Method
-	Method
LSTM	Method
.	O
Training	Task
is	O
carried	O
out	O
by	O
maximising	O
the	O
log	O
probability	O
of	O
the	O
gold	O
sequence	O
:	O
while	O
decoding	Task
can	O
be	O
efficiently	O
performed	O
with	O
the	O
Viterbi	Method
algorithm	Method
.	O
paragraph	O
:	O
Auto	Method
-	Method
encoder	Method
loss	Method
.	O
Alongside	O
sequence	Task
labelling	Task
as	O
the	O
primary	O
task	O
,	O
we	O
also	O
deploy	O
,	O
as	O
auxiliary	Task
tasks	Task
,	O
three	O
auto	Method
-	Method
encoders	Method
for	O
reconstructing	O
the	O
hand	O
-	O
engineered	O
feature	O
vectors	O
.	O
To	O
this	O
end	O
,	O
we	O
add	O
multiple	O
independent	O
fully	Method
-	Method
connected	Method
dense	Method
layers	Method
,	O
all	O
taking	O
as	O
input	O
the	O
Bi	O
-	O
LSTM	O
output	O
with	O
each	O
responsible	O
for	O
reconstructing	O
a	O
particular	O
type	O
of	O
feature	O
:	O
where	O
is	O
the	O
sigmoid	O
activation	O
function	O
,	O
denotes	O
the	O
type	O
of	O
feature	O
,	O
and	O
is	O
a	O
trainable	O
parameter	O
matrix	O
.	O
More	O
formally	O
,	O
we	O
define	O
the	O
auto	Method
-	Method
encoder	Method
loss	Method
as	O
:	O
Model	Task
training	Task
.	O
Training	Task
is	O
carried	O
out	O
by	O
optimising	O
the	O
joint	O
loss	O
:	O
where	O
,	O
in	O
addition	O
to	O
,	O
we	O
also	O
add	O
the	O
auto	O
-	O
encoder	O
loss	O
,	O
weighted	O
by	O
.	O
In	O
all	O
our	O
experiments	O
,	O
we	O
set	O
to	O
for	O
all	O
s.	O
subsection	O
:	O
Hand	Method
-	Method
crafted	Method
Features	Method
We	O
consider	O
three	O
categories	O
of	O
widely	O
used	O
features	O
:	O
(	O
1	O
)	O
POS	O
tags	O
;	O
(	O
2	O
)	O
word	O
shape	O
;	O
and	O
(	O
3	O
)	O
gazetteers	O
and	O
present	O
an	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O
While	O
POS	O
tags	O
carry	O
syntactic	O
information	O
regarding	O
sentence	O
structure	O
,	O
the	O
word	O
shape	O
feature	O
focuses	O
on	O
a	O
more	O
fine	O
-	O
grained	O
level	O
,	O
encoding	O
character	O
-	O
level	O
knowledge	O
to	O
complement	O
the	O
loss	O
of	O
information	O
caused	O
by	O
embedding	O
lookup	O
,	O
such	O
as	O
capitalisation	O
.	O
Both	O
features	O
are	O
based	O
on	O
the	O
implementation	O
of	O
spaCy	Method
.	O
For	O
the	O
gazetteer	O
feature	O
,	O
we	O
focus	O
on	O
PERSON	O
and	O
LOCATION	O
and	O
compile	O
a	O
list	O
for	O
each	O
.	O
The	O
PERSON	Task
gazetteer	Task
is	O
collected	O
from	O
U.S.	Material
census	Material
2000	Material
,	O
U.S.	Material
census	Material
2010	Material
and	O
DBpedia	Material
whereas	O
GeoNames	Material
is	O
the	O
main	O
source	O
for	O
LOCATION	O
,	O
taking	O
in	O
both	O
official	O
and	O
alternative	O
names	O
.	O
All	O
the	O
tokens	O
on	O
both	O
lists	O
are	O
then	O
filtered	O
to	O
exclude	O
frequently	O
occurring	O
common	O
words	O
.	O
Each	O
category	O
is	O
converted	O
into	O
a	O
one	O
-	O
hot	O
sparse	O
feature	O
vector	O
and	O
then	O
concatenated	O
to	O
form	O
a	O
multi	O
-	O
hot	O
vector	O
for	O
the	O
-	O
th	O
word	O
.	O
In	O
addition	O
,	O
we	O
also	O
experimented	O
with	O
including	O
the	O
label	O
of	O
the	O
incoming	O
dependency	O
edge	O
to	O
each	O
word	O
as	O
a	O
feature	O
,	O
but	O
observed	O
performance	O
deterioration	O
on	O
the	O
development	O
set	O
.	O
While	O
we	O
still	O
study	O
and	O
analyse	O
the	O
impacts	O
of	O
this	O
feature	O
in	O
Table	O
[	O
reference	O
]	O
and	O
Section	O
[	O
reference	O
]	O
,	O
it	O
is	O
excluded	O
from	O
our	O
model	O
configuration	O
(	O
not	O
considered	O
as	O
part	O
of	O
unless	O
indicated	O
otherwise	O
)	O
.	O
section	O
:	O
Experiments	O
In	O
this	O
section	O
,	O
we	O
present	O
our	O
experimental	O
setup	O
and	O
results	O
for	O
name	Task
entity	Task
recognition	Task
over	O
the	O
CoNLL	O
2003	O
English	O
NER	Task
shared	O
task	O
dataset	O
.	O
subsection	O
:	O
Experimental	O
Setup	O
paragraph	O
:	O
Dataset	O
.	O
We	O
use	O
the	O
CoNLL	Material
2003	Material
NER	Material
shared	Material
task	Material
dataset	Material
,	O
consisting	O
of	O
14	O
,	O
041	O
/	O
3	O
,	O
250	O
/	O
3	O
,	O
453	O
sentences	O
in	O
the	O
training	O
/	O
development	O
/	O
test	O
set	O
respectively	O
,	O
all	O
extracted	O
from	O
Reuters	O
news	O
articles	O
during	O
the	O
period	O
from	O
1996	O
to	O
1997	O
.	O
The	O
dataset	O
is	O
annotated	O
with	O
four	O
categories	O
of	O
name	O
entities	O
:	O
PERSON	O
,	O
LOCATION	O
,	O
ORGANIZATION	O
and	O
MISC	O
.	O
We	O
use	O
the	O
IOBES	Method
tagging	Method
scheme	Method
,	O
as	O
previous	O
study	O
have	O
shown	O
that	O
this	O
scheme	O
provides	O
a	O
modest	O
improvement	O
to	O
the	O
model	O
performance	O
.	O
paragraph	O
:	O
Model	O
configuration	O
.	O
Following	O
the	O
work	O
of	O
ma2016end	O
,	O
we	O
initialise	O
word	Method
embeddings	Method
with	O
GloVe	Method
(	O
-	O
dimensional	O
,	O
trained	O
on	O
a	O
6B	O
-	O
token	O
corpus	O
)	O
.	O
Character	Method
embeddings	Method
are	O
-	O
dimensional	O
and	O
randomly	O
initialised	O
with	O
a	O
uniform	O
distribution	O
in	O
the	O
range	O
.	O
Parameters	O
are	O
optimised	O
with	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
and	O
momentum	O
of	O
.	O
Exponential	Method
learning	Method
rate	Method
decay	Method
is	O
applied	O
every	O
5	O
epochs	O
with	O
a	O
factor	O
of	O
.	O
To	O
reduce	O
the	O
impact	O
of	O
exploding	O
gradients	O
,	O
we	O
employ	O
gradient	Method
clipping	Method
at	O
.	O
We	O
train	O
our	O
models	O
on	O
a	O
single	O
GeForce	Method
GTX	Method
TITAN	Method
X	Method
GPU	Method
.	O
With	O
the	O
above	O
hyper	O
-	O
parameter	O
setting	O
,	O
training	Task
takes	O
approximately	O
hours	O
for	O
a	O
full	O
run	O
of	O
epochs	O
.	O
paragraph	O
:	O
Evaluation	O
.	O
We	O
measure	O
model	O
performance	O
with	O
the	O
official	O
CoNLL	Metric
evaluation	Metric
script	Metric
and	O
report	O
span	O
-	O
level	O
named	O
entity	O
F	Metric
-	Metric
score	Metric
on	O
the	O
test	O
set	O
using	O
early	Method
stopping	Method
based	O
on	O
the	O
performance	O
on	O
the	O
validation	O
set	O
.	O
We	O
report	O
average	Metric
F	Metric
-	Metric
scores	Metric
and	O
standard	O
deviation	O
over	O
5	O
runs	O
for	O
our	O
model	O
.	O
paragraph	O
:	O
Baseline	O
.	O
In	O
addition	O
to	O
reporting	O
a	O
number	O
of	O
prior	O
results	O
of	O
competitive	Method
baseline	Method
models	Method
,	O
as	O
listed	O
in	O
Table	O
[	O
reference	O
]	O
,	O
we	O
also	O
re	O
-	O
implement	O
the	O
Bi	Method
-	Method
LSTM	Method
-	Method
CNN	Method
-	Method
CRF	Method
model	Method
by	O
ma2016end	O
(	O
referred	O
to	O
as	O
Neural	O
-	O
CRF	Method
in	O
Table	O
[	O
reference	O
]	O
)	O
and	O
report	O
its	O
average	O
performance	O
.	O
subsection	O
:	O
Results	O
The	O
experimental	O
results	O
are	O
presented	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Observe	O
that	O
Neural	Method
-	Method
CRF	Method
+	Method
AE	Method
,	O
trained	O
either	O
on	O
the	O
training	O
set	O
only	O
or	O
with	O
the	O
addition	O
of	O
the	O
development	O
set	O
,	O
achieves	O
substantial	O
improvements	O
in	O
F	Metric
-	Metric
score	Metric
in	O
both	O
settings	O
,	O
superior	O
to	O
all	O
but	O
one	O
of	O
the	O
benchmark	O
models	O
,	O
highlighting	O
the	O
utility	O
of	O
hand	O
-	O
crafted	O
features	O
incorporated	O
with	O
the	O
proposed	O
auto	Method
-	Method
encoder	Method
loss	Method
.	O
Compared	O
against	O
the	O
Neural	O
-	O
CRF	Method
,	O
a	O
very	O
strong	O
model	O
in	O
itself	O
,	O
our	O
model	O
significantly	O
improves	O
performance	O
,	O
showing	O
the	O
positive	O
impact	O
of	O
our	O
technique	O
for	O
exploiting	O
manually	O
-	O
engineered	O
features	O
.	O
Although	O
Peters	O
+	O
:2018	O
report	O
a	O
higher	O
F	Metric
-	Metric
score	Metric
using	O
their	O
ELMo	Method
embedding	Method
technique	Method
,	O
our	O
approach	O
here	O
is	O
orthogonal	O
,	O
and	O
accordingly	O
we	O
would	O
expect	O
a	O
performance	O
increase	O
if	O
we	O
were	O
to	O
incorporate	O
their	O
ELMo	Method
representations	Method
into	O
our	O
model	O
.	O
paragraph	O
:	O
Ablation	Task
Study	Task
To	O
gain	O
a	O
better	O
understanding	O
of	O
the	O
impacts	O
of	O
each	O
feature	O
,	O
we	O
perform	O
an	O
ablation	Task
study	Task
and	O
present	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
observe	O
performance	O
degradation	O
when	O
eliminating	O
POS	O
,	O
word	O
shape	O
and	O
gazetteer	O
features	O
,	O
showing	O
that	O
each	O
feature	O
contributes	O
to	O
NER	Task
performance	O
beyond	O
what	O
is	O
learned	O
through	O
deep	Method
learning	Method
alone	O
.	O
Interestingly	O
,	O
the	O
contribution	O
of	O
gazetteers	O
is	O
much	O
less	O
than	O
that	O
of	O
the	O
other	O
features	O
,	O
which	O
is	O
likely	O
due	O
to	O
the	O
noise	O
introduced	O
in	O
the	O
matching	Method
process	Method
,	O
with	O
many	O
incorrectly	O
identified	O
false	O
positives	O
.	O
Including	O
features	O
based	O
on	O
dependency	O
tags	O
into	O
our	O
model	O
decreases	O
the	O
performance	O
slightly	O
.	O
This	O
might	O
be	O
a	O
result	O
of	O
our	O
simple	O
implementation	O
(	O
as	O
illustrated	O
in	O
Table	O
[	O
reference	O
]	O
)	O
,	O
which	O
does	O
not	O
include	O
dependency	O
direction	O
,	O
nor	O
parent	O
-	O
child	O
relationships	O
.	O
Next	O
,	O
we	O
investigate	O
the	O
impact	O
of	O
different	O
means	O
of	O
incorporating	O
manually	O
-	O
engineered	O
features	O
into	O
the	O
model	O
.	O
To	O
this	O
end	O
,	O
we	O
experiment	O
with	O
three	O
configurations	O
with	O
features	O
as	O
:	O
(	O
1	O
)	O
input	O
only	O
;	O
(	O
2	O
)	O
output	O
only	O
(	O
equivalent	O
to	O
multi	Task
-	Task
task	Task
learning	Task
)	O
;	O
and	O
(	O
3	O
)	O
both	O
input	O
and	O
output	O
(	O
Neural	Method
-	Method
CRF	Method
+	Method
AE	Method
)	O
and	O
present	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Simply	O
using	O
features	O
as	O
either	O
input	O
or	O
output	O
only	O
improves	O
model	O
performance	O
slightly	O
,	O
but	O
insignificantly	O
so	O
.	O
It	O
is	O
only	O
when	O
features	O
are	O
incorporated	O
with	O
the	O
proposed	O
auto	Method
-	Method
encoder	Method
loss	Method
do	O
we	O
observe	O
a	O
significant	O
performance	O
boost	O
.	O
paragraph	O
:	O
Training	O
Requirements	O
every	O
node=	O
[	O
font=	O
]	O
[	O
xlabel	O
=	O
Fraction	O
of	O
training	O
data	O
(	O
%	O
)	O
,	O
ylabel	O
=	O
F1	Metric
score	Metric
,	O
xmin=7	O
,	O
xmax=103	O
,	O
ymin=84	O
,	O
ymax=93	O
,	O
height=4.5	O
cm	O
,	O
ymajorgrids	O
=	O
true	O
,	O
grid	O
style	O
=	O
dashed	O
,	O
width=0.48legend	O
pos	O
=	O
south	O
east	O
]	O
[	O
color	O
=	O
red	O
,	O
error	O
bars	O
/	O
.cd	O
,	O
y	O
dir	O
=	O
both	O
,	O
y	O
explicit	O
]	O
coordinates	O
(	O
10	O
,	O
85.19	O
)+	O
=	O
(	O
10	O
,	O
0.34	O
)-	O
=	O
(	O
10	O
,	O
0.34	O
)	O
(	O
20	O
,	O
88.27	O
)+	O
=	O
(	O
20	O
,	O
0.29	O
)-	O
=	O
(	O
20	O
,	O
0.29	O
)	O
(	O
30	O
,	O
89.35	O
)+	O
=	O
(	O
30	O
,	O
0.28	O
)-	O
=	O
(	O
30	O
,	O
0.28	O
)	O
(	O
40	O
,	O
89.70	O
)+	O
=	O
(	O
40	O
,	O
0.30	O
)-	O
=	O
(	O
40	O
,	O
0.29	O
)	O
(	O
50	O
,	O
90.55	O
)+	O
=	O
(	O
50	O
,	O
0.27	O
)-	O
=	O
(	O
50	O
,	O
0.27	O
)	O
(	O
60	O
,	O
91.09	O
)+	O
=	O
(	O
60	O
,	O
0.26	O
)-	O
=	O
(	O
60	O
,	O
0.26	O
)	O
(	O
70	O
,	O
91.26	O
)+	O
=	O
(	O
70	O
,	O
0.25	O
)-	O
=	O
(	O
70	O
,	O
0.25	O
)	O
(	O
80	O
,	O
91.40	O
)+	O
=	O
(	O
80	O
,	O
0.24	O
)-	O
=	O
(	O
80	O
,	O
0.24	O
)	O
(	O
90	O
,	O
91.64	O
)+	O
=	O
(	O
90	O
,	O
0.25	O
)-	O
=	O
(	O
90	O
,	O
0.25	O
)	O
(	O
100	O
,	O
91.89	O
)+	O
=	O
(	O
100	O
,	O
0.23	O
)-	O
=	O
(	O
100	O
,	O
0.23	O
)	O
;	O
dashed	O
]	O
coordinates	O
(	O
10	O
,	O
91.06	O
)(	O
100	O
,	O
91.06	O
)	O
;	O
Neural	Method
systems	Method
typically	O
require	O
a	O
large	O
amount	O
of	O
annotated	O
data	O
.	O
Here	O
we	O
measure	O
the	O
impact	O
of	O
training	Task
with	O
varying	O
amount	O
of	O
annotated	O
data	O
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Wtih	O
the	O
proposed	O
model	Method
architecture	Method
,	O
the	O
amount	O
of	O
labelled	O
training	O
data	O
can	O
be	O
drastically	O
reduced	O
:	O
our	O
model	O
,	O
achieves	O
comparable	O
performance	O
against	O
the	O
baseline	O
Neural	O
-	O
CRF	Method
,	O
with	O
as	O
little	O
as	O
60	O
%	O
of	O
the	O
training	O
data	O
.	O
Moreover	O
,	O
as	O
we	O
increase	O
the	O
amount	O
of	O
training	O
text	O
,	O
the	O
performance	O
of	O
Neural	Method
-	Method
CRF	Method
+	Method
AE	Method
continues	O
to	O
improve	O
.	O
paragraph	O
:	O
Hyperparameters	O
every	O
node=	O
[	O
font=	O
]	O
[	O
xmode	O
=	O
log	O
,	O
xlabel	O
=	O
Î»i	O
,	O
ylabel	O
=	O
F1	Metric
score	Metric
,	O
xmin=0	O
,	O
xmax=90	O
,	O
ymin=91.3	O
,	O
ymax=92	O
,	O
height=5.5	O
cm	O
,	O
ymajorgrids	O
=	O
true	O
,	O
grid	O
style	O
=	O
dashed	O
,	O
width=0.48legend	O
style	O
=	O
at=	O
(	O
0.87	O
,	O
0	O
),	O
anchor	O
=	O
south	O
east	O
]	O
[	O
color	O
=	O
red	O
,	O
error	O
bars	O
/	O
.cd	O
,	O
y	O
dir	O
=	O
both	O
,	O
y	O
explicit	O
]	O
coordinates	O
(	O
1e	O
-	O
8	O
,	O
91.39	O
)+	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0484	O
)-	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0484	O
)	O
(	O
1e	O
-	O
7	O
,	O
91.43	O
)+	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0361	O
)-	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0361	O
)	O
(	O
1e	O
-	O
6	O
,	O
91.50	O
)+	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.04	O
)-	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.04	O
)	O
(	O
1e	O
-	O
5	O
,	O
91.56	O
)+	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.0529	O
)-	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.0529	O
)	O
(	O
1e	O
-	O
4	O
,	O
91.63	O
)+	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0576	O
)-	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0576	O
)	O
(	O
1e	O
-	O
3	O
,	O
91.62	O
)+	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0484	O
)-	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0484	O
)	O
(	O
1e	O
-	O
2	O
,	O
91.67	O
)+	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.0361	O
)-	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.0361	O
)	O
(	O
1e	O
-	O
1	O
,	O
91.74	O
)+	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.04	O
)-	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.04	O
)	O
(	O
1	O
,	O
91.89	O
)+	O
=	O
(	O
1	O
,	O
0.0529	O
)-	O
=	O
(	O
1	O
,	O
0.0529	O
)	O
(	O
10	O
,	O
91.56	O
)+	O
=	O
(	O
10	O
,	O
0.0289	O
)-	O
=	O
(	O
10	O
,	O
0.0289	O
)	O
;	O
tagging	Task
[	O
color	O
=	O
blue	O
,	O
error	O
bars	O
/	O
.cd	O
,	O
y	O
dir	O
=	O
both	O
,	O
y	O
explicit	O
]	O
coordinates	O
(	O
1e	O
-	O
8	O
,	O
91.41	O
)+	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0289	O
)-	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0289	O
)	O
(	O
1e	O
-	O
7	O
,	O
91.45	O
)+	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0361	O
)-	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0361	O
)	O
(	O
1e	O
-	O
6	O
,	O
91.52	O
)+	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.0324	O
)-	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.0324	O
)	O
(	O
1e	O
-	O
5	O
,	O
91.57	O
)+	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.04	O
)-	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.04	O
)	O
(	O
1e	O
-	O
4	O
,	O
91.66	O
)+	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0441	O
)-	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0441	O
)	O
(	O
1e	O
-	O
3	O
,	O
91.68	O
)+	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0361	O
)-	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0361	O
)	O
(	O
1e	O
-	O
2	O
,	O
91.70	O
)+	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.04	O
)-	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.04	O
)	O
(	O
1e	O
-	O
1	O
,	O
91.78	O
)+	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.0484	O
)-	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.0484	O
)	O
(	O
1	O
,	O
91.89	O
)+	O
=	O
(	O
1	O
,	O
0.0529	O
)-	O
=	O
(	O
1	O
,	O
0.0529	O
)	O
(	O
10	O
,	O
91.50	O
)+	O
=	O
(	O
10	O
,	O
0.0324	O
)-	O
=	O
(	O
10	O
,	O
0.0324	O
)	O
;	O
Shape	O
[	O
color	O
=	O
green	O
,	O
error	O
bars	O
/	O
.cd	O
,	O
y	O
dir	O
=	O
both	O
,	O
y	O
explicit	O
]	O
coordinates	O
(	O
1e	O
-	O
8	O
,	O
91.72	O
)+	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0225	O
)-	O
=	O
(	O
1e	O
-	O
8	O
,	O
0.0225	O
)	O
(	O
1e	O
-	O
7	O
,	O
91.78	O
)+	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0256	O
)-	O
=	O
(	O
1e	O
-	O
7	O
,	O
0.0256	O
)	O
(	O
1e	O
-	O
6	O
,	O
91.76	O
)+	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.0324	O
)-	O
=	O
(	O
1e	O
-	O
6	O
,	O
0.0324	O
)	O
(	O
1e	O
-	O
5	O
,	O
91.79	O
)+	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.0484	O
)-	O
=	O
(	O
1e	O
-	O
5	O
,	O
0.0484	O
)	O
(	O
1e	O
-	O
4	O
,	O
91.80	O
)+	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0576	O
)-	O
=	O
(	O
1e	O
-	O
4	O
,	O
0.0576	O
)	O
(	O
1e	O
-	O
3	O
,	O
91.83	O
)+	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0484	O
)-	O
=	O
(	O
1e	O
-	O
3	O
,	O
0.0484	O
)	O
(	O
1e	O
-	O
2	O
,	O
91.85	O
)+	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.0361	O
)-	O
=	O
(	O
1e	O
-	O
2	O
,	O
0.0361	O
)	O
(	O
1e	O
-	O
1	O
,	O
91.82	O
)+	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.04	O
)-	O
=	O
(	O
1e	O
-	O
1	O
,	O
0.04	O
)	O
(	O
1	O
,	O
91.89	O
)+	O
=	O
(	O
1	O
,	O
0.0529	O
)-	O
=	O
(	O
1	O
,	O
0.0529	O
)	O
(	O
10	O
,	O
91.56	O
)+	O
=	O
(	O
10	O
,	O
0.0289	O
)-	O
=	O
(	O
10	O
,	O
0.0289	O
)	O
;	O
Three	O
extra	O
hyperparameters	O
are	O
introduced	O
into	O
our	O
model	O
,	O
controlling	O
the	O
weight	O
of	O
the	O
autoencoder	O
loss	O
relative	O
to	O
the	O
CRF	Method
loss	O
,	O
for	O
each	O
feature	O
type	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
effect	O
of	O
each	O
hyperparameter	O
on	O
test	O
performance	O
.	O
Observe	O
that	O
setting	O
gives	O
strong	O
performance	O
,	O
and	O
that	O
the	O
impact	O
of	O
the	O
gazetteer	O
is	O
less	O
marked	O
than	O
the	O
other	O
two	O
feature	O
types	O
.	O
While	O
increasing	O
is	O
mostly	O
beneficial	O
,	O
performance	O
drops	O
if	O
the	O
are	O
overly	O
large	O
,	O
that	O
is	O
,	O
the	O
auto	Method
-	Method
encoder	Method
loss	Method
overwhelms	O
the	O
main	O
prediction	Task
task	Task
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
set	O
out	O
to	O
investigate	O
the	O
utility	O
of	O
hand	O
-	O
crafted	O
features	O
.	O
To	O
this	O
end	O
,	O
we	O
have	O
presented	O
a	O
hybrid	Method
neural	Method
architecture	Method
to	O
validate	O
this	O
hypothesis	O
extending	O
a	O
Bi	Method
-	Method
LSTM	Method
-	Method
CNN	Method
-	Method
CRF	Method
by	O
incorporating	O
an	O
auto	Method
-	Method
encoder	Method
loss	Method
to	O
take	O
manual	O
features	O
as	O
input	O
and	O
then	O
reconstruct	O
them	O
.	O
On	O
the	O
task	O
of	O
named	Task
entity	Task
recognition	Task
,	O
we	O
show	O
significant	O
improvements	O
over	O
a	O
collection	O
of	O
competitive	O
baselines	O
,	O
verifying	O
the	O
value	O
of	O
such	O
features	O
.	O
Lastly	O
,	O
the	O
method	O
presented	O
in	O
this	O
work	O
can	O
also	O
be	O
easily	O
applied	O
to	O
other	O
tasks	O
and	O
models	O
,	O
where	O
hand	O
-	O
engineered	O
features	O
provide	O
key	O
insights	O
about	O
the	O
data	O
.	O
bibliography	O
:	O
References	O
