Deep	Method
Residual	Method
Learning	Method
for	O
Image	Task
Recognition	Task
section	O
:	O
Abstract	O
Deeper	Method
neural	Method
networks	Method
are	O
more	O
difficult	O
to	O
train	O
.	O
We	O
present	O
a	O
residual	Method
learning	Method
framework	Method
to	O
ease	O
the	O
training	Task
of	Task
networks	Task
that	O
are	O
substantially	O
deeper	O
than	O
those	O
used	O
previously	O
.	O
We	O
explicitly	O
reformulate	O
the	O
layers	O
as	O
learning	O
residual	O
functions	O
with	O
reference	O
to	O
the	O
layer	O
inputs	O
,	O
instead	O
of	O
learning	O
unreferenced	O
functions	O
.	O
We	O
provide	O
comprehensive	O
empirical	O
evidence	O
showing	O
that	O
these	O
residual	Method
networks	Method
are	O
easier	O
to	O
optimize	O
,	O
and	O
can	O
gain	O
accuracy	Metric
from	O
considerably	O
increased	O
depth	O
.	O
On	O
the	O
ImageNet	Material
dataset	Material
we	O
evaluate	O
residual	Method
nets	Method
with	O
a	O
depth	O
of	O
up	O
to	O
152	O
layers	O
-	O
8×	O
deeper	O
than	O
VGG	Method
nets	Method
[	O
41	O
]	O
but	O
still	O
having	O
lower	O
complexity	Metric
.	O
An	O
ensemble	O
of	O
these	O
residual	Method
nets	Method
achieves	O
3.57	O
%	O
error	Metric
on	O
the	O
ImageNet	Material
test	Material
set	Material
.	O
This	O
result	O
won	O
the	O
1st	O
place	O
on	O
the	O
ILSVRC	Task
2015	O
classification	Task
task	O
.	O
We	O
also	O
present	O
analysis	O
on	O
CIFAR	Material
-	Material
10	Material
with	O
100	O
and	O
1000	O
layers	O
.	O
The	O
depth	Method
of	Method
representations	Method
is	O
of	O
central	O
importance	O
for	O
many	O
visual	Task
recognition	Task
tasks	Task
.	O
Solely	O
due	O
to	O
our	O
extremely	O
deep	Method
representations	Method
,	O
we	O
obtain	O
a	O
28	O
%	O
relative	O
improvement	O
on	O
the	O
COCO	Material
object	Material
detection	Material
dataset	Material
.	O
Deep	O
residual	Method
nets	Method
are	O
foundations	O
of	O
our	O
submissions	O
to	O
ILSVRC	Task
&	O
COCO	Task
2015	Task
competitions	Task
1	O
,	O
where	O
we	O
also	O
won	O
the	O
1st	O
places	O
on	O
the	O
tasks	O
of	O
ImageNet	Task
detection	Task
,	O
ImageNet	O
localization	Task
,	O
COCO	Task
detection	Task
,	O
and	O
COCO	Task
segmentation	Task
.	O
section	O
:	O
Introduction	O
Deep	Method
convolutional	Method
neural	Method
networks	Method
[	O
reference	O
][	O
reference	O
]	O
have	O
led	O
to	O
a	O
series	O
of	O
breakthroughs	O
for	O
image	Task
classification	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
.	O
Deep	Method
networks	Method
naturally	O
integrate	O
low	O
/	O
mid	O
/	O
highlevel	O
features	O
[	O
reference	O
]	O
and	O
classifiers	Method
in	O
an	O
end	O
-	O
to	O
-	O
end	O
multilayer	O
fashion	O
,	O
and	O
the	O
"	O
levels	O
"	O
of	O
features	O
can	O
be	O
enriched	O
by	O
the	O
number	O
of	O
stacked	O
layers	O
(	O
depth	O
)	O
.	O
Recent	O
evidence	O
[	O
reference	O
][	O
reference	O
]	O
reveals	O
that	O
network	O
depth	O
is	O
of	O
crucial	O
importance	O
,	O
and	O
the	O
leading	O
results	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
on	O
the	O
challenging	O
ImageNet	Material
dataset	Material
[	O
reference	O
]	O
all	O
exploit	O
"	O
very	O
deep	O
"	O
[	O
reference	O
]	O
models	O
,	O
with	O
a	O
depth	O
of	O
sixteen	O
[	O
reference	O
]	O
to	O
thirty	O
[	O
reference	O
]	O
.	O
Many	O
other	O
nontrivial	O
visual	Task
recognition	Task
tasks	Task
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
have	O
also	O
[	O
reference	O
]	O
http:	O
//	O
image	O
-	O
net.org	O
/	O
challenges	O
/	O
LSVRC	O
/	O
2015	O
/	O
and	O
http:	O
//	O
mscoco.org	O
/	O
dataset	O
/	O
#detections	O
-	O
challenge2015	O
.	O
greatly	O
benefited	O
from	O
very	O
deep	Method
models	Method
.	O
Driven	O
by	O
the	O
significance	O
of	O
depth	O
,	O
a	O
question	O
arises	O
:	O
Is	O
learning	O
better	O
networks	O
as	O
easy	O
as	O
stacking	O
more	O
layers	O
?	O
An	O
obstacle	O
to	O
answering	O
this	O
question	O
was	O
the	O
notorious	O
problem	O
of	O
vanishing	O
/	O
exploding	O
gradients	O
[	O
reference	O
][	O
reference	O
]	O
,	O
which	O
hamper	O
convergence	O
from	O
the	O
beginning	O
.	O
This	O
problem	O
,	O
however	O
,	O
has	O
been	O
largely	O
addressed	O
by	O
normalized	Method
initialization	Method
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
and	O
intermediate	O
normalization	O
layers	O
[	O
reference	O
]	O
,	O
which	O
enable	O
networks	O
with	O
tens	O
of	O
layers	O
to	O
start	O
converging	O
for	O
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	O
with	O
backpropagation	Method
[	O
reference	O
]	O
.	O
When	O
deeper	Method
networks	Method
are	O
able	O
to	O
start	O
converging	O
,	O
a	O
degradation	Task
problem	Task
has	O
been	O
exposed	O
:	O
with	O
the	O
network	O
depth	O
increasing	O
,	O
accuracy	Metric
gets	O
saturated	O
(	O
which	O
might	O
be	O
unsurprising	O
)	O
and	O
then	O
degrades	O
rapidly	O
.	O
Unexpectedly	O
,	O
such	O
degradation	O
is	O
not	O
caused	O
by	O
overfitting	O
,	O
and	O
adding	O
more	O
layers	O
to	O
a	O
suitably	O
deep	Method
model	Method
leads	O
to	O
higher	O
training	O
error	Metric
,	O
as	O
reported	O
in	O
[	O
reference	O
][	O
reference	O
]	O
and	O
thoroughly	O
verified	O
by	O
our	O
experiments	O
.	O
Fig	O
.	O
1	O
shows	O
a	O
typical	O
example	O
.	O
The	O
degradation	O
(	O
of	O
training	Metric
accuracy	Metric
)	O
indicates	O
that	O
not	O
all	O
systems	O
are	O
similarly	O
easy	O
to	O
optimize	O
.	O
Let	O
us	O
consider	O
a	O
shallower	Method
architecture	Method
and	O
its	O
deeper	O
counterpart	O
that	O
adds	O
more	O
layers	O
onto	O
it	O
.	O
There	O
exists	O
a	O
solution	O
by	O
construction	O
to	O
the	O
deeper	Method
model	Method
:	O
the	O
added	O
layers	O
are	O
identity	O
mapping	O
,	O
and	O
the	O
other	O
layers	O
are	O
copied	O
from	O
the	O
learned	O
shallower	Method
model	Method
.	O
The	O
existence	O
of	O
this	O
constructed	O
solution	O
indicates	O
that	O
a	O
deeper	Method
model	Method
should	O
produce	O
no	O
higher	O
training	O
error	Metric
than	O
its	O
shallower	O
counterpart	O
.	O
But	O
experiments	O
show	O
that	O
our	O
current	O
solvers	O
on	O
hand	O
are	O
unable	O
to	O
find	O
solutions	O
that	O
are	O
comparably	O
good	O
or	O
better	O
than	O
the	O
constructed	O
solution	O
(	O
or	O
unable	O
to	O
do	O
so	O
in	O
feasible	O
time	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
degradation	Task
problem	Task
by	O
introducing	O
a	O
deep	Method
residual	Method
learning	Method
framework	Method
.	O
Instead	O
of	O
hoping	O
each	O
few	O
stacked	O
layers	O
directly	O
fit	O
a	O
desired	O
underlying	O
mapping	O
,	O
we	O
explicitly	O
let	O
these	O
layers	O
fit	O
a	O
residual	Method
mapping	Method
.	O
Formally	O
,	O
denoting	O
the	O
desired	O
underlying	O
mapping	O
as	O
H	O
(	O
x	O
)	O
,	O
we	O
let	O
the	O
stacked	Method
nonlinear	Method
layers	Method
fit	O
another	O
mapping	O
of	O
F	O
(	O
x	O
)	O
:	O
=	O
H	O
(	O
x	O
)	O
−	O
x.	O
The	O
original	O
mapping	O
is	O
recast	O
into	O
F	O
(	O
x	O
)+	O
x	O
.	O
We	O
hypothesize	O
that	O
it	O
is	O
easier	O
to	O
optimize	O
the	O
residual	Task
mapping	Task
than	O
to	O
optimize	O
the	O
original	Task
,	Task
unreferenced	Task
mapping	Task
.	O
To	O
the	O
extreme	O
,	O
if	O
an	O
identity	Method
mapping	Method
were	O
optimal	O
,	O
it	O
would	O
be	O
easier	O
to	O
push	O
the	O
residual	O
to	O
zero	O
than	O
to	O
fit	O
an	O
identity	Method
mapping	Method
by	O
a	O
stack	Method
of	Method
nonlinear	Method
layers	Method
.	O
The	O
formulation	O
of	O
F	O
(	O
x	O
)	O
+	O
x	O
can	O
be	O
realized	O
by	O
feedforward	Method
neural	Method
networks	Method
with	O
"	O
shortcut	O
connections	O
"	O
(	O
Fig	O
.	O
2	O
)	O
.	O
Shortcut	O
connections	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
are	O
those	O
skipping	O
one	O
or	O
more	O
layers	O
.	O
In	O
our	O
case	O
,	O
the	O
shortcut	O
connections	O
simply	O
perform	O
identity	Method
mapping	Method
,	O
and	O
their	O
outputs	O
are	O
added	O
to	O
the	O
outputs	O
of	O
the	O
stacked	O
layers	O
(	O
Fig	O
.	O
2	O
)	O
.	O
Identity	Method
shortcut	Method
connections	Method
add	O
neither	O
extra	O
parameter	O
nor	O
computational	Metric
complexity	Metric
.	O
The	O
entire	O
network	O
can	O
still	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
by	O
SGD	Method
with	O
backpropagation	Method
,	O
and	O
can	O
be	O
easily	O
implemented	O
using	O
common	O
libraries	O
(	O
e.g.	O
,	O
Caffe	Method
[	O
reference	O
]	O
)	O
without	O
modifying	O
the	O
solvers	O
.	O
We	O
present	O
comprehensive	O
experiments	O
on	O
ImageNet	Material
[	O
reference	O
]	O
to	O
show	O
the	O
degradation	Task
problem	Task
and	O
evaluate	O
our	O
method	O
.	O
We	O
show	O
that	O
:	O
1	O
)	O
Our	O
extremely	O
deep	Method
residual	Method
nets	Method
are	O
easy	O
to	O
optimize	O
,	O
but	O
the	O
counterpart	O
"	O
plain	Method
"	O
nets	O
(	O
that	O
simply	O
stack	O
layers	O
)	O
exhibit	O
higher	O
training	O
error	Metric
when	O
the	O
depth	O
increases	O
;	O
2	O
)	O
Our	O
deep	Method
residual	Method
nets	Method
can	O
easily	O
enjoy	O
accuracy	Metric
gains	O
from	O
greatly	O
increased	O
depth	O
,	O
producing	O
results	O
substantially	O
better	O
than	O
previous	O
networks	O
.	O
Similar	O
phenomena	O
are	O
also	O
shown	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
set	Material
[	O
reference	O
]	O
,	O
suggesting	O
that	O
the	O
optimization	Task
difficulties	Task
and	O
the	O
effects	O
of	O
our	O
method	O
are	O
not	O
just	O
akin	O
to	O
a	O
particular	O
dataset	O
.	O
We	O
present	O
successfully	O
trained	O
models	O
on	O
this	O
dataset	O
with	O
over	O
100	O
layers	O
,	O
and	O
explore	O
models	O
with	O
over	O
1000	O
layers	O
.	O
On	O
the	O
ImageNet	O
classification	Task
dataset	O
[	O
reference	O
]	O
,	O
we	O
obtain	O
excellent	O
results	O
by	O
extremely	O
deep	Method
residual	Method
nets	Method
.	O
Our	O
152	Method
-	Method
layer	Method
residual	Method
net	Method
is	O
the	O
deepest	Method
network	Method
ever	O
presented	O
on	O
ImageNet	Material
,	O
while	O
still	O
having	O
lower	O
complexity	Metric
than	O
VGG	Method
nets	Method
[	O
reference	O
]	O
.	O
Our	O
ensemble	O
has	O
3.57	O
%	O
top	O
-	O
5	O
error	Metric
on	O
the	O
ImageNet	Material
test	Material
set	Material
,	O
and	O
won	O
the	O
1st	O
place	O
in	O
the	O
ILSVRC	Task
2015	O
classification	Task
competition	O
.	O
The	O
extremely	O
deep	Method
representations	Method
also	O
have	O
excellent	O
generalization	Metric
performance	O
on	O
other	O
recognition	Task
tasks	Task
,	O
and	O
lead	O
us	O
to	O
further	O
win	O
the	O
1st	O
places	O
on	O
:	O
ImageNet	Task
detection	Task
,	O
ImageNet	O
localization	Task
,	O
COCO	Task
detection	Task
,	O
and	O
COCO	Task
segmentation	Task
in	O
ILSVRC	Task
&	O
COCO	Task
2015	Task
competitions	Task
.	O
This	O
strong	O
evidence	O
shows	O
that	O
the	O
residual	Method
learning	Method
principle	Method
is	O
generic	O
,	O
and	O
we	O
expect	O
that	O
it	O
is	O
applicable	O
in	O
other	O
vision	Task
and	Task
non	Task
-	Task
vision	Task
problems	Task
.	O
section	O
:	O
Related	O
Work	O
Residual	Method
Representations	Method
.	O
In	O
image	Task
recognition	Task
,	O
VLAD	Method
[	O
reference	O
]	O
is	O
a	O
representation	O
that	O
encodes	O
by	O
the	O
residual	O
vectors	O
with	O
respect	O
to	O
a	O
dictionary	O
,	O
and	O
Fisher	O
Vector	O
[	O
reference	O
]	O
can	O
be	O
formulated	O
as	O
a	O
probabilistic	Method
version	Method
[	O
reference	O
]	O
of	O
VLAD	Method
.	O
Both	O
of	O
them	O
are	O
powerful	O
shallow	Method
representations	Method
for	O
image	Task
retrieval	Task
and	O
classification	Task
[	O
reference	O
][	O
reference	O
]	O
.	O
For	O
vector	Task
quantization	Task
,	O
encoding	Task
residual	Task
vectors	Task
[	O
reference	O
]	O
is	O
shown	O
to	O
be	O
more	O
effective	O
than	O
encoding	O
original	O
vectors	O
.	O
In	O
low	Task
-	Task
level	Task
vision	Task
and	Task
computer	Task
graphics	Task
,	O
for	O
solving	O
Partial	Task
Differential	Task
Equations	Task
(	O
PDEs	Task
)	O
,	O
the	O
widely	O
used	O
Multigrid	Method
method	Method
[	O
reference	O
]	O
reformulates	O
the	O
system	O
as	O
subproblems	O
at	O
multiple	O
scales	O
,	O
where	O
each	O
subproblem	O
is	O
responsible	O
for	O
the	O
residual	Task
solution	Task
between	O
a	O
coarser	O
and	O
a	O
finer	O
scale	O
.	O
An	O
alternative	O
to	O
Multigrid	Method
is	O
hierarchical	Method
basis	Method
preconditioning	Method
[	O
reference	O
][	O
reference	O
]	O
,	O
which	O
relies	O
on	O
variables	O
that	O
represent	O
residual	O
vectors	O
between	O
two	O
scales	O
.	O
It	O
has	O
been	O
shown	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
that	O
these	O
solvers	O
converge	O
much	O
faster	O
than	O
standard	O
solvers	O
that	O
are	O
unaware	O
of	O
the	O
residual	O
nature	O
of	O
the	O
solutions	O
.	O
These	O
methods	O
suggest	O
that	O
a	O
good	O
reformulation	O
or	O
preconditioning	Method
can	O
simplify	O
the	O
optimization	Task
.	O
Shortcut	O
Connections	O
.	O
Practices	O
and	O
theories	O
that	O
lead	O
to	O
shortcut	O
connections	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
have	O
been	O
studied	O
for	O
a	O
long	O
time	O
.	O
An	O
early	O
practice	O
of	O
training	O
multi	Method
-	Method
layer	Method
perceptrons	Method
(	Method
MLPs	Method
)	O
is	O
to	O
add	O
a	O
linear	O
layer	O
connected	O
from	O
the	O
network	O
input	O
to	O
the	O
output	O
[	O
reference	O
][	O
reference	O
]	O
.	O
In	O
[	O
reference	O
][	O
reference	O
]	O
,	O
a	O
few	O
intermediate	O
layers	O
are	O
directly	O
connected	O
to	O
auxiliary	Method
classifiers	Method
for	O
addressing	O
vanishing	Task
/	Task
exploding	Task
gradients	Task
.	O
The	O
papers	O
of	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
propose	O
methods	O
for	O
centering	O
layer	O
responses	O
,	O
gradients	O
,	O
and	O
propagated	O
errors	O
,	O
implemented	O
by	O
shortcut	Method
connections	Method
.	O
In	O
[	O
reference	O
]	O
,	O
an	O
"	O
inception	O
"	O
layer	O
is	O
composed	O
of	O
a	O
shortcut	O
branch	O
and	O
a	O
few	O
deeper	O
branches	O
.	O
Concurrent	O
with	O
our	O
work	O
,	O
"	O
highway	Method
networks	Method
"	O
[	O
reference	O
][	O
reference	O
]	O
present	O
shortcut	O
connections	O
with	O
gating	O
functions	O
[	O
reference	O
]	O
.	O
These	O
gates	O
are	O
data	O
-	O
dependent	O
and	O
have	O
parameters	O
,	O
in	O
contrast	O
to	O
our	O
identity	Method
shortcuts	Method
that	O
are	O
parameter	O
-	O
free	O
.	O
When	O
a	O
gated	O
shortcut	O
is	O
"	O
closed	O
"	O
(	O
approaching	O
zero	O
)	O
,	O
the	O
layers	O
in	O
highway	Method
networks	Method
represent	O
non	O
-	O
residual	O
functions	O
.	O
On	O
the	O
contrary	O
,	O
our	O
formulation	O
always	O
learns	O
residual	O
functions	O
;	O
our	O
identity	Method
shortcuts	Method
are	O
never	O
closed	O
,	O
and	O
all	O
information	O
is	O
always	O
passed	O
through	O
,	O
with	O
additional	O
residual	O
functions	O
to	O
be	O
learned	O
.	O
In	O
addition	O
,	O
high	Method
-	Method
way	Method
networks	Method
have	O
not	O
demonstrated	O
accuracy	Metric
gains	O
with	O
extremely	O
increased	O
depth	O
(	O
e.g.	O
,	O
over	O
100	O
layers	O
)	O
.	O
section	O
:	O
Deep	Method
Residual	Method
Learning	Method
section	O
:	O
Residual	Method
Learning	Method
Let	O
us	O
consider	O
H	O
(	O
x	O
)	O
as	O
an	O
underlying	O
mapping	O
to	O
be	O
fit	O
by	O
a	O
few	O
stacked	Method
layers	Method
(	O
not	O
necessarily	O
the	O
entire	O
net	O
)	O
,	O
with	O
x	O
denoting	O
the	O
inputs	O
to	O
the	O
first	O
of	O
these	O
layers	O
.	O
If	O
one	O
hypothesizes	O
that	O
multiple	O
nonlinear	O
layers	O
can	O
asymptotically	O
approximate	O
complicated	O
functions	O
[	O
reference	O
]	O
,	O
then	O
it	O
is	O
equivalent	O
to	O
hypothesize	O
that	O
they	O
can	O
asymptotically	O
approximate	O
the	O
residual	O
functions	O
,	O
i.e.	O
,	O
H	O
(	O
x	O
)	O
−	O
x	O
(	O
assuming	O
that	O
the	O
input	O
and	O
output	O
are	O
of	O
the	O
same	O
dimensions	O
)	O
.	O
So	O
rather	O
than	O
expect	O
stacked	O
layers	O
to	O
approximate	O
H	O
(	O
x	O
)	O
,	O
we	O
explicitly	O
let	O
these	O
layers	O
approximate	O
a	O
residual	O
function	O
F	O
(	O
x	O
)	O
:	O
=	O
H	O
(	O
x	O
)	O
−	O
x.	O
The	O
original	O
function	O
thus	O
becomes	O
F	O
(	O
x	O
)+	O
x	O
.	O
Although	O
both	O
forms	O
should	O
be	O
able	O
to	O
asymptotically	O
approximate	O
the	O
desired	O
functions	O
(	O
as	O
hypothesized	O
)	O
,	O
the	O
ease	O
of	O
learning	Task
might	O
be	O
different	O
.	O
This	O
reformulation	O
is	O
motivated	O
by	O
the	O
counterintuitive	O
phenomena	O
about	O
the	O
degradation	Task
problem	Task
(	O
Fig	O
.	O
1	O
,	O
left	O
)	O
.	O
As	O
we	O
discussed	O
in	O
the	O
introduction	O
,	O
if	O
the	O
added	O
layers	O
can	O
be	O
constructed	O
as	O
identity	Method
mappings	Method
,	O
a	O
deeper	Method
model	Method
should	O
have	O
training	O
error	Metric
no	O
greater	O
than	O
its	O
shallower	Method
counterpart	Method
.	O
The	O
degradation	Task
problem	Task
suggests	O
that	O
the	O
solvers	O
might	O
have	O
difficulties	O
in	O
approximating	Task
identity	Task
mappings	Task
by	O
multiple	O
nonlinear	O
layers	O
.	O
With	O
the	O
residual	Method
learning	Method
reformulation	Method
,	O
if	O
identity	O
mappings	O
are	O
optimal	O
,	O
the	O
solvers	O
may	O
simply	O
drive	O
the	O
weights	O
of	O
the	O
multiple	O
nonlinear	O
layers	O
toward	O
zero	O
to	O
approach	O
identity	O
mappings	O
.	O
In	O
real	O
cases	O
,	O
it	O
is	O
unlikely	O
that	O
identity	O
mappings	O
are	O
optimal	O
,	O
but	O
our	O
reformulation	O
may	O
help	O
to	O
precondition	O
the	O
problem	O
.	O
If	O
the	O
optimal	O
function	O
is	O
closer	O
to	O
an	O
identity	O
mapping	O
than	O
to	O
a	O
zero	O
mapping	O
,	O
it	O
should	O
be	O
easier	O
for	O
the	O
solver	O
to	O
find	O
the	O
perturbations	O
with	O
reference	O
to	O
an	O
identity	O
mapping	O
,	O
than	O
to	O
learn	O
the	O
function	O
as	O
a	O
new	O
one	O
.	O
We	O
show	O
by	O
experiments	O
(	O
Fig	O
.	O
7	O
)	O
that	O
the	O
learned	O
residual	O
functions	O
in	O
general	O
have	O
small	O
responses	O
,	O
suggesting	O
that	O
identity	Method
mappings	Method
provide	O
reasonable	O
preconditioning	O
.	O
section	O
:	O
Identity	Task
Mapping	Task
by	O
Shortcuts	O
We	O
adopt	O
residual	Method
learning	Method
to	O
every	O
few	O
stacked	O
layers	O
.	O
A	O
building	O
block	O
is	O
shown	O
in	O
Fig	O
.	O
2	O
.	O
Formally	O
,	O
in	O
this	O
paper	O
we	O
consider	O
a	O
building	Method
block	Method
defined	O
as	O
:	O
Here	O
x	O
and	O
y	O
are	O
the	O
input	O
and	O
output	O
vectors	O
of	O
the	O
layers	O
considered	O
.	O
The	O
function	O
F	O
(	O
x	O
,	O
{	O
W	O
i	O
}	O
)	O
represents	O
the	O
residual	O
mapping	O
to	O
be	O
learned	O
.	O
For	O
the	O
example	O
in	O
Fig	O
.	O
2	O
that	O
has	O
two	O
layers	O
,	O
F	O
=	O
W	O
2	O
σ	O
(	O
W	O
1	O
x	O
)	O
in	O
which	O
σ	O
denotes	O
ReLU	O
[	O
reference	O
]	O
and	O
the	O
biases	O
are	O
omitted	O
for	O
simplifying	O
notations	O
.	O
The	O
operation	O
F	O
+	O
x	O
is	O
performed	O
by	O
a	O
shortcut	Method
connection	Method
and	O
element	Method
-	Method
wise	Method
addition	Method
.	O
We	O
adopt	O
the	O
second	O
nonlinearity	O
after	O
the	O
addition	O
(	O
i.e.	O
,	O
σ	O
(	O
y	O
)	O
,	O
see	O
Fig	O
.	O
2	O
)	O
.	O
The	O
shortcut	O
connections	O
in	O
Eqn	O
.	O
(	O
1	O
)	O
introduce	O
neither	O
extra	O
parameter	O
nor	O
computation	Metric
complexity	Metric
.	O
This	O
is	O
not	O
only	O
attractive	O
in	O
practice	O
but	O
also	O
important	O
in	O
our	O
comparisons	O
between	O
plain	Method
and	O
residual	Method
networks	Method
.	O
We	O
can	O
fairly	O
compare	O
plain	Method
/	O
residual	Method
networks	Method
that	O
simultaneously	O
have	O
the	O
same	O
number	O
of	O
parameters	O
,	O
depth	O
,	O
width	O
,	O
and	O
computational	Metric
cost	Metric
(	O
except	O
for	O
the	O
negligible	O
element	O
-	O
wise	O
addition	O
)	O
.	O
The	O
dimensions	O
of	O
x	O
and	O
F	O
must	O
be	O
equal	O
in	O
Eqn	O
.	O
[	O
reference	O
]	O
.	O
If	O
this	O
is	O
not	O
the	O
case	O
(	O
e.g.	O
,	O
when	O
changing	O
the	O
input	O
/	O
output	O
channels	O
)	O
,	O
we	O
can	O
perform	O
a	O
linear	Method
projection	Method
W	Method
s	Method
by	O
the	O
shortcut	O
connections	O
to	O
match	O
the	O
dimensions	O
:	O
We	O
can	O
also	O
use	O
a	O
square	O
matrix	O
W	O
s	O
in	O
Eqn	O
.	O
[	O
reference	O
]	O
.	O
But	O
we	O
will	O
show	O
by	O
experiments	O
that	O
the	O
identity	Method
mapping	Method
is	O
sufficient	O
for	O
addressing	O
the	O
degradation	Task
problem	Task
and	O
is	O
economical	O
,	O
and	O
thus	O
W	Method
s	Method
is	O
only	O
used	O
when	O
matching	O
dimensions	O
.	O
The	O
form	O
of	O
the	O
residual	O
function	O
F	O
is	O
flexible	O
.	O
Experiments	O
in	O
this	O
paper	O
involve	O
a	O
function	O
F	O
that	O
has	O
two	O
or	O
three	O
layers	O
(	O
Fig	O
.	O
5	O
)	O
,	O
while	O
more	O
layers	O
are	O
possible	O
.	O
But	O
if	O
F	O
has	O
only	O
a	O
single	O
layer	O
,	O
Eqn	O
.	O
(	O
1	O
)	O
is	O
similar	O
to	O
a	O
linear	Method
layer	Method
:	O
y	O
=	O
W	O
1	O
x	O
+	O
x	O
,	O
for	O
which	O
we	O
have	O
not	O
observed	O
advantages	O
.	O
We	O
also	O
note	O
that	O
although	O
the	O
above	O
notations	O
are	O
about	O
fully	O
-	O
connected	O
layers	O
for	O
simplicity	O
,	O
they	O
are	O
applicable	O
to	O
convolutional	Method
layers	Method
.	O
The	O
function	O
F	O
(	O
x	O
,	O
{	O
W	O
i	O
}	O
)	O
can	O
represent	O
multiple	O
convolutional	Method
layers	Method
.	O
The	O
element	Method
-	Method
wise	Method
addition	Method
is	O
performed	O
on	O
two	O
feature	O
maps	O
,	O
channel	O
by	O
channel	O
.	O
section	O
:	O
Network	Method
Architectures	Method
We	O
have	O
tested	O
various	O
plain	Method
/	O
residual	Method
nets	Method
,	O
and	O
have	O
observed	O
consistent	O
phenomena	O
.	O
To	O
provide	O
instances	O
for	O
discussion	O
,	O
we	O
describe	O
two	O
models	O
for	O
ImageNet	Task
as	O
follows	O
.	O
Plain	Method
Network	Method
.	O
Our	O
plain	Method
baselines	O
(	O
Fig	O
.	O
3	O
,	O
middle	O
)	O
are	O
mainly	O
inspired	O
by	O
the	O
philosophy	O
of	O
VGG	Method
nets	Method
[	O
reference	O
]	O
(	O
Fig	O
.	O
3	O
,	O
left	O
)	O
.	O
The	O
convolutional	Method
layers	Method
mostly	O
have	O
3×3	O
filters	O
and	O
follow	O
two	O
simple	O
design	O
rules	O
:	O
(	O
i	O
)	O
for	O
the	O
same	O
output	O
feature	O
map	O
size	O
,	O
the	O
layers	O
have	O
the	O
same	O
number	O
of	O
filters	O
;	O
and	O
(	O
ii	O
)	O
if	O
the	O
feature	O
map	O
size	O
is	O
halved	O
,	O
the	O
number	O
of	O
filters	O
is	O
doubled	O
so	O
as	O
to	O
preserve	O
the	O
time	Metric
complexity	Metric
per	O
layer	O
.	O
We	O
perform	O
downsampling	Task
directly	O
by	O
convolutional	Method
layers	Method
that	O
have	O
a	O
stride	O
of	O
2	O
.	O
The	O
network	O
ends	O
with	O
a	O
global	Method
average	Method
pooling	Method
layer	Method
and	O
a	O
1000	Method
-	Method
way	Method
fully	Method
-	Method
connected	Method
layer	Method
with	O
softmax	Method
.	O
The	O
total	O
number	O
of	O
weighted	O
layers	O
is	O
34	O
in	O
Fig	O
.	O
3	O
section	O
:	O
(	O
middle	O
)	O
.	O
It	O
is	O
worth	O
noticing	O
that	O
our	O
model	O
has	O
fewer	O
filters	O
and	O
lower	O
complexity	Metric
than	O
VGG	Method
nets	Method
[	O
reference	O
]	O
(	O
Fig	O
.	O
3	O
,	O
left	O
)	O
.	O
Our	O
34	O
-	O
layer	O
baseline	O
has	O
3.6	O
billion	O
FLOPs	O
(	O
multiply	O
-	O
adds	O
)	O
,	O
which	O
is	O
only	O
18	O
%	O
of	O
VGG	Method
-	Method
19	Method
(	O
19.6	O
billion	O
FLOPs	O
)	O
.	O
Residual	Method
Network	Method
.	O
Based	O
on	O
the	O
above	O
plain	Method
network	O
,	O
we	O
insert	O
shortcut	O
connections	O
(	O
Fig	O
.	O
3	O
,	O
right	O
)	O
which	O
turn	O
the	O
network	O
into	O
its	O
counterpart	O
residual	O
version	O
.	O
The	O
identity	O
shortcuts	O
(	O
Eqn	O
.	O
(	O
1	O
)	O
)	O
can	O
be	O
directly	O
used	O
when	O
the	O
input	O
and	O
output	O
are	O
of	O
the	O
same	O
dimensions	O
(	O
solid	O
line	O
shortcuts	O
in	O
Fig	O
.	O
3	O
)	O
.	O
When	O
the	O
dimensions	O
increase	O
(	O
dotted	O
line	O
shortcuts	O
in	O
Fig	O
.	O
3	O
)	O
,	O
we	O
consider	O
two	O
options	O
:	O
(	O
A	O
)	O
The	O
shortcut	Method
still	O
performs	O
identity	Task
mapping	Task
,	O
with	O
extra	O
zero	O
entries	O
padded	O
for	O
increasing	O
dimensions	O
.	O
This	O
option	O
introduces	O
no	O
extra	O
parameter	O
;	O
(	O
B	O
)	O
The	O
projection	O
shortcut	O
in	O
Eqn	O
.	O
(	O
2	O
)	O
is	O
used	O
to	O
match	O
dimensions	O
(	O
done	O
by	O
1×1	Method
convolutions	Method
)	O
.	O
For	O
both	O
options	O
,	O
when	O
the	O
shortcuts	O
go	O
across	O
feature	O
maps	O
of	O
two	O
sizes	O
,	O
they	O
are	O
performed	O
with	O
a	O
stride	O
of	O
2	O
.	O
section	O
:	O
Implementation	O
Our	O
implementation	O
for	O
ImageNet	Task
follows	O
the	O
practice	O
in	O
[	O
reference	O
][	O
reference	O
]	O
.	O
The	O
image	O
is	O
resized	O
with	O
its	O
shorter	O
side	O
randomly	O
sampled	O
in	O
[	O
256	O
,	O
480	O
]	O
for	O
scale	Task
augmentation	Task
[	O
reference	O
]	O
.	O
A	O
224×224	O
crop	O
is	O
randomly	O
sampled	O
from	O
an	O
image	O
or	O
its	O
horizontal	O
flip	O
,	O
with	O
the	O
per	O
-	O
pixel	O
mean	O
subtracted	O
[	O
reference	O
]	O
.	O
The	O
standard	O
color	Method
augmentation	Method
in	O
[	O
reference	O
]	O
is	O
used	O
.	O
We	O
adopt	O
batch	Method
normalization	Method
(	O
BN	Method
)	O
[	O
reference	O
]	O
right	O
after	O
each	O
convolution	O
and	O
before	O
activation	O
,	O
following	O
[	O
reference	O
]	O
.	O
We	O
initialize	O
the	O
weights	O
as	O
in	O
[	O
reference	O
]	O
and	O
train	O
all	O
plain	Method
/	O
residual	Method
nets	Method
from	O
scratch	O
.	O
We	O
use	O
SGD	Method
with	O
a	O
mini	O
-	O
batch	O
size	O
of	O
256	O
.	O
The	O
learning	Metric
rate	Metric
starts	O
from	O
0.1	O
and	O
is	O
divided	O
by	O
10	O
when	O
the	O
error	Metric
plateaus	O
,	O
and	O
the	O
models	O
are	O
trained	O
for	O
up	O
to	O
60	O
×	O
10	O
4	O
iterations	O
.	O
We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
a	O
momentum	O
of	O
0.9	O
.	O
We	O
do	O
not	O
use	O
dropout	Method
[	O
reference	O
]	O
,	O
following	O
the	O
practice	O
in	O
[	O
reference	O
]	O
.	O
In	O
testing	O
,	O
for	O
comparison	O
studies	O
we	O
adopt	O
the	O
standard	O
10	Method
-	Method
crop	Method
testing	Method
[	O
reference	O
]	O
.	O
For	O
best	O
results	O
,	O
we	O
adopt	O
the	O
fullyconvolutional	Method
form	Method
as	O
in	O
[	O
reference	O
][	O
reference	O
]	O
,	O
and	O
average	O
the	O
scores	O
at	O
multiple	O
scales	O
(	O
images	O
are	O
resized	O
such	O
that	O
the	O
shorter	O
side	O
is	O
in	O
{	O
224	O
,	O
256	O
,	O
384	O
,	O
480	O
,	O
640	O
}	O
)	O
.	O
section	O
:	O
Experiments	O
4.1	O
.	O
ImageNet	Task
Classification	Task
We	O
evaluate	O
our	O
method	O
on	O
the	O
ImageNet	O
2012	O
classification	Task
dataset	O
[	O
reference	O
]	O
that	O
consists	O
of	O
1000	O
classes	O
.	O
The	O
models	O
are	O
trained	O
on	O
the	O
1.28	O
million	O
training	O
images	O
,	O
and	O
evaluated	O
on	O
the	O
50k	O
validation	O
images	O
.	O
We	O
also	O
obtain	O
a	O
final	O
result	O
on	O
the	O
100k	O
test	O
images	O
,	O
reported	O
by	O
the	O
test	O
server	O
.	O
We	O
evaluate	O
both	O
top	Metric
-	Metric
1	Metric
and	O
top	Metric
-	Metric
5	Metric
error	Metric
rates	Metric
.	O
Plain	Method
Networks	Method
.	O
We	O
first	O
evaluate	O
18	O
-	O
layer	O
and	O
34	O
-	O
layer	O
plain	Method
nets	O
.	O
The	O
34	O
-	O
layer	O
plain	Method
net	O
is	O
in	O
Fig	O
.	O
3	O
(	O
middle	O
)	O
.	O
The	O
18	O
-	O
layer	O
plain	Method
net	O
is	O
of	O
a	O
similar	O
form	O
.	O
See	O
Table	O
1	O
for	O
detailed	O
architectures	O
.	O
The	O
results	O
in	O
Table	O
2	O
show	O
that	O
the	O
deeper	O
34	O
-	O
layer	O
plain	Method
net	O
has	O
higher	O
validation	O
error	Metric
than	O
the	O
shallower	O
18	O
-	O
layer	O
plain	Method
net	O
.	O
To	O
reveal	O
the	O
reasons	O
,	O
in	O
Fig	O
.	O
4	O
Table	O
2	O
.	O
Top	O
-	O
1	O
error	Metric
(	O
%	O
,	O
10	Method
-	Method
crop	Method
testing	Method
)	O
on	O
ImageNet	Task
validation	Task
.	O
Here	O
the	O
ResNets	Method
have	O
no	O
extra	O
parameter	O
compared	O
to	O
their	O
plain	Method
counterparts	O
.	O
Fig	O
.	O
4	O
shows	O
the	O
training	O
procedures	O
.	O
34	O
-	O
layer	O
plain	Method
net	O
has	O
higher	O
training	O
error	Metric
throughout	O
the	O
whole	O
training	O
procedure	O
,	O
even	O
though	O
the	O
solution	O
space	O
of	O
the	O
18	O
-	O
layer	O
plain	Method
network	O
is	O
a	O
subspace	O
of	O
that	O
of	O
the	O
34	O
-	O
layer	O
one	O
.	O
We	O
argue	O
that	O
this	O
optimization	Task
difficulty	Task
is	O
unlikely	O
to	O
be	O
caused	O
by	O
vanishing	O
gradients	O
.	O
These	O
plain	Method
networks	O
are	O
trained	O
with	O
BN	Method
[	O
reference	O
]	O
,	O
which	O
ensures	O
forward	O
propagated	O
signals	O
to	O
have	O
non	O
-	O
zero	O
variances	O
.	O
We	O
also	O
verify	O
that	O
the	O
backward	O
propagated	O
gradients	O
exhibit	O
healthy	O
norms	O
with	O
BN	Method
.	O
So	O
neither	O
forward	O
nor	O
backward	O
signals	O
vanish	O
.	O
In	O
fact	O
,	O
the	O
34	O
-	O
layer	O
plain	Method
net	O
is	O
still	O
able	O
to	O
achieve	O
competitive	O
accuracy	Metric
(	O
Table	O
3	O
)	O
,	O
suggesting	O
that	O
the	O
solver	O
works	O
to	O
some	O
extent	O
.	O
We	O
conjecture	O
that	O
the	O
deep	O
plain	Method
nets	O
may	O
have	O
exponentially	O
low	Metric
convergence	Metric
rates	Metric
,	O
which	O
impact	O
the	O
reducing	O
of	O
the	O
training	O
error	Metric
[	O
reference	O
]	O
.	O
The	O
reason	O
for	O
such	O
optimization	Task
difficulties	Task
will	O
be	O
studied	O
in	O
the	O
future	O
.	O
Residual	Method
Networks	Method
.	O
Next	O
we	O
evaluate	O
18	O
-	O
layer	O
and	O
34	O
-	O
layer	O
residual	Method
nets	Method
(	O
ResNets	Method
)	O
.	O
The	O
baseline	O
architectures	O
are	O
the	O
same	O
as	O
the	O
above	O
plain	Method
nets	O
,	O
expect	O
that	O
a	O
shortcut	O
connection	O
is	O
added	O
to	O
each	O
pair	O
of	O
3×3	O
filters	O
as	O
in	O
Fig	O
.	O
3	O
(	O
right	O
)	O
.	O
In	O
the	O
first	O
comparison	O
(	O
Table	O
2	O
and	O
Fig	O
.	O
4	O
right	O
)	O
,	O
we	O
use	O
identity	Method
mapping	Method
for	O
all	O
shortcuts	O
and	O
zero	Method
-	Method
padding	Method
for	O
increasing	O
dimensions	O
(	O
option	O
A	O
)	O
.	O
So	O
they	O
have	O
no	O
extra	O
parameter	O
compared	O
to	O
the	O
plain	Method
counterparts	O
.	O
We	O
have	O
three	O
major	O
observations	O
from	O
Table	O
2	O
and	O
Fig	O
.	O
4	O
.	O
First	O
,	O
the	O
situation	O
is	O
reversed	O
with	O
residual	Method
learning	Method
-	O
the	O
34	O
-	O
layer	O
ResNet	Method
is	O
better	O
than	O
the	O
18	O
-	O
layer	O
ResNet	Method
(	O
by	O
2.8	O
%	O
)	O
.	O
More	O
importantly	O
,	O
the	O
34	O
-	O
layer	O
ResNet	Method
exhibits	O
considerably	O
lower	O
training	O
error	Metric
and	O
is	O
generalizable	O
to	O
the	O
validation	O
data	O
.	O
This	O
indicates	O
that	O
the	O
degradation	Task
problem	Task
is	O
well	O
addressed	O
in	O
this	O
setting	O
and	O
we	O
manage	O
to	O
obtain	O
accuracy	Metric
gains	O
from	O
increased	O
depth	O
.	O
Second	O
,	O
compared	O
to	O
its	O
plain	Method
counterpart	O
,	O
the	O
34	O
-	O
layer	O
6.8	O
PReLU	Method
-	Method
net	Method
[	O
reference	O
]	O
4.94	O
BN	Method
-	O
inception	O
[	O
reference	O
]	O
4.82	O
ResNet	Method
(	O
ILSVRC	Task
[	O
reference	O
]	O
3.57	O
Table	O
5	O
.	O
Error	Metric
rates	Metric
(	O
%	O
)	O
of	O
ensembles	O
.	O
The	O
top	O
-	O
5	O
error	Metric
is	O
on	O
the	O
test	O
set	O
of	O
ImageNet	Material
and	O
reported	O
by	O
the	O
test	O
server	O
.	O
ResNet	Method
reduces	O
the	O
top	Metric
-	Metric
1	Metric
error	Metric
by	O
3.5	O
%	O
(	O
Table	O
2	O
)	O
,	O
resulting	O
from	O
the	O
successfully	O
reduced	O
training	O
error	Metric
(	O
Fig	O
.	O
4	O
right	O
vs.	O
left	O
)	O
.	O
This	O
comparison	O
verifies	O
the	O
effectiveness	O
of	O
residual	Method
learning	Method
on	O
extremely	O
deep	Task
systems	Task
.	O
Last	O
,	O
we	O
also	O
note	O
that	O
the	O
18	O
-	O
layer	O
plain	Method
/	O
residual	Method
nets	Method
are	O
comparably	O
accurate	O
(	O
Table	O
2	O
)	O
,	O
but	O
the	O
18	O
-	O
layer	O
ResNet	Method
converges	O
faster	O
(	O
Fig	O
.	O
4	O
right	O
vs.	O
left	O
)	O
.	O
When	O
the	O
net	O
is	O
"	O
not	O
overly	O
deep	O
"	O
(	O
18	O
layers	O
here	O
)	O
,	O
the	O
current	O
SGD	Method
solver	Method
is	O
still	O
able	O
to	O
find	O
good	O
solutions	O
to	O
the	O
plain	Method
net	O
.	O
In	O
this	O
case	O
,	O
the	O
ResNet	Method
eases	O
the	O
optimization	Task
by	O
providing	O
faster	O
convergence	O
at	O
the	O
early	O
stage	O
.	O
Identity	Method
vs.	O
Projection	O
Shortcuts	O
.	O
We	O
have	O
shown	O
that	O
parameter	O
-	O
free	O
,	O
identity	Method
shortcuts	Method
help	O
with	O
training	Task
.	O
Next	O
we	O
investigate	O
projection	O
shortcuts	O
(	O
Eqn	O
.	O
(	O
2	O
)	O
)	O
.	O
In	O
Table	O
3	O
we	O
compare	O
three	O
options	O
:	O
(	O
A	O
)	O
zero	O
-	O
padding	O
shortcuts	O
are	O
used	O
for	O
increasing	O
dimensions	O
,	O
and	O
all	O
shortcuts	O
are	O
parameterfree	O
(	O
the	O
same	O
as	O
Table	O
2	O
and	O
Fig	O
.	O
4	O
right	O
)	O
;	O
(	O
B	O
)	O
projection	O
shortcuts	O
are	O
used	O
for	O
increasing	O
dimensions	O
,	O
and	O
other	O
shortcuts	O
are	O
identity	O
;	O
and	O
(	O
C	O
)	O
all	O
shortcuts	O
are	O
projections	O
.	O
Table	O
3	O
shows	O
that	O
all	O
three	O
options	O
are	O
considerably	O
better	O
than	O
the	O
plain	Method
counterpart	O
.	O
B	O
is	O
slightly	O
better	O
than	O
A.	O
We	O
argue	O
that	O
this	O
is	O
because	O
the	O
zero	O
-	O
padded	O
dimensions	O
in	O
A	O
indeed	O
have	O
no	O
residual	Method
learning	Method
.	O
C	O
is	O
marginally	O
better	O
than	O
B	O
,	O
and	O
we	O
attribute	O
this	O
to	O
the	O
extra	O
parameters	O
introduced	O
by	O
many	O
(	O
thirteen	O
)	O
projection	O
shortcuts	O
.	O
But	O
the	O
small	O
differences	O
among	O
A	O
/	O
B	O
/	O
C	O
indicate	O
that	O
projection	O
shortcuts	O
are	O
not	O
essential	O
for	O
addressing	O
the	O
degradation	Task
problem	Task
.	O
So	O
we	O
do	O
not	O
use	O
option	O
C	O
in	O
the	O
rest	O
of	O
this	O
paper	O
,	O
to	O
reduce	O
memory	Metric
/	Metric
time	Metric
complexity	Metric
and	O
model	O
sizes	O
.	O
Identity	Method
shortcuts	Method
are	O
particularly	O
important	O
for	O
not	O
increasing	O
the	O
complexity	Metric
of	O
the	O
bottleneck	Method
architectures	Method
that	O
are	O
introduced	O
below	O
.	O
Deeper	Method
Bottleneck	Method
Architectures	Method
.	O
Next	O
we	O
describe	O
our	O
deeper	Method
nets	Method
for	O
ImageNet	Task
.	O
Because	O
of	O
concerns	O
on	O
the	O
training	Metric
time	Metric
that	O
we	O
can	O
afford	O
,	O
we	O
modify	O
the	O
building	Method
block	Method
as	O
a	O
bottleneck	Method
design	Method
[	O
reference	O
]	O
.	O
For	O
each	O
residual	O
function	O
F	O
,	O
we	O
use	O
a	O
stack	O
of	O
3	O
layers	O
instead	O
of	O
2	O
(	O
Fig	O
.	O
5	O
)	O
.	O
The	O
three	O
layers	O
are	O
1×1	O
,	O
3×3	O
,	O
and	O
1×1	Method
convolutions	Method
,	O
where	O
the	O
1×1	Method
layers	Method
are	O
responsible	O
for	O
reducing	O
and	O
then	O
increasing	O
(	O
restoring	O
)	O
dimensions	O
,	O
leaving	O
the	O
3×3	O
layer	O
a	O
bottleneck	O
with	O
smaller	O
input	O
/	O
output	O
dimensions	O
.	O
Fig	O
.	O
5	O
shows	O
an	O
example	O
,	O
where	O
both	O
designs	O
have	O
similar	O
time	Metric
complexity	Metric
.	O
The	O
parameter	Method
-	Method
free	Method
identity	Method
shortcuts	Method
are	O
particularly	O
important	O
for	O
the	O
bottleneck	Method
architectures	Method
.	O
If	O
the	O
identity	O
shortcut	O
in	O
Fig	O
.	O
5	O
(	O
right	O
)	O
is	O
replaced	O
with	O
projection	O
,	O
one	O
can	O
show	O
that	O
the	O
time	Metric
complexity	Metric
and	O
model	Metric
size	Metric
are	O
doubled	O
,	O
as	O
the	O
shortcut	O
is	O
connected	O
to	O
the	O
two	O
high	O
-	O
dimensional	O
ends	O
.	O
So	O
identity	O
shortcuts	O
lead	O
to	O
more	O
efficient	O
models	O
for	O
the	O
bottleneck	Method
designs	Method
.	O
50	O
-	O
layer	O
ResNet	Method
:	O
We	O
replace	O
each	O
2	O
-	O
layer	O
block	O
in	O
the	O
34	O
-	O
layer	O
net	O
with	O
this	O
3	O
-	O
layer	O
bottleneck	O
block	O
,	O
resulting	O
in	O
a	O
50	O
-	O
layer	O
ResNet	Method
(	O
(	O
Table	O
3	O
and	O
4	O
)	O
.	O
We	O
do	O
not	O
observe	O
the	O
degradation	Task
problem	Task
and	O
thus	O
enjoy	O
significant	O
accuracy	Metric
gains	O
from	O
considerably	O
increased	O
depth	O
.	O
The	O
benefits	O
of	O
depth	O
are	O
witnessed	O
for	O
all	O
evaluation	Metric
metrics	Metric
(	O
Table	O
3	O
section	O
:	O
and	O
4	O
)	O
.	O
Comparisons	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
Methods	O
.	O
In	O
Table	O
4	O
we	O
compare	O
with	O
the	O
previous	O
best	O
single	O
-	O
model	O
results	O
.	O
Our	O
baseline	O
34	O
-	O
layer	O
ResNets	Method
have	O
achieved	O
very	O
competitive	O
accuracy	Metric
.	O
Our	O
152	O
-	O
layer	O
ResNet	Method
has	O
a	O
single	Method
-	Method
model	Method
top	O
-	O
5	O
validation	O
error	Metric
of	O
4.49	O
%	O
.	O
This	O
single	O
-	O
model	O
result	O
outperforms	O
all	O
previous	O
ensemble	O
results	O
(	O
Table	O
5	O
)	O
.	O
We	O
combine	O
six	O
models	O
of	O
different	O
depth	O
to	O
form	O
an	O
ensemble	O
(	O
only	O
with	O
two	O
152	O
-	O
layer	O
ones	O
at	O
the	O
time	O
of	O
submitting	O
)	O
.	O
This	O
leads	O
to	O
3.57	O
%	O
top	O
-	O
5	O
error	Metric
on	O
the	O
test	O
set	O
(	O
Table	O
5	O
)	O
.	O
This	O
entry	O
won	O
the	O
1st	O
place	O
in	O
ILSVRC	Task
2015	O
.	O
section	O
:	O
CIFAR	Task
-	Task
10	Task
and	O
Analysis	O
We	O
conducted	O
more	O
studies	O
on	O
the	O
CIFAR	Material
-	Material
10	Material
dataset	Material
[	O
reference	O
]	O
,	O
which	O
consists	O
of	O
50k	O
training	O
images	O
and	O
10k	O
testing	O
images	O
in	O
10	O
classes	O
.	O
We	O
present	O
experiments	O
trained	O
on	O
the	O
training	O
set	O
and	O
evaluated	O
on	O
the	O
test	O
set	O
.	O
Our	O
focus	O
is	O
on	O
the	O
behaviors	O
of	O
extremely	O
deep	Method
networks	Method
,	O
but	O
not	O
on	O
pushing	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
so	O
we	O
intentionally	O
use	O
simple	O
architectures	O
as	O
follows	O
.	O
The	O
plain	Method
/	O
residual	O
architectures	O
follow	O
the	O
form	O
in	O
Fig	O
.	O
3	O
(	O
middle	O
/	O
right	O
)	O
.	O
The	O
network	O
inputs	O
are	O
32×32	O
images	O
,	O
with	O
the	O
per	O
-	O
pixel	O
mean	O
subtracted	O
.	O
The	O
first	O
layer	O
is	O
3×3	O
convolutions	Method
.	O
Then	O
we	O
use	O
a	O
stack	Method
of	Method
6n	Method
layers	Method
with	O
3×3	Method
convolutions	Method
on	O
the	O
feature	O
maps	O
of	O
sizes	O
{	O
32	O
,	O
16	O
,	O
8	O
}	O
respectively	O
,	O
with	O
2n	O
layers	O
for	O
each	O
feature	O
map	O
size	O
.	O
The	O
numbers	O
of	O
filters	O
are	O
{	O
16	O
,	O
32	O
,	O
64	O
}	O
respectively	O
.	O
The	O
subsampling	Method
is	O
performed	O
by	O
convolutions	Method
with	O
a	O
stride	O
of	O
2	O
.	O
The	O
network	O
ends	O
with	O
a	O
global	Method
average	Method
pooling	Method
,	O
a	O
10	Method
-	Method
way	Method
fully	Method
-	Method
connected	Method
layer	Method
,	O
and	O
softmax	Method
.	O
There	O
are	O
totally	O
6n	O
+	O
2	O
stacked	O
weighted	Method
layers	Method
.	O
The	O
following	O
table	O
summarizes	O
the	O
architecture	O
:	O
output	O
map	O
size	O
32×32	O
16×16	O
8×8	O
#	O
layers	O
1	O
+	O
2n	O
2n	O
2n	O
#	O
filters	O
16	O
32	O
64	O
When	O
shortcut	O
connections	O
are	O
used	O
,	O
they	O
are	O
connected	O
to	O
the	O
pairs	O
of	O
3×3	O
layers	O
(	O
totally	O
3n	O
shortcuts	O
)	O
.	O
On	O
this	O
dataset	O
we	O
use	O
identity	O
shortcuts	O
in	O
all	O
cases	O
(	O
i.e.	O
,	O
option	O
A	O
)	O
,	O
method	O
error	Metric
(	O
%	O
)	O
Maxout	O
[	O
reference	O
]	O
9.38	O
NIN	O
[	O
reference	O
]	O
8.81	O
DSN	Method
[	O
reference	O
]	O
8	O
Table	O
6	O
.	O
Classification	O
error	Metric
on	O
the	O
CIFAR	Material
-	Material
10	Material
test	Material
set	Material
.	O
All	O
methods	O
are	O
with	O
data	Task
augmentation	Task
.	O
For	O
ResNet	Method
-	Method
110	Method
,	O
we	O
run	O
it	O
5	O
times	O
and	O
show	O
"	O
best	O
(	O
mean±std	O
)	O
"	O
as	O
in	O
[	O
reference	O
]	O
.	O
so	O
our	O
residual	Method
models	Method
have	O
exactly	O
the	O
same	O
depth	O
,	O
width	O
,	O
and	O
number	O
of	O
parameters	O
as	O
the	O
plain	Method
counterparts	O
.	O
We	O
use	O
a	O
weight	O
decay	O
of	O
0.0001	O
and	O
momentum	O
of	O
0.9	O
,	O
and	O
adopt	O
the	O
weight	Method
initialization	Method
in	O
[	O
reference	O
]	O
and	O
BN	Method
[	O
reference	O
]	O
but	O
with	O
no	O
dropout	Method
.	O
These	O
models	O
are	O
trained	O
with	O
a	O
minibatch	O
size	O
of	O
128	O
on	O
two	O
GPUs	O
.	O
We	O
start	O
with	O
a	O
learning	Metric
rate	Metric
of	O
0.1	O
,	O
divide	O
it	O
by	O
10	O
at	O
32k	O
and	O
48k	O
iterations	O
,	O
and	O
terminate	O
training	O
at	O
64k	O
iterations	O
,	O
which	O
is	O
determined	O
on	O
a	O
45k	O
/	O
5k	O
train	O
/	O
val	O
split	O
.	O
We	O
follow	O
the	O
simple	O
data	Method
augmentation	Method
in	O
[	O
reference	O
]	O
for	O
training	O
:	O
4	O
pixels	O
are	O
padded	O
on	O
each	O
side	O
,	O
and	O
a	O
32×32	O
crop	O
is	O
randomly	O
sampled	O
from	O
the	O
padded	O
image	O
or	O
its	O
horizontal	O
flip	O
.	O
For	O
testing	O
,	O
we	O
only	O
evaluate	O
the	O
single	O
view	O
of	O
the	O
original	O
32×32	O
image	O
.	O
We	O
compare	O
n	O
=	O
{	O
3	O
,	O
5	O
,	O
7	O
,	O
9	O
}	O
,	O
leading	O
to	O
20	O
,	O
32	O
,	O
44	O
,	O
and	O
56	O
-	O
layer	O
networks	O
.	O
Fig	O
.	O
6	O
(	O
left	O
)	O
shows	O
the	O
behaviors	O
of	O
the	O
plain	Method
nets	O
.	O
The	O
deep	O
plain	Method
nets	O
suffer	O
from	O
increased	O
depth	O
,	O
and	O
exhibit	O
higher	O
training	O
error	Metric
when	O
going	O
deeper	O
.	O
This	O
phenomenon	O
is	O
similar	O
to	O
that	O
on	O
ImageNet	Material
(	O
Fig	O
.	O
4	O
,	O
left	O
)	O
and	O
on	O
MNIST	Material
(	O
see	O
[	O
reference	O
]	O
)	O
,	O
suggesting	O
that	O
such	O
an	O
optimization	Task
difficulty	Task
is	O
a	O
fundamental	O
problem	O
.	O
Fig	O
.	O
6	O
(	O
middle	O
)	O
shows	O
the	O
behaviors	O
of	O
ResNets	Method
.	O
Also	O
similar	O
to	O
the	O
ImageNet	Material
cases	Material
(	O
Fig	O
.	O
4	O
,	O
right	O
)	O
,	O
our	O
ResNets	Method
manage	O
to	O
overcome	O
the	O
optimization	O
difficulty	O
and	O
demonstrate	O
accuracy	Metric
gains	O
when	O
the	O
depth	O
increases	O
.	O
We	O
further	O
explore	O
n	O
=	O
18	O
that	O
leads	O
to	O
a	O
110	Method
-	Method
layer	Method
ResNet	Method
.	O
In	O
this	O
case	O
,	O
we	O
find	O
that	O
the	O
initial	O
learning	Metric
rate	Metric
of	Metric
0.1	Metric
is	O
slightly	O
too	O
large	O
to	O
start	O
converging	O
[	O
reference	O
]	O
.	O
So	O
we	O
use	O
0.01	O
to	O
warm	O
up	O
the	O
training	O
until	O
the	O
training	O
error	Metric
is	O
below	O
80	O
%	O
(	O
about	O
400	O
iterations	O
)	O
,	O
and	O
then	O
go	O
back	O
to	O
0.1	O
and	O
continue	O
training	O
.	O
The	O
rest	O
of	O
the	O
learning	Method
schedule	Method
is	O
as	O
done	O
previously	O
.	O
This	O
110	Method
-	Method
layer	Method
network	Method
converges	O
well	O
(	O
Fig	O
.	O
6	O
,	O
middle	O
)	O
.	O
It	O
has	O
fewer	O
parameters	O
than	O
other	O
deep	O
and	O
thin	O
networks	O
such	O
as	O
FitNet	Method
[	O
reference	O
]	O
and	O
Highway	Method
[	O
reference	O
]	O
(	O
Table	O
6	O
)	O
,	O
yet	O
is	O
among	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
(	O
6.43	O
%	O
,	O
Table	O
6	O
)	O
.	O
Analysis	Task
of	Task
Layer	Task
Responses	Task
.	O
Fig	O
.	O
7	O
shows	O
the	O
standard	O
deviations	O
(	O
std	O
)	O
of	O
the	O
layer	O
responses	O
.	O
The	O
responses	O
are	O
the	O
outputs	O
of	O
each	O
3×3	O
layer	O
,	O
after	O
BN	Method
and	O
before	O
other	O
nonlinearity	O
(	O
ReLU	Method
/	Method
addition	Method
)	O
.	O
For	O
ResNets	Method
,	O
this	O
analysis	O
reveals	O
the	O
response	O
strength	O
of	O
the	O
residual	O
functions	O
.	O
Fig	O
.	O
7	O
shows	O
that	O
ResNets	Method
have	O
generally	O
smaller	O
responses	O
than	O
their	O
plain	Method
counterparts	O
.	O
These	O
results	O
support	O
our	O
basic	O
motivation	O
(	O
Sec.3.1	O
)	O
that	O
the	O
residual	O
functions	O
might	O
be	O
generally	O
closer	O
to	O
zero	O
than	O
the	O
non	O
-	O
residual	O
functions	O
.	O
We	O
also	O
notice	O
that	O
the	O
deeper	O
ResNet	Method
has	O
smaller	O
magnitudes	O
of	O
responses	O
,	O
as	O
evidenced	O
by	O
the	O
comparisons	O
among	O
ResNet	Method
-	Method
20	Method
,	O
56	O
,	O
and	O
110	O
in	O
Fig	O
.	O
7	O
.	O
When	O
there	O
are	O
more	O
layers	O
,	O
an	O
individual	O
layer	O
of	O
ResNets	Method
tends	O
to	O
modify	O
the	O
signal	O
less	O
.	O
Exploring	O
Over	O
1000	O
layers	O
.	O
We	O
explore	O
an	O
aggressively	O
deep	Method
model	Method
of	O
over	O
1000	O
layers	O
.	O
We	O
set	O
n	O
=	O
200	O
that	O
leads	O
to	O
a	O
1202	Method
-	Method
layer	Method
network	Method
,	O
which	O
is	O
trained	O
as	O
described	O
above	O
.	O
Our	O
method	O
shows	O
no	O
optimization	O
difficulty	O
,	O
and	O
this	O
10	O
3	Method
-	Method
layer	Method
network	Method
is	O
able	O
to	O
achieve	O
training	O
error	Metric
<	O
0.1	O
%	O
(	O
Fig	O
.	O
6	O
,	O
right	O
)	O
.	O
Its	O
test	O
error	Metric
is	O
still	O
fairly	O
good	O
(	O
7.93	O
%	O
,	O
Table	O
8	O
.	O
Object	Metric
detection	Metric
mAP	Metric
(	O
%	O
)	O
on	O
the	O
COCO	Metric
validation	Metric
set	Metric
using	O
baseline	Method
Faster	Method
R	Method
-	Method
CNN	Method
.	O
See	O
also	O
Table	O
9	O
for	O
better	O
results	O
.	O
have	O
similar	O
training	O
error	Metric
.	O
We	O
argue	O
that	O
this	O
is	O
because	O
of	O
overfitting	O
.	O
The	O
1202	Method
-	Method
layer	Method
network	Method
may	O
be	O
unnecessarily	O
large	O
(	O
19.4	O
M	O
)	O
for	O
this	O
small	O
dataset	O
.	O
Strong	Method
regularization	Method
such	O
as	O
maxout	Method
[	O
reference	O
]	O
or	O
dropout	Method
[	O
reference	O
]	O
is	O
applied	O
to	O
obtain	O
the	O
best	O
results	O
(	O
[	O
reference	O
][	O
reference	O
][	O
reference	O
][	O
reference	O
]	O
)	O
on	O
this	O
dataset	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
no	O
maxout	Method
/	Method
dropout	Method
and	O
just	O
simply	O
impose	O
regularization	Method
via	O
deep	Method
and	Method
thin	Method
architectures	Method
by	O
design	O
,	O
without	O
distracting	O
from	O
the	O
focus	O
on	O
the	O
difficulties	O
of	O
optimization	Task
.	O
But	O
combining	O
with	O
stronger	O
regularization	Method
may	O
improve	O
results	O
,	O
which	O
we	O
will	O
study	O
in	O
the	O
future	O
.	O
section	O
:	O
Object	Task
Detection	Task
on	O
PASCAL	Material
and	O
MS	Material
COCO	Material
Our	O
method	O
has	O
good	O
generalization	Task
performance	O
on	O
other	O
recognition	Task
tasks	Task
.	O
Table	O
7	O
and	O
8	O
show	O
the	O
object	Metric
detection	Metric
baseline	Metric
results	O
on	O
PASCAL	Material
VOC	Material
2007	Material
and	O
2012	O
[	O
reference	O
]	O
and	O
COCO	Material
[	O
reference	O
]	O
.	O
We	O
adopt	O
Faster	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
as	O
the	O
detection	Method
method	Method
.	O
Here	O
we	O
are	O
interested	O
in	O
the	O
improvements	O
of	O
replacing	O
VGG	Method
-	Method
16	Method
[	O
reference	O
]	O
with	O
ResNet	Method
-	Method
101	Method
.	O
The	O
detection	Task
implementation	Task
(	O
see	O
appendix	O
)	O
of	O
using	O
both	O
models	O
is	O
the	O
same	O
,	O
so	O
the	O
gains	O
can	O
only	O
be	O
attributed	O
to	O
better	O
networks	O
.	O
Most	O
remarkably	O
,	O
on	O
the	O
challenging	O
COCO	Material
dataset	Material
we	O
obtain	O
a	O
6.0	O
%	O
increase	O
in	O
COCO	Metric
's	Metric
standard	Metric
metric	Metric
(	O
mAP@	Metric
[	Metric
.5	Metric
,	O
.95	O
]	O
)	O
,	O
which	O
is	O
a	O
28	O
%	O
relative	O
improvement	O
.	O
This	O
gain	O
is	O
solely	O
due	O
to	O
the	O
learned	Method
representations	Method
.	O
Based	O
on	O
deep	Method
residual	Method
nets	Method
,	O
we	O
won	O
the	O
1st	O
places	O
in	O
several	O
tracks	O
in	O
ILSVRC	Task
&	O
COCO	Task
2015	Task
competitions	Task
:	O
ImageNet	Task
detection	Task
,	O
ImageNet	O
localization	Task
,	O
COCO	Task
detection	Task
,	O
and	O
COCO	Task
segmentation	Task
.	O
The	O
details	O
are	O
in	O
the	O
appendix	O
.	O
section	O
:	O
A.	O
Object	Method
Detection	Method
Baselines	Method
In	O
this	O
section	O
we	O
introduce	O
our	O
detection	Method
method	Method
based	O
on	O
the	O
baseline	Method
Faster	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
system	O
.	O
The	O
models	O
are	O
initialized	O
by	O
the	O
ImageNet	O
classification	Task
models	O
,	O
and	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
object	O
detection	O
data	O
.	O
We	O
have	O
experimented	O
with	O
ResNet	Method
-	Method
50	Method
/	Method
101	Method
at	O
the	O
time	O
of	O
the	O
ILSVRC	Task
&	O
COCO	Task
2015	Task
detection	Task
competitions	Task
.	O
Unlike	O
VGG	Method
-	Method
16	Method
used	O
in	O
[	O
reference	O
]	O
,	O
our	O
ResNet	Method
has	O
no	O
hidden	Method
fc	Method
layers	Method
.	O
We	O
adopt	O
the	O
idea	O
of	O
"	O
Networks	Method
on	Method
Conv	Method
feature	Method
maps	Method
"	O
(	O
NoC	Method
)	O
[	O
reference	O
]	O
to	O
address	O
this	O
issue	O
.	O
We	O
compute	O
the	O
full	O
-	O
image	O
shared	O
conv	O
feature	O
maps	O
using	O
those	O
layers	O
whose	O
strides	O
on	O
the	O
image	O
are	O
no	O
greater	O
than	O
16	O
pixels	O
(	O
i.e.	O
,	O
conv1	O
,	O
conv2	O
x	O
,	O
conv3	O
x	O
,	O
and	O
conv4	O
x	O
,	O
totally	O
91	O
conv	O
layers	O
in	O
ResNet	Method
-	O
101	O
;	O
Table	O
1	O
)	O
.	O
We	O
consider	O
these	O
layers	O
as	O
analogous	O
to	O
the	O
13	O
conv	O
layers	O
in	O
VGG	Method
-	Method
16	Method
,	O
and	O
by	O
doing	O
so	O
,	O
both	O
ResNet	Method
and	O
VGG	Method
-	Method
16	Method
have	O
conv	O
feature	O
maps	O
of	O
the	O
same	O
total	O
stride	O
(	O
16	O
pixels	O
)	O
.	O
These	O
layers	O
are	O
shared	O
by	O
a	O
region	Method
proposal	Method
network	Method
(	O
RPN	Method
,	O
generating	O
300	O
proposals	O
)	O
[	O
reference	O
]	O
and	O
a	O
Fast	O
R	Method
-	Method
CNN	Method
detection	O
network	O
[	O
reference	O
]	O
.	O
RoI	Method
pooling	Method
[	O
reference	O
]	O
is	O
performed	O
before	O
conv5	Method
1	O
.	O
On	O
this	O
RoI	O
-	O
pooled	O
feature	O
,	O
all	O
layers	O
of	O
conv5	O
x	O
and	O
up	O
are	O
adopted	O
for	O
each	O
region	O
,	O
playing	O
the	O
roles	O
of	O
VGG	Method
-	Method
16	Method
's	Method
fc	Method
layers	Method
.	O
The	O
final	O
classification	Task
layer	O
is	O
replaced	O
by	O
two	O
sibling	Method
layers	Method
(	O
classification	Task
and	O
box	Method
regression	Method
[	O
reference	O
]	O
)	O
.	O
For	O
the	O
usage	O
of	O
BN	Method
layers	O
,	O
after	O
pre	Task
-	Task
training	Task
,	O
we	O
compute	O
the	O
BN	Method
statistics	O
(	O
means	O
and	O
variances	O
)	O
for	O
each	O
layer	O
on	O
the	O
ImageNet	Material
training	Material
set	Material
.	O
Then	O
the	O
BN	Method
layers	O
are	O
fixed	O
during	O
fine	Task
-	Task
tuning	Task
for	O
object	Task
detection	Task
.	O
As	O
such	O
,	O
the	O
BN	Method
layers	O
become	O
linear	O
activations	O
with	O
constant	O
offsets	O
and	O
scales	O
,	O
and	O
BN	Method
statistics	O
are	O
not	O
updated	O
by	O
fine	Method
-	Method
tuning	Method
.	O
We	O
fix	O
the	O
BN	Method
layers	O
mainly	O
for	O
reducing	O
memory	Metric
consumption	Metric
in	O
Faster	Method
R	Method
-	Method
CNN	Method
training	O
.	O
section	O
:	O
PASCAL	Material
VOC	O
Following	O
[	O
reference	O
][	O
reference	O
]	O
,	O
for	O
the	O
PASCAL	Material
VOC	O
2007	O
test	O
set	O
,	O
we	O
use	O
the	O
5k	O
trainval	O
images	O
in	O
VOC	Material
2007	Material
and	O
16k	O
trainval	O
images	O
in	O
VOC	Material
2012	Material
for	O
training	O
(	O
"	O
07	O
+	O
12	O
"	O
)	O
.	O
For	O
the	O
PASCAL	Material
VOC	Material
2012	Material
test	Material
set	Material
,	O
we	O
use	O
the	O
10k	O
trainval	O
+	O
test	O
images	O
in	O
VOC	Material
2007	Material
and	O
16k	O
trainval	O
images	O
in	O
VOC	Material
2012	Material
for	O
training	O
(	O
"	O
07	O
++	O
12	O
"	O
)	O
.	O
The	O
hyper	O
-	O
parameters	O
for	O
training	O
Faster	O
R	Method
-	Method
CNN	Method
are	O
the	O
same	O
as	O
in	O
[	O
reference	O
]	O
.	O
Table	O
7	O
shows	O
the	O
results	O
.	O
ResNet	Method
-	Method
101	Method
improves	O
the	O
mAP	Metric
by	O
>	O
3	O
%	O
over	O
VGG	Method
-	Method
16	Method
.	O
This	O
gain	O
is	O
solely	O
because	O
of	O
the	O
improved	O
features	O
learned	O
by	O
ResNet	Method
.	O
section	O
:	O
MS	Material
COCO	Material
The	O
MS	Material
COCO	Material
dataset	O
[	O
reference	O
]	O
involves	O
80	O
object	O
categories	O
.	O
We	O
evaluate	O
the	O
PASCAL	Material
VOC	O
metric	O
(	O
mAP	Metric
@	Metric
IoU	Metric
=	Metric
0.5	Metric
)	O
and	O
the	O
standard	O
COCO	Metric
metric	Metric
(	O
mAP	Metric
@	Metric
IoU	Metric
=	O
.5:.05:.95	O
)	O
.	O
We	O
use	O
the	O
80k	O
images	O
on	O
the	O
train	O
set	O
for	O
training	O
and	O
the	O
40k	O
images	O
on	O
the	O
val	O
set	O
for	O
evaluation	O
.	O
Our	O
detection	Method
system	Method
for	O
COCO	Task
is	O
similar	O
to	O
that	O
for	O
PASCAL	Material
VOC	O
.	O
We	O
train	O
the	O
COCO	Method
models	Method
with	O
an	O
8	Method
-	Method
GPU	Method
implementation	Method
,	O
and	O
thus	O
the	O
RPN	Method
step	Method
has	O
a	O
mini	O
-	O
batch	O
size	O
of	O
8	O
images	O
(	O
i.e.	O
,	O
1	O
per	O
GPU	O
)	O
and	O
the	O
Fast	O
R	Method
-	Method
CNN	Method
step	O
has	O
a	O
mini	O
-	O
batch	O
size	O
of	O
16	O
images	O
.	O
The	O
RPN	Method
step	Method
and	O
Fast	O
R	Method
-	Method
CNN	Method
step	O
are	O
both	O
trained	O
for	O
240k	O
iterations	O
with	O
a	O
learning	Metric
rate	Metric
of	O
0.001	O
and	O
then	O
for	O
80k	O
iterations	O
with	O
0.0001	O
.	O
Table	O
8	O
shows	O
the	O
results	O
on	O
the	O
MS	Material
COCO	Material
validation	O
set	O
.	O
ResNet	Method
-	Method
101	Method
has	O
a	O
6	O
%	O
increase	O
of	O
mAP@	Metric
[	Metric
.5	Metric
,	O
.95	O
]	O
over	O
VGG	Method
-	Method
16	Method
,	O
which	O
is	O
a	O
28	O
%	O
relative	O
improvement	O
,	O
solely	O
contributed	O
by	O
the	O
features	O
learned	O
by	O
the	O
better	O
network	O
.	O
Remarkably	O
,	O
the	O
mAP@	O
[	O
.5	O
,	O
.95	O
]	O
's	O
absolute	O
increase	O
(	O
6.0	O
%	O
)	O
is	O
nearly	O
as	O
big	O
as	O
mAP@.5	O
's	O
(	O
6.9	O
%	O
)	O
.	O
This	O
suggests	O
that	O
a	O
deeper	Method
network	Method
can	O
improve	O
both	O
recognition	Task
and	O
localization	Task
.	O
section	O
:	O
B.	O
Object	Task
Detection	Task
Improvements	O
For	O
completeness	O
,	O
we	O
report	O
the	O
improvements	O
made	O
for	O
the	O
competitions	O
.	O
These	O
improvements	O
are	O
based	O
on	O
deep	O
features	O
and	O
thus	O
should	O
benefit	O
from	O
residual	Method
learning	Method
.	O
MS	Material
COCO	Material
Box	Method
refinement	Method
.	O
Our	O
box	Method
refinement	Method
partially	O
follows	O
the	O
iterative	O
localization	Task
in	O
[	O
reference	O
]	O
.	O
In	O
Faster	O
R	Method
-	Method
CNN	Method
,	O
the	O
final	O
output	O
is	O
a	O
regressed	O
box	O
that	O
is	O
different	O
from	O
its	O
proposal	O
box	O
.	O
So	O
for	O
inference	Task
,	O
we	O
pool	O
a	O
new	O
feature	O
from	O
the	O
regressed	O
box	O
and	O
obtain	O
a	O
new	O
classification	Task
score	O
and	O
a	O
new	O
regressed	O
box	O
.	O
We	O
combine	O
these	O
300	O
new	O
predictions	O
with	O
the	O
original	O
300	O
predictions	O
.	O
Non	Method
-	Method
maximum	Method
suppression	Method
(	O
NMS	Method
)	O
is	O
applied	O
on	O
the	O
union	O
set	O
of	O
predicted	O
boxes	O
using	O
an	O
IoU	O
threshold	O
of	O
0.3	O
[	O
reference	O
]	O
,	O
followed	O
by	O
box	Method
voting	Method
[	O
reference	O
]	O
.	O
Box	Method
refinement	Method
improves	O
mAP	Metric
by	O
about	O
2	O
points	O
(	O
Table	O
9	O
)	O
.	O
Global	O
context	O
.	O
We	O
combine	O
global	O
context	O
in	O
the	O
Fast	O
R	Method
-	Method
CNN	Method
step	O
.	O
Given	O
the	O
full	Task
-	Task
image	Task
conv	Task
feature	Task
map	Task
,	O
we	O
pool	O
a	O
feature	O
by	O
global	Method
Spatial	Method
Pyramid	Method
Pooling	Method
[	O
reference	O
]	O
(	O
with	O
a	O
"	O
single	O
-	O
level	O
"	O
pyramid	O
)	O
which	O
can	O
be	O
implemented	O
as	O
"	O
RoI	Method
"	Method
pooling	Method
using	O
the	O
entire	O
image	O
's	O
bounding	O
box	O
as	O
the	O
RoI.	O
This	O
pooled	O
feature	O
is	O
fed	O
into	O
the	O
post	Method
-	Method
RoI	Method
layers	Method
to	O
obtain	O
a	O
global	O
context	O
feature	O
.	O
This	O
global	O
feature	O
is	O
concatenated	O
with	O
the	O
original	O
per	O
-	O
region	O
feature	O
,	O
followed	O
by	O
the	O
sibling	O
classification	Task
and	O
box	O
regression	O
layers	O
.	O
This	O
new	O
structure	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O
Global	O
context	O
improves	O
mAP@.5	Metric
by	O
about	O
1	O
point	O
(	O
Table	O
9	O
)	O
.	O
Multi	Task
-	Task
scale	Task
testing	Task
.	O
In	O
the	O
above	O
,	O
all	O
results	O
are	O
obtained	O
by	O
single	Method
-	Method
scale	Method
training	Method
/	Method
testing	Method
as	O
in	O
[	O
reference	O
]	O
,	O
where	O
the	O
image	O
's	O
shorter	O
side	O
is	O
s	O
=	O
600	O
pixels	O
.	O
Multi	Task
-	Task
scale	Task
training	Task
/	Task
testing	Task
has	O
been	O
developed	O
in	O
[	O
reference	O
][	O
reference	O
]	O
by	O
selecting	O
a	O
scale	O
from	O
a	O
feature	O
pyramid	O
,	O
and	O
in	O
[	O
reference	O
]	O
by	O
using	O
maxout	Method
layers	Method
.	O
In	O
our	O
current	O
implementation	O
,	O
we	O
have	O
performed	O
multi	Task
-	Task
scale	Task
testing	Task
following	O
[	O
reference	O
]	O
;	O
we	O
have	O
not	O
performed	O
multi	Method
-	Method
scale	Method
training	Method
because	O
of	O
limited	O
time	O
.	O
In	O
addition	O
,	O
we	O
have	O
performed	O
multi	Task
-	Task
scale	Task
testing	Task
only	O
for	O
the	O
Fast	O
R	Method
-	Method
CNN	Method
step	O
(	O
but	O
not	O
yet	O
for	O
the	O
RPN	Method
step	Method
)	O
.	O
With	O
a	O
trained	O
model	O
,	O
we	O
compute	O
conv	Method
feature	Method
maps	Method
on	O
an	O
image	O
pyramid	O
,	O
where	O
the	O
image	O
's	O
shorter	O
sides	O
are	O
s	O
∈	O
{	O
200	O
,	O
400	O
,	O
600	O
,	O
800	O
,	O
1000}.	O
Table	O
10	O
.	O
Detection	Task
results	O
on	O
the	O
PASCAL	Material
VOC	O
2007	O
test	O
set	O
.	O
The	O
baseline	O
is	O
the	O
Faster	Method
R	Method
-	Method
CNN	Method
system	O
.	O
The	O
system	O
"	O
baseline	O
+++	O
"	O
include	O
box	O
refinement	O
,	O
context	O
,	O
and	O
multi	O
-	O
scale	O
testing	O
in	O
Table	O
9	O
Table	O
11	O
.	O
Detection	Task
results	O
on	O
the	O
PASCAL	Material
VOC	Material
2012	Material
test	Material
set	Material
(	O
http:	O
//	O
host.robots.ox.ac.uk:8080	O
/	O
leaderboard	O
/	O
displaylb.php?challengeid=11	O
&	O
compid=4	O
)	O
.	O
The	O
baseline	O
is	O
the	O
Faster	O
R	Method
-	Method
CNN	Method
system	Method
.	O
The	O
system	O
"	O
baseline	O
+++	O
"	O
include	O
box	O
refinement	O
,	O
context	O
,	O
and	O
multi	O
-	O
scale	O
testing	O
in	O
Table	O
9	O
.	O
We	O
select	O
two	O
adjacent	O
scales	O
from	O
the	O
pyramid	O
following	O
[	O
reference	O
]	O
.	O
RoI	Method
pooling	Method
and	O
subsequent	O
layers	O
are	O
performed	O
on	O
the	O
feature	O
maps	O
of	O
these	O
two	O
scales	O
[	O
reference	O
]	O
,	O
which	O
are	O
merged	O
by	O
maxout	Method
as	O
in	O
[	O
reference	O
]	O
.	O
Multi	Method
-	Method
scale	Method
testing	Method
improves	O
the	O
mAP	O
by	O
over	O
2	O
points	O
(	O
Table	O
9	O
)	O
.	O
Using	O
validation	O
data	O
.	O
Next	O
we	O
use	O
the	O
80k	O
+	O
40k	O
trainval	O
set	O
for	O
training	O
and	O
the	O
20k	O
test	Metric
-	Metric
dev	Metric
set	Metric
for	O
evaluation	O
.	O
The	O
testdev	O
set	O
has	O
no	O
publicly	O
available	O
ground	O
truth	O
and	O
the	O
result	O
is	O
reported	O
by	O
the	O
evaluation	O
server	O
.	O
Under	O
this	O
setting	O
,	O
the	O
results	O
are	O
an	O
mAP@.5	Metric
of	O
55.7	O
%	O
and	O
an	O
mAP@	Metric
[	Metric
.5	Metric
,	O
.95	O
]	O
of	O
34.9	O
%	O
(	O
Table	O
9	O
)	O
.	O
This	O
is	O
our	O
single	O
-	O
model	O
result	O
.	O
Ensemble	Method
.	O
In	O
Faster	O
R	Method
-	Method
CNN	Method
,	O
the	O
system	O
is	O
designed	O
to	O
learn	O
region	O
proposals	O
and	O
also	O
object	Method
classifiers	Method
,	O
so	O
an	O
ensemble	Method
can	O
be	O
used	O
to	O
boost	O
both	O
tasks	O
.	O
We	O
use	O
an	O
ensemble	O
for	O
proposing	Task
regions	Task
,	O
and	O
the	O
union	O
set	O
of	O
proposals	O
are	O
processed	O
by	O
an	O
ensemble	Method
of	Method
per	Method
-	Method
region	Method
classifiers	Method
.	O
Table	O
9	O
shows	O
our	O
result	O
based	O
on	O
an	O
ensemble	O
of	O
3	O
networks	O
.	O
The	O
mAP	Metric
is	O
59.0	O
%	O
and	O
37.4	O
%	O
on	O
the	O
test	O
-	O
dev	O
set	O
.	O
This	O
result	O
won	O
the	O
1st	O
place	O
in	O
the	O
detection	Task
task	Task
in	O
COCO	Task
2015	Task
.	O
section	O
:	O
PASCAL	Material
VOC	O
We	O
revisit	O
the	O
PASCAL	Material
VOC	O
dataset	O
based	O
on	O
the	O
above	O
model	O
.	O
With	O
the	O
single	O
model	O
on	O
the	O
COCO	Material
dataset	Material
(	O
55.7	O
%	O
mAP@.5	Metric
in	O
Table	O
9	O
)	O
,	O
we	O
fine	O
-	O
tune	O
this	O
model	O
on	O
the	O
PAS	Material
-	Material
CAL	Material
VOC	Material
sets	Material
.	O
The	O
improvements	O
of	O
box	Task
refinement	Task
,	O
context	O
,	O
and	O
multi	Task
-	Task
scale	Task
testing	Task
are	O
also	O
adopted	O
.	O
Table	O
12	O
.	O
Our	O
results	O
(	O
mAP	Metric
,	O
%	O
)	O
on	O
the	O
ImageNet	Material
detection	Material
dataset	Material
.	O
Our	O
detection	Method
system	Method
is	O
Faster	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
with	O
the	O
improvements	O
in	O
Table	O
9	O
,	O
using	O
ResNet	Method
-	Method
101	Method
.	O
we	O
achieve	O
85.6	O
%	O
mAP	Metric
on	O
PASCAL	Material
VOC	Material
2007	Material
(	O
Table	O
10	O
)	O
and	O
83.8	O
%	O
on	O
PASCAL	Material
VOC	Material
2012	Material
(	O
Table	O
11	O
)	O
6	O
.	O
The	O
result	O
on	O
PASCAL	Material
VOC	Material
2012	Material
is	O
10	O
points	O
higher	O
than	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
[	O
reference	O
]	O
.	O
section	O
:	O
ImageNet	Task
Detection	Task
The	O
ImageNet	Task
Detection	Task
(	O
DET	Task
)	O
task	O
involves	O
200	O
object	O
categories	O
.	O
The	O
accuracy	Metric
is	O
evaluated	O
by	O
mAP@.5	Method
.	O
Our	O
object	Method
detection	Method
algorithm	Method
for	O
ImageNet	O
DET	Task
is	O
the	O
same	O
as	O
that	O
for	O
MS	Material
COCO	Material
in	O
Table	O
9	O
.	O
The	O
networks	O
are	O
pretrained	O
on	O
the	O
1000	O
-	O
class	O
ImageNet	O
classification	Task
set	O
,	O
and	O
are	O
fine	O
-	O
tuned	O
on	O
the	O
DET	Task
data	O
.	O
We	O
split	O
the	O
validation	O
set	O
into	O
two	O
parts	O
(	O
val1	O
/	O
val2	O
)	O
following	O
[	O
reference	O
]	O
.	O
We	O
fine	O
-	O
tune	O
the	O
detection	Method
models	Method
using	O
the	O
DET	Task
training	O
set	O
and	O
the	O
val1	Material
set	Material
.	O
The	O
val2	Material
set	Material
is	O
used	O
for	O
validation	Task
.	O
We	O
do	O
not	O
use	O
other	O
ILSVRC	Task
2015	O
data	O
.	O
Table	O
13	O
.	O
Localization	O
error	Metric
(	O
%	O
)	O
on	O
the	O
ImageNet	Task
validation	Task
.	O
In	O
the	O
column	O
of	O
"	O
LOC	O
error	Metric
on	O
GT	Metric
class	Metric
"	O
(	O
[	O
reference	O
]	O
)	O
,	O
the	O
ground	O
truth	O
class	O
is	O
used	O
.	O
In	O
the	O
"	O
testing	O
"	O
column	O
,	O
"	O
1	O
-	O
crop	O
"	O
denotes	O
testing	O
on	O
a	O
center	O
crop	O
of	O
224×224	O
pixels	O
,	O
"	O
dense	O
"	O
denotes	O
dense	O
(	O
fully	Method
convolutional	Method
)	O
and	O
multi	Task
-	Task
scale	Task
testing	Task
.	O
58.8	O
%	O
mAP	Metric
and	O
our	O
ensemble	O
of	O
3	O
models	O
has	O
62.1	O
%	O
mAP	Metric
on	O
the	O
DET	Task
test	O
set	O
(	O
Table	O
12	O
)	O
.	O
This	O
result	O
won	O
the	O
1st	O
place	O
in	O
the	O
ImageNet	Task
detection	Task
task	Task
in	O
ILSVRC	Task
2015	O
,	O
surpassing	O
the	O
second	O
place	O
by	O
8.5	O
points	O
(	O
absolute	O
)	O
.	O
section	O
:	O
C.	O
ImageNet	Task
Localization	Task
The	O
ImageNet	Task
Localization	Task
(	Task
LOC	Task
)	Task
task	Task
[	O
reference	O
]	O
requires	O
to	O
classify	O
and	O
localize	O
the	O
objects	O
.	O
Following	O
[	O
reference	O
][	O
reference	O
]	O
,	O
we	O
assume	O
that	O
the	O
image	Method
-	Method
level	Method
classifiers	Method
are	O
first	O
adopted	O
for	O
predicting	O
the	O
class	Task
labels	Task
of	Task
an	Task
image	Task
,	O
and	O
the	O
localization	Task
algorithm	O
only	O
accounts	O
for	O
predicting	Task
bounding	Task
boxes	Task
based	O
on	O
the	O
predicted	O
classes	O
.	O
We	O
adopt	O
the	O
"	O
per	Method
-	Method
class	Method
regression	Method
"	O
(	O
PCR	Method
)	O
strategy	O
[	O
reference	O
][	O
reference	O
]	O
,	O
learning	O
a	O
bounding	Method
box	Method
regressor	Method
for	O
each	O
class	O
.	O
We	O
pre	O
-	O
train	O
the	O
networks	O
for	O
ImageNet	O
classification	Task
and	O
then	O
fine	O
-	O
tune	O
them	O
for	O
localization	Task
.	O
We	O
train	O
networks	O
on	O
the	O
provided	O
1000	Material
-	Material
class	Material
ImageNet	Material
training	Material
set	Material
.	O
Our	O
localization	Task
algorithm	O
is	O
based	O
on	O
the	O
RPN	Method
framework	Method
of	O
[	O
reference	O
]	O
with	O
a	O
few	O
modifications	O
.	O
Unlike	O
the	O
way	O
in	O
[	O
reference	O
]	O
that	O
is	O
category	O
-	O
agnostic	O
,	O
our	O
RPN	Method
for	O
localization	Task
is	O
designed	O
in	O
a	O
per	O
-	O
class	O
form	O
.	O
This	O
RPN	Method
ends	O
with	O
two	O
sibling	Method
1×1	Method
convolutional	Method
layers	Method
for	O
binary	O
classification	Task
(	O
cls	Method
)	O
and	O
box	Method
regression	Method
(	O
reg	Method
)	O
,	O
as	O
in	O
[	O
reference	O
]	O
.	O
The	O
cls	Method
and	O
reg	Method
layers	O
are	O
both	O
in	O
a	O
per	Task
-	Task
class	Task
from	Task
,	O
in	O
contrast	O
to	O
[	O
reference	O
]	O
.	O
Specifically	O
,	O
the	O
cls	Method
layer	O
has	O
a	O
1000	O
-	O
d	O
output	O
,	O
and	O
each	O
dimension	O
is	O
binary	Method
logistic	Method
regression	Method
for	O
predicting	O
being	O
or	O
not	O
being	O
an	O
object	O
class	O
;	O
the	O
reg	Method
layer	O
has	O
a	O
1000×4	O
-	O
d	O
output	O
consisting	O
of	O
box	Method
regressors	Method
for	O
1000	O
classes	O
.	O
As	O
in	O
[	O
reference	O
]	O
,	O
our	O
bounding	Method
box	Method
regression	Method
is	O
with	O
reference	O
to	O
multiple	O
translation	O
-	O
invariant	O
"	O
anchor	O
"	O
boxes	O
at	O
each	O
position	O
.	O
As	O
in	O
our	O
ImageNet	O
classification	Task
training	O
(	O
Sec	O
.	O
3.4	O
)	O
,	O
we	O
randomly	O
sample	O
224×224	O
crops	O
for	O
data	Task
augmentation	Task
.	O
We	O
use	O
a	O
mini	O
-	O
batch	O
size	O
of	O
256	O
images	O
for	O
fine	Task
-	Task
tuning	Task
.	O
To	O
avoid	O
negative	O
samples	O
being	O
dominate	O
,	O
8	O
anchors	O
are	O
randomly	O
sampled	O
for	O
each	O
image	O
,	O
where	O
the	O
sampled	O
positive	O
and	O
negative	O
anchors	O
have	O
a	O
ratio	O
of	O
1:1	O
[	O
reference	O
]	O
.	O
For	O
testing	O
,	O
the	O
network	O
is	O
applied	O
on	O
the	O
image	Task
fully	Task
-	Task
convolutionally	Task
.	O
Table	O
13	O
compares	O
the	O
localization	Task
results	O
.	O
Following	O
[	O
reference	O
]	O
,	O
we	O
first	O
perform	O
"	O
oracle	Method
"	Method
testing	Method
using	O
the	O
ground	O
truth	O
class	O
as	O
the	O
classification	Task
prediction	O
.	O
VGG	Method
's	O
paper	O
[	O
reference	O
]	O
ports	O
a	O
center	O
-	O
crop	O
error	Metric
of	O
33.1	O
%	O
(	O
Table	O
13	O
)	O
using	O
ground	O
truth	O
classes	O
.	O
Under	O
the	O
same	O
setting	O
,	O
our	O
RPN	Method
method	Method
using	O
ResNet	Method
-	O
101	O
net	O
significantly	O
reduces	O
the	O
center	O
-	O
crop	O
error	Metric
to	O
13.3	O
%	O
.	O
This	O
comparison	O
demonstrates	O
the	O
excellent	O
performance	O
of	O
our	O
framework	O
.	O
With	O
dense	Method
(	Method
fully	Method
convolutional	Method
)	O
and	O
multi	Task
-	Task
scale	Task
testing	Task
,	O
our	O
ResNet	Method
-	Method
101	Method
has	O
an	O
error	Metric
of	O
11.7	O
%	O
using	O
ground	Metric
truth	Metric
classes	Metric
.	O
Using	O
ResNet	Method
-	Method
101	Method
for	O
predicting	Task
classes	Task
(	O
4.6	O
%	O
top	Metric
-	Metric
5	Metric
classification	Metric
error	Metric
,	O
Table	O
4	O
)	O
,	O
the	O
top	O
-	O
5	O
localization	Task
error	Metric
is	O
14.4	O
%	O
.	O
The	O
above	O
results	O
are	O
only	O
based	O
on	O
the	O
proposal	Method
network	Method
(	O
RPN	Method
)	O
in	O
Faster	Method
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
.	O
One	O
may	O
use	O
the	O
detection	Method
network	Method
(	O
Fast	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
)	O
in	O
Faster	Method
R	Method
-	Method
CNN	Method
to	O
improve	O
the	O
results	O
.	O
But	O
we	O
notice	O
that	O
on	O
this	O
dataset	O
,	O
one	O
image	O
usually	O
contains	O
a	O
single	O
dominate	O
object	O
,	O
and	O
the	O
proposal	O
regions	O
highly	O
overlap	O
with	O
each	O
other	O
and	O
thus	O
have	O
very	O
similar	O
RoI	O
-	O
pooled	O
features	O
.	O
As	O
a	O
result	O
,	O
the	O
image	Method
-	Method
centric	Method
training	Method
of	O
Fast	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
generates	O
samples	O
of	O
small	O
variations	O
,	O
which	O
may	O
not	O
be	O
desired	O
for	O
stochastic	Task
training	Task
.	O
Motivated	O
by	O
this	O
,	O
in	O
our	O
current	O
experiment	O
we	O
use	O
the	O
original	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
that	O
is	O
RoI	O
-	O
centric	O
,	O
in	O
place	O
of	O
Fast	O
R	Method
-	Method
CNN	Method
.	O
Our	O
R	Method
-	Method
CNN	Method
implementation	O
is	O
as	O
follows	O
.	O
We	O
apply	O
the	O
per	Method
-	Method
class	Method
RPN	Method
trained	O
as	O
above	O
on	O
the	O
training	O
images	O
to	O
predict	O
bounding	O
boxes	O
for	O
the	O
ground	O
truth	O
class	O
.	O
These	O
predicted	O
boxes	O
play	O
a	O
role	O
of	O
class	O
-	O
dependent	O
proposals	O
.	O
For	O
each	O
training	O
image	O
,	O
the	O
highest	O
scored	O
200	O
proposals	O
are	O
extracted	O
as	O
training	O
samples	O
to	O
train	O
an	O
R	Method
-	Method
CNN	Method
classifier	O
.	O
The	O
image	O
region	O
is	O
cropped	O
from	O
a	O
proposal	O
,	O
warped	O
to	O
224×224	O
pixels	O
,	O
and	O
fed	O
into	O
the	O
classification	Task
network	O
as	O
in	O
R	Method
-	Method
CNN	Method
[	O
reference	O
]	O
.	O
The	O
outputs	O
of	O
this	O
network	O
consist	O
of	O
two	O
sibling	Method
fc	Method
layers	Method
for	O
cls	Method
and	O
reg	Method
,	O
also	O
in	O
a	O
per	O
-	O
class	O
form	O
.	O
This	O
R	Method
-	Method
CNN	Method
network	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
training	O
set	O
using	O
a	O
mini	O
-	O
batch	O
size	O
of	O
256	O
in	O
the	O
RoI	O
-	O
centric	O
fashion	O
.	O
For	O
testing	O
,	O
the	O
RPN	Method
generates	O
the	O
highest	O
scored	O
200	O
proposals	O
for	O
each	O
predicted	O
class	O
,	O
and	O
the	O
R	Method
-	Method
CNN	Method
network	O
is	O
used	O
to	O
update	O
these	O
proposals	O
'	O
scores	O
and	O
box	O
positions	O
.	O
This	O
method	O
reduces	O
the	O
top	O
-	O
5	O
localization	Task
error	Metric
to	O
10.6	O
%	O
(	O
Table	O
13	O
)	O
.	O
This	O
is	O
our	O
single	O
-	O
model	O
result	O
on	O
the	O
validation	O
set	O
.	O
Using	O
an	O
ensemble	Method
of	Method
networks	Method
for	O
both	O
classification	Task
and	O
localization	Task
,	O
we	O
achieve	O
a	O
top	O
-	O
5	O
localization	Task
error	Metric
of	O
9.0	O
%	O
on	O
the	O
test	O
set	O
.	O
This	O
number	O
significantly	O
outperforms	O
the	O
ILSVRC	Task
14	O
results	O
(	O
Table	O
14	O
)	O
,	O
showing	O
a	O
64	O
%	O
relative	O
reduction	O
of	O
error	Metric
.	O
This	O
result	O
won	O
the	O
1st	O
place	O
in	O
the	O
ImageNet	O
localization	Task
task	O
in	O
ILSVRC	Task
2015	O
.	O
section	O
:	O
