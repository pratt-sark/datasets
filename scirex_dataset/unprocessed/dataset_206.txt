document	O
:	O
Learning	O
to	O
Make	O
Predictions	Task
on	Task
Graphs	Task
with	O
Autoencoders	Method
We	O
examine	O
two	O
fundamental	O
tasks	O
associated	O
with	O
graph	Task
representation	Task
learning	Task
:	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
We	O
present	O
a	O
novel	O
autoencoder	Method
architecture	Method
capable	O
of	O
learning	O
a	O
joint	Method
representation	Method
of	O
both	O
local	O
graph	O
structure	O
and	O
available	O
node	O
features	O
for	O
the	O
multi	Task
-	Task
task	Task
learning	Task
of	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
.	O
Our	O
autoencoder	Method
architecture	Method
is	O
efficiently	O
trained	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
single	O
learning	Method
stage	Method
to	O
simultaneously	O
perform	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
,	O
whereas	O
previous	O
related	O
methods	O
require	O
multiple	O
training	O
steps	O
that	O
are	O
difficult	O
to	O
optimize	O
.	O
We	O
provide	O
a	O
comprehensive	O
empirical	O
evaluation	O
of	O
our	O
models	O
on	O
nine	O
benchmark	O
graph	O
-	O
structured	O
datasets	O
and	O
demonstrate	O
significant	O
improvement	O
over	O
related	O
methods	O
for	O
graph	Task
representation	Task
learning	Task
.	O
Reference	O
code	O
and	O
data	O
are	O
available	O
at	O
.	O
network	Task
embedding	Task
,	O
link	Task
prediction	Task
,	O
semi	Task
-	Task
supervised	Task
learning	Task
,	O
multi	Task
-	Task
task	Task
learning	Task
section	O
:	O
Introduction	O
A	O
s	O
the	O
world	O
is	O
becoming	O
increasingly	O
interconnected	O
,	O
graph	O
-	O
structured	O
data	O
are	O
also	O
growing	O
in	O
ubiquity	O
.	O
In	O
this	O
work	O
,	O
we	O
examine	O
the	O
task	O
of	O
learning	Task
to	O
make	O
predictions	Task
on	Task
graphs	Task
for	O
a	O
broad	O
range	O
of	O
real	Task
-	Task
world	Task
applications	Task
.	O
Specifically	O
,	O
we	O
study	O
two	O
canonical	Task
subtasks	Task
associated	O
with	O
graph	O
-	O
structured	O
datasets	O
:	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
(	O
LPNC	Task
)	O
.	O
A	O
graph	O
is	O
a	O
partially	O
observed	O
set	O
of	O
edges	O
and	O
nodes	O
(	O
or	O
vertices	O
)	O
,	O
and	O
the	O
learning	Task
task	Task
is	O
to	O
predict	O
the	O
labels	O
for	O
edges	O
and	O
nodes	O
.	O
In	O
real	Task
-	Task
world	Task
applications	Task
,	O
the	O
input	O
graph	O
is	O
a	O
network	O
with	O
nodes	O
representing	O
unique	O
entities	O
,	O
and	O
edges	O
representing	O
relationships	O
(	O
or	O
links	O
)	O
between	O
entities	O
.	O
Further	O
,	O
the	O
labels	O
of	O
nodes	O
and	O
edges	O
in	O
a	O
graph	O
are	O
often	O
correlated	O
,	O
exhibiting	O
complex	O
relational	O
structures	O
that	O
violate	O
the	O
general	O
assumption	O
of	O
independent	Method
and	Method
identical	Method
distribution	Method
fundamental	O
in	O
traditional	O
machine	Method
learning	Method
.	O
Therefore	O
,	O
models	O
capable	O
of	O
exploiting	O
topological	O
structures	O
of	O
graphs	O
have	O
been	O
shown	O
to	O
achieve	O
superior	O
predictive	Metric
performances	O
on	O
many	O
LPNC	Task
tasks	O
.	O
We	O
present	O
a	O
novel	O
densely	Method
connected	Method
autoencoder	Method
architecture	Method
capable	O
of	O
learning	O
a	O
shared	Method
representation	Method
of	Method
latent	Method
node	Method
embeddings	Method
from	O
both	O
local	O
graph	O
topology	O
and	O
available	O
explicit	O
node	O
features	O
for	O
LPNC	Task
.	O
The	O
resulting	O
autoencoder	Method
models	Method
are	O
useful	O
for	O
many	O
applications	O
across	O
multiple	O
domains	O
,	O
including	O
analysis	Task
of	Task
metabolic	Task
networks	Task
for	O
drug	Task
-	Task
target	Task
interaction	Task
,	O
bibliographic	Task
networks	Task
,	O
social	Task
networks	Task
such	O
as	O
Facebook	O
(	O
“	O
People	O
You	O
May	O
Know	O
”	O
)	O
,	O
terrorist	Task
networks	Task
,	O
communication	Task
networks	Task
,	O
cybersecurity	Task
,	O
recommender	Task
systems	Task
,	O
and	O
knowledge	O
bases	O
such	O
as	O
DBpedia	Material
and	O
Wikidata	Material
.	O
There	O
are	O
a	O
number	O
of	O
technical	O
challenges	O
associated	O
with	O
learning	Task
to	O
make	O
meaningful	Task
predictions	Task
on	O
complex	Task
graphs	Task
:	O
Extreme	Task
class	Task
imbalance	Task
:	O
in	O
link	Task
prediction	Task
,	O
the	O
number	O
of	O
known	O
present	O
(	O
positive	O
)	O
edges	O
is	O
often	O
significantly	O
less	O
than	O
the	O
number	O
of	O
known	O
absent	O
(	O
negative	O
)	O
edges	O
,	O
making	O
it	O
difficult	O
to	O
reliably	O
learn	O
from	O
rare	O
examples	O
;	O
Learn	O
from	O
complex	O
graph	O
structures	O
:	O
edges	O
may	O
be	O
directed	O
or	O
undirected	O
,	O
weighted	O
or	O
unweighted	O
,	O
highly	O
sparse	O
in	O
occurrence	O
,	O
and	O
/	O
or	O
consisting	O
of	O
multiple	O
types	O
.	O
A	O
useful	O
model	O
should	O
be	O
versatile	O
to	O
address	O
a	O
variety	O
of	O
graph	Task
types	Task
,	O
including	O
bipartite	O
graphs	O
;	O
Incorporate	O
side	O
information	O
:	O
nodes	O
(	O
and	O
maybe	O
edges	O
)	O
are	O
sometimes	O
described	O
by	O
a	O
set	O
of	O
features	O
,	O
called	O
side	O
information	O
,	O
that	O
could	O
encode	O
information	O
complementary	O
to	O
topological	O
features	O
of	O
the	O
input	O
graph	O
.	O
Such	O
explicit	O
data	O
on	O
nodes	O
and	O
edges	O
are	O
not	O
always	O
readily	O
available	O
and	O
are	O
considered	O
optional	O
.	O
A	O
useful	O
model	O
should	O
be	O
able	O
to	O
incorporate	O
optional	O
side	O
information	O
about	O
nodes	O
and	O
/	O
or	O
edges	O
,	O
whenever	O
available	O
,	O
to	O
potentially	O
improve	O
predictive	Task
performance	O
;	O
Efficiency	Metric
and	O
scalability	Metric
:	O
real	O
-	O
world	O
graph	O
datasets	O
contain	O
large	O
numbers	O
of	O
nodes	O
and	O
/	O
or	O
edges	O
.	O
It	O
is	O
essential	O
for	O
a	O
model	O
to	O
be	O
memory	O
and	O
computationally	O
efficient	O
to	O
achieve	O
practical	O
utility	O
on	O
real	Task
-	Task
world	Task
applications	Task
.	O
Our	O
contribution	O
in	O
this	O
work	O
is	O
a	O
simple	O
,	O
yet	O
versatile	O
autoencoder	Method
architecture	Method
that	O
addresses	O
all	O
of	O
the	O
above	O
technical	O
challenges	O
.	O
We	O
demonstrate	O
that	O
our	O
autoencoder	Method
models	Method
:	O
1	O
)	O
can	O
handle	O
extreme	O
class	O
imbalance	O
common	O
in	O
link	Task
prediction	Task
problems	O
;	O
2	O
)	O
can	O
learn	O
expressive	O
latent	O
features	O
for	O
nodes	O
from	O
topological	O
structures	O
of	O
sparse	O
,	O
bipartite	O
graphs	O
that	O
may	O
have	O
directed	O
and	O
/	O
or	O
weighted	O
edges	O
;	O
3	O
)	O
is	O
flexible	O
to	O
incorporate	O
explicit	O
side	O
features	O
about	O
nodes	O
as	O
an	O
optional	O
component	O
to	O
improve	O
predictive	Task
performance	O
;	O
and	O
4	O
)	O
utilize	O
extensive	O
parameter	O
sharing	O
to	O
reduce	O
memory	Metric
footprint	Metric
and	O
computational	Metric
complexity	Metric
,	O
while	O
leveraging	O
available	O
GPU	Method
-	Method
based	Method
implementations	Method
for	O
increased	O
scalability	O
.	O
Further	O
,	O
the	O
autoencoder	Method
architecture	Method
has	O
the	O
novelty	O
of	O
being	O
efficiently	O
trained	O
end	O
-	O
to	O
-	O
end	O
for	O
the	O
joint	O
,	O
multi	Task
-	Task
task	Task
learning	Task
(	O
MTL	Task
)	O
of	O
both	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
tasks	Task
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
architecture	O
capable	O
of	O
performing	O
simultaneous	Task
link	Task
prediction	Task
and	O
node	Task
classification	Task
in	O
a	O
single	O
learning	Method
stage	Method
,	O
whereas	O
previous	O
related	O
methods	O
require	O
multiple	O
training	O
stages	O
that	O
are	O
difficult	O
to	O
optimize	O
.	O
Lastly	O
,	O
we	O
conduct	O
a	O
comprehensive	O
evaluation	O
of	O
the	O
proposed	O
autoencoder	Method
architecture	Method
on	O
nine	O
challenging	O
benchmark	O
graph	O
-	O
structured	O
datasets	O
comprising	O
a	O
wide	O
range	O
of	O
LPNC	Task
applications	O
.	O
Numerical	O
experiments	O
validate	O
the	O
efficacy	O
of	O
our	O
models	O
by	O
showing	O
significant	O
improvement	O
on	O
multiple	O
evaluation	Metric
measures	Metric
over	O
related	O
methods	O
designed	O
for	O
link	Task
prediction	Task
and	O
/	O
or	O
node	Task
classification	Task
.	O
section	O
:	O
Autoencoder	Method
Architecture	Method
for	O
Link	Task
Prediction	Task
and	O
Node	Task
Classification	Task
We	O
now	O
characterize	O
our	O
proposed	O
autoencoder	Method
architecture	Method
,	O
schematically	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
for	O
LPNC	Task
and	O
formalize	O
the	O
notation	O
used	O
in	O
this	O
paper	O
.	O
The	O
input	O
to	O
the	O
autoencoder	Method
is	O
a	O
graph	O
of	O
nodes	O
.	O
Graph	Method
is	O
represented	O
by	O
its	O
adjacency	O
matrix	O
.	O
For	O
a	O
partially	O
observed	O
graph	O
,	O
,	O
where	O
denotes	O
a	O
known	O
present	O
positive	O
edge	O
,	O
denotes	O
a	O
known	O
absent	O
negative	O
edge	O
,	O
and	O
unk	O
denotes	O
an	O
unknown	O
status	O
(	O
missing	O
or	O
unobserved	O
)	O
edge	O
.	O
In	O
general	O
,	O
the	O
input	O
to	O
the	O
autoencoder	Method
can	O
be	O
directed	O
or	O
undirected	O
,	O
weighted	O
or	O
unweighted	O
,	O
and	O
/	O
or	O
bipartite	O
graphs	O
.	O
However	O
,	O
for	O
the	O
remainder	O
of	O
this	O
paper	O
and	O
throughout	O
the	O
numerical	O
experiments	O
,	O
we	O
assume	O
undirected	O
and	O
symmetric	O
graphs	O
with	O
binary	O
edges	O
to	O
maintain	O
parity	O
with	O
previous	O
related	O
work	O
.	O
Optionally	O
,	O
we	O
are	O
given	O
a	O
matrix	O
of	O
available	O
explicit	O
node	O
features	O
,	O
i.e.	O
side	O
information	O
.	O
The	O
aim	O
of	O
the	O
autoencoder	Method
model	Method
is	O
to	O
learn	O
a	O
set	O
of	O
low	O
-	O
dimensional	O
latent	O
variables	O
for	O
the	O
nodes	O
that	O
can	O
produce	O
an	O
approximate	O
reconstruction	O
output	O
such	O
that	O
the	O
error	O
between	O
and	O
is	O
minimized	O
,	O
thereby	O
preserving	O
the	O
global	O
graph	O
structure	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
capital	O
variables	O
(	O
e.g.	O
,	O
)	O
to	O
denote	O
matrices	O
and	O
lower	O
-	O
case	O
variables	O
(	O
e.g.	O
,	O
)	O
to	O
denote	O
row	O
vectors	O
.	O
For	O
example	O
,	O
we	O
use	O
to	O
mean	O
the	O
th	O
row	O
of	O
the	O
matrix	O
.	O
subsection	O
:	O
Link	Task
Prediction	Task
Research	O
on	O
link	Task
prediction	Task
attempts	O
to	O
answer	O
the	O
principal	O
question	O
:	O
given	O
two	O
entities	O
,	O
should	O
there	O
be	O
a	O
connection	O
between	O
them	O
?	O
We	O
focus	O
on	O
the	O
structural	O
link	Task
prediction	Task
problem	O
,	O
where	O
the	O
task	O
is	O
to	O
compute	O
the	O
likelihood	O
that	O
an	O
unobserved	O
or	O
missing	O
edge	O
exists	O
between	O
two	O
nodes	O
in	O
a	O
partially	O
observed	O
graph	O
.	O
For	O
a	O
comprehensive	O
survey	O
on	O
link	Task
prediction	Task
,	O
to	O
include	O
structural	O
and	O
temporal	O
link	Task
prediction	Task
using	O
unsupervised	Method
and	Method
supervised	Method
models	Method
,	O
see	O
.	O
Link	Task
Prediction	Task
from	O
Graph	O
Topology	O
Let	O
be	O
an	O
adjacency	O
vector	O
of	O
that	O
contains	O
the	O
local	O
neighborhood	O
of	O
the	O
th	O
node	O
.	O
Our	O
proposed	O
autoencoder	Method
architecture	Method
comprises	O
a	O
set	O
of	O
non	Method
-	Method
linear	Method
transformations	Method
on	O
summarized	O
in	O
two	O
component	O
parts	O
:	O
encoder	Method
,	O
and	O
decoder	Method
.	O
We	O
stack	O
two	O
layers	O
of	O
the	O
encoder	Method
part	Method
to	O
derive	O
-	O
dimensional	O
latent	Method
feature	Method
representation	Method
of	O
the	O
th	O
node	O
,	O
and	O
then	O
stack	O
two	O
layers	O
of	O
the	O
decoder	Method
part	Method
to	O
obtain	O
an	O
approximate	O
reconstruction	O
output	O
,	O
resulting	O
in	O
a	O
four	Method
-	Method
layer	Method
autoencoder	Method
architecture	Method
.	O
Note	O
that	O
is	O
highly	O
sparse	O
,	O
with	O
up	O
to	O
90	O
percent	O
of	O
the	O
edges	O
missing	O
at	O
random	O
in	O
some	O
of	O
our	O
experiments	O
,	O
and	O
the	O
dense	O
reconstructed	O
output	O
contains	O
the	O
predictions	O
for	O
the	O
missing	O
edges	O
.	O
The	O
hidden	Method
representations	Method
for	O
the	O
encoder	O
and	O
decoder	O
parts	O
are	O
computed	O
as	O
follows	O
:	O
The	O
choice	O
of	O
non	Method
-	Method
linear	Method
,	Method
element	Method
-	Method
wise	Method
activation	Method
function	Method
is	O
the	O
rectified	Method
linear	Method
unit	Method
.	O
The	O
last	O
decoder	Method
layer	Method
computes	O
a	O
linear	Method
transformation	Method
to	O
score	O
the	O
missing	O
links	O
as	O
part	O
of	O
the	O
reconstruction	Task
.	O
We	O
constrain	O
the	O
autoencoder	Method
to	O
be	O
symmetrical	O
with	O
shared	O
parameters	O
for	O
between	O
the	O
encoder	O
and	O
decoder	O
parts	O
,	O
resulting	O
in	O
almost	O
fewer	O
parameters	O
than	O
an	O
unconstrained	Method
architecture	Method
.	O
Parameter	Method
sharing	Method
is	O
a	O
powerful	O
form	O
of	O
regularization	Method
that	O
helps	O
improve	O
learning	Task
and	O
generalization	Task
,	O
and	O
is	O
also	O
the	O
main	O
motivation	O
for	O
MTL	Task
,	O
first	O
explored	O
in	O
and	O
most	O
recently	O
in	O
.	O
Notice	O
the	O
bias	O
units	O
do	O
not	O
share	O
parameters	O
,	O
and	O
,	O
are	O
transposed	O
copies	O
of	O
,	O
.	O
For	O
brevity	O
of	O
notation	O
,	O
we	O
summarize	O
the	O
parameters	O
to	O
be	O
learned	O
in	O
.	O
Since	O
our	O
autoencoder	Method
learns	O
node	O
embeddings	O
from	O
local	O
neighborhood	O
structures	O
of	O
the	O
graph	O
,	O
we	O
refer	O
to	O
it	O
as	O
LoNGAE	Method
for	O
Local	Method
Neighborhood	Method
Graph	Method
Autoencoder	Method
.	O
Link	Task
Prediction	Task
with	O
Node	O
Features	O
Optionally	O
,	O
if	O
a	O
matrix	O
of	O
explicit	O
node	O
features	O
is	O
available	O
,	O
then	O
we	O
concatenate	O
to	O
obtain	O
an	O
augmented	O
adjacency	O
matrix	O
and	O
perform	O
the	O
above	O
encoder	Method
-	Method
decoder	Method
transformations	Method
on	O
for	O
link	Task
prediction	Task
.	O
We	O
refer	O
to	O
this	O
variant	O
as	O
LoNGAE	Method
.	O
Notice	O
the	O
augmented	O
adjacency	O
matrix	O
is	O
no	O
longer	O
square	O
and	O
symmetric	O
.	O
The	O
intuition	O
behind	O
the	O
concatenation	O
of	O
node	O
features	O
is	O
to	O
enable	O
a	O
shared	O
representation	O
of	O
both	O
graph	O
and	O
node	O
features	O
throughout	O
the	O
autoencoding	O
transformations	O
by	O
way	O
of	O
the	O
tied	O
parameters	O
.	O
This	O
idea	O
draws	O
inspiration	O
from	O
recent	O
work	O
by	O
Vukotić	O
et	O
al	O
.	O
,	O
where	O
they	O
successfully	O
applied	O
symmetrical	Method
autoencoders	Method
with	O
parameter	Method
sharing	Method
for	O
multi	Task
-	Task
modal	Task
and	Task
cross	Task
-	Task
modal	Task
representation	Task
learning	Task
of	Task
textual	Task
and	Task
visual	Task
features	Task
.	O
The	O
training	Metric
complexity	Metric
of	O
LoNGAE	Method
is	O
,	O
where	O
is	O
the	O
number	O
of	O
nodes	O
,	O
is	O
the	O
dimensionality	O
of	O
node	O
features	O
,	O
is	O
the	O
size	O
of	O
the	O
hidden	O
layer	O
,	O
and	O
is	O
the	O
number	O
of	O
iterations	O
.	O
In	O
practice	O
,	O
,	O
,	O
and	O
are	O
independent	O
of	O
.	O
Thus	O
,	O
the	O
overall	O
complexity	Metric
of	O
the	O
autoencoder	Method
is	O
,	O
linear	O
in	O
the	O
number	O
of	O
nodes	O
.	O
Inference	Task
and	O
Learning	Task
During	O
the	O
forward	Task
pass	Task
,	O
or	O
inference	Task
,	O
the	O
model	O
takes	O
as	O
input	O
an	O
adjacency	O
vector	O
and	O
computes	O
its	O
reconstructed	O
output	O
for	O
link	Task
prediction	Task
.	O
The	O
parameters	O
are	O
learned	O
via	O
backpropagation	Method
.	O
During	O
the	O
backward	O
pass	O
,	O
we	O
estimate	O
by	O
minimizing	O
the	O
Masked	Metric
Balanced	Metric
Cross	Metric
-	Metric
Entropy	Metric
(	Metric
MBCE	Metric
)	Metric
loss	Metric
,	O
which	O
only	O
allows	O
for	O
the	O
contributions	O
of	O
those	O
parameters	O
associated	O
with	O
observed	O
edges	O
,	O
as	O
in	O
.	O
Moreover	O
,	O
can	O
exhibit	O
extreme	O
class	O
imbalance	O
between	O
known	O
present	O
and	O
absent	O
links	O
,	O
as	O
is	O
common	O
in	O
link	Task
prediction	Task
problems	O
.	O
We	O
handle	O
class	O
imbalance	O
by	O
defining	O
a	O
weighting	O
factor	O
to	O
be	O
used	O
as	O
a	O
multiplier	O
for	O
the	O
positive	O
class	O
in	O
the	O
cross	Method
-	Method
entropy	Method
loss	Method
formulation	Method
.	O
This	O
approach	O
is	O
referred	O
to	O
as	O
balanced	Method
cross	Method
-	Method
entropy	Method
.	O
Other	O
approaches	O
to	O
class	Task
imbalance	Task
include	O
optimizing	O
for	O
a	O
ranking	Task
loss	Task
and	O
the	O
recent	O
work	O
on	O
focal	Task
loss	Task
by	O
Lin	O
et	O
al	O
.	O
.	O
For	O
a	O
single	O
example	O
and	O
its	O
reconstructed	O
output	O
,	O
we	O
compute	O
the	O
MBCE	Metric
loss	Metric
as	O
follows	O
:	O
Here	O
,	O
is	O
the	O
balanced	O
cross	O
-	O
entropy	O
loss	O
with	O
weighting	O
factor	O
,	O
is	O
the	O
sigmoid	Method
function	Method
,	O
is	O
the	O
Hadamard	O
(	O
element	O
-	O
wise	O
)	O
product	O
,	O
and	O
is	O
the	O
boolean	O
function	O
:	O
if	O
,	O
else	O
.	O
The	O
same	O
autoencoder	Method
architecture	Method
can	O
be	O
applied	O
to	O
a	O
row	O
vector	O
in	O
the	O
augmented	O
adjacency	O
matrix	O
.	O
However	O
,	O
at	O
the	O
final	O
decoder	Method
layer	Method
,	O
we	O
slice	O
the	O
reconstruction	O
into	O
two	O
outputs	O
:	O
corresponding	O
to	O
the	O
reconstructed	O
example	O
in	O
the	O
original	O
adjacency	O
matrix	O
,	O
and	O
corresponding	O
to	O
the	O
reconstructed	O
example	O
in	O
the	O
matrix	O
of	O
node	O
features	O
.	O
During	O
learning	Task
,	O
we	O
optimize	O
on	O
the	O
concatenation	O
of	O
graph	O
topology	O
and	O
side	O
node	O
features	O
,	O
but	O
compute	O
the	O
losses	O
for	O
the	O
reconstructed	O
outputs	O
separately	O
with	O
different	O
loss	O
functions	O
.	O
The	O
motivation	O
behind	O
this	O
design	O
is	O
to	O
maintain	O
flexibility	O
to	O
handle	O
different	O
input	O
formats	O
;	O
the	O
input	O
is	O
usually	O
binary	O
,	O
but	O
the	O
input	O
can	O
be	O
binary	O
,	O
real	O
-	O
valued	O
,	O
or	O
both	O
.	O
In	O
this	O
work	O
,	O
we	O
enforce	O
both	O
inputs	O
to	O
be	O
in	O
the	O
range	O
for	O
simplicity	O
and	O
improved	O
performance	O
,	O
and	O
compute	O
the	O
augmented	Method
MBCE	Method
loss	Method
as	O
follows	O
:	O
where	O
is	O
the	O
standard	O
cross	Method
-	Method
entropy	Method
loss	Method
with	O
sigmoid	Method
function	Method
.	O
At	O
inference	O
time	O
,	O
we	O
use	O
the	O
reconstructed	O
output	O
for	O
link	Task
prediction	Task
and	O
disregard	O
the	O
output	O
.	O
subsection	O
:	O
Semi	O
-	O
Supervised	O
Node	Task
Classification	Task
The	O
LoNGAE	Method
model	O
can	O
also	O
be	O
used	O
to	O
perform	O
efficient	O
information	Task
propagation	Task
on	Task
graphs	Task
for	O
the	O
task	O
of	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
Node	Task
classification	Task
is	O
the	O
task	O
of	O
predicting	Task
the	Task
labels	Task
or	Task
types	Task
of	Task
entities	Task
in	Task
a	Task
graph	Task
,	O
such	O
as	O
the	O
types	O
of	O
molecules	O
in	O
a	O
metabolic	O
network	O
or	O
document	O
categories	O
in	O
a	O
citation	Task
network	Task
.	O
For	O
a	O
given	O
augmented	O
adjacency	O
vector	O
,	O
the	O
autoencoder	Method
learns	O
the	O
corresponding	O
node	O
embeddings	O
to	O
obtain	O
an	O
optimal	Task
reconstruction	Task
.	O
Intuitively	O
,	O
encodes	O
a	O
vector	O
of	O
latent	O
features	O
derived	O
from	O
the	O
concatenation	O
of	O
both	O
graph	O
and	O
node	O
features	O
,	O
and	O
can	O
be	O
used	O
to	O
predict	O
the	O
label	O
of	O
the	O
th	O
node	O
.	O
For	O
multi	Task
-	Task
class	Task
classification	Task
,	O
we	O
can	O
decode	O
using	O
the	O
softmax	Method
activation	Method
function	Method
to	O
learn	O
a	O
probability	O
distribution	O
over	O
node	O
labels	O
.	O
More	O
precisely	O
,	O
we	O
predict	O
node	O
labels	O
via	O
the	O
following	O
transformation	O
:	O
,	O
where	O
and	O
.	O
In	O
many	O
applications	O
,	O
only	O
a	O
small	O
fraction	O
of	O
the	O
nodes	O
are	O
labeled	O
.	O
For	O
semi	Task
-	Task
supervised	Task
learning	Task
,	O
it	O
is	O
advantageous	O
to	O
utilize	O
unlabeled	O
examples	O
in	O
conjunction	O
with	O
labeled	O
instances	O
to	O
better	O
capture	O
the	O
underlying	O
data	O
patterns	O
for	O
improved	O
learning	Task
and	O
generalization	Task
.	O
We	O
achieve	O
this	O
by	O
jointly	O
training	O
the	O
autoencoder	Method
with	O
a	O
masked	Method
softmax	Method
classifier	Method
to	O
collectively	O
learn	O
node	O
labels	O
from	O
minimizing	O
their	O
combined	O
losses	O
:	O
where	O
is	O
the	O
set	O
of	O
node	O
labels	O
,	O
if	O
node	O
belongs	O
to	O
class	O
,	O
is	O
the	O
softmax	O
probability	O
that	O
node	O
belongs	O
to	O
class	O
,	O
is	O
the	O
loss	O
defined	O
for	O
the	O
autoencoder	Method
,	O
and	O
the	O
boolean	O
function	O
if	O
node	O
has	O
a	O
label	O
,	O
otherwise	O
.	O
Notice	O
in	O
this	O
configuration	O
,	O
we	O
can	O
perform	O
multi	Task
-	Task
task	Task
learning	Task
for	O
both	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
,	O
simultaneously	O
.	O
section	O
:	O
Related	O
Work	O
The	O
field	O
of	O
graph	Task
representation	Task
learning	Task
is	O
seeing	O
a	O
resurgence	O
of	O
research	O
interest	O
in	O
recent	O
years	O
,	O
driven	O
in	O
part	O
by	O
the	O
latest	O
advances	O
in	O
deep	Method
learning	Method
.	O
The	O
aim	O
is	O
to	O
learn	O
a	O
mapping	Method
that	O
encodes	O
the	O
input	O
graph	O
into	O
low	O
-	O
dimensional	O
feature	O
embeddings	O
while	O
preserving	O
its	O
original	O
global	O
structure	O
.	O
Hamilton	O
et	O
al	O
.	O
succinctly	O
articulate	O
the	O
diverse	O
set	O
of	O
previously	O
proposed	O
approaches	O
for	O
graph	Task
representation	Task
learning	Task
,	O
or	O
graph	Task
embedding	Task
,	O
as	O
belonging	O
within	O
a	O
unified	O
encoder	Method
-	Method
decoder	Method
framework	Method
.	O
In	O
this	O
section	O
,	O
we	O
summarize	O
three	O
classes	O
of	O
encoder	Method
-	Method
decoder	Method
models	Method
most	O
related	O
to	O
our	O
work	O
:	O
matrix	Method
factorization	Method
(	O
MF	Method
)	O
,	O
autoencoders	Method
,	O
and	O
graph	Method
convolutional	Method
networks	Method
(	O
GCNs	Method
)	O
.	O
MF	Method
has	O
its	O
roots	O
in	O
dimensionality	Task
reduction	Task
and	O
gained	O
popularity	O
with	O
extensive	O
applications	O
in	O
collaborative	Task
filtering	Task
(	O
CF	Task
)	Task
and	O
recommender	Task
systems	Task
.	O
MF	Method
models	O
take	O
an	O
input	O
matrix	O
,	O
learn	O
a	O
shared	Method
linear	Method
latent	Method
representation	Method
for	O
rows	O
(	O
)	O
and	O
columns	O
(	O
)	O
during	O
an	O
encoder	O
step	O
,	O
and	O
then	O
use	O
a	O
bilinear	Method
(	Method
pairwise	Method
)	Method
decoder	Method
based	O
on	O
the	O
inner	Method
product	Method
to	O
produce	O
a	O
reconstructed	O
matrix	O
.	O
CF	Method
is	O
mathematically	O
similar	O
to	O
link	Task
prediction	Task
,	O
where	O
the	O
goal	O
is	O
essentially	O
matrix	Task
completion	Task
.	O
Menon	O
and	O
Elkan	O
proposed	O
an	O
MF	Method
model	O
capable	O
of	O
incorporating	O
side	O
information	O
about	O
nodes	O
and	O
/	O
or	O
edges	O
to	O
demonstrate	O
strong	O
link	Task
prediction	Task
results	O
on	O
several	O
challenging	O
network	O
datasets	O
.	O
Other	O
recent	O
approaches	O
similar	O
to	O
MF	Method
that	O
learn	O
node	O
embeddings	O
via	O
some	O
encoder	Method
transformation	Method
and	O
then	O
use	O
a	O
bilinear	Method
decoder	Method
for	O
the	O
reconstruction	Task
include	O
DeepWalk	Method
and	O
its	O
variants	O
LINE	Method
and	O
node2vec	Method
.	O
DeepWalk	Method
,	O
LINE	Method
,	O
and	O
node2vec	Method
do	O
not	O
support	O
external	O
node	O
/	O
edge	O
features	O
.	O
Our	O
work	O
is	O
inspired	O
by	O
recent	O
successful	O
applications	O
of	O
autoencoder	Method
architectures	Method
for	O
collaborative	Task
filtering	Task
that	O
outperform	O
popular	O
matrix	Method
factorization	Method
methods	O
,	O
and	O
is	O
related	O
to	O
Structural	Method
Deep	Method
Network	Method
Embedding	Method
(	O
SDNE	Method
)	O
for	O
link	Task
prediction	Task
.	O
Similar	O
to	O
SDNE	Method
,	O
our	O
models	O
rely	O
on	O
the	O
autoencoder	Method
to	O
learn	O
non	O
-	O
linear	O
node	O
embeddings	O
from	O
local	O
graph	O
neighborhoods	O
.	O
However	O
,	O
our	O
models	O
have	O
several	O
important	O
distinctions	O
:	O
1	O
)	O
we	O
leverage	O
extensive	O
parameter	O
sharing	O
between	O
the	O
encoder	Method
and	Method
decoder	Method
parts	Method
to	O
enhance	O
representation	Task
learning	Task
;	O
2	O
)	O
our	O
LoNGAE	Method
model	O
can	O
optionally	O
concatenate	O
side	O
node	O
features	O
to	O
the	O
adjacency	O
matrix	O
for	O
improved	O
link	Task
prediction	Task
performance	O
;	O
and	O
3	O
)	O
the	O
LoNGAE	Method
model	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
single	O
stage	O
for	O
multi	Task
-	Task
task	Task
learning	Task
of	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
On	O
the	O
other	O
hand	O
,	O
training	O
SDNE	Method
requires	O
multiple	O
steps	O
that	O
are	O
difficult	O
to	O
jointly	O
optimize	O
:	O
i	O
)	O
pre	O
-	O
training	O
via	O
a	O
deep	Method
belief	Method
network	Method
;	O
and	O
ii	O
)	O
utilizing	O
a	O
separate	O
downstream	Method
classifier	Method
on	O
top	O
of	O
node	Method
embeddings	Method
for	O
LPNC	Task
.	O
Lastly	O
,	O
GCNs	Method
are	O
a	O
recent	O
class	O
of	O
algorithms	O
based	O
on	O
convolutional	Method
encoders	Method
for	O
learning	Task
node	Task
embeddings	Task
.	O
The	O
GCN	Method
model	Method
is	O
motivated	O
by	O
a	O
localized	Method
first	Method
-	Method
order	Method
approximation	Method
of	Method
spectral	Method
convolutions	Method
for	O
layer	Task
-	Task
wise	Task
information	Task
propagation	Task
on	Task
graphs	Task
.	O
Similar	O
to	O
our	O
LoNGAE	Method
model	O
,	O
the	O
GCN	Method
model	Method
can	O
learn	O
hidden	Method
layer	Method
representations	Method
that	O
encode	O
both	O
local	O
graph	O
structure	O
and	O
features	O
of	O
nodes	O
.	O
The	O
choice	O
of	O
the	O
decoder	O
depends	O
on	O
the	O
task	O
.	O
For	O
link	Task
prediction	Task
,	O
the	O
bilinear	Method
inner	Method
product	Method
is	O
used	O
in	O
the	O
context	O
of	O
the	O
variational	Method
graph	Method
autoencoder	Method
(	O
VGAE	Method
)	O
.	O
For	O
semi	O
-	O
supervised	O
node	Task
classification	Task
,	O
the	O
softmax	O
activation	O
function	O
is	O
employed	O
.	O
The	O
GCN	Method
model	Method
provides	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
learning	Method
framework	Method
that	O
scales	O
linearly	O
in	O
the	O
number	O
of	O
graph	O
edges	O
and	O
has	O
been	O
shown	O
to	O
achieve	O
strong	O
LPNC	Task
results	O
on	O
a	O
number	O
of	O
graph	O
-	O
structured	O
datasets	O
.	O
However	O
,	O
the	O
GCN	Method
model	Method
has	O
a	O
drawback	O
of	O
being	O
memory	O
intensive	O
because	O
it	O
is	O
trained	O
on	O
the	O
full	O
dataset	O
using	O
batch	Method
gradient	Method
descent	Method
for	O
every	O
training	O
iteration	O
.	O
We	O
show	O
that	O
our	O
models	O
outperform	O
GCN	Method
-	Method
based	Method
models	Method
for	O
LPNC	Task
while	O
consuming	O
a	O
constant	O
memory	O
budget	O
by	O
way	O
of	O
mini	Method
-	Method
batch	Method
training	Method
.	O
section	O
:	O
Experimental	O
Design	O
In	O
this	O
section	O
,	O
we	O
expound	O
our	O
protocol	O
for	O
the	O
empirical	O
evaluation	O
of	O
our	O
models	O
’	O
capability	O
for	O
learning	Task
and	Task
generalization	Task
on	O
the	O
tasks	O
of	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
Secondarily	O
,	O
we	O
also	O
present	O
results	O
of	O
the	O
models	O
’	O
representation	Method
capacity	Method
on	O
the	O
task	O
of	O
network	Task
reconstruction	Task
.	O
subsection	O
:	O
Datasets	O
and	O
Baselines	O
We	O
evaluate	O
our	O
proposed	O
autoencoder	Method
models	Method
on	O
nine	O
graph	O
-	O
structured	O
datasets	O
,	O
spanning	O
multiple	O
application	O
domains	O
,	O
from	O
which	O
previous	O
graph	Method
embedding	Method
methods	Method
have	O
achieved	O
strong	O
results	O
for	O
LPNC	Task
.	O
The	O
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
and	O
include	O
networks	O
for	O
Protein	Material
interactions	O
,	O
Metabolic	Material
pathways	O
,	O
military	O
Conflict	Material
between	O
countries	O
,	O
the	O
U.S.	O
PowerGrid	O
,	O
collaboration	O
between	O
users	O
on	O
the	O
BlogCatalog	Material
social	O
website	O
,	O
and	O
publication	O
citations	O
from	O
the	O
Cora	Material
,	O
Citeseer	Material
,	O
Pubmed	Material
,	O
Arxiv	Material
-	Material
GRQC	Material
databases	O
.	O
{	O
Protein	Material
,	O
Metabolic	Material
,	O
Conflict	Material
,	O
PowerGrid	O
}	O
are	O
reported	O
in	O
.	O
{	O
Cora	Material
,	O
Citeseer	Material
,	O
Pubmed	Material
}	O
are	O
from	O
and	O
reported	O
in	O
.	O
And	O
{	O
Arxiv	Material
-	Material
GRQC	Material
,	O
BlogCatalog	Material
}	O
are	O
reported	O
in	O
.	O
width=0.5	O
width=0.5	O
We	O
empirically	O
compare	O
our	O
autoencoder	Method
models	Method
against	O
four	O
strong	O
baselines	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
,	O
which	O
were	O
designed	O
specifically	O
for	O
link	Task
prediction	Task
and	O
/	O
or	O
node	Task
classification	Task
.	O
We	O
begin	O
our	O
empirical	O
evaluation	O
with	O
the	O
SDNE	Method
baseline	O
,	O
where	O
we	O
compare	O
the	O
representation	Metric
capacity	Metric
of	O
our	O
models	O
on	O
the	O
network	Task
reconstruction	Task
task	Task
using	O
the	O
Arxiv	Material
-	Material
GRQC	Material
and	O
BlogCatalog	Material
datasets	Material
.	O
For	O
the	O
MF	Method
baseline	O
,	O
we	O
closely	O
follow	O
the	O
experimental	O
protocol	O
in	O
,	O
where	O
we	O
randomly	O
sample	O
10	O
percent	O
of	O
the	O
observed	O
links	O
for	O
training	O
and	O
evaluate	O
link	Task
prediction	Task
performance	O
on	O
the	O
other	O
disjoint	O
90	O
percent	O
for	O
the	O
{	O
Protein	Material
,	O
Metabolic	Material
,	O
Conflict	Material
}	O
datasets	O
.	O
For	O
PowerGrid	Material
,	O
we	O
use	O
90	O
percent	O
of	O
observed	O
links	O
for	O
training	O
and	O
evaluate	O
on	O
the	O
remaining	O
10	O
percent	O
.	O
And	O
for	O
the	O
VGAE	Method
and	O
GCN	O
baselines	O
,	O
we	O
use	O
the	O
same	O
train	Metric
/	Metric
validation	Metric
/	Metric
test	Metric
segments	Metric
described	O
in	O
and	O
for	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
,	O
respectively	O
,	O
on	O
the	O
{	O
Cora	Material
,	O
Citeseer	Material
,	O
Pubmed	Material
}	O
citation	O
networks	O
.	O
subsection	O
:	O
Implementation	O
Details	O
We	O
implement	O
the	O
autoencoder	Method
architecture	Method
using	O
Keras	Method
on	O
top	O
of	O
the	O
GPU	Method
-	Method
enabled	Method
TensorFlow	Method
backend	Method
,	O
along	O
with	O
several	O
additional	O
details	O
.	O
The	O
diagonal	O
elements	O
of	O
the	O
adjacency	O
matrix	O
are	O
set	O
to	O
with	O
the	O
interpretation	O
that	O
every	O
node	O
is	O
connected	O
to	O
itself	O
.	O
We	O
impute	O
missing	O
or	O
unk	O
elements	O
in	O
the	O
adjacency	O
matrix	O
with	O
.	O
Note	O
that	O
imputed	O
edges	O
are	O
not	O
observed	O
elements	O
in	O
the	O
adjacency	O
matrix	O
and	O
hence	O
do	O
not	O
contribute	O
to	O
the	O
masked	Task
loss	Task
computations	Task
during	O
training	Task
.	O
We	O
are	O
free	O
to	O
impute	O
any	O
values	O
for	O
the	O
missing	O
edges	O
,	O
but	O
through	O
cross	Metric
-	Metric
validation	Metric
we	O
found	O
that	O
the	O
uniform	O
value	O
of	O
produces	O
the	O
best	O
results	O
.	O
Hyper	Method
-	Method
parameter	Method
tuning	Method
is	O
performed	O
via	O
cross	Method
-	Method
validation	Method
or	O
on	O
the	O
available	O
validation	O
set	O
.	O
Key	O
hyper	O
-	O
parameters	O
include	O
mini	O
-	O
batch	O
size	O
,	O
dimensionality	O
of	O
the	O
hidden	O
layers	O
,	O
and	O
the	O
percentage	O
of	O
dropout	Method
regularization	Method
.	O
In	O
general	O
,	O
we	O
strive	O
to	O
keep	O
a	O
similar	O
set	O
of	O
hyper	O
-	O
parameters	O
across	O
datasets	O
to	O
highlight	O
the	O
consistency	O
of	O
our	O
models	O
.	O
In	O
all	O
experiments	O
,	O
the	O
dimensionality	O
of	O
the	O
hidden	O
layers	O
in	O
the	O
autoencoder	Method
architecture	Method
is	O
fixed	O
at	O
-	O
256	O
-	O
128	O
-	O
256	O
-	O
.	O
For	O
reconstruction	O
and	O
link	Task
prediction	Task
,	O
we	O
train	O
for	O
50	O
epochs	O
using	O
mini	O
-	O
batch	O
size	O
of	O
8	O
samples	O
.	O
For	O
node	Task
classification	Task
,	O
we	O
train	O
for	O
100	O
epochs	O
using	O
mini	O
-	O
batch	O
size	O
of	O
64	O
samples	O
.	O
We	O
utilize	O
early	O
stopping	O
as	O
a	O
form	O
of	O
regularization	O
in	O
time	O
when	O
the	O
model	O
shows	O
signs	O
of	O
overfitting	O
on	O
the	O
validation	O
set	O
.	O
We	O
apply	O
mean	Method
-	Method
variance	Method
normalization	Method
(	O
MVN	Method
)	O
after	O
each	O
ReLU	Method
activation	Method
layer	Method
to	O
help	O
improve	O
link	Task
prediction	Task
performance	O
,	O
where	O
it	O
compensates	O
for	O
noise	O
between	O
train	O
and	O
test	O
instances	O
by	O
normalizing	O
the	O
activations	O
to	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O
MVN	Method
enables	O
efficient	O
learning	Method
and	O
has	O
been	O
shown	O
effective	O
in	O
cardiac	Task
semantic	Task
segmentation	Task
and	O
speech	Task
recognition	Task
.	O
During	O
training	Task
,	O
we	O
apply	O
dropout	Method
regularization	Method
throughout	O
the	O
architecture	O
to	O
mitigate	O
overfitting	O
,	O
depending	O
on	O
the	O
sparsity	O
of	O
the	O
input	O
graph	O
.	O
For	O
link	Task
prediction	Task
,	O
dropout	Method
is	O
also	O
applied	O
at	O
the	O
input	O
layer	O
to	O
produce	O
an	O
effect	O
similar	O
to	O
using	O
a	O
denoising	Method
autoencoder	Method
.	O
This	O
denoising	Method
technique	Method
was	O
previously	O
employed	O
for	O
link	Task
prediction	Task
in	O
.	O
We	O
initialize	O
weights	O
according	O
to	O
the	O
Xavier	Method
scheme	Method
described	O
in	O
.	O
We	O
do	O
not	O
apply	O
weight	Method
decay	Method
regularization	Method
.	O
We	O
employ	O
the	O
Adam	Method
algorithm	Method
for	O
gradient	Task
descent	Task
optimization	Task
with	O
a	O
fixed	O
learning	Metric
rate	Metric
of	O
.	O
As	O
part	O
of	O
our	O
experimental	O
design	O
,	O
we	O
also	O
performed	O
experiments	O
without	O
parameter	O
sharing	O
between	O
the	O
encoder	Method
and	Method
decoder	Method
parts	Method
of	O
the	O
architecture	O
and	O
found	O
severely	O
degraded	O
predictive	Metric
performance	Metric
.	O
This	O
observation	O
is	O
consistent	O
with	O
prior	O
findings	O
that	O
parameter	Method
sharing	Method
helps	O
improve	O
generalization	Task
by	O
providing	O
additional	O
regularization	O
to	O
mitigate	O
the	O
adverse	O
effects	O
of	O
overfitting	O
and	O
enhance	O
representation	Method
learning	Method
.	O
subsection	O
:	O
Results	O
and	O
Analysis	O
Reconstruction	Task
Results	O
of	O
the	O
reconstruction	Task
task	Task
for	O
the	O
Arxiv	Material
-	Material
GRQC	Material
and	O
BlogCatalog	Material
network	O
datasets	O
are	O
illustrated	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
In	O
this	O
experiment	O
,	O
we	O
compare	O
the	O
results	O
obtained	O
by	O
our	O
LoNGAE	Method
model	O
to	O
those	O
obtained	O
by	O
the	O
related	O
autoencoder	O
-	O
based	O
SDNE	Method
model	O
.	O
The	O
evaluation	Metric
metric	Metric
is	O
precision@	Metric
,	O
which	O
is	O
a	O
rank	Metric
-	Metric
based	Metric
measure	Metric
used	O
in	O
information	Task
retrieval	Task
and	O
is	O
defined	O
as	O
the	O
proportion	O
of	O
retrieved	O
edges	O
/	O
links	O
in	O
the	O
top	O
-	O
set	O
that	O
are	O
relevant	O
.	O
We	O
use	O
precision@	Metric
to	O
evaluate	O
the	O
model	O
’s	O
ability	O
to	O
retrieve	O
edges	O
known	O
to	O
be	O
present	O
(	O
positive	O
edges	O
)	O
as	O
part	O
of	O
the	O
reconstruction	Task
.	O
width=0.8	O
In	O
comparison	O
to	O
SDNE	Method
,	O
we	O
show	O
that	O
our	O
LoNGAE	Method
model	O
achieves	O
better	O
precision@	Metric
performance	O
for	O
all	O
values	O
,	O
up	O
to	O
for	O
Arxiv	Material
-	Material
GRQC	Material
and	O
for	O
BlogCatalog	Material
,	O
when	O
trained	O
on	O
the	O
complete	O
datasets	O
.	O
We	O
also	O
systematically	O
test	O
the	O
capacity	O
of	O
the	O
LoNGAE	Method
model	O
to	O
reconstruct	O
the	O
original	O
networks	O
when	O
up	O
to	O
80	O
percent	O
of	O
the	O
edges	O
are	O
randomly	O
removed	O
,	O
akin	O
to	O
the	O
link	Task
prediction	Task
task	O
.	O
We	O
show	O
that	O
the	O
LoNGAE	Method
model	O
only	O
gets	O
worse	O
precision@	Metric
performance	O
than	O
SDNE	Method
on	O
the	O
Arxiv	Material
-	Material
GRQC	Material
dataset	O
when	O
more	O
than	O
40	O
percent	O
of	O
the	O
edges	O
are	O
missing	O
at	O
random	O
.	O
On	O
the	O
BlogCatalog	Material
dataset	O
,	O
the	O
LoNGAE	Method
model	O
achieves	O
better	O
precision@	Metric
performance	O
than	O
SDNE	Method
for	O
large	O
values	O
even	O
when	O
80	O
percent	O
of	O
the	O
edges	O
are	O
missing	O
at	O
random	O
.	O
This	O
experiment	O
demonstrates	O
the	O
superior	O
representation	Metric
capacity	Metric
of	O
our	O
LoNGAE	Method
model	O
compared	O
to	O
SDNE	Method
.	O
Link	Task
Prediction	Task
Table	O
[	O
reference	O
]	O
shows	O
the	O
comparison	O
between	O
our	O
autoencoder	Method
models	Method
and	O
the	O
matrix	Method
factorization	Method
(	O
MF	Method
)	O
model	O
proposed	O
in	O
for	O
link	Task
prediction	Task
with	O
and	O
without	O
node	O
features	O
.	O
Recall	O
that	O
our	O
goal	O
is	O
to	O
recover	O
the	O
statuses	O
of	O
the	O
missing	O
or	O
unknown	O
links	O
in	O
the	O
input	O
graph	O
.	O
As	O
part	O
of	O
the	O
experimental	O
design	O
,	O
we	O
pretend	O
that	O
a	O
randomly	O
selected	O
set	O
of	O
elements	O
in	O
the	O
adjacency	O
matrix	O
are	O
missing	O
and	O
collect	O
their	O
indices	O
to	O
be	O
used	O
as	O
a	O
validation	O
set	O
.	O
Our	O
task	O
is	O
to	O
train	O
the	O
autoencoder	Method
to	O
produce	O
a	O
set	O
of	O
predictions	O
,	O
a	O
list	O
of	O
ones	O
and	O
zeros	O
,	O
on	O
those	O
missing	O
indices	O
and	O
see	O
how	O
well	O
the	O
model	O
performs	O
when	O
compared	O
to	O
the	O
ground	O
-	O
truth	O
.	O
The	O
evaluation	Metric
metric	Metric
is	O
the	O
area	O
under	O
the	O
ROC	Metric
curve	Metric
(	O
AUC	Metric
)	O
.	O
Results	O
are	O
reported	O
as	O
mean	O
AUC	Metric
and	O
standard	O
deviation	O
over	O
10	Metric
-	Metric
fold	Metric
cross	Metric
-	Metric
validation	Metric
.	O
The	O
datasets	O
under	O
consideration	O
for	O
link	Task
prediction	Task
exhibit	O
varying	O
degrees	O
of	O
class	O
imbalance	O
.	O
For	O
featureless	O
link	Task
prediction	Task
,	O
our	O
LoNGAE	Method
model	O
marginally	O
outperforms	O
MF	Method
on	O
{	O
Protein	Material
,	O
Metabolic	Material
,	O
Conflict	Material
}	O
and	O
is	O
significantly	O
better	O
than	O
MF	Method
on	O
PowerGrid	Method
.	O
Consistent	O
with	O
MF	Method
results	O
,	O
we	O
observe	O
that	O
incorporating	O
external	O
node	O
features	O
provides	O
a	O
boost	O
in	O
link	Task
prediction	Task
accuracy	O
,	O
especially	O
for	O
the	O
Protein	Material
dataset	O
where	O
we	O
achieve	O
a	O
6	O
percent	O
increase	O
in	O
performance	O
.	O
Metabolic	Material
and	O
Conflict	Material
also	O
come	O
with	O
external	O
edge	O
features	O
,	O
which	O
were	O
exploited	O
by	O
the	O
MF	Method
model	O
for	O
further	O
performance	O
gains	O
.	O
We	O
leave	O
the	O
task	O
of	O
combining	O
edge	O
features	O
for	O
future	O
work	O
.	O
Each	O
node	O
in	O
Conflict	Material
only	O
has	O
three	O
features	O
,	O
which	O
are	O
unable	O
to	O
significantly	O
boost	O
link	Task
prediction	Task
accuracy	O
.	O
PowerGrid	Method
does	O
not	O
have	O
node	O
features	O
so	O
there	O
are	O
no	O
results	O
for	O
the	O
respective	O
rows	O
.	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
performances	O
between	O
our	O
autoencoder	Method
models	Method
and	O
related	O
graph	Method
embedding	Method
methods	Method
for	O
link	Task
prediction	Task
with	O
and	O
without	O
node	O
features	O
.	O
Following	O
the	O
protocol	O
described	O
in	O
,	O
we	O
report	O
AUC	Metric
and	O
average	Metric
precision	Metric
(	O
AP	Metric
)	O
scores	O
for	O
each	O
model	O
on	O
the	O
held	O
-	O
out	O
test	O
set	O
containing	O
10	O
percent	O
of	O
randomly	O
sampled	O
positive	O
links	O
and	O
the	O
same	O
number	O
of	O
negative	O
links	O
.	O
We	O
show	O
mean	O
AUC	Metric
and	O
AP	Metric
with	O
standard	O
error	O
over	O
10	O
runs	O
with	O
random	Method
weight	Method
initializations	Method
on	O
fixed	O
data	O
splits	O
.	O
Results	O
for	O
the	O
baseline	O
methods	O
are	O
taken	O
from	O
Kipf	Method
and	O
Welling	O
,	O
where	O
we	O
pick	O
the	O
best	O
performing	O
models	O
for	O
comparison	O
.	O
Similar	O
to	O
the	O
MF	Method
model	O
,	O
the	O
graph	Method
embedding	Method
methods	Method
that	O
can	O
combine	O
side	O
node	O
features	O
always	O
produce	O
a	O
boost	O
in	O
link	Task
prediction	Task
accuracy	O
.	O
In	O
this	O
comparison	O
,	O
we	O
significantly	O
outperform	O
the	O
best	O
graph	Method
embedding	Method
methods	Method
by	O
as	O
much	O
as	O
10	O
percent	O
,	O
with	O
and	O
without	O
node	O
features	O
.	O
Our	O
LoNGAE	Method
model	O
achieves	O
competitive	O
link	Task
prediction	Task
performance	O
when	O
compared	O
against	O
the	O
best	O
model	O
presented	O
in	O
on	O
the	O
Pubmed	Material
dataset	O
.	O
width=0.9	O
Node	Task
Classification	Task
Results	O
of	O
semi	O
-	O
supervised	O
node	Task
classification	Task
for	O
the	O
{	O
Cora	Material
,	O
Citeseer	Material
,	O
Pubmed	Material
}	O
datasets	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
In	O
this	O
context	O
of	O
citation	Task
networks	Task
,	O
node	Task
classification	Task
is	O
equivalent	O
to	O
the	O
task	O
of	O
document	Task
classification	Task
.	O
We	O
closely	O
follow	O
the	O
experimental	O
setup	O
of	O
Kipf	Method
and	O
Welling	O
,	O
where	O
we	O
use	O
their	O
provided	O
train	O
/	O
validation	O
/	O
test	O
splits	O
for	O
evaluation	O
.	O
Accuracy	Metric
performance	O
is	O
measured	O
on	O
the	O
held	O
-	O
out	O
test	O
set	O
of	O
1	O
,	O
000	O
examples	O
.	O
We	O
tune	O
hyper	O
-	O
parameters	O
on	O
the	O
validation	O
set	O
of	O
500	O
examples	O
.	O
The	O
train	O
set	O
only	O
contains	O
20	O
examples	O
per	O
class	O
.	O
All	O
methods	O
use	O
the	O
complete	O
adjacency	O
matrix	O
,	O
and	O
available	O
node	O
features	O
,	O
to	O
learn	O
latent	O
embeddings	O
for	O
node	Task
classification	Task
.	O
For	O
comparison	O
,	O
we	O
train	O
and	O
test	O
our	O
LoNGAE	Method
model	O
on	O
the	O
same	O
data	O
splits	O
over	O
10	O
runs	O
with	O
random	Method
weight	Method
initializations	Method
and	O
report	O
mean	Metric
accuracy	Metric
.	O
Kipf	O
and	O
Welling	O
report	O
their	O
mean	O
GCN	Method
and	Method
ICA	Method
results	O
on	O
the	O
same	O
data	O
splits	O
over	O
100	O
runs	O
with	O
random	Method
weight	Method
initializations	Method
.	O
The	O
other	O
baseline	O
methods	O
are	O
taken	O
from	O
Yang	O
et	O
al	O
.	O
.	O
In	O
this	O
comparison	O
,	O
our	O
LoNGAE	Method
model	O
achieves	O
competitive	O
performance	O
when	O
compared	O
against	O
the	O
GCN	Method
model	Method
on	O
the	O
Cora	Material
dataset	O
,	O
but	O
outperforms	O
GCN	Method
and	O
all	O
other	O
baseline	O
methods	O
on	O
the	O
Citeseer	Material
and	O
Pubmed	Material
datasets	Material
.	O
width=0.5	O
Multi	Task
-	Task
task	Task
Learning	Task
Lastly	O
,	O
we	O
report	O
LPNC	Task
results	O
obtained	O
by	O
our	O
LoNGAE	Method
model	O
in	O
the	O
MTL	Task
setting	O
over	O
10	O
runs	O
with	O
random	Method
weight	Method
initializations	Method
.	O
In	O
the	O
MTL	Task
scenario	O
,	O
the	O
LoNGAE	Method
model	O
takes	O
as	O
input	O
an	O
incomplete	O
graph	O
with	O
10	O
percent	O
of	O
the	O
positive	O
edges	O
,	O
and	O
the	O
same	O
number	O
of	O
negative	O
edges	O
,	O
missing	O
at	O
random	O
and	O
all	O
available	O
node	O
features	O
to	O
simultaneously	O
produce	O
predictions	O
for	O
the	O
missing	O
edges	O
and	O
labels	O
for	O
the	O
nodes	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
efficacy	O
of	O
the	O
LoNGAE	Method
model	O
for	O
MTL	Task
when	O
compared	O
against	O
the	O
best	O
performing	O
task	O
-	O
specific	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
models	O
,	O
which	O
require	O
the	O
complete	O
adjacency	O
matrix	O
as	O
input	O
.	O
For	O
link	Task
prediction	Task
,	O
multi	O
-	O
task	O
LoNGAE	Method
achieves	O
competitive	O
performance	O
against	O
task	O
-	O
specific	O
LoNGAE	Method
,	O
and	O
significantly	O
outperforms	O
the	O
best	O
VGAE	Method
model	O
from	O
Kipf	Method
and	O
Welling	Method
on	O
Cora	Material
and	O
Citeseer	Material
datasets	Material
.	O
For	O
node	Task
classification	Task
,	O
multi	O
-	O
task	O
LoNGAE	Method
is	O
the	O
best	O
performing	O
model	O
across	O
the	O
board	O
,	O
only	O
trailing	O
behind	O
the	O
GCN	Method
model	Method
on	O
the	O
Cora	Material
dataset	O
.	O
width=0.5	O
section	O
:	O
Discussion	O
In	O
our	O
experiments	O
,	O
we	O
show	O
that	O
a	O
simple	O
autoencoder	Method
architecture	Method
with	O
parameter	Method
sharing	Method
consistently	O
outperforms	O
previous	O
related	O
methods	O
on	O
a	O
range	O
of	O
challenging	O
graph	O
-	O
structured	O
benchmarks	O
for	O
three	O
separate	O
tasks	O
:	O
reconstruction	Task
,	O
link	Task
prediction	Task
,	O
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
For	O
the	O
reconstruction	Task
task	Task
,	O
our	O
LoNGAE	Method
model	O
achieves	O
superior	O
precision@	Metric
performance	O
when	O
compared	O
to	O
the	O
related	O
SDNE	Method
model	O
.	O
Although	O
both	O
models	O
leverage	O
a	O
deep	Method
autoencoder	Method
architecture	Method
for	O
graph	Method
representation	Method
learning	Method
,	O
the	O
SDNE	Method
model	O
lacks	O
several	O
key	O
implementations	O
necessary	O
for	O
enhanced	Task
representation	Task
capacity	Task
,	O
namely	O
parameter	Method
sharing	Method
between	O
the	O
encoder	Method
-	Method
decoder	Method
parts	Method
and	O
end	O
-	O
to	O
-	O
end	O
training	O
of	O
deep	Method
architectures	Method
.	O
For	O
link	Task
prediction	Task
,	O
we	O
observe	O
that	O
combining	O
available	O
node	O
features	O
always	O
produces	O
a	O
significant	O
boost	O
in	O
predictive	Metric
performance	Metric
.	O
This	O
observation	O
was	O
previously	O
reported	O
in	O
,	O
among	O
others	O
.	O
Intuitively	O
,	O
we	O
expect	O
topological	O
graph	O
features	O
provide	O
complementary	O
information	O
not	O
present	O
in	O
the	O
node	O
features	O
,	O
and	O
the	O
combination	O
of	O
both	O
feature	O
sets	O
should	O
improve	O
predictive	Metric
power	Metric
.	O
Although	O
explicit	O
node	O
features	O
may	O
not	O
always	O
be	O
readily	O
available	O
,	O
a	O
link	Task
prediction	Task
model	O
capable	O
of	O
incorporating	O
optional	O
side	O
information	O
has	O
broader	O
applicability	O
.	O
Our	O
LoNGAE	Method
model	O
also	O
performs	O
favorably	O
well	O
on	O
the	O
task	O
of	O
semi	O
-	O
supervised	O
node	Task
classification	Task
.	O
The	O
model	O
is	O
capable	O
of	O
encoding	O
non	O
-	O
linear	O
node	O
embeddings	O
from	O
both	O
local	O
graph	O
structure	O
and	O
explicit	O
node	O
features	O
,	O
which	O
can	O
be	O
decoded	O
by	O
a	O
softmax	Method
activation	Method
function	Method
to	O
yield	O
accurate	O
node	O
labels	O
.	O
The	O
efficacy	O
of	O
the	O
proposed	O
LoNGAE	Method
model	O
is	O
evident	O
especially	O
on	O
the	O
Pubmed	Material
dataset	O
,	O
where	O
the	O
label	Metric
rate	Metric
is	O
only	O
0.003	O
.	O
This	O
efficacy	O
is	O
attributed	O
to	O
parameter	Method
sharing	Method
being	O
used	O
in	O
the	O
autoencoder	Method
architecture	Method
,	O
which	O
provides	O
regularization	O
to	O
help	O
improve	O
representation	Task
learning	Task
and	O
generalization	Task
.	O
Our	O
autoencoder	Method
architecture	Method
naturally	O
supports	O
multi	Task
-	Task
task	Task
learning	Task
,	O
where	O
a	O
joint	Method
representation	Method
for	O
both	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
is	O
enabled	O
via	O
parameter	O
sharing	O
.	O
MTL	Task
aims	O
to	O
exploit	O
commonalities	O
and	O
differences	O
across	O
multiple	O
tasks	O
to	O
find	O
a	O
shared	O
representation	O
that	O
can	O
result	O
in	O
improved	O
performance	O
for	O
each	O
task	O
-	O
specific	O
metric	O
.	O
In	O
this	O
work	O
,	O
we	O
show	O
that	O
our	O
multi	O
-	O
task	O
LoNGAE	Method
model	O
improves	O
node	Task
classification	Task
accuracy	O
by	O
learning	O
to	O
predict	O
missing	O
edges	O
at	O
the	O
same	O
time	O
.	O
Our	O
multi	Method
-	Method
task	Method
model	Method
has	O
broad	O
practical	O
utility	O
to	O
address	O
real	Task
-	Task
world	Task
applications	Task
where	O
the	O
input	O
graphs	O
may	O
have	O
both	O
missing	O
edges	O
and	O
node	O
labels	O
.	O
Finally	O
,	O
we	O
address	O
one	O
major	O
limitation	O
associated	O
with	O
our	O
autoencoder	Method
models	Method
having	O
complexity	Metric
scale	O
linearly	O
in	O
the	O
number	O
of	O
nodes	O
.	O
Hamilton	O
et	O
al	O
.	O
express	O
that	O
the	O
complexity	O
in	O
nodes	O
may	O
limit	O
the	O
utility	O
of	O
the	O
models	O
on	O
massive	O
graphs	O
with	O
hundreds	O
of	O
millions	O
of	O
nodes	O
.	O
In	O
practice	O
,	O
we	O
would	O
implement	O
our	O
models	O
to	O
leverage	O
data	O
parallelism	O
across	O
commodity	O
CPU	O
and	O
/	O
or	O
GPU	O
resources	O
for	O
effective	O
distributed	Task
learning	Task
on	O
massive	Task
graphs	Task
.	O
Data	Task
parallelism	Task
is	O
possible	O
because	O
our	O
models	O
learn	O
node	O
embeddings	O
from	O
each	O
row	O
vector	O
of	O
the	O
adjacency	O
matrix	O
independently	O
.	O
Nevertheless	O
,	O
the	O
area	O
of	O
improvement	O
in	O
future	O
work	O
is	O
to	O
take	O
advantage	O
of	O
the	O
sparsity	O
of	O
edges	O
in	O
the	O
graphs	O
to	O
scale	O
our	O
models	O
linearly	O
in	O
the	O
number	O
of	O
observed	O
edges	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
presented	O
a	O
new	O
autoencoder	Method
architecture	Method
for	O
link	Task
prediction	Task
and	O
semi	O
-	O
supervised	O
node	Task
classification	Task
,	O
and	O
showed	O
that	O
the	O
resulting	O
models	O
outperform	O
related	O
methods	O
in	O
accuracy	Metric
performance	O
on	O
a	O
range	O
of	O
real	O
-	O
world	O
graph	O
-	O
structured	O
datasets	O
.	O
The	O
success	O
of	O
our	O
models	O
is	O
primarily	O
attributed	O
to	O
extensive	O
parameter	O
sharing	O
between	O
the	O
encoder	Method
and	Method
decoder	Method
parts	Method
of	O
the	O
architecture	O
,	O
coupled	O
with	O
the	O
capability	O
to	O
learn	O
expressive	O
non	Method
-	Method
linear	Method
latent	Method
node	Method
representations	Method
from	O
both	O
local	O
graph	O
neighborhoods	O
and	O
explicit	O
node	O
features	O
.	O
Further	O
,	O
our	O
novel	O
architecture	O
is	O
capable	O
of	O
simultaneous	O
multi	Task
-	Task
task	Task
learning	Task
of	O
both	O
link	Task
prediction	Task
and	O
node	Task
classification	Task
in	O
one	O
efficient	O
end	O
-	O
to	O
-	O
end	Task
training	Task
stage	Task
.	O
Our	O
work	O
provides	O
a	O
useful	O
framework	O
to	O
make	O
accurate	O
and	O
meaningful	O
predictions	O
on	O
a	O
diverse	O
set	O
of	O
complex	O
graph	O
structures	O
for	O
a	O
wide	O
range	O
of	O
real	Task
-	Task
world	Task
applications	Task
.	O
section	O
:	O
Acknowledgment	O
The	O
author	O
thanks	O
Edward	O
Raff	O
and	O
Jared	O
Sylvester	O
for	O
insightful	O
discussions	O
,	O
and	O
gracious	O
reviewers	O
for	O
constructive	O
feedback	O
on	O
the	O
paper	O
.	O
bibliography	O
:	O
References	O
