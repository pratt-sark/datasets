document	O
:	O
Wide	Method
Residual	Method
Networks	Method
Deep	Method
residual	Method
networks	Method
were	O
shown	O
to	O
be	O
able	O
to	O
scale	O
up	O
to	O
thousands	O
of	O
layers	O
and	O
still	O
have	O
improving	O
performance	O
.	O
However	O
,	O
each	O
fraction	O
of	O
a	O
percent	O
of	O
improved	O
accuracy	Metric
costs	O
nearly	O
doubling	O
the	O
number	O
of	O
layers	O
,	O
and	O
so	O
training	O
very	O
deep	Method
residual	Method
networks	Method
has	O
a	O
problem	O
of	O
diminishing	Task
feature	Task
reuse	Task
,	O
which	O
makes	O
these	O
networks	O
very	O
slow	O
to	O
train	O
.	O
To	O
tackle	O
these	O
problems	O
,	O
in	O
this	O
paper	O
we	O
conduct	O
a	O
detailed	O
experimental	O
study	O
on	O
the	O
architecture	O
of	O
ResNet	Method
blocks	Method
,	O
based	O
on	O
which	O
we	O
propose	O
a	O
novel	O
architecture	O
where	O
we	O
decrease	O
depth	O
and	O
increase	O
width	O
of	O
residual	Method
networks	Method
.	O
We	O
call	O
the	O
resulting	O
network	O
structures	O
wide	Method
residual	Method
networks	Method
(	O
WRNs	Method
)	O
and	O
show	O
that	O
these	O
are	O
far	O
superior	O
over	O
their	O
commonly	O
used	O
thin	O
and	O
very	O
deep	O
counterparts	O
.	O
For	O
example	O
,	O
we	O
demonstrate	O
that	O
even	O
a	O
simple	O
16	Method
-	Method
layer	Method
-	Method
deep	Method
wide	Method
residual	Method
network	Method
outperforms	O
in	O
accuracy	Metric
and	O
efficiency	O
all	O
previous	O
deep	Method
residual	Method
networks	Method
,	O
including	O
thousand	Method
-	Method
layer	Method
-	Method
deep	Method
networks	Method
,	O
achieving	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
CIFAR	Material
,	O
SVHN	Material
,	O
COCO	O
,	O
and	O
significant	O
improvements	O
on	O
ImageNet	O
.	O
Our	O
code	O
and	O
models	O
are	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
szagoruyko	O
/	O
wide	O
-	O
residual	O
-	O
networks	O
.	O
[	O
reference	O
]	O
[	O
reference	O
]	O
SergeyZagoruykosergey.zagoruyko@enpc.fr1	O
NikosKomodakisnikos.komodakis@enpc.fr1	O
UniversitéParis	O
-	O
Est	O
,	O
ÉcoledesPontsParisTech	O
Paris	O
,	O
France	O
SergeyZagoruykoandNikosKomodakisWideresidualnetworks	O
section	O
:	O
Introduction	O
Convolutional	Method
neural	Method
networks	Method
have	O
seen	O
a	O
gradual	O
increase	O
of	O
the	O
number	O
of	O
layers	O
in	O
the	O
last	O
few	O
years	O
,	O
starting	O
from	O
AlexNet	Method
,	O
VGG	Method
,	O
Inception	O
to	O
Residual	Method
networks	Method
,	O
corresponding	O
to	O
improvements	O
in	O
many	O
image	Task
recognition	Task
tasks	Task
.	O
The	O
superiority	O
of	O
deep	Method
networks	Method
has	O
been	O
spotted	O
in	O
several	O
works	O
in	O
the	O
recent	O
years	O
.	O
However	O
,	O
training	O
deep	Method
neural	Method
networks	Method
has	O
several	O
difficulties	O
,	O
including	O
exploding	O
/	O
vanishing	O
gradients	O
and	O
degradation	O
.	O
Various	O
techniques	O
were	O
suggested	O
to	O
enable	O
training	O
of	O
deeper	Method
neural	Method
networks	Method
,	O
such	O
as	O
well	O
-	O
designed	O
initialization	Method
strategies	Method
,	O
better	O
optimizers	Method
,	O
skip	O
connections	O
,	O
knowledge	Method
transfer	Method
and	O
layer	Method
-	Method
wise	Method
training	Method
.	O
The	O
latest	O
residual	Method
networks	Method
had	O
a	O
large	O
success	O
winning	O
ImageNet	O
and	O
COCO	O
2015	O
competition	O
and	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
several	O
benchmarks	O
,	O
including	O
object	Task
classification	Task
on	O
ImageNet	O
and	O
CIFAR	Material
,	O
object	Task
detection	Task
and	Task
segmentation	Task
on	O
PASCAL	O
VOC	O
and	O
MS	O
COCO	O
.	O
Compared	O
to	O
Inception	Method
architectures	Method
they	O
show	O
better	O
generalization	Task
,	O
meaning	O
the	O
features	O
can	O
be	O
utilized	O
in	O
transfer	Method
learning	Method
with	O
better	O
efficiency	O
.	O
Also	O
,	O
follow	O
-	O
up	O
work	O
showed	O
that	O
residual	O
links	O
speed	O
up	O
convergence	Task
of	O
deep	Method
networks	Method
.	O
Recent	O
follow	O
-	O
up	O
work	O
explored	O
the	O
order	O
of	O
activations	O
in	O
residual	Method
networks	Method
,	O
presenting	O
identity	O
mappings	O
in	O
residual	O
blocks	O
and	O
improving	O
training	O
of	O
very	Method
deep	Method
networks	Method
.	O
Successful	O
training	O
of	O
very	Task
deep	Task
networks	Task
was	O
also	O
shown	O
to	O
be	O
possible	O
through	O
the	O
use	O
of	O
highway	Method
networks	Method
,	O
which	O
is	O
an	O
architecture	O
that	O
had	O
been	O
proposed	O
prior	O
to	O
residual	Method
networks	Method
.	O
The	O
essential	O
difference	O
between	O
residual	Method
and	Method
highway	Method
networks	Method
is	O
that	O
in	O
the	O
latter	O
residual	O
links	O
are	O
gated	O
and	O
weights	O
of	O
these	O
gates	O
are	O
learned	O
.	O
Therefore	O
,	O
up	O
to	O
this	O
point	O
,	O
the	O
study	O
of	O
residual	Task
networks	Task
has	O
focused	O
mainly	O
on	O
the	O
order	O
of	O
activations	O
inside	O
a	O
ResNet	O
block	O
and	O
the	O
depth	Method
of	Method
residual	Method
networks	Method
.	O
In	O
this	O
work	O
we	O
attempt	O
to	O
conduct	O
an	O
experimental	O
study	O
that	O
goes	O
beyond	O
the	O
above	O
points	O
.	O
By	O
doing	O
so	O
,	O
our	O
goal	O
is	O
to	O
explore	O
a	O
much	O
richer	O
set	O
of	O
network	Method
architectures	Method
of	O
ResNet	Method
blocks	Method
and	O
thoroughly	O
examine	O
how	O
several	O
other	O
different	O
aspects	O
besides	O
the	O
order	O
of	O
activations	O
affect	O
performance	O
.	O
As	O
we	O
explain	O
below	O
,	O
such	O
an	O
exploration	O
of	O
architectures	O
has	O
led	O
to	O
new	O
interesting	O
findings	O
with	O
great	O
practical	O
importance	O
concerning	O
residual	Method
networks	Method
.	O
Width	O
vs	O
depth	O
in	O
residual	Method
networks	Method
.	O
The	O
problem	O
of	O
shallow	Task
vs	Task
deep	Task
networks	Task
has	O
been	O
in	O
discussion	O
for	O
a	O
long	O
time	O
in	O
machine	Task
learning	Task
with	O
pointers	O
to	O
the	O
circuit	Metric
complexity	Metric
theory	Metric
literature	Metric
showing	O
that	O
shallow	O
circuits	O
can	O
require	O
exponentially	O
more	O
components	O
than	O
deeper	O
circuits	O
.	O
The	O
authors	O
of	O
residual	Method
networks	Method
tried	O
to	O
make	O
them	O
as	O
thin	O
as	O
possible	O
in	O
favor	O
of	O
increasing	O
their	O
depth	O
and	O
having	O
less	O
parameters	O
,	O
and	O
even	O
introduced	O
a	O
¡	O
¡	O
bottleneck	O
¿	O
¿	O
block	O
which	O
makes	O
ResNet	Method
blocks	Method
even	O
thinner	O
.	O
[	O
basic	O
]	O
[	O
scale=0.4	O
]	O
images	O
/	O
basic_a.pdf	O
[	O
bottleneck	O
]	O
[	O
scale=0.4	O
]	O
images	O
/	O
basic_b.pdf	O
[	O
basic	O
-	O
wide	O
]	O
[	O
scale=0.4	O
]	O
images	O
/	O
basic_c.pdf	O
[	O
wide	Method
-	Method
dropout	Method
]	O
[	O
scale=0.4	O
]	O
images	O
/	O
basic_d.pdf	O
We	O
note	O
,	O
however	O
,	O
that	O
the	O
residual	Method
block	Method
with	O
identity	Method
mapping	Method
that	O
allows	O
to	O
train	O
very	O
deep	Method
networks	Method
is	O
at	O
the	O
same	O
time	O
a	O
weakness	O
of	O
residual	Method
networks	Method
.	O
As	O
gradient	O
flows	O
through	O
the	O
network	O
there	O
is	O
nothing	O
to	O
force	O
it	O
to	O
go	O
through	O
residual	O
block	O
weights	O
and	O
it	O
can	O
avoid	O
learning	O
anything	O
during	O
training	O
,	O
so	O
it	O
is	O
possible	O
that	O
there	O
is	O
either	O
only	O
a	O
few	O
blocks	O
that	O
learn	O
useful	O
representations	O
,	O
or	O
many	O
blocks	O
share	O
very	O
little	O
information	O
with	O
small	O
contribution	O
to	O
the	O
final	O
goal	O
.	O
This	O
problem	O
was	O
formulated	O
as	O
diminishing	Task
feature	Task
reuse	Task
in	O
.	O
The	O
authors	O
of	O
tried	O
to	O
address	O
this	O
problem	O
with	O
the	O
idea	O
of	O
randomly	O
disabling	O
residual	O
blocks	O
during	O
training	O
.	O
This	O
method	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
dropout	Method
,	O
where	O
each	O
residual	O
block	O
has	O
an	O
identity	O
scalar	O
weight	O
on	O
which	O
dropout	Method
is	O
applied	O
.	O
The	O
effectiveness	O
of	O
this	O
approach	O
proves	O
the	O
hypothesis	O
above	O
.	O
Motivated	O
by	O
the	O
above	O
observation	O
,	O
our	O
work	O
builds	O
on	O
top	O
of	O
and	O
tries	O
to	O
answer	O
the	O
question	O
of	O
how	O
wide	O
deep	Method
residual	Method
networks	Method
should	O
be	O
and	O
address	O
the	O
problem	O
of	O
training	Task
.	O
In	O
this	O
context	O
,	O
we	O
show	O
that	O
the	O
widening	O
of	O
ResNet	Method
blocks	Method
(	O
if	O
done	O
properly	O
)	O
provides	O
a	O
much	O
more	O
effective	O
way	O
of	O
improving	O
performance	O
of	O
residual	Method
networks	Method
compared	O
to	O
increasing	O
their	O
depth	O
.	O
In	O
particular	O
,	O
we	O
present	O
wider	O
deep	Method
residual	Method
networks	Method
that	O
significantly	O
improve	O
over	O
,	O
having	O
50	O
times	O
less	O
layers	O
and	O
being	O
more	O
than	O
2	O
times	O
faster	O
.	O
We	O
call	O
the	O
resulting	O
network	Method
architectures	Method
wide	Method
residual	Method
networks	Method
.	O
For	O
instance	O
,	O
our	O
wide	Method
16	Method
-	Method
layer	Method
deep	Method
network	Method
has	O
the	O
same	O
accuracy	Metric
as	O
a	O
1000	Method
-	Method
layer	Method
thin	Method
deep	Method
network	Method
and	O
a	O
comparable	O
number	O
of	O
parameters	O
,	O
although	O
being	O
several	O
times	O
faster	O
to	O
train	O
.	O
This	O
type	O
of	O
experiments	O
thus	O
seem	O
to	O
indicate	O
that	O
the	O
main	O
power	O
of	O
deep	Method
residual	Method
networks	Method
is	O
in	O
residual	O
blocks	O
,	O
and	O
that	O
the	O
effect	O
of	O
depth	O
is	O
supplementary	O
.	O
We	O
note	O
that	O
one	O
can	O
train	O
even	O
better	O
wide	Method
residual	Method
networks	Method
that	O
have	O
twice	O
as	O
many	O
parameters	O
(	O
and	O
more	O
)	O
,	O
which	O
suggests	O
that	O
to	O
further	O
improve	O
performance	O
by	O
increasing	O
depth	O
of	O
thin	Method
networks	Method
one	O
needs	O
to	O
add	O
thousands	O
of	O
layers	O
in	O
this	O
case	O
.	O
Use	O
of	O
dropout	Method
in	O
ResNet	Method
blocks	Method
.	O
Dropout	Method
was	O
first	O
introduced	O
in	O
and	O
then	O
was	O
adopted	O
by	O
many	O
successful	O
architectures	O
as	O
etc	O
.	O
It	O
was	O
mostly	O
applied	O
on	O
top	O
layers	O
that	O
had	O
a	O
large	O
number	O
of	O
parameters	O
to	O
prevent	O
feature	O
coadaptation	O
and	O
overfitting	O
.	O
It	O
was	O
then	O
mainly	O
substituted	O
by	O
batch	Method
normalization	Method
which	O
was	O
introduced	O
as	O
a	O
technique	O
to	O
reduce	O
internal	O
covariate	O
shift	O
in	O
neural	O
network	O
activations	O
by	O
normalizing	O
them	O
to	O
have	O
specific	O
distribution	O
.	O
It	O
also	O
works	O
as	O
a	O
regularizer	Method
and	O
the	O
authors	O
experimentally	O
showed	O
that	O
a	O
network	O
with	O
batch	Method
normalization	Method
achieves	O
better	O
accuracy	Metric
than	O
a	O
network	O
with	O
dropout	Method
.	O
In	O
our	O
case	O
,	O
as	O
widening	Task
of	Task
residual	Task
blocks	Task
results	O
in	O
an	O
increase	O
of	O
the	O
number	O
of	O
parameters	O
,	O
we	O
studied	O
the	O
effect	O
of	O
dropout	Method
to	O
regularize	Task
training	Task
and	O
prevent	O
overfitting	O
.	O
Previously	O
,	O
dropout	Method
in	Method
residual	Method
networks	Method
was	O
studied	O
in	O
with	O
dropout	Method
being	O
inserted	O
in	O
the	O
identity	O
part	O
of	O
the	O
block	O
,	O
and	O
the	O
authors	O
showed	O
negative	O
effects	O
of	O
that	O
.	O
Instead	O
,	O
we	O
argue	O
here	O
that	O
dropout	O
should	O
be	O
inserted	O
between	O
convolutional	Method
layers	Method
.	O
Experimental	O
results	O
on	O
wide	Method
residual	Method
networks	Method
show	O
that	O
this	O
leads	O
to	O
consistent	O
gains	O
,	O
yielding	O
even	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
(	O
e.g	O
,	O
16	Method
-	Method
layer	Method
-	Method
deep	Method
wide	Method
residual	Method
network	Method
with	O
dropout	Method
achieves	O
1.64	O
%	O
error	Metric
on	O
SVHN	Material
)	O
.	O
In	O
summary	O
,	O
the	O
contributions	O
of	O
this	O
work	O
are	O
as	O
follows	O
:	O
We	O
present	O
a	O
detailed	O
experimental	O
study	O
of	O
residual	Method
network	Method
architectures	Method
that	O
thoroughly	O
examines	O
several	O
important	O
aspects	O
of	O
ResNet	O
block	O
structure	O
.	O
We	O
propose	O
a	O
novel	O
widened	Method
architecture	Method
for	O
ResNet	Method
blocks	Method
that	O
allows	O
for	O
residual	Method
networks	Method
with	O
significantly	O
improved	O
performance	O
.	O
We	O
propose	O
a	O
new	O
way	O
of	O
utilizing	O
dropout	Method
within	O
deep	Method
residual	Method
networks	Method
so	O
as	O
to	O
properly	O
regularize	O
them	O
and	O
prevent	O
overfitting	O
during	O
training	O
.	O
Last	O
,	O
we	O
show	O
that	O
our	O
proposed	O
ResNet	Method
architectures	Method
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
several	O
datasets	O
dramatically	O
improving	O
accuracy	Metric
and	O
speed	Metric
of	O
residual	Method
networks	Method
.	O
section	O
:	O
Wide	Method
residual	Method
networks	Method
Residual	Method
block	Method
with	O
identity	Method
mapping	Method
can	O
be	O
represented	O
by	O
the	O
following	O
formula	O
:	O
where	O
and	O
are	O
input	O
and	O
output	O
of	O
the	O
-	O
th	O
unit	O
in	O
the	O
network	O
,	O
is	O
a	O
residual	O
function	O
and	O
are	O
parameters	O
of	O
the	O
block	O
.	O
Residual	Method
network	Method
consists	O
of	O
sequentially	O
stacked	O
residual	O
blocks	O
.	O
In	O
residual	Method
networks	Method
consisted	O
of	O
two	O
type	O
of	O
blocks	O
:	O
basic	O
-	O
with	O
two	O
consecutive	Method
convolutions	Method
with	O
batch	Method
normalization	Method
and	O
ReLU	Method
preceding	Method
convolution	Method
:	O
conv×33	Method
-	Method
conv×33	Method
Fig	O
.	O
[	O
reference	O
]	O
bottleneck	O
-	O
with	O
one	O
convolution	Method
surrounded	O
by	O
dimensionality	Method
reducing	Method
and	Method
expanding	Method
convolution	Method
layers	Method
:	O
conv×11	Method
-	Method
conv×33	Method
-	Method
conv×11	Method
Fig	O
.	O
[	O
reference	O
]	O
Compared	O
to	O
the	O
original	O
architecture	O
in	O
the	O
order	O
of	O
batch	O
normalization	O
,	O
activation	O
and	O
convolution	O
in	O
residual	O
block	O
was	O
changed	O
from	O
conv	Method
-	Method
BN	Method
-	Method
ReLU	Method
to	O
BN	Method
-	Method
ReLU	Method
-	Method
conv	Method
.	O
As	O
the	O
latter	O
was	O
shown	O
to	O
train	O
faster	O
and	O
achieve	O
better	O
results	O
we	O
do	O
n’t	O
consider	O
the	O
original	O
version	O
.	O
Furthermore	O
,	O
so	O
-	O
called	O
¡	O
¡	O
bottleneck	O
¿	O
¿	O
blocks	O
were	O
initially	O
used	O
to	O
make	O
blocks	O
less	O
computationally	O
expensive	O
to	O
increase	O
the	O
number	O
of	O
layers	O
.	O
As	O
we	O
want	O
to	O
study	O
the	O
effect	O
of	O
widening	O
and	O
¡	O
¡	O
bottleneck	O
¿	O
¿	O
is	O
used	O
to	O
make	O
networks	O
thinner	O
we	O
do	O
n’t	O
consider	O
it	O
too	O
,	O
focusing	O
instead	O
on	O
¡	O
¡	O
basic	O
¿	O
¿	O
residual	Method
architecture	Method
.	O
There	O
are	O
essentially	O
three	O
simple	O
ways	O
to	O
increase	O
representational	O
power	O
of	O
residual	O
blocks	O
:	O
to	O
add	O
more	O
convolutional	Method
layers	Method
per	O
block	O
to	O
widen	O
the	O
convolutional	Method
layers	Method
by	O
adding	O
more	O
feature	O
planes	O
to	O
increase	O
filter	O
sizes	O
in	O
convolutional	O
layers	O
As	O
small	O
filters	O
were	O
shown	O
to	O
be	O
very	O
effective	O
in	O
several	O
works	O
including	O
we	O
do	O
not	O
consider	O
using	O
filters	O
larger	O
than	O
.	O
Let	O
us	O
also	O
introduce	O
two	O
factors	O
,	O
deepening	O
factor	O
and	O
widening	O
factor	O
,	O
where	O
is	O
the	O
number	O
of	O
convolutions	O
in	O
a	O
block	O
and	O
multiplies	O
the	O
number	O
of	O
features	O
in	O
convolutional	O
layers	O
,	O
thus	O
the	O
baseline	O
¡	O
¡	O
basic	O
¿	O
¿	O
block	O
corresponds	O
to	O
,	O
.	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
schematic	O
examples	O
of	O
¡	O
¡	O
basic	O
¿	O
¿	O
and	O
¡	O
¡	O
basic	O
-	O
wide	O
¿	O
¿	O
blocks	O
respectively	O
.	O
The	O
general	O
structure	O
of	O
our	O
residual	Method
networks	Method
is	O
illustrated	O
in	O
table	O
[	O
reference	O
]	O
:	O
it	O
consists	O
of	O
an	O
initial	O
convolutional	Method
layer	Method
conv1	Method
that	O
is	O
followed	O
by	O
3	O
groups	O
(	O
each	O
of	O
size	O
)	O
of	O
residual	O
blocks	O
conv2	Method
,	O
conv3	Method
and	O
conv4	Method
,	O
followed	O
by	O
average	Method
pooling	Method
and	O
final	O
classification	Method
layer	Method
.	O
The	O
size	O
of	O
conv1	O
is	O
fixed	O
in	O
all	O
of	O
our	O
experiments	O
,	O
while	O
the	O
introduced	O
widening	O
factor	O
scales	O
the	O
width	O
of	O
the	O
residual	O
blocks	O
in	O
the	O
three	O
groups	O
conv2	O
-	O
4	O
(	O
e.g	O
,	O
the	O
original	O
¡	Method
¡	Method
basic	Method
¿	Method
¿	Method
architecture	Method
is	O
equivalent	O
to	O
)	O
.	O
We	O
want	O
to	O
study	O
the	O
effect	O
of	O
representational	O
power	O
of	O
residual	O
block	O
and	O
,	O
to	O
that	O
end	O
,	O
we	O
perform	O
and	O
test	O
several	O
modifications	O
to	O
the	O
¡	O
¡	O
basic	O
¿	O
¿	O
architecture	O
,	O
which	O
are	O
detailed	O
in	O
the	O
following	O
subsections	O
.	O
subsection	O
:	O
Type	O
of	O
convolutions	O
in	O
residual	O
block	O
Let	O
denote	O
residual	O
block	O
structure	O
,	O
where	O
is	O
a	O
list	O
with	O
the	O
kernel	O
sizes	O
of	O
the	O
convolutional	Method
layers	Method
in	O
a	O
block	O
.	O
For	O
example	O
,	O
denotes	O
a	O
residual	O
block	O
with	O
and	O
convolutional	O
layers	O
(	O
we	O
always	O
assume	O
square	O
spatial	O
kernels	O
)	O
.	O
Note	O
that	O
,	O
as	O
we	O
do	O
not	O
consider	O
¡	O
¡	O
bottleneck	O
¿	O
¿	O
blocks	O
as	O
explained	O
earlier	O
,	O
the	O
number	O
of	O
feature	O
planes	O
is	O
always	O
kept	O
the	O
same	O
across	O
the	O
block	O
.	O
We	O
would	O
like	O
to	O
answer	O
the	O
question	O
of	O
how	O
important	O
each	O
of	O
the	O
convolutional	Method
layers	Method
of	O
the	O
¡	O
¡	O
basic	O
¿	O
¿	O
residual	Method
architecture	Method
is	O
and	O
if	O
they	O
can	O
be	O
substituted	O
by	O
a	O
less	O
computationally	O
expensive	O
layer	O
or	O
even	O
a	O
combination	Method
of	Method
and	Method
convolutional	Method
layers	Method
,	O
e.g	O
,	O
or	O
.	O
This	O
can	O
increase	O
or	O
decrease	O
the	O
representational	O
power	O
of	O
the	O
block	O
.	O
We	O
thus	O
experiment	O
with	O
the	O
following	O
combinations	O
(	O
note	O
that	O
the	O
last	O
combination	O
,	O
i.e.	O
,	O
is	O
similar	O
to	O
effective	O
Network	Method
-	Method
in	Method
-	Method
Network	Method
architecture	Method
)	O
:	O
-	O
original	O
¡	O
¡	O
basic	O
¿	O
¿	O
block	O
-	O
with	O
one	O
extra	O
layer	O
-	O
with	O
the	O
same	O
dimensionality	O
of	O
all	O
convolutions	O
,	O
¡	O
¡	O
straightened	O
¿	O
¿	O
bottleneck	O
-	O
the	O
network	O
has	O
alternating	O
-	O
convolutions	O
everywhere	O
-	O
similar	O
idea	O
to	O
the	O
previous	O
block	Method
-	Method
Network	Method
-	Method
in	Method
-	Method
Network	Method
style	Method
block	Method
subsection	O
:	O
Number	O
of	O
convolutional	O
layers	O
per	O
residual	O
block	O
We	O
also	O
experiment	O
with	O
the	O
block	O
deepening	O
factor	O
to	O
see	O
how	O
it	O
affects	O
performance	O
.	O
The	O
comparison	O
has	O
to	O
be	O
done	O
among	O
networks	O
with	O
the	O
same	O
number	O
of	O
parameters	O
,	O
so	O
in	O
this	O
case	O
we	O
need	O
to	O
build	O
networks	O
with	O
different	O
and	O
(	O
where	O
denotes	O
the	O
total	O
number	O
of	O
blocks	O
)	O
while	O
ensuring	O
that	O
network	Metric
complexity	Metric
is	O
kept	O
roughly	O
constant	O
.	O
This	O
means	O
,	O
for	O
instance	O
,	O
that	O
should	O
decrease	O
whenever	O
increases	O
.	O
subsection	O
:	O
Width	O
of	O
residual	O
blocks	O
In	O
addition	O
to	O
the	O
above	O
modifications	O
,	O
we	O
experiment	O
with	O
the	O
widening	O
factor	O
of	O
a	O
block	O
.	O
While	O
the	O
number	O
of	O
parameters	O
increases	O
linearly	O
with	O
(	O
the	O
deepening	O
factor	O
)	O
and	O
(	O
the	O
number	O
of	O
ResNet	Method
blocks	Method
)	O
,	O
number	O
of	O
parameters	O
and	O
computational	Metric
complexity	Metric
are	O
quadratic	O
in	O
.	O
However	O
,	O
it	O
is	O
more	O
computationally	O
effective	O
to	O
widen	O
the	O
layers	O
than	O
have	O
thousands	O
of	O
small	O
kernels	O
as	O
GPU	Method
is	O
much	O
more	O
efficient	O
in	O
parallel	Task
computations	Task
on	O
large	Task
tensors	Task
,	O
so	O
we	O
are	O
interested	O
in	O
an	O
optimal	O
to	Metric
ratio	Metric
.	O
One	O
argument	O
for	O
wider	O
residual	Method
networks	Method
would	O
be	O
that	O
almost	O
all	O
architectures	O
before	O
residual	Method
networks	Method
,	O
including	O
the	O
most	O
successful	O
Inception	Method
and	O
VGG	Method
,	O
were	O
much	O
wider	O
compared	O
to	O
.	O
For	O
example	O
,	O
residual	Method
networks	Method
WRN	Method
-	Method
22	Method
-	Method
8	Method
and	O
WRN	Method
-	Method
16	Method
-	Method
10	Method
(	O
see	O
next	O
paragraph	O
for	O
explanation	O
of	O
this	O
notation	O
)	O
are	O
very	O
similar	O
in	O
width	O
,	O
depth	O
and	O
number	O
of	O
parameters	O
to	O
VGG	Method
architectures	Method
.	O
We	O
further	O
refer	O
to	O
original	O
residual	Method
networks	Method
with	O
as	O
¡	O
¡	O
thin	O
¿	O
¿	O
and	O
to	O
networks	O
with	O
as	O
¡	O
¡	O
wide¿¿.	O
In	O
the	O
rest	O
of	O
the	O
paper	O
we	O
use	O
the	O
following	O
notation	O
:	O
WRN	Method
-	Method
-	O
denotes	O
a	O
residual	Method
network	Method
that	O
has	O
a	O
total	O
number	O
of	O
convolutional	O
layers	O
and	O
a	O
widening	O
factor	O
(	O
for	O
example	O
,	O
network	O
with	O
40	O
layers	O
and	O
times	O
wider	O
than	O
original	O
would	O
be	O
denoted	O
as	O
WRN	Method
-	Method
40	Method
-	Method
2	Method
)	O
.	O
Also	O
,	O
when	O
applicable	O
we	O
append	O
block	O
type	O
,	O
e.g	O
WRN	Method
-	Method
40	Method
-	Method
2	Method
-	Method
.	O
subsection	O
:	O
Dropout	Task
in	Task
residual	Task
blocks	Task
As	O
widening	O
increases	O
the	O
number	O
of	O
parameters	O
we	O
would	O
like	O
to	O
study	O
ways	O
of	O
regularization	Task
.	O
Residual	Method
networks	Method
already	O
have	O
batch	Method
normalization	Method
that	O
provides	O
a	O
regularization	O
effect	O
,	O
however	O
it	O
requires	O
heavy	O
data	Task
augmentation	Task
,	O
which	O
we	O
would	O
like	O
to	O
avoid	O
,	O
and	O
it	O
’s	O
not	O
always	O
possible	O
.	O
We	O
add	O
a	O
dropout	Method
layer	Method
into	O
each	O
residual	O
block	O
between	O
convolutions	O
as	O
shown	O
in	O
fig	O
.	O
[	O
reference	O
]	O
and	O
after	O
ReLU	Method
to	O
perturb	O
batch	Task
normalization	Task
in	O
the	O
next	O
residual	O
block	O
and	O
prevent	O
it	O
from	O
overfitting	O
.	O
In	O
very	O
deep	Method
residual	Method
networks	Method
that	O
should	O
help	O
deal	O
with	O
diminishing	Task
feature	Task
reuse	Task
problem	Task
enforcing	Task
learning	Task
in	O
different	O
residual	O
blocks	O
.	O
section	O
:	O
Experimental	O
results	O
For	O
experiments	O
we	O
chose	O
well	O
-	O
known	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
SVHN	Material
and	O
ImageNet	O
image	O
classification	O
datasets	O
.	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	O
100	O
datasets	O
consist	O
of	O
color	O
images	O
drawn	O
from	O
10	O
and	O
100	O
classes	O
split	O
into	O
50	O
,	O
000	O
train	O
and	O
10	O
,	O
000	O
test	O
images	O
.	O
For	O
data	Task
augmentation	Task
we	O
do	O
horizontal	O
flips	O
and	O
take	O
random	O
crops	O
from	O
image	O
padded	O
by	O
4	O
pixels	O
on	O
each	O
side	O
,	O
filling	O
missing	O
pixels	O
with	O
reflections	O
of	O
original	O
image	O
.	O
We	O
do	O
n’t	O
use	O
heavy	O
data	Task
augmentation	Task
as	O
proposed	O
in	O
.	O
SVHN	Material
is	O
a	O
dataset	O
of	O
Google	Material
’s	Material
Street	Material
View	Material
House	Material
Numbers	Material
images	Material
and	O
contains	O
about	O
600	O
,	O
000	O
digit	O
images	O
,	O
coming	O
from	O
a	O
significantly	O
harder	O
real	O
world	O
problem	O
.	O
For	O
experiments	O
on	O
SVHN	Material
we	O
do	O
n’t	O
do	O
any	O
image	Task
preprocessing	Task
,	O
except	O
dividing	O
images	O
by	O
255	O
to	O
provide	O
them	O
in	O
[	O
0	O
,	O
1	O
]	O
range	O
as	O
input	O
.	O
All	O
of	O
our	O
experiments	O
except	O
ImageNet	O
are	O
based	O
on	O
architecture	Method
with	O
pre	O
-	O
activation	O
residual	O
blocks	O
and	O
we	O
use	O
it	O
as	O
baseline	O
.	O
For	O
ImageNet	O
,	O
we	O
find	O
that	O
using	O
pre	Method
-	Method
activation	Method
in	O
networks	O
with	O
less	O
than	O
100	O
layers	O
does	O
not	O
make	O
any	O
significant	O
difference	O
and	O
so	O
we	O
decide	O
to	O
use	O
the	O
original	O
ResNet	Method
architecture	Method
in	O
this	O
case	O
.	O
Unless	O
mentioned	O
otherwise	O
,	O
for	O
CIFAR	Material
we	O
follow	O
the	O
image	Method
preprocessing	Method
of	O
with	O
ZCA	Method
whitening	Method
.	O
However	O
,	O
for	O
some	O
CIFAR	Material
experiments	O
we	O
instead	O
use	O
simple	O
mean	Method
/	Method
std	Method
normalization	Method
such	O
that	O
we	O
can	O
directly	O
compare	O
with	O
and	O
other	O
ResNet	O
related	O
works	O
that	O
make	O
use	O
of	O
this	O
type	O
of	O
preprocessing	O
.	O
In	O
the	O
following	O
we	O
describe	O
our	O
findings	O
w.r.t	O
.	O
the	O
different	O
ResNet	Method
block	Method
architectures	Method
and	O
also	O
analyze	O
the	O
performance	O
of	O
our	O
proposed	O
wide	Method
residual	Method
networks	Method
.	O
We	O
note	O
that	O
for	O
all	O
experiments	O
related	O
to	O
¡	O
¡	O
type	O
of	O
convolutions	O
in	O
a	O
block	O
¿	O
¿	O
and	O
¡	O
¡	O
number	O
of	O
convolutions	O
per	O
block	O
¿	O
¿	O
we	O
use	O
and	O
reduced	O
depth	O
compared	O
to	O
in	O
order	O
to	O
speed	O
up	O
training	Task
.	O
subsubsection	O
:	O
Type	O
of	O
convolutions	Method
in	O
a	O
block	O
We	O
start	O
by	O
reporting	O
results	O
using	O
trained	Method
networks	Method
with	O
different	O
block	O
types	O
(	O
reported	O
results	O
are	O
on	O
CIFAR	Material
-	Material
10	Material
)	O
.	O
We	O
used	O
WRN	Method
-	Method
40	Method
-	Method
2	Method
for	O
blocks	O
,	O
,	O
and	O
as	O
these	O
blocks	O
have	O
only	O
one	O
convolution	O
.	O
To	O
keep	O
the	O
number	O
of	O
parameters	O
comparable	O
we	O
trained	O
other	O
networks	O
with	O
less	O
layers	O
:	O
WRN	Method
-	Method
28	Method
-	Method
2	Method
-	Method
and	O
WRN	Method
-	Method
22	Method
-	Method
2	Method
-	Method
.	O
We	O
provide	O
the	O
results	O
including	O
test	Metric
accuracy	Metric
in	O
median	O
over	O
5	O
runs	O
and	O
time	O
per	O
training	O
epoch	O
in	O
the	O
table	O
[	O
reference	O
]	O
.	O
Block	Method
turned	O
out	O
to	O
be	O
the	O
best	O
by	O
a	O
little	O
margin	O
,	O
and	O
with	O
are	O
very	O
close	O
to	O
in	O
accuracy	Metric
having	O
less	O
parameters	O
and	O
less	O
layers	O
.	O
is	O
faster	O
than	O
others	O
by	O
a	O
small	O
margin	O
.	O
Based	O
on	O
the	O
above	O
,	O
blocks	O
with	O
comparable	O
number	O
of	O
parameters	O
turned	O
out	O
to	O
give	O
more	O
or	O
less	O
the	O
same	O
results	O
.	O
Due	O
to	O
this	O
fact	O
,	O
we	O
hereafter	O
restrict	O
our	O
attention	O
to	O
only	O
WRNs	Method
with	O
convolutions	Method
so	O
as	O
to	O
be	O
also	O
consistent	O
with	O
other	O
methods	O
.	O
subsubsection	O
:	O
Number	O
of	O
convolutions	Method
per	O
block	O
We	O
next	O
proceed	O
with	O
the	O
experiments	O
related	O
to	O
varying	O
the	O
deepening	O
factor	O
(	O
which	O
represents	O
the	O
number	O
of	O
convolutional	O
layers	O
per	O
block	O
)	O
.	O
We	O
show	O
indicative	O
results	O
in	O
table	O
[	O
reference	O
]	O
,	O
where	O
in	O
this	O
case	O
we	O
took	O
WRN	Method
-	Method
40	Method
-	O
2	O
with	O
convolutions	Method
and	O
trained	O
several	O
networks	O
with	O
different	O
deepening	O
factor	O
,	O
same	O
number	O
of	O
parameters	O
(	O
2.2	O
)	O
and	O
same	O
number	O
of	O
convolutional	Method
layers	Method
.	O
As	O
can	O
be	O
noticed	O
,	O
turned	O
out	O
to	O
be	O
the	O
best	O
,	O
whereas	O
and	O
had	O
the	O
worst	O
performance	O
.	O
We	O
speculate	O
that	O
this	O
is	O
probably	O
due	O
to	O
the	O
increased	O
difficulty	O
in	O
optimization	Task
as	O
a	O
result	O
of	O
the	O
decreased	O
number	O
of	O
residual	O
connections	O
in	O
the	O
last	O
two	O
cases	O
.	O
Furthermore	O
,	O
turned	O
out	O
to	O
be	O
quite	O
worse	O
.	O
The	O
conclusion	O
is	O
that	O
is	O
optimal	O
in	O
terms	O
of	O
number	O
of	O
convolutions	O
per	O
block	O
.	O
For	O
this	O
reason	O
,	O
in	O
the	O
remaining	O
experiments	O
we	O
only	O
consider	O
wide	Method
residual	Method
networks	Method
with	O
a	O
block	O
of	O
type	O
.	O
subsubsection	O
:	O
Width	O
of	O
residual	O
blocks	O
As	O
we	O
try	O
to	O
increase	O
widening	O
parameter	O
we	O
have	O
to	O
decrease	O
total	O
number	O
of	O
layers	O
.	O
To	O
find	O
an	O
optimal	O
ratio	O
we	O
experimented	O
with	O
from	O
2	O
to	O
12	O
and	O
depth	O
from	O
16	O
to	O
40	O
.	O
The	O
results	O
are	O
presented	O
in	O
table	O
[	O
reference	O
]	O
.	O
As	O
can	O
be	O
seen	O
,	O
all	O
networks	O
with	O
40	O
,	O
22	O
and	O
16	O
layers	O
see	O
consistent	O
gains	O
when	O
width	O
is	O
increased	O
by	O
1	O
to	O
12	O
times	O
.	O
On	O
the	O
other	O
hand	O
,	O
when	O
keeping	O
the	O
same	O
fixed	O
widening	O
factor	O
or	O
and	O
varying	O
depth	O
from	O
16	O
to	O
28	O
there	O
is	O
a	O
consistent	O
improvement	O
,	O
however	O
when	O
we	O
further	O
increase	O
depth	O
to	O
40	O
accuracy	Metric
decreases	O
(	O
e.g	O
,	O
WRN	Method
-	Method
40	Method
-	Method
8	Method
loses	O
in	O
accuracy	Metric
to	O
WRN	Method
-	Method
22	Method
-	Method
8	Method
)	O
.	O
We	O
show	O
additional	O
results	O
in	O
table	O
[	O
reference	O
]	O
where	O
we	O
compare	O
thin	O
and	O
wide	Method
residual	Method
networks	Method
.	O
As	O
can	O
be	O
observed	O
,	O
wide	O
WRN	Method
-	Method
40	Method
-	O
4	O
compares	O
favorably	O
to	O
thin	O
ResNet	Method
-	Method
1001	Method
as	O
it	O
achieves	O
better	O
accuracy	Metric
on	O
both	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
.	O
Yet	O
,	O
it	O
is	O
interesting	O
that	O
these	O
networks	O
have	O
comparable	O
number	O
of	O
parameters	O
,	O
8.9	O
and	O
10.2	O
,	O
suggesting	O
that	O
depth	O
does	O
not	O
add	O
regularization	O
effects	O
compared	O
to	O
width	O
at	O
this	O
level	O
.	O
As	O
we	O
show	O
further	O
in	O
benchmarks	O
,	O
WRN	Method
-	Method
40	Method
-	Method
4	Method
is	O
8	O
times	O
faster	O
to	O
train	O
,	O
so	O
evidently	O
depth	O
to	O
width	O
ratio	O
in	O
the	O
original	O
thin	Method
residual	Method
networks	Method
is	O
far	O
from	O
optimal	O
.	O
Also	O
,	O
wide	Method
WRN	Method
-	Method
28	Method
-	Method
10	Method
outperforms	O
thin	O
ResNet	Method
-	Method
1001	Method
by	O
0.92	O
%	O
(	O
with	O
the	O
same	O
minibatch	O
size	O
during	O
training	O
)	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
3.46	O
%	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
having	O
36	O
times	O
less	O
layers	O
(	O
see	O
table	O
[	O
reference	O
]	O
)	O
.	O
We	O
note	O
that	O
the	O
result	O
of	O
4.64	O
%	O
with	O
ResNet	Method
-	Method
1001	Method
was	O
obtained	O
with	O
batch	O
size	O
64	O
,	O
whereas	O
we	O
use	O
a	O
batch	O
size	O
128	O
in	O
all	O
of	O
our	O
experiments	O
(	O
i.e.	O
,	O
all	O
other	O
results	O
reported	O
in	O
table	O
[	O
reference	O
]	O
are	O
with	O
batch	O
size	O
128	O
)	O
.	O
Training	O
curves	O
for	O
these	O
networks	O
are	O
presented	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Despite	O
previous	O
arguments	O
that	O
depth	O
gives	O
regularization	O
effects	O
and	O
width	O
causes	O
network	O
to	O
overfit	O
,	O
we	O
successfully	O
train	O
networks	O
with	O
several	O
times	O
more	O
parameters	O
than	O
ResNet	Method
-	Method
1001	Method
.	O
For	O
instance	O
,	O
wide	O
WRN	Method
-	Method
28	Method
-	Method
10	Method
(	O
table	O
[	O
reference	O
]	O
)	O
and	O
wide	Method
WRN	Method
-	Method
40	Method
-	O
10	O
(	O
table	O
[	O
reference	O
]	O
)	O
have	O
respectively	O
and	O
times	O
more	O
parameters	O
than	O
ResNet	Method
-	Method
1001	Method
and	O
both	O
outperform	O
it	O
by	O
a	O
significant	O
margin	O
.	O
In	O
general	O
,	O
we	O
observed	O
that	O
CIFAR	Material
mean	O
/	O
std	O
preprocessing	O
allows	O
training	O
wider	O
and	O
deeper	Method
networks	Method
with	O
better	O
accuracy	Metric
,	O
and	O
achieved	O
18.3	O
%	O
on	O
CIFAR	Material
-	Material
100	Material
using	O
WRN	Method
-	Method
40	Method
-	O
10	O
with	O
parameters	O
(	O
table	O
[	O
reference	O
]	O
)	O
,	O
giving	O
a	O
total	O
improvement	O
of	O
4.4	O
%	O
over	O
ResNet	Method
-	Method
1001	Method
and	O
establishing	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
this	O
dataset	O
.	O
To	O
summarize	O
:	O
widening	O
consistently	O
improves	O
performance	O
across	O
residual	Method
networks	Method
of	O
different	O
depth	O
;	O
increasing	O
both	O
depth	O
and	O
width	O
helps	O
until	O
the	O
number	O
of	O
parameters	O
becomes	O
too	O
high	O
and	O
stronger	O
regularization	O
is	O
needed	O
;	O
there	O
does	O
n’t	O
seem	O
to	O
be	O
a	O
regularization	O
effect	O
from	O
very	O
high	O
depth	O
in	O
residual	Method
networks	Method
as	O
wide	Method
networks	Method
with	O
the	O
same	O
number	O
of	O
parameters	O
as	O
thin	O
ones	O
can	O
learn	O
same	O
or	O
better	O
representations	O
.	O
Furthermore	O
,	O
wide	Method
networks	Method
can	O
successfully	O
learn	O
with	O
a	O
2	O
or	O
more	O
times	O
larger	O
number	O
of	O
parameters	O
than	O
thin	O
ones	O
,	O
which	O
would	O
require	O
doubling	O
the	O
depth	O
of	O
thin	Method
networks	Method
,	O
making	O
them	O
infeasibly	O
expensive	O
to	O
train	O
.	O
[	O
scale=0.42	O
]	O
.	O
/	O
images	O
/	O
cifar10.pdf	O
[	O
scale=0.42	O
]	O
.	O
/	O
images	O
/	O
cifar100.pdf	O
subsubsection	O
:	O
Dropout	Method
in	O
residual	O
blocks	O
We	O
trained	O
networks	O
with	O
dropout	O
inserted	O
into	O
residual	O
block	O
between	O
convolutions	Method
on	O
all	O
datasets	O
.	O
We	O
used	O
cross	Method
-	Method
validation	Method
to	O
determine	O
dropout	Metric
probability	Metric
values	Metric
,	O
0.3	O
on	O
CIFAR	Material
and	O
0.4	O
on	O
SVHN	Material
.	O
Also	O
,	O
we	O
did	O
n’t	O
have	O
to	O
increase	O
number	O
of	O
training	O
epochs	O
compared	O
to	O
baseline	Method
networks	Method
without	O
dropout	Method
.	O
Dropout	Method
decreases	O
test	O
error	Metric
on	O
CIFAR	Material
-	Material
10	Material
and	O
CIFAR	Material
-	Material
100	Material
by	O
0.11	O
%	O
and	O
0.4	O
%	O
correnspondingly	O
(	O
over	O
median	O
of	O
5	O
runs	O
and	O
mean	Method
/	Method
std	Method
preprocessing	Method
)	O
with	O
WRN	Method
-	Method
28	Method
-	O
10	O
,	O
and	O
gives	O
improvements	O
with	O
other	O
ResNets	Method
as	O
well	O
(	O
table	O
[	O
reference	O
]	O
)	O
.	O
To	O
our	O
knowledge	O
,	O
that	O
was	O
the	O
first	O
result	O
to	O
approach	O
20	O
%	O
error	Metric
on	O
CIFAR	Material
-	Material
100	Material
,	O
even	O
outperforming	O
methods	O
with	O
heavy	O
data	Method
augmentation	Method
.	O
There	O
is	O
only	O
a	O
slight	O
drop	O
in	O
accuracy	Metric
with	O
WRN	Method
-	Method
16	Method
-	Method
4	Method
on	O
CIFAR	Material
-	Material
10	Material
which	O
we	O
speculate	O
is	O
due	O
to	O
the	O
relatively	O
small	O
number	O
of	O
parameters	O
.	O
We	O
notice	O
a	O
disturbing	O
effect	O
in	O
residual	Method
network	Method
training	Method
after	O
the	O
first	O
learning	Metric
rate	Metric
drop	Metric
when	O
both	O
loss	Metric
and	O
validation	Metric
error	Metric
suddenly	O
start	O
to	O
go	O
up	O
and	O
oscillate	O
on	O
high	O
values	O
until	O
the	O
next	O
learning	Metric
rate	Metric
drop	Metric
.	O
We	O
found	O
out	O
that	O
it	O
is	O
caused	O
by	O
weight	Method
decay	Method
,	O
however	O
making	O
it	O
lower	O
leads	O
to	O
a	O
significant	O
drop	O
in	O
accuracy	Metric
.	O
Interestingly	O
,	O
dropout	Method
partially	O
removes	O
this	O
effect	O
in	O
most	O
cases	O
,	O
see	O
figures	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
.	O
The	O
effect	O
of	O
dropout	O
becomes	O
more	O
evident	O
on	O
SVHN	Material
.	O
This	O
is	O
probably	O
due	O
to	O
the	O
fact	O
that	O
we	O
do	O
n’t	Method
do	O
any	O
data	Method
augmentation	Method
and	O
batch	Method
normalization	Method
overfits	O
,	O
so	O
dropout	Method
adds	O
a	O
regularization	O
effect	O
.	O
Evidence	O
for	O
this	O
can	O
be	O
found	O
on	O
training	O
curves	O
in	O
figure	O
[	O
reference	O
]	O
where	O
the	O
loss	Metric
without	O
dropout	Method
drops	O
to	O
very	O
low	O
values	O
.	O
The	O
results	O
are	O
presented	O
in	O
table	O
[	O
reference	O
]	O
.	O
We	O
observe	O
significant	O
improvements	O
from	O
using	O
dropout	Method
on	O
both	O
thin	O
and	O
wide	Method
networks	Method
.	O
Thin	Method
50	Method
-	Method
layer	Method
deep	Method
network	Method
even	O
outperforms	O
thin	Method
152	Method
-	Method
layer	Method
deep	Method
network	Method
with	O
stochastic	O
depth	O
.	O
We	O
additionally	O
trained	O
WRN	Method
-	Method
16	Method
-	Method
8	Method
with	O
dropout	Method
on	O
SVHN	Material
(	O
table	O
[	O
reference	O
]	O
)	O
,	O
which	O
achieves	O
1.54	O
%	O
on	O
SVHN	Material
-	O
the	O
best	O
published	O
result	O
to	O
our	O
knowledge	O
.	O
Without	O
dropout	Method
it	O
achieves	O
1.81	O
%	O
.	O
Overall	O
,	O
despite	O
the	O
arguments	O
of	O
combining	O
with	O
batch	Method
normalization	Method
,	O
dropout	Method
shows	O
itself	O
as	O
an	O
effective	O
techique	O
of	O
regularization	O
of	O
thin	O
and	O
wide	Method
networks	Method
.	O
It	O
can	O
be	O
used	O
to	O
further	O
improve	O
results	O
from	O
widening	Task
,	O
while	O
also	O
being	O
complementary	O
to	O
it	O
.	O
[	O
scale=0.42	O
]	O
.	O
/	O
images	O
/	O
svhn.pdf	O
[	O
scale=0.42	O
]	O
.	O
/	O
images	O
/	O
svhn	O
-	O
dropout.pdf	O
subsubsection	O
:	O
ImageNet	O
and	O
COCO	O
experiments	O
For	O
ImageNet	O
we	O
first	O
experiment	O
with	O
non	O
-	O
bottleneck	O
ResNet	O
-	O
18	O
and	O
ResNet	Method
-	Method
34	Method
,	O
trying	O
to	O
gradually	O
increase	O
their	O
width	O
from	O
1.0	O
to	O
3.0	O
.	O
The	O
results	O
are	O
shown	O
in	O
table	O
[	O
reference	O
]	O
.	O
Increasing	O
width	O
gradually	O
increases	O
accuracy	Metric
of	O
both	O
networks	O
,	O
and	O
networks	O
with	O
a	O
comparable	O
number	O
of	O
parameters	O
achieve	O
similar	O
results	O
,	O
despite	O
having	O
different	O
depth	O
.	O
Althouth	O
these	O
networks	O
have	O
a	O
large	O
number	O
of	O
parameters	O
,	O
they	O
are	O
outperfomed	O
by	O
bottleneck	Method
networks	Method
,	O
which	O
is	O
probably	O
either	O
due	O
to	O
that	O
bottleneck	Method
architecture	Method
is	O
simply	O
better	O
suited	O
for	O
ImageNet	Task
classification	Task
task	Task
,	O
or	O
due	O
to	O
that	O
this	O
more	O
complex	O
task	O
needs	O
a	O
deeper	Method
network	Method
.	O
To	O
test	O
this	O
,	O
we	O
took	O
the	O
ResNet	O
-	O
50	O
,	O
and	O
tried	O
to	O
make	O
it	O
wider	O
by	O
increasing	O
inner	O
layer	O
width	O
.	O
With	O
widening	O
factor	O
of	O
2.0	O
the	O
resulting	O
WRN	Method
-	Method
50	Method
-	Method
2	Method
-	Method
bottleneck	Method
outperforms	O
ResNet	Method
-	Method
152	Method
having	O
3	O
times	O
less	O
layers	O
,	O
and	O
being	O
significantly	O
faster	O
.	O
WRN	Method
-	Method
50	Method
-	Method
2	Method
-	Method
bottleneck	Method
is	O
only	O
slightly	O
worse	O
and	O
almost	O
faster	O
than	O
the	O
best	O
-	O
performing	O
pre	Method
-	Method
activation	Method
ResNet	Method
-	Method
200	Method
,	O
althouth	O
having	O
slightly	O
more	O
parameters	O
(	O
table	O
[	O
reference	O
]	O
)	O
.	O
In	O
general	O
,	O
we	O
find	O
that	O
,	O
unlike	O
CIFAR	Material
,	O
ImageNet	Method
networks	Method
need	O
more	O
width	O
at	O
the	O
same	O
depth	O
to	O
achieve	O
the	O
same	O
accuracy	Metric
.	O
It	O
is	O
however	O
clear	O
that	O
it	O
is	O
unnecessary	O
to	O
have	O
residual	Method
networks	Method
with	O
more	O
than	O
50	O
layers	O
due	O
to	O
computational	O
reasons	O
.	O
We	O
did	O
n’t	O
try	O
to	O
train	O
bigger	O
bottleneck	Method
networks	Method
as	O
8	Method
-	Method
GPU	Method
machines	Method
are	O
needed	O
for	O
that	O
.	O
We	O
also	O
used	O
WRN	Method
-	Method
34	Method
-	Method
2	Method
to	O
participate	O
in	O
COCO	Task
2016	Task
object	Task
detection	Task
challenge	Task
,	O
using	O
a	O
combination	O
of	O
MultiPathNet	Method
and	O
LocNet	Method
.	O
Despite	O
having	O
only	O
34	O
layers	O
,	O
this	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
single	O
model	O
performance	O
,	O
outperforming	O
even	O
ResNet	Method
-	Method
152	Method
and	O
Inception	Method
-	Method
v4	Method
-	O
based	O
models	O
.	O
Finally	O
,	O
in	O
table	O
[	O
reference	O
]	O
we	O
summarize	O
our	O
best	O
WRN	O
results	O
over	O
various	O
commonly	O
used	O
datasets	O
.	O
subsubsection	O
:	O
Computational	Metric
efficiency	Metric
Thin	Method
and	Method
deep	Method
residual	Method
networks	Method
with	O
small	Method
kernels	Method
are	O
against	O
the	O
nature	O
of	O
GPU	Method
computations	Method
because	O
of	O
their	O
sequential	O
structure	O
.	O
Increasing	O
width	O
helps	O
effectively	O
balance	O
computations	O
in	O
much	O
more	O
optimal	O
way	O
,	O
so	O
that	O
wide	Method
networks	Method
are	O
many	O
times	O
more	O
efficient	O
than	O
thin	O
ones	O
as	O
our	O
benchmarks	O
show	O
.	O
We	O
use	O
cudnn	Method
v5	Method
and	O
Titan	Method
X	Method
to	O
measure	O
forward	Metric
+	Metric
backward	Metric
update	Metric
times	Metric
with	O
minibatch	O
size	O
32	O
for	O
several	O
networks	O
,	O
the	O
results	O
are	O
in	O
the	O
figure	O
[	O
reference	O
]	O
.	O
We	O
show	O
that	O
our	O
best	O
CIFAR	Material
wide	O
WRN	Method
-	Method
28	Method
-	O
10	O
is	O
1.6	O
times	O
faster	O
than	O
thin	Method
ResNet	Method
-	Method
1001	Method
.	O
Furthermore	O
,	O
wide	Method
WRN	Method
-	Method
40	Method
-	Method
4	Method
,	O
which	O
has	O
approximately	O
the	O
same	O
accuracy	Metric
as	O
ResNet	Method
-	Method
1001	Method
,	O
is	O
8	O
times	O
faster	O
.	O
[	O
scale=0.6	O
]	O
.	O
/	O
images	O
/	O
benchmark	O
-	O
edited.pdf	O
subsubsection	O
:	O
Implementation	O
details	O
In	O
all	O
our	O
experiments	O
we	O
use	O
SGD	Method
with	O
Nesterov	Method
momentum	Method
and	O
cross	O
-	O
entropy	O
loss	Metric
.	O
The	O
initial	O
learning	Metric
rate	Metric
is	O
set	O
to	O
0.1	O
,	O
weight	O
decay	O
to	O
0.0005	O
,	O
dampening	O
to	O
0	O
,	O
momentum	O
to	O
0.9	O
and	O
minibatch	O
size	O
to	O
128	O
.	O
On	O
CIFAR	Material
learning	O
rate	O
dropped	O
by	O
0.2	O
at	O
60	O
,	O
120	O
and	O
160	O
epochs	O
and	O
we	O
train	O
for	O
total	O
200	O
epochs	O
.	O
On	O
SVHN	Material
initial	O
learning	O
rate	O
is	O
set	O
to	O
0.01	O
and	O
we	O
drop	O
it	O
at	O
80	O
and	O
120	O
epochs	O
by	O
0.1	O
,	O
training	O
for	O
total	O
160	O
epochs	O
.	O
Our	O
implementation	O
is	O
based	O
on	O
Torch	Method
.	O
We	O
use	O
to	O
reduce	O
memory	O
footprints	O
of	O
all	O
our	O
networks	O
.	O
For	O
ImageNet	Task
experiments	O
we	O
used	O
fb.resnet.torch	Method
implementation	Method
.	O
Our	O
code	O
and	O
models	O
are	O
available	O
at	O
https:	O
//	O
github.com	O
/	O
szagoruyko	O
/	O
wide	O
-	O
residual	O
-	O
networks	O
.	O
section	O
:	O
Conclusions	O
We	O
presented	O
a	O
study	O
on	O
the	O
width	O
of	O
residual	Method
networks	Method
as	O
well	O
as	O
on	O
the	O
use	O
of	O
dropout	Method
in	Method
residual	Method
architectures	Method
.	O
Based	O
on	O
this	O
study	O
,	O
we	O
proposed	O
a	O
wide	Method
residual	Method
network	Method
architecture	Method
that	O
provides	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
several	O
commonly	O
used	O
benchmark	O
datasets	O
(	O
including	O
CIFAR	Material
-	Material
10	Material
,	O
CIFAR	Material
-	Material
100	Material
,	O
SVHN	Material
and	O
COCO	O
)	O
,	O
as	O
well	O
as	O
significant	O
improvements	O
on	O
ImageNet	O
.	O
We	O
demonstrate	O
that	O
wide	Method
networks	Method
with	O
only	O
16	Method
layers	Method
can	O
significantly	O
outperform	O
1000	Method
-	Method
layer	Method
deep	Method
networks	Method
on	O
CIFAR	Material
,	O
as	O
well	O
as	O
that	O
50	Method
-	Method
layer	Method
outperform	O
152	O
-	O
layer	O
on	O
ImageNet	O
,	O
thus	O
showing	O
that	O
the	O
main	O
power	O
of	O
residual	Method
networks	Method
is	O
in	O
residual	O
blocks	O
,	O
and	O
not	O
in	O
extreme	O
depth	O
as	O
claimed	O
earlier	O
.	O
Also	O
,	O
wide	Method
residual	Method
networks	Method
are	O
several	O
times	O
faster	O
to	O
train	O
.	O
We	O
think	O
that	O
these	O
intriguing	O
findings	O
will	O
help	O
further	O
advances	O
in	O
research	O
in	O
deep	Task
neural	Task
networks	Task
.	O
section	O
:	O
Acknowledgements	O
We	O
thank	O
startup	O
company	O
VisionLabs	O
and	O
Eugenio	O
Culurciello	O
for	O
giving	O
us	O
access	O
to	O
their	O
clusters	O
,	O
without	O
them	O
ImageNet	O
experiments	O
would	O
n’t	O
be	O
possible	O
.	O
We	O
also	O
thank	O
Adam	O
Lerer	O
and	O
Sam	O
Gross	O
for	O
helpful	O
discussions	O
.	O
Work	O
supported	O
by	O
EC	O
project	O
FP7	O
-	O
ICT	O
-	O
611145	O
ROBOSPECT	O
.	O
bibliography	O
:	O
References	O
