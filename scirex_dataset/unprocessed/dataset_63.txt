document	O
:	O
Fast	O
and	O
Accurate	O
Deep	Method
Network	Method
Learning	Method
by	O
Exponential	Method
Linear	Method
Units	Method
(	O
ELUs	Method
)	O
We	O
introduce	O
the	O
“	O
exponential	Method
linear	Method
unit	Method
”	Method
(	O
ELU	Method
)	O
which	O
speeds	O
up	O
learning	Task
in	O
deep	Method
neural	Method
networks	Method
and	O
leads	O
to	O
higher	O
classification	Metric
accuracies	Metric
.	O
Like	O
rectified	Method
linear	Method
units	Method
(	O
ReLUs	Method
)	O
,	O
leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
and	O
parametrized	Method
ReLUs	Method
(	O
PReLUs	Method
)	O
,	O
ELUs	Method
alleviate	O
the	O
vanishing	Task
gradient	Task
problem	Task
via	O
the	O
identity	O
for	O
positive	O
values	O
.	O
However	O
ELUs	Method
have	O
improved	O
learning	Metric
characteristics	Metric
compared	O
to	O
the	O
units	O
with	O
other	O
activation	O
functions	O
.	O
In	O
contrast	O
to	O
ReLUs	Method
,	O
ELUs	Method
have	O
negative	O
values	O
which	O
allows	O
them	O
to	O
push	O
mean	O
unit	O
activations	O
closer	O
to	O
zero	O
like	O
batch	Method
normalization	Method
but	O
with	O
lower	O
computational	Metric
complexity	Metric
.	O
Mean	O
shifts	O
toward	O
zero	O
speed	O
up	O
learning	Task
by	O
bringing	O
the	O
normal	O
gradient	O
closer	O
to	O
the	O
unit	O
natural	O
gradient	O
because	O
of	O
a	O
reduced	O
bias	O
shift	O
effect	O
.	O
While	O
LReLUs	Method
and	O
PReLUs	Method
have	O
negative	O
values	O
,	O
too	O
,	O
they	O
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O
ELUs	Method
saturate	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
inputs	O
and	O
thereby	O
decrease	O
the	O
forward	O
propagated	O
variation	O
and	O
information	O
.	O
Therefore	O
ELUs	Method
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
while	O
they	O
do	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O
In	O
experiments	O
,	O
ELUs	Method
lead	O
not	O
only	O
to	O
faster	O
learning	Task
,	O
but	O
also	O
to	O
significantly	O
better	O
generalization	Metric
performance	Metric
than	O
ReLUs	Method
and	O
LReLUs	Method
on	O
networks	O
with	O
more	O
than	O
5	O
layers	O
.	O
On	O
CIFAR	Method
-	Method
100	Method
ELUs	Method
networks	Method
significantly	O
outperform	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
while	O
batch	Method
normalization	Method
does	O
not	O
improve	O
ELU	Method
networks	Method
.	O
ELU	Method
networks	Method
are	O
among	O
the	O
top	O
10	O
reported	O
CIFAR	Material
-	Material
10	Material
results	O
and	O
yield	O
the	O
best	O
published	O
result	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
without	O
resorting	O
to	O
multi	Method
-	Method
view	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O
On	O
ImageNet	O
,	O
ELU	Method
networks	Method
considerably	O
speed	O
up	O
learning	Task
compared	O
to	O
a	O
ReLU	Method
network	O
with	O
the	O
same	O
architecture	O
,	O
obtaining	O
less	O
than	O
10	O
%	O
classification	Metric
error	Metric
for	O
a	O
single	O
crop	Method
,	Method
single	Method
model	Method
network	Method
.	O
section	O
:	O
Introduction	O
Currently	O
the	O
most	O
popular	O
activation	Method
function	Method
for	O
neural	Method
networks	Method
is	O
the	O
rectified	Method
linear	Method
unit	Method
(	O
ReLU	Method
)	O
,	O
which	O
was	O
first	O
proposed	O
for	O
restricted	Method
Boltzmann	Method
machines	Method
and	O
then	O
successfully	O
used	O
for	O
neural	Task
networks	Task
.	O
The	O
ReLU	Method
activation	O
function	O
is	O
the	O
identity	O
for	O
positive	O
arguments	O
and	O
zero	O
otherwise	O
.	O
Besides	O
producing	O
sparse	Method
codes	Method
,	O
the	O
main	O
advantage	O
of	O
ReLUs	Method
is	O
that	O
they	O
alleviate	O
the	O
vanishing	Task
gradient	Task
problem	Task
since	O
the	O
derivative	O
of	O
1	O
for	O
positive	O
values	O
is	O
not	O
contractive	O
.	O
However	O
ReLUs	O
are	O
non	O
-	O
negative	O
and	O
,	O
therefore	O
,	O
have	O
a	O
mean	O
activation	O
larger	O
than	O
zero	O
.	O
Units	O
that	O
have	O
a	O
non	O
-	O
zero	O
mean	O
activation	O
act	O
as	O
bias	O
for	O
the	O
next	O
layer	O
.	O
If	O
such	O
units	O
do	O
not	O
cancel	O
each	O
other	O
out	O
,	O
learning	O
causes	O
a	O
bias	O
shift	O
for	O
units	O
in	O
next	O
layer	O
.	O
The	O
more	O
the	O
units	O
are	O
correlated	O
,	O
the	O
higher	O
their	O
bias	O
shift	O
.	O
We	O
will	O
see	O
that	O
Fisher	Method
optimal	Method
learning	Method
,	O
i.e.	O
,	O
the	O
natural	O
gradient	O
,	O
would	O
correct	O
for	O
the	O
bias	O
shift	O
by	O
adjusting	O
the	O
weight	O
updates	O
.	O
Thus	O
,	O
less	O
bias	O
shift	O
brings	O
the	O
standard	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
and	O
speeds	O
up	O
learning	Task
.	O
We	O
aim	O
at	O
activation	O
functions	O
that	O
push	O
activation	O
means	O
closer	O
to	O
zero	O
to	O
decrease	O
the	O
bias	O
shift	O
effect	O
.	O
Centering	O
the	O
activations	O
at	O
zero	O
has	O
been	O
proposed	O
in	O
order	O
to	O
keep	O
the	O
off	O
-	O
diagonal	O
entries	O
of	O
the	O
Fisher	O
information	O
matrix	O
small	O
.	O
For	O
neural	Method
network	Method
it	O
is	O
known	O
that	O
centering	O
the	O
activations	O
speeds	O
up	O
learning	Task
.	O
“	O
Batch	Method
normalization	Method
”	O
also	O
centers	O
activations	O
with	O
the	O
goal	O
to	O
counter	O
the	O
internal	O
covariate	O
shift	O
.	O
Also	O
the	O
Projected	Method
Natural	Method
Gradient	Method
Descent	Method
algorithm	Method
(	O
PRONG	Method
)	O
centers	O
the	O
activations	O
by	O
implicitly	O
whitening	O
them	O
.	O
An	O
alternative	O
to	O
centering	O
is	O
to	O
push	O
the	O
mean	O
activation	O
toward	O
zero	O
by	O
an	O
appropriate	O
activation	Method
function	Method
.	O
Therefore	O
has	O
been	O
preferred	O
over	O
logistic	Method
functions	Method
.	O
Recently	O
“	O
Leaky	Method
ReLUs	Method
”	Method
(	O
LReLUs	Method
)	O
that	O
replace	O
the	O
negative	O
part	O
of	O
the	O
ReLU	Method
with	O
a	O
linear	O
function	O
have	O
been	O
shown	O
to	O
be	O
superior	O
to	O
ReLUs	Method
.	O
Parametric	Method
Rectified	Method
Linear	Method
Units	Method
(	O
PReLUs	Method
)	O
generalize	O
LReLUs	Method
by	O
learning	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
yielded	O
improved	O
learning	O
behavior	O
on	O
large	O
image	O
benchmark	O
data	O
sets	O
.	O
Another	O
variant	O
are	O
Randomized	Method
Leaky	Method
Rectified	Method
Linear	Method
Units	Method
(	O
RReLUs	Method
)	O
which	O
randomly	O
sample	O
the	O
slope	O
of	O
the	O
negative	O
part	O
which	O
raised	O
the	O
performance	O
on	O
image	O
benchmark	O
datasets	O
and	O
convolutional	Method
networks	Method
.	O
In	O
contrast	O
to	O
ReLUs	O
,	O
activation	O
functions	O
like	O
LReLUs	Method
,	O
PReLUs	O
,	O
and	O
RReLUs	Method
do	O
not	O
ensure	O
a	O
noise	O
-	O
robust	O
deactivation	O
state	O
.	O
We	O
propose	O
an	O
activation	Method
function	Method
that	O
has	O
negative	O
values	O
to	O
allow	O
for	O
mean	O
activations	O
close	O
to	O
zero	O
,	O
but	O
which	O
saturates	O
to	O
a	O
negative	O
value	O
with	O
smaller	O
arguments	O
.	O
The	O
saturation	O
decreases	O
the	O
variation	O
of	O
the	O
units	O
if	O
deactivated	O
,	O
so	O
the	O
precise	O
deactivation	O
argument	O
is	O
less	O
relevant	O
.	O
Such	O
an	O
activation	Method
function	Method
can	O
code	O
the	O
degree	O
of	O
presence	O
of	O
particular	O
phenomena	O
in	O
the	O
input	O
,	O
but	O
does	O
not	O
quantitatively	O
model	O
the	O
degree	O
of	O
their	O
absence	O
.	O
Therefore	O
,	O
such	O
an	O
activation	Method
function	Method
is	O
more	O
robust	O
to	O
noise	O
.	O
Consequently	O
,	O
dependencies	O
between	O
coding	O
units	O
are	O
much	O
easier	O
to	O
model	O
and	O
much	O
easier	O
to	O
interpret	O
since	O
only	O
activated	O
code	O
units	O
carry	O
much	O
information	O
.	O
Furthermore	O
,	O
distinct	O
concepts	O
are	O
much	O
less	O
likely	O
to	O
interfere	O
with	O
such	O
activation	O
functions	O
since	O
the	O
deactivation	O
state	O
is	O
non	O
-	O
informative	O
,	O
i.e.	O
variance	O
decreasing	O
.	O
section	O
:	O
Bias	Method
Shift	Method
Correction	Method
Speeds	O
Up	O
Learning	Task
To	O
derive	O
and	O
analyze	O
the	O
bias	O
shift	O
effect	O
mentioned	O
in	O
the	O
introduction	O
,	O
we	O
utilize	O
the	O
natural	O
gradient	O
.	O
The	O
natural	O
gradient	O
corrects	O
the	O
gradient	O
direction	O
with	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
and	O
,	O
thereby	O
,	O
enables	O
Fisher	Method
optimal	Method
learning	Method
,	O
which	O
ensures	O
the	O
steepest	O
descent	O
in	O
the	O
Riemannian	O
parameter	O
manifold	O
and	O
Fisher	Metric
efficiency	Metric
for	O
online	Task
learning	Task
.	O
The	O
recently	O
introduced	O
Hessian	Method
-	Method
Free	Method
Optimization	Method
technique	Method
and	O
the	O
Krylov	Method
Subspace	Method
Descent	Method
methods	Method
use	O
an	O
extended	Method
Gauss	Method
-	Method
Newton	Method
approximation	Method
of	O
the	O
Hessian	O
,	O
therefore	O
they	O
can	O
be	O
interpreted	O
as	O
versions	O
of	O
natural	Method
gradient	Method
descent	Method
.	O
Since	O
for	O
neural	Method
networks	Method
the	O
Fisher	O
information	O
matrix	O
is	O
typically	O
too	O
expensive	O
to	O
compute	O
,	O
different	O
approximations	O
of	O
the	O
natural	O
gradient	O
have	O
been	O
proposed	O
.	O
Topmoumoute	Method
Online	Method
natural	Method
Gradient	Method
Algorithm	Method
(	O
TONGA	Method
)	O
uses	O
a	O
low	Method
-	Method
rank	Method
approximation	Method
of	Method
natural	Method
gradient	Method
descent	Method
.	O
FActorized	Method
Natural	Method
Gradient	Method
(	O
FANG	Method
)	O
estimates	O
the	O
natural	O
gradient	O
via	O
an	O
approximation	Method
of	Method
the	Method
Fisher	Method
information	Method
matrix	Method
by	O
a	O
Gaussian	Method
graphical	Method
model	Method
.	O
The	O
Fisher	O
information	O
matrix	O
can	O
be	O
approximated	O
by	O
a	O
block	Method
-	Method
diagonal	Method
matrix	Method
,	O
where	O
unit	O
or	O
quasi	O
-	O
diagonal	O
natural	O
gradients	O
are	O
used	O
.	O
Unit	O
natural	O
gradients	O
or	O
“	O
Unitwise	Method
Fisher	Method
’s	Method
scoring	Method
”	Method
are	O
based	O
on	O
natural	O
gradients	O
for	O
perceptrons	Method
.	O
We	O
will	O
base	O
our	O
analysis	O
on	O
the	O
unit	O
natural	O
gradient	O
.	O
We	O
assume	O
a	O
parameterized	Method
probabilistic	Method
model	Method
with	O
parameter	O
vector	O
and	O
data	O
.	O
The	O
training	O
data	O
are	O
with	O
,	O
where	O
is	O
the	O
input	O
for	O
example	O
and	O
is	O
its	O
label	O
.	O
is	O
the	O
loss	O
of	O
example	O
using	O
model	O
.	O
The	O
average	Metric
loss	Metric
on	O
the	O
training	O
data	O
is	O
the	O
empirical	O
risk	O
.	O
Gradient	Method
descent	Method
updates	O
the	O
weight	O
vector	O
by	O
where	O
is	O
the	O
learning	Metric
rate	Metric
.	O
The	O
natural	O
gradient	O
is	O
the	O
inverse	O
Fisher	O
information	O
matrix	O
multiplied	O
by	O
the	O
gradient	O
of	O
the	O
empirical	O
risk	O
:	O
.	O
For	O
a	O
multi	Method
-	Method
layer	Method
perceptron	Method
is	O
the	O
unit	O
activation	O
vector	O
and	O
is	O
the	O
bias	O
unit	O
activation	O
.	O
We	O
consider	O
the	O
ingoing	O
weights	O
to	O
unit	O
,	O
therefore	O
we	O
drop	O
the	O
index	O
:	O
for	O
the	O
weight	O
from	O
unit	O
to	O
unit	O
,	O
for	O
the	O
activation	O
,	O
and	O
for	O
the	O
bias	O
weight	O
of	O
unit	O
.	O
The	O
activation	Method
function	Method
maps	O
the	O
net	O
input	O
of	O
unit	O
to	O
its	O
activation	O
.	O
For	O
computing	O
the	O
Fisher	O
information	O
matrix	O
,	O
the	O
derivative	O
of	O
the	O
log	O
-	O
output	O
probability	O
is	O
required	O
.	O
Therefore	O
we	O
define	O
the	O
at	O
unit	O
as	O
,	O
which	O
can	O
be	O
computed	O
via	O
backpropagation	Method
,	O
but	O
using	O
the	O
log	O
-	O
output	O
probability	O
instead	O
of	O
the	O
conventional	O
loss	O
function	O
.	O
The	O
derivative	O
is	O
.	O
We	O
restrict	O
the	O
Fisher	O
information	O
matrix	O
to	O
weights	O
leading	O
to	O
unit	O
which	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
.	O
captures	O
only	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
.	O
Consequently	O
,	O
the	O
unit	O
natural	O
gradient	O
only	O
corrects	O
the	O
interactions	O
of	O
weights	O
to	O
unit	O
,	O
i.e.	O
considers	O
the	O
Riemannian	O
parameter	O
manifold	O
only	O
in	O
a	O
subspace	O
.	O
The	O
unit	O
Fisher	O
information	O
matrix	O
is	O
Weighting	O
the	O
activations	O
by	O
is	O
equivalent	O
to	O
adjusting	O
the	O
probability	O
of	O
drawing	O
inputs	O
.	O
Inputs	O
with	O
large	O
are	O
drawn	O
with	O
higher	O
probability	O
.	O
Since	O
,	O
we	O
can	O
define	O
a	O
distribution	O
:	O
Using	O
,	O
the	O
entries	O
of	O
can	O
be	O
expressed	O
as	O
second	O
moments	O
:	O
If	O
the	O
bias	O
unit	O
is	O
with	O
weight	O
then	O
the	O
weight	O
vector	O
can	O
be	O
divided	O
into	O
a	O
bias	O
part	O
and	O
the	O
rest	O
:	O
.	O
For	O
the	O
row	O
that	O
corresponds	O
to	O
the	O
bias	O
weight	O
,	O
we	O
have	O
:	O
The	O
next	O
Theorem	O
[	O
reference	O
]	O
gives	O
the	O
correction	O
of	O
the	O
standard	O
gradient	O
by	O
the	O
unit	O
natural	O
gradient	O
where	O
the	O
bias	O
weight	O
is	O
treated	O
separately	O
(	O
see	O
also	O
)	O
.	O
theorem	O
:	O
.	O
The	O
unit	O
natural	O
gradient	O
corrects	O
the	O
weight	O
update	O
(	O
⁢ΔwT	O
,	O
⁢Δw0	O
)	O
T	O
to	O
a	O
unit	O
i	O
by	O
following	O
affine	O
transformation	O
of	O
the	O
gradient	O
=	O
∇	O
(	O
wT	O
,	O
w0	O
)	O
TRemp	O
(	O
gT	O
,	O
g0	O
)	O
T	O
:	O
where	O
A=	O
[	O
⁢F	O
(	O
w	O
)]	O
⁢¬0	O
,	O
⁢¬0=⁢E⁢p	O
(	O
z	O
)(	O
δ2	O
)	O
E⁢q	O
(	O
z	O
)(	O
⁢aaT	O
)	O
is	O
the	O
unit	O
Fisher	O
information	O
matrix	O
without	O
row	O
0	O
and	O
column	O
0	O
corresponding	O
to	O
the	O
bias	O
weight	O
.	O
The	O
vector	O
=	O
b	O
[	O
⁢F	O
(	O
w	O
)]	O
0	O
is	O
the	O
zeroth	O
column	O
of	O
F	O
corresponding	O
to	O
the	O
bias	O
weight	O
,	O
and	O
the	O
positive	O
scalar	O
s	O
is	O
where	O
a	O
is	O
the	O
vector	O
of	O
activations	O
of	O
units	O
with	O
weights	O
to	O
unit	O
i	O
and	O
=	O
⁢q	O
(	O
z	O
)	O
⁢δ2	O
(	O
z	O
)	O
p	O
(	O
z	O
)	O
E⁢p	O
(	O
z	O
)-	O
1	O
(	O
δ2	O
)	O
.	O
proof	O
:	O
Proof	O
.	O
Multiplying	O
the	O
inverse	O
Fisher	O
matrix	O
with	O
the	O
separated	O
gradient	O
gives	O
the	O
weight	Method
update	Method
:	O
where	O
The	O
previous	O
formula	O
is	O
derived	O
in	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O
Using	O
in	O
the	O
update	O
gives	O
The	O
right	O
hand	O
side	O
is	O
obtained	O
by	O
inserting	O
in	O
the	O
left	O
hand	O
side	O
update	O
.	O
Since	O
,	O
,	O
and	O
,	O
we	O
obtain	O
Applying	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
gives	O
the	O
formula	O
for	O
.	O
∎	O
The	O
bias	O
shift	O
(	O
mean	O
shift	O
)	O
of	O
unit	O
is	O
the	O
change	O
of	O
unit	O
’s	O
mean	O
value	O
due	O
to	O
the	O
weight	Method
update	Method
.	O
Bias	O
shifts	O
of	O
unit	O
lead	O
to	O
oscillations	O
and	O
impede	O
learning	Task
.	O
See	O
Section	O
4.4	O
in	O
for	O
demonstrating	O
this	O
effect	O
at	O
the	O
inputs	O
and	O
in	O
for	O
explaining	O
this	O
effect	O
using	O
the	O
input	O
covariance	O
matrix	O
.	O
Such	O
bias	O
shifts	O
are	O
mitigated	O
or	O
even	O
prevented	O
by	O
the	O
unit	O
natural	O
gradient	O
.	O
The	O
bias	Task
shift	Task
correction	Task
of	O
the	O
unit	O
natural	O
gradient	O
is	O
the	O
effect	O
on	O
the	O
bias	O
shift	O
due	O
to	O
which	O
captures	O
the	O
interaction	O
between	O
the	O
bias	O
unit	O
and	O
the	O
incoming	O
units	O
.	O
Without	O
bias	Method
shift	Method
correction	Method
,	O
i.e.	O
,	O
and	O
,	O
the	O
weight	O
updates	O
are	O
and	O
.	O
As	O
only	O
the	O
activations	O
depend	O
on	O
the	O
input	O
,	O
the	O
bias	O
shift	O
can	O
be	O
computed	O
by	O
multiplying	O
the	O
weight	O
update	O
by	O
the	O
mean	O
of	O
the	O
activation	O
vector	O
.	O
Thus	O
we	O
obtain	O
the	O
bias	O
shift	O
.	O
The	O
bias	O
shift	O
strongly	O
depends	O
on	O
the	O
correlation	O
of	O
the	O
incoming	O
units	O
which	O
is	O
captured	O
by	O
.	O
Next	O
,	O
Theorem	O
[	O
reference	O
]	O
states	O
that	O
the	O
bias	Method
shift	Method
correction	Method
by	O
the	O
unit	O
natural	O
gradient	O
can	O
be	O
considered	O
to	O
correct	O
the	O
incoming	O
mean	O
proportional	O
to	O
toward	O
zero	O
.	O
theorem	O
:	O
.	O
The	O
bias	O
shift	O
correction	O
by	O
the	O
unit	O
natural	O
gradient	O
is	O
equivalent	O
to	O
an	O
additive	Method
correction	Method
of	O
the	O
incoming	O
mean	O
by	O
-	O
⁢kE⁢q	O
(	O
z	O
)(	O
a	O
)	O
and	O
a	O
multiplicative	Method
correction	Method
of	O
the	O
bias	O
unit	O
by	O
k	O
,	O
where	O
proof	O
:	O
Proof	O
.	O
Using	O
,	O
the	O
bias	O
shift	O
is	O
:	O
The	O
mean	O
correction	O
term	O
,	O
indicated	O
by	O
an	O
underbrace	O
in	O
previous	O
formula	O
,	O
is	O
The	O
expression	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
for	O
follows	O
from	O
Lemma	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
.	O
The	O
bias	O
unit	O
correction	O
term	O
is	O
.	O
∎	O
In	O
Theorem	O
[	O
reference	O
]	O
we	O
can	O
reformulate	O
.	O
Therefore	O
increases	O
with	O
the	O
length	O
of	O
for	O
given	O
variances	O
and	O
covariances	O
.	O
Consequently	O
the	O
bias	Task
shift	Task
correction	Task
through	O
the	O
unit	O
natural	O
gradient	O
is	O
governed	O
by	O
the	O
length	O
of	O
.	O
The	O
bias	Method
shift	Method
correction	Method
is	O
zero	O
for	O
since	O
does	O
not	O
correct	O
the	O
bias	O
unit	O
multiplicatively	O
.	O
Using	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
is	O
split	O
into	O
an	O
offset	O
and	O
an	O
information	O
containing	O
term	O
:	O
In	O
general	O
,	O
smaller	O
positive	O
⁢E⁢p	O
(	O
z	O
)(	O
a	O
)	O
lead	O
to	O
smaller	O
positive	O
⁢E⁢q	O
(	O
z	O
)(	O
a	O
)	O
,	O
therefore	O
to	O
smaller	O
corrections	O
.	O
The	O
reason	O
is	O
that	O
in	O
general	O
the	O
largest	O
absolute	O
components	O
of	O
are	O
positive	O
,	O
since	O
activated	O
inputs	O
will	O
activate	O
the	O
unit	O
which	O
in	O
turn	O
will	O
have	O
large	O
impact	O
on	O
the	O
output	O
.	O
To	O
summarize	O
,	O
the	O
unit	O
natural	O
gradient	O
corrects	O
the	O
bias	O
shift	O
of	O
unit	O
via	O
the	O
interactions	O
of	O
incoming	O
units	O
with	O
the	O
bias	O
unit	O
to	O
ensure	O
efficient	O
learning	Task
.	O
This	O
correction	O
is	O
equivalent	O
to	O
shifting	O
the	O
mean	O
activations	O
of	O
the	O
incoming	O
units	O
toward	O
zero	O
and	O
scaling	O
up	O
the	O
bias	O
unit	O
.	O
To	O
reduce	O
the	O
undesired	O
bias	O
shift	O
effect	O
without	O
the	O
natural	O
gradient	O
,	O
either	O
the	O
(	O
i	O
)	O
activation	O
of	O
incoming	O
units	O
can	O
be	O
centered	O
at	O
zero	O
or	O
(	O
ii	O
)	O
activation	O
functions	O
with	O
negative	O
values	O
can	O
be	O
used	O
.	O
We	O
introduce	O
a	O
new	O
activation	Method
function	Method
with	O
negative	O
values	O
while	O
keeping	O
the	O
identity	O
for	O
positive	O
arguments	O
where	O
it	O
is	O
not	O
contradicting	O
.	O
section	O
:	O
Exponential	Method
Linear	Method
Units	Method
(	O
ELUs	Method
)	O
The	O
exponential	Method
linear	Method
unit	Method
(	O
ELU	Method
)	O
with	O
is	O
The	O
ELU	Method
hyperparameter	O
controls	O
the	O
value	O
to	O
which	O
an	O
ELU	Method
saturates	O
for	O
negative	O
net	O
inputs	O
(	O
see	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
ELUs	Method
diminish	O
the	O
vanishing	O
gradient	O
effect	O
as	O
rectified	O
linear	O
units	O
(	O
ReLUs	Method
)	O
and	O
leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
do	O
.	O
The	O
vanishing	Task
gradient	Task
problem	Task
is	O
alleviated	O
because	O
the	O
positive	O
part	O
of	O
these	O
functions	O
is	O
the	O
identity	O
,	O
therefore	O
their	O
derivative	O
is	O
one	O
and	O
not	O
contractive	O
.	O
In	O
contrast	O
,	O
and	O
sigmoid	Method
activation	Method
functions	Method
are	O
contractive	O
almost	O
everywhere	O
.	O
In	O
contrast	O
to	O
ReLUs	O
,	O
ELUs	Method
have	O
negative	O
values	O
which	O
pushes	O
the	O
mean	O
of	O
the	O
activations	O
closer	O
to	O
zero	O
.	O
Mean	O
activations	O
that	O
are	O
closer	O
to	O
zero	O
enable	O
faster	O
learning	Task
as	O
they	O
bring	O
the	O
gradient	O
closer	O
to	O
the	O
natural	O
gradient	O
(	O
see	O
Theorem	O
[	O
reference	O
]	O
and	O
text	O
thereafter	O
)	O
.	O
ELUs	Method
saturate	O
to	O
a	O
negative	O
value	O
when	O
the	O
argument	O
gets	O
smaller	O
.	O
Saturation	O
means	O
a	O
small	O
derivative	O
which	O
decreases	O
the	O
variation	O
and	O
the	O
information	O
that	O
is	O
propagated	O
to	O
the	O
next	O
layer	O
.	O
Therefore	O
the	O
representation	O
is	O
both	O
noise	O
-	O
robust	O
and	O
low	O
-	O
complex	O
.	O
ELUs	Method
code	O
the	O
degree	O
of	O
presence	O
of	O
input	O
concepts	O
,	O
while	O
they	O
neither	O
quantify	O
the	O
degree	O
of	O
their	O
absence	O
nor	O
distinguish	O
the	O
causes	O
of	O
their	O
absence	O
.	O
This	O
property	O
of	O
non	O
-	O
informative	O
deactivation	O
states	O
is	O
also	O
present	O
at	O
ReLUs	Method
and	O
allowed	O
to	O
detect	O
biclusters	O
corresponding	O
to	O
biological	O
modules	O
in	O
gene	O
expression	O
datasets	O
and	O
to	O
identify	O
toxicophores	Task
in	Task
toxicity	Task
prediction	Task
.	O
The	O
enabling	O
features	O
for	O
these	O
interpretations	O
is	O
that	O
activation	Task
can	O
be	O
clearly	O
distinguished	O
from	O
deactivation	Method
and	O
that	O
only	O
active	O
units	O
carry	O
relevant	O
information	O
and	O
can	O
crosstalk	O
.	O
section	O
:	O
Experiments	O
Using	O
ELUs	Method
In	O
this	O
section	O
,	O
we	O
assess	O
the	O
performance	O
of	O
exponential	Method
linear	Method
units	Method
(	O
ELUs	Method
)	O
if	O
used	O
for	O
unsupervised	Task
and	O
supervised	Task
learning	Task
of	Task
deep	Task
autoencoders	Task
and	O
deep	Method
convolutional	Method
networks	Method
.	O
ELUs	Method
with	O
are	O
compared	O
to	O
(	O
i	O
)	O
Rectified	Method
Linear	Method
Units	Method
(	O
ReLUs	Method
)	O
with	O
activation	O
,	O
(	O
ii	O
)	O
Leaky	Method
ReLUs	Method
(	O
LReLUs	Method
)	O
with	O
activation	O
(	O
)	O
,	O
and	O
(	O
iii	O
)	O
Shifted	Method
ReLUs	Method
(	O
SReLUs	Method
)	O
with	O
activation	O
.	O
Comparisons	O
are	O
done	O
with	O
and	O
without	O
batch	Method
normalization	Method
.	O
The	O
following	O
benchmark	O
datasets	O
are	O
used	O
:	O
(	O
i	O
)	O
MNIST	O
(	O
gray	O
images	O
in	O
10	O
classes	O
,	O
60k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
ii	O
)	O
CIFAR	Material
-	Material
10	Material
(	O
color	O
images	O
in	O
10	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
(	O
iii	O
)	O
CIFAR	Material
-	Material
100	Material
(	O
color	O
images	O
in	O
100	O
classes	O
,	O
50k	O
train	O
and	O
10k	O
test	O
)	O
,	O
and	O
(	O
iv	O
)	O
ImageNet	O
(	O
color	O
images	O
in	O
1	O
,	O
000	O
classes	O
,	O
1.3	O
M	O
train	O
and	O
100k	O
tests	O
)	O
.	O
subsection	O
:	O
MNIST	O
subsubsection	O
:	O
Learning	Task
Behavior	Task
We	O
first	O
want	O
to	O
verify	O
that	O
ELUs	Method
keep	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
than	O
other	O
units	O
.	O
Fully	Method
connected	Method
deep	Method
neural	Method
networks	Method
with	O
ELUs	Method
(	Method
)	Method
,	O
ReLUs	Method
,	O
and	O
LReLUs	Method
(	O
)	O
were	O
trained	O
on	O
the	O
MNIST	O
digit	O
classification	O
dataset	O
while	O
each	O
hidden	O
unit	O
’s	O
activation	O
was	O
tracked	O
.	O
Each	O
network	O
had	O
eight	O
hidden	O
layers	O
of	O
128	O
units	O
each	O
,	O
and	O
was	O
trained	O
for	O
300	O
epochs	O
by	O
stochastic	Method
gradient	Method
descent	Method
with	O
learning	Method
rate	Method
and	O
mini	O
-	O
batches	O
of	O
size	O
64	O
.	O
The	O
weights	O
have	O
been	O
initialized	O
according	O
to	O
.	O
After	O
each	O
epoch	O
we	O
calculated	O
the	O
units	O
’	O
average	O
activations	O
on	O
a	O
fixed	O
subset	O
of	O
the	O
training	O
data	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
median	O
over	O
all	O
units	O
along	O
learning	O
.	O
ELUs	Method
stay	O
have	O
smaller	O
median	O
throughout	O
the	O
training	O
process	O
.	O
The	O
training	Metric
error	Metric
of	O
ELU	Method
networks	Method
decreases	O
much	O
more	O
rapidly	O
than	O
for	O
the	O
other	O
networks	O
.	O
Section	O
[	O
reference	O
]	O
in	O
the	O
appendix	O
compares	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	Method
and	O
ELU	Method
networks	Method
.	O
The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	Method
networks	O
.	O
This	O
indicates	O
that	O
ReLU	Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	Method
networks	Method
.	O
subsubsection	O
:	O
Autoencoder	Method
Learning	Method
To	O
evaluate	O
ELU	Method
networks	Method
at	O
unsupervised	Task
settings	O
,	O
we	O
followed	O
and	O
and	O
trained	O
a	O
deep	Method
autoencoder	Method
on	O
the	O
MNIST	O
dataset	O
.	O
The	O
encoder	Method
part	Method
consisted	O
of	O
four	O
fully	Method
connected	Method
hidden	Method
layers	Method
with	O
sizes	O
1000	O
,	O
500	O
,	O
250	O
and	O
30	O
,	O
respectively	O
.	O
The	O
decoder	O
part	O
was	O
symmetrical	O
to	O
the	O
encoder	O
.	O
For	O
learning	Task
we	O
applied	O
stochastic	Method
gradient	Method
descent	Method
with	O
mini	O
-	O
batches	O
of	O
64	O
samples	O
for	O
500	O
epochs	O
using	O
the	O
fixed	O
learning	Metric
rates	Metric
(	O
)	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
,	O
that	O
ELUs	Method
outperform	O
the	O
competing	O
activation	Method
functions	Method
in	O
terms	O
of	O
training	Metric
/	Metric
test	Metric
set	Metric
reconstruction	Metric
error	Metric
for	O
all	O
learning	Metric
rates	Metric
.	O
As	O
already	O
noted	O
by	O
,	O
higher	O
learning	Metric
rates	Metric
seem	O
to	O
perform	O
better	O
.	O
subsection	O
:	O
Comparison	O
of	O
Activation	Method
Functions	Method
In	O
this	O
subsection	O
we	O
show	O
that	O
ELUs	Method
indeed	O
possess	O
a	O
superior	O
learning	Metric
behavior	Metric
compared	O
to	O
other	O
activation	Method
functions	Method
as	O
postulated	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Furthermore	O
we	O
show	O
that	O
ELU	Method
networks	Method
perform	O
better	O
than	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
.	O
We	O
use	O
as	O
benchmark	O
dataset	O
CIFAR	Material
-	Material
100	Material
and	O
use	O
a	O
relatively	O
simple	O
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
architecture	O
to	O
keep	O
the	O
computational	Metric
complexity	Metric
reasonable	O
for	O
comparisons	O
.	O
[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
[	O
-	O
2.0ex	O
]	O
The	O
CNN	Method
for	O
these	O
CIFAR	Material
-	Material
100	Material
experiments	O
consists	O
of	O
11	O
convolutional	Method
layers	Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
.	O
2	O
2	O
max	Method
-	Method
pooling	Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
.	O
For	O
network	Task
regularization	Task
we	O
used	O
the	O
following	O
drop	Metric
-	Metric
out	Metric
rate	Metric
for	O
the	O
last	O
layer	O
of	O
each	O
stack	O
(	O
)	O
.	O
The	O
-	O
weight	O
decay	O
regularization	O
term	O
was	O
set	O
to	O
.	O
The	O
following	O
learning	Metric
rate	Metric
schedule	Metric
was	O
applied	O
(	O
)	O
(	O
iterations	O
[	O
learning	O
rate	O
]	O
)	O
.	O
For	O
fair	O
comparisons	O
,	O
we	O
used	O
this	O
learning	Method
rate	Method
schedule	Method
for	O
all	O
networks	O
.	O
During	O
previous	O
experiments	O
,	O
this	O
schedule	O
was	O
optimized	O
for	O
ReLU	Method
networks	O
,	O
however	O
as	O
ELUs	Method
converge	O
faster	O
they	O
would	O
benefit	O
from	O
an	O
adjusted	O
schedule	O
.	O
The	O
momentum	Metric
term	Metric
learning	Metric
rate	Metric
was	O
fixed	O
to	O
0.9	O
.	O
The	O
dataset	O
was	O
preprocessed	O
as	O
described	O
in	O
with	O
global	Method
contrast	Method
normalization	Method
and	O
ZCA	Method
whitening	Method
.	O
Additionally	O
,	O
the	O
images	O
were	O
padded	O
with	O
four	O
zero	O
pixels	O
at	O
all	O
borders	O
.	O
The	O
model	O
was	O
trained	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O
Besides	O
that	O
,	O
we	O
no	O
further	O
augmented	O
the	O
dataset	O
during	O
training	O
.	O
Each	O
network	O
was	O
run	O
10	O
times	O
with	O
different	O
weight	Method
initialization	Method
.	O
Across	O
networks	O
with	O
different	O
activation	O
functions	O
the	O
same	O
run	O
number	O
had	O
the	O
same	O
initial	O
weights	O
.	O
Mean	Metric
test	Metric
error	Metric
results	O
of	O
networks	O
with	O
different	O
activation	O
functions	O
are	O
compared	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
which	O
also	O
shows	O
the	O
standard	O
deviation	O
.	O
ELUs	Method
yield	O
on	O
average	O
a	O
test	Metric
error	Metric
of	O
28.75	O
(	O
0.24	O
)	O
%	O
,	O
while	O
SReLUs	O
,	O
ReLUs	O
and	O
LReLUs	Method
yield	O
29.35	O
(	O
0.29	O
)	O
%	O
,	O
31.56	O
(	O
0.37	O
)	O
%	O
and	O
30.59	O
(	O
0.29	O
)	O
%	O
,	O
respectively	O
.	O
ELUs	Method
achieve	O
both	O
lower	O
training	Metric
loss	Metric
and	O
lower	O
test	Metric
error	Metric
than	O
ReLUs	Method
,	O
LReLUs	Method
,	O
and	O
SReLUs	Method
.	O
Both	O
the	O
ELU	Method
training	Method
and	O
test	O
performance	O
is	O
significantly	O
better	O
than	O
for	O
other	O
activation	Method
functions	Method
(	O
Wilcoxon	O
signed	O
-	O
rank	O
test	O
with	O
-	O
value	O
0.001	O
)	O
.	O
Batch	Method
normalization	Method
improved	O
ReLU	Method
and	O
LReLU	Method
networks	Method
,	O
but	O
did	O
not	O
improve	O
ELU	Method
and	O
SReLU	Method
networks	Method
(	O
see	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
ELU	Method
networks	Method
significantly	O
outperform	O
ReLU	Method
networks	O
with	O
batch	Method
normalization	Method
(	O
Wilcoxon	Metric
signed	Metric
-	Metric
rank	Metric
test	Metric
with	O
-	O
value	O
0.001	O
)	O
.	O
subsection	O
:	O
Classification	Metric
Performance	Metric
on	O
CIFAR	Material
-	Material
100	Material
and	O
CIFAR	Material
-	Material
10	Material
The	O
following	O
experiments	O
should	O
highlight	O
the	O
generalization	Method
capabilities	Method
of	O
ELU	Method
networks	Method
.	O
The	O
CNN	Method
architecture	O
is	O
more	O
sophisticated	O
than	O
in	O
the	O
previous	O
subsection	O
and	O
consists	O
of	O
18	O
convolutional	Method
layers	Method
arranged	O
in	O
stacks	O
of	O
(	O
)	O
.	O
Initial	O
drop	Metric
-	Metric
out	Metric
rate	Metric
,	O
Max	Method
-	Method
pooling	Method
after	O
each	O
stack	O
,	O
-	O
weight	O
decay	O
,	O
momentum	O
term	O
,	O
data	Method
preprocessing	Method
,	O
padding	O
,	O
and	O
cropping	O
were	O
as	O
in	O
previous	O
section	O
.	O
The	O
initial	O
learning	Metric
rate	Metric
was	O
set	O
to	O
0.01	O
and	O
decreased	O
by	O
a	O
factor	O
of	O
10	O
after	O
35k	O
iterations	O
.	O
The	O
mini	O
-	O
batch	O
size	O
was	O
100	O
.	O
For	O
the	O
final	O
50k	O
iterations	O
fine	O
-	O
tuning	O
we	O
increased	O
the	O
drop	Metric
-	Metric
out	Metric
rate	Metric
for	O
all	O
layers	O
in	O
a	O
stack	O
to	O
(	O
)	O
,	O
thereafter	O
increased	O
the	O
drop	Metric
-	Metric
out	Metric
rate	Metric
by	O
a	O
factor	O
of	O
1.5	O
for	O
40k	O
additional	O
iterations	O
.	O
ELU	Method
networks	Method
are	O
compared	O
to	O
following	O
recent	O
successful	O
CNN	Method
architectures	O
:	O
AlexNet	Method
,	O
DSN	Method
,	O
NiN	Method
,	O
Maxout	Method
,	O
All	O
-	O
CNN	Method
,	O
Highway	Method
Network	Method
and	O
Fractional	Method
Max	Method
-	Method
Pooling	Method
.	O
The	O
test	Metric
error	Metric
in	O
percent	Metric
misclassification	Metric
are	O
given	O
in	O
Tab	O
.	O
[	O
reference	O
]	O
.	O
ELU	Method
-	Method
networks	Method
are	O
the	O
second	O
best	O
on	O
CIFAR	Material
-	Material
10	Material
with	O
a	O
test	Metric
error	Metric
of	O
6.55	O
%	O
but	O
still	O
they	O
are	O
among	O
the	O
top	O
10	O
best	O
results	O
reported	O
for	O
CIFAR	Material
-	Material
10	Material
.	O
ELU	Method
networks	Method
performed	O
best	O
on	O
CIFAR	Material
-	Material
100	Material
with	O
a	O
test	Metric
error	Metric
of	O
24.28	O
%	O
.	O
This	O
is	O
the	O
best	O
published	O
result	O
on	O
CIFAR	Material
-	Material
100	Material
,	O
without	O
even	O
resorting	O
to	O
multi	Method
-	Method
view	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O
subsection	O
:	O
ImageNet	O
Challenge	O
Dataset	O
Finally	O
,	O
we	O
evaluated	O
ELU	Method
-	Method
networks	Method
on	O
the	O
1000	O
-	O
class	O
ImageNet	O
dataset	O
.	O
It	O
contains	O
about	O
1.3	O
M	O
training	O
color	O
images	O
as	O
well	O
as	O
additional	O
50k	O
images	O
and	O
100k	O
images	O
for	O
validation	O
and	O
testing	O
,	O
respectively	O
.	O
For	O
this	O
task	O
,	O
we	O
designed	O
a	O
15	O
layer	O
CNN	Method
,	O
which	O
was	O
arranged	O
in	O
stacks	O
of	O
(	O
)	O
layers	O
units	O
receptive	O
fields	O
or	O
fully	Method
-	Method
connected	Method
(	O
FC	Method
)	O
.	O
2	O
2	O
max	Method
-	Method
pooling	Method
with	O
a	O
stride	O
of	O
2	O
was	O
applied	O
after	O
each	O
stack	O
and	O
spatial	Method
pyramid	Method
pooling	Method
(	O
SPP	Method
)	O
with	O
3	O
levels	O
before	O
the	O
first	O
FC	Method
layer	O
.	O
For	O
network	Task
regularization	Task
we	O
set	O
the	O
-	O
weight	O
decay	O
term	O
to	O
and	O
used	O
50	O
%	O
drop	O
-	O
out	O
in	O
the	O
two	O
penultimate	O
FC	Method
layers	O
.	O
Images	O
were	O
re	O
-	O
sized	O
to	O
256	O
256	O
pixels	O
and	O
per	O
-	O
pixel	O
mean	O
subtracted	O
.	O
Trained	O
was	O
on	O
random	O
crops	O
with	O
random	O
horizontal	O
flipping	O
.	O
Besides	O
that	O
,	O
we	O
did	O
not	O
augment	O
the	O
dataset	O
during	O
training	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
learning	O
behavior	O
of	O
ELU	Method
vs.	O
ReLU	Method
networks	O
.	O
Panel	O
(	O
b	O
)	O
shows	O
that	O
ELUs	Method
start	O
reducing	O
the	O
error	Metric
earlier	O
.	O
The	O
ELU	Method
-	Method
network	Method
already	O
reaches	O
the	O
20	O
%	O
top	Metric
-	Metric
5	Metric
error	Metric
after	O
160k	O
iterations	O
,	O
while	O
the	O
ReLU	Method
network	O
needs	O
200k	O
iterations	O
to	O
reach	O
the	O
same	O
error	Metric
rate	Metric
.	O
The	O
single	O
-	O
model	O
performance	O
was	O
evaluated	O
on	O
the	O
single	O
center	Task
crop	Task
with	O
no	O
further	O
augmentation	O
and	O
yielded	O
a	O
top	Metric
-	Metric
5	Metric
validation	Metric
error	Metric
below	O
10	O
%	O
.	O
Currently	O
ELU	Method
nets	Method
are	O
5	O
%	O
slower	O
on	O
ImageNet	Method
than	O
ReLU	Method
nets	O
.	O
The	O
difference	O
is	O
small	O
because	O
activation	O
functions	O
generally	O
have	O
only	O
minor	O
influence	O
on	O
the	O
overall	O
training	Metric
time	Metric
.	O
In	O
terms	O
of	O
wall	Metric
clock	Metric
time	Metric
,	O
ELUs	Method
require	O
12.15h	O
vs.	O
ReLUs	Method
with	O
11.48h	O
for	O
10k	O
iterations	O
.	O
We	O
expect	O
that	O
ELU	Method
implementations	Method
can	O
be	O
improved	O
,	O
e.g.	O
by	O
faster	O
exponential	Method
functions	Method
.	O
section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
the	O
exponential	Method
linear	Method
units	Method
(	O
ELUs	Method
)	O
for	O
faster	O
and	O
more	O
precise	O
learning	Task
in	O
deep	Task
neural	Task
networks	Task
.	O
ELUs	Method
have	O
negative	O
values	O
,	O
which	O
allows	O
the	O
network	O
to	O
push	O
the	O
mean	O
activations	O
closer	O
to	O
zero	O
.	O
Therefore	O
ELUs	Method
decrease	O
the	O
gap	O
between	O
the	O
normal	O
gradient	O
and	O
the	O
unit	O
natural	O
gradient	O
and	O
,	O
thereby	O
speed	O
up	O
learning	Task
.	O
We	O
believe	O
that	O
this	O
property	O
is	O
also	O
the	O
reason	O
for	O
the	O
success	O
of	O
activation	O
functions	O
like	O
LReLUs	Method
and	O
PReLUs	Method
and	O
of	O
batch	Method
normalization	Method
.	O
In	O
contrast	O
to	O
LReLUs	Method
and	O
PReLUs	Method
,	O
ELUs	Method
have	O
a	O
clear	O
saturation	O
plateau	O
in	O
its	O
negative	O
regime	O
,	O
allowing	O
them	O
to	O
learn	O
a	O
more	O
robust	O
and	O
stable	O
representation	O
.	O
Experimental	O
results	O
show	O
that	O
ELUs	Method
significantly	O
outperform	O
other	O
activation	Method
functions	Method
on	O
different	O
vision	O
datasets	O
.	O
Further	O
ELU	Method
networks	Method
perform	O
significantly	O
better	O
than	O
ReLU	Method
networks	O
trained	O
with	O
batch	Method
normalization	Method
.	O
ELU	Method
networks	Method
achieved	O
one	O
of	O
the	O
top	O
10	O
best	O
reported	O
results	O
on	O
CIFAR	Material
-	Material
10	Material
and	O
set	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
CIFAR	Material
-	Material
100	Material
without	O
the	O
need	O
for	O
multi	Method
-	Method
view	Method
test	Method
evaluation	Method
or	O
model	Method
averaging	Method
.	O
Furthermore	O
,	O
ELU	Method
networks	Method
produced	O
competitive	O
results	O
on	O
the	O
ImageNet	O
in	O
much	O
fewer	O
epochs	O
than	O
a	O
corresponding	O
ReLU	Method
network	O
.	O
Given	O
their	O
outstanding	O
performance	O
,	O
we	O
expect	O
ELU	Method
networks	Method
to	O
become	O
a	O
real	O
time	O
saver	O
in	O
convolutional	Method
networks	Method
,	O
which	O
are	O
notably	O
time	O
-	O
intensive	O
to	O
train	O
from	O
scratch	O
otherwise	O
.	O
paragraph	O
:	O
Acknowledgment	O
.	O
We	O
thank	O
the	O
NVIDIA	O
Corporation	O
for	O
supporting	O
this	O
research	O
with	O
several	O
Titan	O
X	O
GPUs	O
and	O
Roland	O
Vollgraf	O
and	O
Martin	O
Heusel	O
for	O
helpful	O
discussions	O
and	O
comments	O
on	O
this	O
work	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Inverse	Method
of	Method
Block	Method
Matrices	Method
theorem	O
:	O
.	O
The	O
positive	O
definite	O
matrix	O
M	O
is	O
in	O
block	O
format	O
with	O
matrix	O
A	O
,	O
vector	O
b	O
,	O
and	O
scalar	O
c.	O
The	O
inverse	O
of	O
M	O
is	O
where	O
proof	O
:	O
Proof	O
.	O
For	O
block	O
matrices	O
the	O
inverse	O
is	O
where	O
the	O
matrices	O
on	O
the	O
right	O
hand	O
side	O
are	O
:	O
Further	O
if	O
follows	O
that	O
We	O
now	O
use	O
this	O
formula	O
for	O
being	O
a	O
vector	O
and	O
a	O
scalar	O
.	O
We	O
obtain	O
where	O
the	O
right	O
hand	O
side	O
matrices	O
,	O
vectors	O
,	O
and	O
the	O
scalar	O
are	O
:	O
Again	O
it	O
follows	O
that	O
A	O
reformulation	O
using	O
gives	O
∎	O
appendix	O
:	O
Quadratic	O
Form	O
of	O
Mean	O
and	O
Inverse	Method
Second	Method
Moment	Method
theorem	O
:	O
.	O
For	O
a	O
random	O
variable	O
a	O
holds	O
and	O
Furthermore	O
holds	O
proof	O
:	O
Proof	O
.	O
The	O
Sherman	Method
-	Method
Morrison	Method
Theorem	Method
states	O
Therefore	O
we	O
have	O
Using	O
the	O
identity	O
for	O
the	O
second	O
moment	O
and	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
we	O
get	O
The	O
last	O
inequality	O
follows	O
from	O
the	O
fact	O
that	O
is	O
positive	O
definite	O
.	O
From	O
last	O
equation	O
,	O
we	O
obtain	O
further	O
For	O
the	O
mixed	O
quadratic	O
form	O
we	O
get	O
from	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
From	O
this	O
equation	O
follows	O
Therefore	O
we	O
get	O
∎	O
appendix	O
:	O
Variance	O
of	O
Mean	O
Activations	O
in	O
ELU	Method
and	O
ReLU	Method
Networks	O
To	O
compare	O
the	O
variance	O
of	O
median	O
activation	O
in	O
ReLU	Method
and	O
ELU	Method
networks	Method
,	O
we	O
trained	O
a	O
neural	Method
network	Method
with	O
5	O
hidden	O
layers	O
of	O
256	O
hidden	O
units	O
for	O
200	O
epochs	O
using	O
a	O
learning	O
rate	O
of	O
0.01	O
,	O
once	O
using	O
ReLU	Method
and	O
once	O
using	O
ELU	Method
activation	Method
functions	Method
on	O
the	O
MNIST	O
dataset	O
.	O
After	O
each	O
epoch	O
,	O
we	O
calculated	O
the	O
median	O
activation	O
of	O
each	O
hidden	Method
unit	Method
on	O
the	O
whole	O
training	O
set	O
.	O
We	O
then	O
calculated	O
the	O
variance	O
of	O
these	O
changes	O
,	O
which	O
is	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
median	O
varies	O
much	O
more	O
in	O
ReLU	Method
networks	O
.	O
This	O
indicates	O
that	O
ReLU	Method
networks	O
continuously	O
try	O
to	O
correct	O
the	O
bias	O
shift	O
introduced	O
by	O
previous	O
weight	O
updates	O
while	O
this	O
effect	O
is	O
much	O
less	O
prominent	O
in	O
ELU	Method
networks	Method
.	O
