document	O
:	O
PoseCNN	Method
:	O
A	O
Convolutional	Method
Neural	Method
Network	Method
for	O
6D	Task
Object	Task
Pose	Task
Estimation	Task
in	Task
Cluttered	Task
Scenes	Task
Estimating	Task
the	Task
6D	Task
pose	Task
of	Task
known	Task
objects	Task
is	O
important	O
for	O
robots	O
to	O
interact	O
with	O
the	O
real	O
world	O
.	O
The	O
problem	O
is	O
challenging	O
due	O
to	O
the	O
variety	O
of	O
objects	O
as	O
well	O
as	O
the	O
complexity	O
of	O
a	O
scene	O
caused	O
by	O
clutter	O
and	O
occlusions	O
between	O
objects	O
.	O
In	O
this	O
work	O
,	O
we	O
introduce	O
PoseCNN	Method
,	O
a	O
new	O
Convolutional	Method
Neural	Method
Network	Method
for	O
6D	O
object	O
pose	Task
estimation	Task
.	O
PoseCNN	Method
estimates	O
the	O
3D	Task
translation	Task
of	O
an	O
object	O
by	O
localizing	O
its	O
center	O
in	O
the	O
image	O
and	O
predicting	O
its	O
distance	O
from	O
the	O
camera	O
.	O
The	O
3D	O
rotation	O
of	O
the	O
object	O
is	O
estimated	O
by	O
regressing	Method
to	O
a	O
quaternion	Method
representation	Method
.	O
We	O
also	O
introduce	O
a	O
novel	O
loss	Method
function	Method
that	O
enables	O
PoseCNN	Method
to	O
handle	O
symmetric	O
objects	O
.	O
In	O
addition	O
,	O
we	O
contribute	O
a	O
large	O
scale	O
video	O
dataset	O
for	O
6D	O
object	O
pose	Task
estimation	Task
named	O
the	O
YCB	Material
-	Material
Video	Material
dataset	Material
.	O
Our	O
dataset	O
provides	O
accurate	O
6D	O
poses	O
of	O
21	O
objects	O
from	O
the	O
YCB	Material
dataset	Material
observed	O
in	O
92	O
videos	O
with	O
133	O
,	O
827	O
frames	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
our	O
YCB	Material
-	Material
Video	Material
dataset	Material
and	O
the	O
OccludedLINEMOD	Material
dataset	Material
to	O
show	O
that	O
PoseCNN	Method
is	O
highly	O
robust	O
to	O
occlusions	O
,	O
can	O
handle	O
symmetric	O
objects	O
,	O
and	O
provide	O
accurate	O
pose	Task
estimation	Task
using	O
only	O
color	O
images	O
as	O
input	O
.	O
When	O
using	O
depth	O
data	O
to	O
further	O
refine	O
the	O
poses	O
,	O
our	O
approach	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
challenging	O
OccludedLINEMOD	Material
dataset	Material
.	O
Our	O
code	O
and	O
dataset	O
are	O
available	O
at	O
.	O
section	O
:	O
INTRODUCTION	O
Recognizing	Task
objects	Task
and	O
estimating	Task
their	Task
poses	Task
in	O
3D	Task
has	O
a	O
wide	O
range	O
of	O
applications	O
in	O
robotic	Task
tasks	Task
.	O
For	O
instance	O
,	O
recognizing	O
the	O
3D	Task
location	Task
and	Task
orientation	Task
of	Task
objects	Task
is	O
important	O
for	O
robot	Task
manipulation	Task
.	O
It	O
is	O
also	O
useful	O
in	O
human	Task
-	Task
robot	Task
interaction	Task
tasks	Task
such	O
as	O
learning	Task
from	Task
demonstration	Task
.	O
However	O
,	O
the	O
problem	O
is	O
challenging	O
due	O
to	O
the	O
variety	O
of	O
objects	O
in	O
the	O
real	O
world	O
.	O
They	O
have	O
different	O
3D	O
shapes	O
,	O
and	O
their	O
appearances	O
on	O
images	O
are	O
affected	O
by	O
lighting	O
conditions	O
,	O
clutter	O
in	O
the	O
scene	O
and	O
occlusions	O
between	O
objects	O
.	O
Traditionally	O
,	O
the	O
problem	O
of	O
6D	O
object	O
pose	Task
estimation	Task
is	O
tackled	O
by	O
matching	O
feature	O
points	O
between	O
3D	Method
models	Method
and	O
images	O
.	O
However	O
,	O
these	O
methods	O
require	O
that	O
there	O
are	O
rich	O
textures	O
on	O
the	O
objects	O
in	O
order	O
to	O
detect	O
feature	O
points	O
for	O
matching	Task
.	O
As	O
a	O
result	O
,	O
they	O
are	O
unable	O
to	O
handle	O
texture	O
-	O
less	O
objects	O
.	O
With	O
the	O
emergence	O
of	O
depth	O
cameras	O
,	O
several	O
methods	O
have	O
been	O
proposed	O
for	O
recognizing	Task
texture	Task
-	Task
less	Task
objects	Task
using	O
RGB	Material
-	Material
D	Material
data	Material
.	O
For	O
template	Method
-	Method
based	Method
methods	Method
,	O
occlusions	O
significantly	O
reduce	O
the	O
recognition	Task
performance	O
.	O
Alternatively	O
,	O
methods	O
that	O
perform	O
learning	O
to	O
regress	O
image	O
pixels	O
to	O
3D	O
object	O
coordinates	O
in	O
order	O
to	O
establish	O
the	O
2D	O
-	O
3D	O
correspondences	O
for	O
6D	Task
pose	Task
estimation	Task
can	O
not	O
handle	O
symmetric	O
objects	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
generic	Method
framework	Method
for	O
6D	O
object	O
pose	Task
estimation	Task
where	O
we	O
attempt	O
to	O
overcome	O
the	O
limitations	O
of	O
existing	O
methods	O
.	O
We	O
introduce	O
a	O
novel	O
Convolutional	Method
Neural	Method
Network	Method
(	O
CNN	Method
)	O
for	O
end	O
-	O
to	O
-	O
end	O
6D	Task
pose	Task
estimation	Task
named	O
PoseCNN	Method
.	O
A	O
key	O
idea	O
behind	O
PoseCNN	Method
is	O
to	O
decouple	O
the	O
pose	Task
estimation	Task
task	Task
into	O
different	O
components	O
,	O
which	O
enables	O
the	O
network	O
to	O
explicitly	O
model	O
the	O
dependencies	O
and	O
independencies	O
between	O
them	O
.	O
Specifically	O
,	O
PoseCNN	Method
performs	O
three	O
related	O
tasks	O
as	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
First	O
,	O
it	O
predicts	O
an	O
object	O
label	O
for	O
each	O
pixel	O
in	O
the	O
input	O
image	O
.	O
Second	O
,	O
it	O
estimates	O
the	O
2D	O
pixel	O
coordinates	O
of	O
the	O
object	O
center	O
by	O
predicting	O
a	O
unit	O
vector	O
from	O
each	O
pixel	O
towards	O
the	O
center	O
.	O
Using	O
the	O
semantic	O
labels	O
,	O
image	O
pixels	O
associated	O
with	O
an	O
object	O
vote	O
on	O
the	O
object	O
center	O
location	O
in	O
the	O
image	O
.	O
In	O
addition	O
,	O
the	O
network	O
also	O
estimates	O
the	O
distance	O
of	O
the	O
object	O
center	O
.	O
Assuming	O
known	O
camera	O
intrinsics	O
,	O
estimation	Task
of	O
the	O
2D	O
object	O
center	O
and	O
its	O
distance	O
enables	O
us	O
to	O
recover	O
its	O
3D	O
translation	O
.	O
Finally	O
,	O
the	O
3D	O
Rotation	O
is	O
estimated	O
by	O
regressing	O
convolutional	O
features	O
extracted	O
inside	O
the	O
bounding	O
box	O
of	O
the	O
object	O
to	O
a	O
quaternion	Method
representation	Method
of	O
.	O
As	O
we	O
will	O
show	O
,	O
the	O
2D	Method
center	Method
voting	Method
followed	O
by	O
rotation	Method
regression	Method
to	O
estimate	O
and	O
can	O
be	O
applied	O
to	O
textured	Task
/	Task
texture	Task
-	Task
less	Task
objects	Task
and	O
is	O
robust	O
to	O
occlusions	O
since	O
the	O
network	O
is	O
trained	O
to	O
vote	O
on	O
object	O
centers	O
even	O
when	O
they	O
are	O
occluded	O
.	O
Handling	Task
symmetric	Task
objects	Task
is	O
another	O
challenge	O
for	O
pose	Task
estimation	Task
,	O
since	O
different	O
object	O
orientations	O
may	O
generate	O
identical	O
observations	O
.	O
For	O
instance	O
,	O
it	O
is	O
not	O
possible	O
to	O
uniquely	O
estimate	O
the	O
orientation	O
of	O
the	O
red	O
bowl	O
or	O
the	O
wood	O
block	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
While	O
pose	O
benchmark	O
datasets	O
such	O
as	O
the	O
OccludedLINEMOD	Material
dataset	Material
consider	O
a	O
special	O
symmetric	Metric
evaluation	Metric
for	O
such	O
objects	O
,	O
symmetries	O
are	O
typically	O
ignored	O
during	O
network	Method
training	Method
.	O
However	O
,	O
this	O
can	O
result	O
in	O
bad	O
training	Task
performance	O
since	O
a	O
network	O
receives	O
inconsistent	O
loss	O
signals	O
,	O
such	O
as	O
a	O
high	O
loss	O
on	O
an	O
object	O
orientation	O
even	O
though	O
the	O
estimation	Task
from	O
the	O
network	O
is	O
correct	O
with	O
respect	O
to	O
the	O
symmetry	O
of	O
the	O
object	O
.	O
Inspired	O
by	O
this	O
observation	O
,	O
we	O
introduce	O
ShapeMatch	Method
-	Method
Loss	Method
,	O
a	O
new	O
loss	Method
function	Method
that	O
focuses	O
on	O
matching	O
the	O
3D	O
shape	O
of	O
an	O
object	O
.	O
We	O
will	O
show	O
that	O
this	O
loss	Method
function	Method
produces	O
superior	O
estimation	Task
for	O
objects	O
with	O
shape	O
symmetries	O
.	O
We	O
evaluate	O
our	O
method	O
on	O
the	O
OccludedLINEMOD	Material
dataset	Material
,	O
a	O
benchmark	O
dataset	O
for	O
6D	Task
pose	Task
estimation	Task
.	O
On	O
this	O
challenging	O
dataset	O
,	O
PoseCNN	Method
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
both	O
color	Task
only	Task
and	O
RGB	Task
-	Task
D	Task
pose	Task
estimation	Task
(	O
we	O
use	O
depth	O
images	O
in	O
the	O
Iterative	Method
Closest	Method
Point	Method
(	O
ICP	Method
)	O
algorithm	O
for	O
pose	Task
refinement	Task
)	O
.	O
To	O
thoroughly	O
evaluate	O
our	O
method	O
,	O
we	O
additionally	O
collected	O
a	O
large	Material
scale	Material
RGB	Material
-	Material
D	Material
video	Material
dataset	Material
named	O
YCB	Material
-	Material
Video	Material
,	O
which	O
contains	O
6D	O
poses	O
of	O
21	O
objects	O
from	O
the	O
YCB	O
object	O
set	O
in	O
92	O
videos	O
with	O
a	O
total	O
of	O
133	O
,	O
827	O
frames	O
.	O
Objects	O
in	O
the	O
dataset	O
exhibit	O
different	O
symmetries	O
and	O
are	O
arranged	O
in	O
various	O
poses	O
and	O
spatial	O
configurations	O
,	O
generating	O
severe	O
occlusions	O
between	O
them	O
.	O
In	O
summary	O
,	O
our	O
work	O
has	O
the	O
following	O
key	O
contributions	O
:	O
We	O
propose	O
a	O
novel	O
convolutional	Method
neural	Method
network	Method
for	O
6D	O
object	O
pose	Task
estimation	Task
named	O
PoseCNN	Method
.	O
Our	O
network	O
achieves	O
end	O
-	O
to	O
-	O
end	O
6D	O
pose	Task
estimation	Task
and	O
is	O
very	O
robust	O
to	O
occlusions	O
between	O
objects	O
.	O
We	O
introduce	O
ShapeMatch	Method
-	Method
Loss	Method
,	O
a	O
new	O
training	Method
loss	Method
function	Method
for	O
pose	Task
estimation	Task
of	Task
symmetric	Task
objects	Task
.	O
We	O
contribute	O
a	O
large	Material
scale	Material
RGB	Material
-	Material
D	Material
video	Material
dataset	Material
for	O
6D	O
object	O
pose	Task
estimation	Task
,	O
where	O
we	O
provide	O
6D	O
pose	O
annotations	O
for	O
21	O
YCB	O
objects	O
.	O
This	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
After	O
discussing	O
related	O
work	O
,	O
we	O
introduce	O
PoseCNN	Method
for	O
6D	O
object	O
pose	Task
estimation	Task
,	O
followed	O
by	O
experimental	O
results	O
and	O
a	O
conclusion	O
.	O
section	O
:	O
RELATED	O
WORK	O
6D	O
object	O
pose	Task
estimation	Task
methods	O
in	O
the	O
literature	O
can	O
be	O
roughly	O
classified	O
into	O
template	Method
-	Method
based	Method
methods	Method
and	O
feature	Method
-	Method
based	Method
methods	Method
.	O
In	O
template	Method
-	Method
based	Method
methods	Method
,	O
a	O
rigid	Method
template	Method
is	O
constructed	O
and	O
used	O
to	O
scan	O
different	O
locations	O
in	O
the	O
input	O
image	O
.	O
At	O
each	O
location	O
,	O
a	O
similarity	Metric
score	Metric
is	O
computed	O
,	O
and	O
the	O
best	O
match	O
is	O
obtained	O
by	O
comparing	O
these	O
similarity	Metric
scores	Metric
.	O
In	O
6D	Task
pose	Task
estimation	Task
,	O
a	O
template	O
is	O
usually	O
obtained	O
by	O
rendering	O
the	O
corresponding	O
3D	Method
model	Method
.	O
Recently	O
,	O
2D	Method
object	Method
detection	Method
methods	Method
are	O
used	O
as	O
template	Method
matching	Method
and	O
augmented	O
for	O
6D	Task
pose	Task
estimation	Task
,	O
especially	O
with	O
deep	Method
learning	Method
-	Method
based	Method
object	Method
detectors	Method
.	O
Template	Method
-	Method
based	Method
methods	Method
are	O
useful	O
in	O
detecting	Task
texture	Task
-	Task
less	Task
objects	Task
.	O
However	O
,	O
they	O
can	O
not	O
handle	O
occlusions	O
between	O
objects	O
very	O
well	O
,	O
since	O
the	O
template	O
will	O
have	O
low	Metric
similarity	Metric
score	Metric
if	O
the	O
object	O
is	O
occluded	O
.	O
In	O
feature	Method
-	Method
based	Method
methods	Method
,	O
local	O
features	O
are	O
extracted	O
from	O
either	O
points	O
of	O
interest	O
or	O
every	O
pixel	O
in	O
the	O
image	O
and	O
matched	O
to	O
features	O
on	O
the	O
3D	Method
models	Method
to	O
establish	O
the	O
2D	O
-	O
3D	O
correspondences	O
,	O
from	O
which	O
6D	O
poses	O
can	O
be	O
recovered	O
.	O
Feature	Method
-	Method
based	Method
methods	Method
are	O
able	O
to	O
handle	O
occlusions	O
between	O
objects	O
.	O
However	O
,	O
they	O
require	O
sufficient	O
textures	O
on	O
the	O
objects	O
in	O
order	O
to	O
compute	O
the	O
local	O
features	O
.	O
To	O
deal	O
with	O
texture	O
-	O
less	O
objects	O
,	O
several	O
methods	O
are	O
proposed	O
to	O
learn	O
feature	Method
descriptors	Method
using	O
machine	Method
learning	Method
techniques	Method
.	O
A	O
few	O
approaches	O
have	O
been	O
proposed	O
to	O
directly	O
regress	O
to	O
3D	O
object	O
coordinate	O
location	O
for	O
each	O
pixel	O
to	O
establish	O
the	O
2D	O
-	O
3D	O
correspondences	O
.	O
But	O
3D	Method
coordinate	Method
regression	Method
encounters	O
ambiguities	O
in	O
dealing	O
with	O
symmetric	O
objects	O
.	O
In	O
this	O
work	O
,	O
we	O
combine	O
the	O
advantages	O
of	O
both	O
template	Method
-	Method
based	Method
methods	Method
and	O
feature	Method
-	Method
based	Method
methods	Method
in	O
a	O
deep	Method
learning	Method
framework	Method
,	O
where	O
the	O
network	O
combines	O
bottom	Method
-	Method
up	Method
pixel	Method
-	Method
wise	Method
labeling	Method
with	O
top	Method
-	Method
down	Method
object	Method
pose	Method
regression	Method
.	O
Recently	O
,	O
the	O
6D	O
object	O
pose	Task
estimation	Task
problem	O
has	O
received	O
more	O
attention	O
thanks	O
to	O
the	O
competition	O
in	O
the	O
Amazon	Material
Picking	Material
Challenge	Material
(	O
APC	Material
)	O
.	O
Several	O
datasets	O
and	O
approaches	O
have	O
been	O
introduced	O
for	O
the	O
specific	O
setting	O
in	O
the	O
APC	Task
.	O
Our	O
network	O
has	O
the	O
potential	O
to	O
be	O
applied	O
to	O
the	O
APC	Task
setting	Task
as	O
long	O
as	O
the	O
appropriate	O
training	O
data	O
is	O
provided	O
.	O
section	O
:	O
PoseCNN	Method
Given	O
an	O
input	O
image	O
,	O
the	O
task	O
of	O
6D	O
object	O
pose	Task
estimation	Task
is	O
to	O
estimate	O
the	O
rigid	O
transformation	O
from	O
the	O
object	O
coordinate	O
system	O
to	O
the	O
camera	O
coordinate	O
system	O
.	O
We	O
assume	O
that	O
the	O
3D	Method
model	Method
of	O
the	O
object	O
is	O
available	O
and	O
the	O
object	Method
coordinate	Method
system	Method
is	O
defined	O
in	O
the	O
3D	O
space	O
of	O
the	O
model	O
.	O
The	O
rigid	O
transformation	O
here	O
consists	O
of	O
an	O
SE	Method
(	Method
3	Method
)	Method
transform	Method
containing	O
a	O
3D	O
rotation	O
and	O
a	O
3D	O
translation	O
,	O
where	O
specifies	O
the	O
rotation	O
angles	O
around	O
the	O
-	O
axis	O
,	O
-	O
axis	O
and	O
-	O
axis	O
of	O
the	O
object	O
coordinate	O
system	O
,	O
and	O
is	O
the	O
coordinate	O
of	O
the	O
origin	O
of	O
in	O
the	O
camera	O
coordinate	O
system	O
.	O
In	O
the	O
imaging	Task
process	Task
,	O
determines	O
the	O
object	O
location	O
and	O
scale	O
in	O
the	O
image	O
,	O
while	O
affects	O
the	O
image	O
appearance	O
of	O
the	O
object	O
according	O
to	O
the	O
3D	O
shape	O
and	O
texture	O
of	O
the	O
object	O
.	O
Since	O
these	O
two	O
parameters	O
have	O
distinct	O
visual	O
properties	O
,	O
we	O
propose	O
a	O
convolutional	Method
neural	Method
network	Method
architecture	Method
that	O
internally	O
decouples	O
the	O
estimation	Task
of	O
and	O
.	O
subsection	O
:	O
Overview	O
of	O
the	O
Network	O
Fig	O
.	O
[	O
reference	O
]	O
illustrates	O
the	O
architecture	O
of	O
our	O
network	O
for	O
6D	O
object	O
pose	Task
estimation	Task
.	O
The	O
network	O
contains	O
two	O
stages	O
.	O
The	O
first	O
stage	O
consists	O
of	O
13	O
convolutional	Method
layers	Method
and	O
4	O
max	Method
-	Method
pooling	Method
layers	Method
,	O
which	O
extract	O
feature	O
maps	O
with	O
different	O
resolutions	O
from	O
the	O
input	O
image	O
.	O
This	O
stage	O
is	O
the	O
backbone	O
of	O
the	O
network	O
since	O
the	O
extracted	O
features	O
are	O
shared	O
across	O
all	O
the	O
tasks	O
performed	O
by	O
the	O
network	O
.	O
The	O
second	O
stage	O
consists	O
of	O
an	O
embedding	Method
step	Method
that	O
embeds	O
the	O
high	O
-	O
dimensional	O
feature	O
maps	O
generated	O
by	O
the	O
first	O
stage	O
into	O
low	O
-	O
dimensional	O
,	O
task	O
-	O
specific	O
features	O
.	O
Then	O
,	O
the	O
network	O
performs	O
three	O
different	O
tasks	O
that	O
lead	O
to	O
the	O
6D	Task
pose	Task
estimation	Task
,	O
i.e.	O
,	O
semantic	Task
labeling	Task
,	O
3D	Task
translation	Task
estimation	Task
,	O
and	O
3D	Task
rotation	Task
regression	Task
,	O
as	O
described	O
next	O
.	O
subsection	O
:	O
Semantic	Task
Labeling	Task
In	O
order	O
to	O
detect	O
objects	O
in	O
images	O
,	O
we	O
resort	O
to	O
semantic	Task
labeling	Task
,	O
where	O
the	O
network	O
classifies	O
each	O
image	O
pixel	O
into	O
an	O
object	O
class	O
.	O
Compared	O
to	O
recent	O
6D	O
pose	Task
estimation	Task
methods	O
that	O
resort	O
to	O
object	Task
detection	Task
with	O
bounding	O
boxes	O
,	O
semantic	Method
labeling	Method
provides	O
richer	O
information	O
about	O
the	O
objects	O
and	O
handles	O
occlusions	O
better	O
.	O
The	O
embedding	Method
step	Method
of	O
the	O
semantic	O
labeling	O
branch	O
,	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
takes	O
two	O
feature	O
maps	O
with	O
channel	O
dimension	O
512	O
generated	O
by	O
the	O
feature	Method
extraction	Method
stage	Method
as	O
inputs	O
.	O
The	O
resolutions	O
of	O
the	O
two	O
feature	Method
maps	Method
are	O
and	O
of	O
the	O
original	O
image	O
size	O
,	O
respectively	O
.	O
The	O
network	O
first	O
reduces	O
the	O
channel	O
dimension	O
of	O
the	O
two	O
feature	O
maps	O
to	O
64	O
using	O
two	O
convolutional	Method
layers	Method
.	O
Then	O
it	O
doubles	O
the	O
resolution	O
of	O
the	O
feature	Method
map	Method
with	O
a	O
deconvolutional	Method
layer	Method
.	O
After	O
that	O
,	O
the	O
two	O
feature	Method
maps	Method
are	O
summed	O
and	O
another	O
deconvolutional	Method
layer	Method
is	O
used	O
to	O
increase	O
the	O
resolution	O
by	O
8	O
times	O
in	O
order	O
to	O
obtain	O
a	O
feature	O
map	O
with	O
the	O
original	O
image	O
size	O
.	O
Finally	O
,	O
a	O
convolutional	Method
layer	Method
operates	O
on	O
the	O
feature	O
map	O
and	O
generates	O
semantic	O
labeling	O
scores	O
for	O
pixels	O
.	O
The	O
output	O
of	O
this	O
layer	O
has	O
channels	O
with	O
the	O
number	O
of	O
the	O
semantic	O
classes	O
.	O
In	O
training	Task
,	O
a	O
softmax	O
cross	O
entropy	O
loss	O
is	O
applied	O
to	O
train	O
the	O
semantic	Task
labeling	Task
branch	Task
.	O
While	O
in	O
testing	Task
,	O
a	O
softmax	O
function	O
is	O
used	O
to	O
compute	O
the	O
class	O
probabilities	O
of	O
the	O
pixels	O
.	O
The	O
design	O
of	O
the	O
semantic	Task
labeling	Task
branch	Task
is	O
inspired	O
by	O
the	O
fully	Method
convolutional	Method
network	Method
in	O
for	O
semantic	Task
labeling	Task
.	O
It	O
is	O
also	O
used	O
in	O
our	O
previous	O
work	O
for	O
scene	Task
labeling	Task
.	O
subsection	O
:	O
3D	Task
Translation	Task
Estimation	Task
As	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
the	O
3D	O
translation	O
is	O
the	O
coordinate	O
of	O
the	O
object	O
origin	O
in	O
the	O
camera	O
coordinate	O
system	O
.	O
A	O
naive	O
way	O
of	O
estimating	Task
is	O
to	O
directly	O
regress	O
the	O
image	O
features	O
to	O
.	O
However	O
,	O
this	O
approach	O
is	O
not	O
generalizable	O
since	O
objects	O
can	O
appear	O
in	O
any	O
location	O
in	O
the	O
image	O
.	O
Also	O
,	O
it	O
can	O
not	O
handle	O
multiple	O
object	O
instances	O
in	O
the	O
same	O
category	O
.	O
Therefore	O
,	O
we	O
propose	O
to	O
estimate	O
the	O
3D	O
translation	O
by	O
localizing	O
the	O
2D	O
object	O
center	O
in	O
the	O
image	O
and	O
estimating	O
object	O
distance	O
from	O
the	O
camera	O
.	O
To	O
see	O
,	O
suppose	O
the	O
projection	O
of	O
on	O
the	O
image	O
is	O
.	O
If	O
the	O
network	O
can	O
localize	O
in	O
the	O
image	O
and	O
estimate	O
the	O
depth	O
,	O
then	O
we	O
can	O
recover	O
and	O
according	O
to	O
the	O
following	O
projection	Method
equation	Method
assuming	O
a	O
pinhole	O
camera	O
:	O
where	O
and	O
denote	O
the	O
focal	O
lengths	O
of	O
the	O
camera	O
,	O
and	O
is	O
the	O
principal	O
point	O
.	O
If	O
the	O
object	O
origin	O
is	O
the	O
centroid	O
of	O
the	O
object	O
,	O
we	O
call	O
the	O
2D	O
center	O
of	O
the	O
object	O
.	O
A	O
straightforward	O
way	O
for	O
localizing	Task
the	Task
2D	Task
object	Task
center	Task
is	O
to	O
directly	O
detect	O
the	O
center	O
point	O
as	O
in	O
existing	O
key	Method
point	Method
detection	Method
methods	Method
.	O
However	O
,	O
these	O
methods	O
would	O
not	O
work	O
if	O
the	O
object	O
center	O
is	O
occluded	O
.	O
Inspired	O
by	O
the	O
traditional	O
Implicit	Method
Shape	Method
Model	Method
(	O
ISM	Method
)	Method
in	O
which	O
image	O
patches	O
vote	O
for	O
the	O
object	O
center	O
for	O
detection	Task
,	O
we	O
design	O
our	O
network	O
to	O
regress	O
to	O
the	O
center	O
direction	O
for	O
each	O
pixel	O
in	O
the	O
image	O
.	O
Specifically	O
,	O
for	O
a	O
pixel	O
on	O
the	O
image	O
,	O
it	O
regresses	O
to	O
three	O
variables	O
:	O
Note	O
that	O
instead	O
of	O
directly	O
regressing	O
to	O
the	O
displacement	O
vector	O
,	O
we	O
design	O
the	O
network	O
to	O
regress	O
to	O
the	O
unit	O
length	O
vector	O
,	O
i.e	O
.	O
,	O
2D	O
center	O
direction	O
,	O
which	O
is	O
scale	O
-	O
invariant	O
and	O
therefore	O
easier	O
to	O
be	O
trained	O
(	O
as	O
we	O
verified	O
experimentally	O
)	O
.	O
The	O
center	O
regression	O
branch	O
of	O
our	O
network	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
uses	O
the	O
same	O
architecture	O
as	O
the	O
semantic	O
labeling	O
branch	O
,	O
except	O
that	O
the	O
channel	O
dimensions	O
of	O
the	O
convolutional	Method
layers	Method
and	O
the	O
deconvolutional	Method
layers	Method
are	O
different	O
.	O
We	O
embed	O
the	O
high	O
-	O
dimensional	O
features	O
into	O
a	O
128	O
-	O
dimensional	O
space	O
instead	O
of	O
64	O
-	O
dimensional	O
since	O
this	O
branch	O
needs	O
to	O
regress	O
to	O
three	O
variables	O
for	O
each	O
object	O
class	O
.	O
The	O
last	O
convolutional	Method
layer	Method
in	O
this	O
branch	O
has	O
channel	O
dimension	O
with	O
the	O
number	O
of	O
object	O
classes	O
.	O
In	O
training	Task
,	O
a	O
smoothed	Method
L1	Method
loss	Method
function	Method
is	O
applied	O
for	O
regression	Task
as	O
in	O
.	O
In	O
order	O
to	O
find	O
the	O
2D	O
object	O
center	O
of	O
an	O
object	O
,	O
a	O
Hough	Method
voting	Method
layer	Method
is	O
designed	O
and	O
integrated	O
into	O
the	O
network	O
.	O
The	O
Hough	Method
voting	Method
layer	Method
takes	O
the	O
pixel	O
-	O
wise	O
semantic	O
labeling	O
results	O
and	O
the	O
center	O
regression	O
results	O
as	O
inputs	O
.	O
For	O
each	O
object	O
class	O
,	O
it	O
first	O
computes	O
the	O
voting	Metric
score	Metric
for	O
every	O
location	O
in	O
the	O
image	O
.	O
The	O
voting	Metric
score	Metric
indicates	O
how	O
likely	O
the	O
corresponding	O
image	O
location	O
is	O
the	O
center	O
of	O
an	O
object	O
in	O
the	O
class	O
.	O
Specifically	O
,	O
each	O
pixel	O
in	O
the	O
object	O
class	O
adds	O
votes	O
for	O
image	O
locations	O
along	O
the	O
ray	O
predicted	O
from	O
the	O
network	O
(	O
see	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
After	O
processing	O
all	O
the	O
pixels	O
in	O
the	O
object	O
class	O
,	O
we	O
obtain	O
the	O
voting	O
scores	O
for	O
all	O
the	O
image	O
locations	O
.	O
Then	O
the	O
object	O
center	O
is	O
selected	O
as	O
the	O
location	O
with	O
the	O
maximum	O
score	O
.	O
For	O
cases	O
where	O
multiple	O
instances	O
of	O
the	O
same	O
object	O
class	O
may	O
appear	O
in	O
the	O
image	O
,	O
we	O
apply	O
non	Method
-	Method
maximum	Method
suppression	Method
to	O
the	O
voting	O
scores	O
,	O
and	O
then	O
select	O
locations	O
with	O
scores	O
larger	O
than	O
a	O
certain	O
threshold	O
.	O
After	O
generating	O
a	O
set	O
of	O
object	O
centers	O
,	O
we	O
consider	O
the	O
pixels	O
that	O
vote	O
for	O
an	O
object	O
center	O
to	O
be	O
the	O
inliers	O
of	O
the	O
center	O
.	O
Then	O
the	O
depth	Task
prediction	Task
of	O
the	O
center	O
,	O
,	O
is	O
simply	O
computed	O
as	O
the	O
mean	O
of	O
the	O
depths	O
predicted	O
by	O
the	O
inliers	O
.	O
Finally	O
,	O
using	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
we	O
can	O
estimate	O
the	O
3D	O
translation	O
.	O
In	O
addition	O
,	O
the	O
network	O
generates	O
the	O
bounding	O
box	O
of	O
the	O
object	O
as	O
the	O
2D	O
rectangle	O
that	O
bounds	O
all	O
the	O
inliers	O
,	O
and	O
the	O
bounding	O
box	O
is	O
used	O
for	O
3D	Task
rotation	Task
regression	Task
.	O
subsection	O
:	O
3D	Method
Rotation	Method
Regression	Method
The	O
lowest	O
part	O
of	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
3D	Method
rotation	Method
regression	Method
branch	Method
.	O
Using	O
the	O
object	O
bounding	O
boxes	O
predicted	O
from	O
the	O
Hough	Method
voting	Method
layer	Method
,	O
we	O
utilize	O
two	O
RoI	Method
pooling	Method
layers	Method
to	O
“	O
crop	O
and	O
pool	O
”	O
the	O
visual	O
features	O
generated	O
by	O
the	O
first	O
stage	O
of	O
the	O
network	O
for	O
the	O
3D	Task
rotation	Task
regression	Task
.	O
The	O
pooled	O
feature	O
maps	O
are	O
added	O
together	O
and	O
fed	O
into	O
three	O
Fully	Method
-	Method
Connected	Method
(	O
FC	Method
)	O
layers	O
.	O
The	O
first	O
two	O
FC	Method
layers	O
have	O
dimension	O
4096	O
,	O
and	O
the	O
last	O
FC	Method
layer	O
has	O
dimension	O
with	O
the	O
number	O
of	O
object	O
classes	O
.	O
For	O
each	O
class	O
,	O
the	O
last	O
FC	Method
layer	O
outputs	O
a	O
3D	O
rotation	O
represented	O
by	O
a	O
quaternion	O
.	O
To	O
train	O
the	O
quaternion	Method
regression	Method
,	O
we	O
propose	O
two	O
loss	Method
functions	Method
,	O
one	O
of	O
which	O
is	O
specifically	O
designed	O
to	O
handle	O
symmetric	O
objects	O
.	O
The	O
first	O
loss	O
,	O
called	O
PoseLoss	Method
(	O
PLoss	Method
)	O
,	O
operates	O
in	O
the	O
3D	O
model	O
space	O
and	O
measures	O
the	O
average	Metric
squared	Metric
distance	Metric
between	O
points	O
on	O
the	O
correct	O
model	O
pose	O
and	O
their	O
corresponding	O
points	O
on	O
the	O
model	O
using	O
the	O
estimated	O
orientation	O
.	O
PLoss	Method
is	O
defined	O
as	O
where	O
denotes	O
the	O
set	O
of	O
3D	O
model	O
points	O
and	O
is	O
the	O
number	O
of	O
points	O
.	O
and	O
indicate	O
the	O
rotation	O
matrices	O
computed	O
from	O
the	O
the	O
estimated	O
quaternion	O
and	O
the	O
ground	O
truth	O
quaternion	O
,	O
respectively	O
.	O
This	O
loss	O
has	O
its	O
unique	O
minimum	O
when	O
the	O
estimated	O
orientation	O
is	O
identical	O
to	O
the	O
ground	O
truth	O
orientation	O
.	O
Unfortunately	O
,	O
PLoss	Method
does	O
not	O
handle	O
symmetric	O
objects	O
appropriately	O
,	O
since	O
a	O
symmetric	O
object	O
can	O
have	O
multiple	O
correct	O
3D	O
rotations	O
.	O
Using	O
such	O
a	O
loss	O
function	O
on	O
symmetric	O
objects	O
unnecessarily	O
penalizes	O
the	O
network	O
for	O
regressing	O
to	O
one	O
of	O
the	O
alternative	O
3D	O
rotations	O
,	O
thereby	O
giving	O
possibly	O
inconsistent	O
training	O
signals	O
.	O
While	O
PLoss	Method
could	O
potentially	O
be	O
modified	O
to	O
handle	O
symmetric	O
objects	O
by	O
manually	O
specifying	O
object	O
symmetries	O
and	O
then	O
considering	O
all	O
correct	O
orientations	O
as	O
ground	O
truth	O
options	O
,	O
we	O
here	O
introduce	O
ShapeMatch	Method
-	Method
Loss	Method
(	O
SLoss	Method
)	O
,	O
a	O
loss	Method
function	Method
that	O
does	O
not	O
require	O
the	O
specification	O
of	O
symmetries	O
.	O
SLoss	Method
is	O
defined	O
as	O
As	O
we	O
can	O
see	O
,	O
just	O
like	O
ICP	Method
,	O
this	O
loss	O
measures	O
the	O
offset	O
between	O
each	O
point	O
on	O
the	O
estimated	O
model	O
orientation	O
and	O
the	O
closest	O
point	O
on	O
the	O
ground	Method
truth	Method
model	Method
.	O
SLoss	Method
is	O
minimized	O
when	O
the	O
two	O
3D	Method
models	Method
match	O
each	O
other	O
.	O
In	O
this	O
way	O
,	O
the	O
SLoss	O
will	O
not	O
penalize	O
rotations	O
that	O
are	O
equivalent	O
with	O
respect	O
to	O
the	O
3D	O
shape	O
symmetry	O
of	O
the	O
object	O
.	O
section	O
:	O
The	O
YCB	Material
-	Material
Video	Material
Dataset	Material
Object	O
-	O
centric	O
datasets	O
providing	O
ground	O
-	O
truth	O
annotations	O
for	O
object	Task
poses	Task
and	O
/	O
or	O
segmentations	Task
are	O
limited	O
in	O
size	O
by	O
the	O
fact	O
that	O
the	O
annotations	O
are	O
typically	O
provided	O
manually	O
.	O
For	O
example	O
,	O
the	O
popular	O
LINEMOD	Material
dataset	Material
provides	O
manual	O
annotations	O
for	O
around	O
1	O
,	O
000	O
images	O
for	O
each	O
of	O
the	O
15	O
objects	O
in	O
the	O
dataset	O
.	O
While	O
such	O
a	O
dataset	O
is	O
useful	O
for	O
evaluation	O
of	O
model	O
-	O
based	O
pose	Task
estimation	Task
techniques	O
,	O
it	O
is	O
orders	O
of	O
magnitude	O
smaller	O
than	O
a	O
typical	O
dataset	O
for	O
training	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	Method
neural	Method
networks	Method
.	O
One	O
solution	O
to	O
this	O
problem	O
is	O
to	O
augment	O
the	O
data	O
with	O
synthetic	O
images	O
.	O
However	O
,	O
care	O
must	O
be	O
taken	O
to	O
ensure	O
that	O
performance	O
generalizes	O
between	O
real	O
and	O
rendered	O
scenes	O
.	O
subsection	O
:	O
6D	Task
Pose	Task
Annotation	Task
To	O
avoid	O
annotating	O
all	O
the	O
video	O
frames	O
manually	O
,	O
we	O
manually	O
specify	O
the	O
poses	O
of	O
the	O
objects	O
only	O
in	O
the	O
first	O
frame	O
of	O
each	O
video	O
.	O
Using	O
Signed	Method
Distance	Method
Function	Method
(	O
SDF	Method
)	O
representations	O
of	O
each	O
object	O
,	O
we	O
refine	O
the	O
pose	O
of	O
each	O
object	O
in	O
the	O
first	O
depth	O
frame	O
.	O
Next	O
,	O
the	O
camera	O
trajectory	O
is	O
initialized	O
by	O
fixing	O
the	O
object	O
poses	O
relative	O
to	O
one	O
another	O
and	O
tracking	O
the	O
object	O
configuration	O
through	O
the	O
depth	O
video	O
.	O
Finally	O
,	O
the	O
camera	O
trajectory	O
and	O
relative	O
object	O
poses	O
are	O
refined	O
in	O
a	O
global	Task
optimization	Task
step	Task
.	O
subsection	O
:	O
Dataset	O
Characteristics	O
The	O
objects	O
we	O
used	O
are	O
a	O
subset	O
of	O
21	O
of	O
the	O
YCB	O
objects	O
as	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
selected	O
due	O
to	O
high	O
-	O
quality	O
3D	Method
models	Method
and	O
good	O
visibility	O
in	O
depth	O
.	O
The	O
videos	O
are	O
collected	O
using	O
an	O
Asus	Method
Xtion	Method
Pro	Method
Live	Method
RGB	Method
-	Method
D	Method
camera	Method
in	O
fast	Method
-	Method
cropping	Method
mode	Method
,	O
which	O
provides	O
RGB	Material
images	Material
at	O
a	O
resolution	O
of	O
640x480	O
at	O
30	O
FPS	O
by	O
capturing	O
a	O
1280x960	O
image	O
locally	O
on	O
the	O
device	O
and	O
transmitting	O
only	O
the	O
center	O
region	O
over	O
USB	O
.	O
This	O
results	O
in	O
higher	O
effective	Metric
resolution	Metric
of	O
RGB	Material
images	Material
at	O
the	O
cost	O
of	O
a	O
lower	O
FOV	O
,	O
but	O
given	O
the	O
minimum	O
range	O
of	O
the	O
depth	O
sensor	O
this	O
was	O
an	O
acceptable	O
trade	O
-	O
off	O
.	O
The	O
full	O
dataset	O
comprises	O
133	O
,	O
827	O
images	O
,	O
two	O
full	O
orders	O
of	O
magnitude	O
larger	O
than	O
the	O
LINEMOD	Material
dataset	Material
.	O
For	O
more	O
statistics	O
relating	O
to	O
the	O
dataset	O
,	O
see	O
Table	O
[	O
reference	O
]	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
one	O
annotation	O
example	O
in	O
our	O
dataset	O
where	O
we	O
render	O
the	O
3D	Method
models	Method
according	O
to	O
the	O
annotated	O
ground	O
truth	O
pose	O
.	O
Note	O
that	O
our	O
annotation	Metric
accuracy	Metric
suffers	O
from	O
several	O
sources	O
of	O
error	Metric
,	O
including	O
the	O
rolling	O
shutter	O
of	O
the	O
RGB	O
sensor	O
,	O
inaccuracies	O
in	O
the	O
object	Method
models	Method
,	O
slight	O
asynchrony	O
between	O
RGB	O
and	O
depth	O
sensors	O
,	O
and	O
uncertainties	O
in	O
the	O
intrinsic	O
and	O
extrinsic	O
parameters	O
of	O
the	O
cameras	O
.	O
section	O
:	O
EXPERIMENTS	O
subsection	O
:	O
Datasets	O
In	O
our	O
YCB	Material
-	Material
Video	Material
dataset	Material
,	O
we	O
use	O
80	O
videos	O
for	O
training	O
,	O
and	O
test	O
on	O
2	O
,	O
949	O
key	O
frames	O
extracted	O
from	O
the	O
rest	O
12	O
test	O
videos	O
.	O
We	O
also	O
evaluate	O
our	O
method	O
on	O
the	O
OccludedLINEMOD	Material
dataset	Material
.	O
The	O
authors	O
of	O
selected	O
one	O
video	O
with	O
1	O
,	O
214	O
frames	O
from	O
the	O
original	O
LINEMOD	Material
dataset	Material
,	O
and	O
annotated	O
ground	O
truth	O
poses	O
for	O
eight	O
objects	O
in	O
that	O
video	O
:	O
Ape	O
,	O
Can	O
,	O
Cat	O
,	O
Driller	O
,	O
Duck	O
,	O
Eggbox	O
,	O
Glue	O
and	O
Holepuncher	O
.	O
There	O
are	O
significant	O
occlusions	O
between	O
objects	O
in	O
this	O
video	O
sequence	O
,	O
which	O
makes	O
this	O
dataset	O
challenging	O
.	O
For	O
training	O
,	O
we	O
use	O
the	O
eight	O
sequences	O
from	O
the	O
original	O
LINEMOD	Material
dataset	Material
corresponding	O
to	O
these	O
eight	O
objects	O
.	O
In	O
addition	O
,	O
we	O
generate	O
80	O
,	O
000	O
synthetic	O
images	O
for	O
training	O
on	O
both	O
datasets	O
by	O
randomly	O
placing	O
objects	O
in	O
a	O
scene	O
.	O
subsection	O
:	O
Evaluation	Metric
Metrics	Metric
We	O
adopt	O
the	O
average	Metric
distance	Metric
(	O
ADD	Metric
)	O
metric	O
as	O
proposed	O
in	O
for	O
evaluation	O
.	O
Given	O
the	O
ground	O
truth	O
rotation	O
and	O
translation	O
and	O
the	O
estimated	O
rotation	O
and	O
translation	O
,	O
the	O
average	Metric
distance	Metric
computes	O
the	O
mean	O
of	O
the	O
pairwise	O
distances	O
between	O
the	O
3D	O
model	O
points	O
transformed	O
according	O
to	O
the	O
ground	O
truth	O
pose	O
and	O
the	O
estimated	O
pose	O
:	O
where	O
denotes	O
the	O
set	O
of	O
3D	O
model	O
points	O
and	O
is	O
the	O
number	O
of	O
points	O
.	O
The	O
6D	O
pose	O
is	O
considered	O
to	O
be	O
correct	O
if	O
the	O
average	O
distance	O
is	O
smaller	O
than	O
a	O
predefined	O
threshold	O
.	O
In	O
the	O
OccludedLINEMOD	Material
dataset	Material
,	O
the	O
threshold	O
is	O
set	O
to	O
10	O
%	O
of	O
the	O
3D	O
model	O
diameter	O
.	O
For	O
symmetric	O
objects	O
such	O
as	O
the	O
Eggbox	O
and	O
Glue	O
,	O
the	O
matching	O
between	O
points	O
is	O
ambiguous	O
for	O
some	O
views	O
.	O
Therefore	O
,	O
the	O
average	Metric
distance	Metric
is	O
computed	O
using	O
the	O
closest	O
point	O
distance	O
:	O
Our	O
design	O
of	O
the	O
loss	Method
function	Method
for	O
rotation	Task
regression	Task
is	O
motivated	O
by	O
these	O
two	O
evaluation	Metric
metrics	Metric
.	O
Using	O
a	O
fixed	O
threshold	O
in	O
computing	O
pose	Metric
accuracy	Metric
can	O
not	O
reveal	O
how	O
a	O
method	O
performs	O
on	O
these	O
incorrect	O
poses	O
with	O
respect	O
to	O
that	O
threshold	O
.	O
Therefore	O
,	O
we	O
vary	O
the	O
distance	O
threshold	O
in	O
evaluation	Task
.	O
In	O
this	O
case	O
,	O
we	O
can	O
plot	O
an	O
accuracy	Metric
-	Metric
threshold	Metric
curve	Metric
,	O
and	O
compute	O
the	O
area	O
under	O
the	O
curve	O
for	O
pose	Task
evaluation	Task
.	O
Instead	O
of	O
computing	O
distances	O
in	O
the	O
3D	O
space	O
,	O
we	O
can	O
project	O
the	O
transformed	O
points	O
onto	O
the	O
image	O
,	O
and	O
then	O
compute	O
the	O
pairwise	O
distances	O
in	O
the	O
image	O
space	O
.	O
This	O
metric	O
is	O
called	O
the	O
reprojection	Metric
error	Metric
that	O
is	O
widely	O
used	O
for	O
6D	Task
pose	Task
estimation	Task
when	O
only	O
color	O
images	O
are	O
used	O
.	O
subsection	O
:	O
Implementation	O
Details	O
PoseCNN	Method
is	O
implemented	O
using	O
the	O
TensorFlow	Method
library	Method
.	O
The	O
Hough	Method
voting	Method
layer	Method
is	O
implemented	O
on	O
GPU	Method
as	O
in	O
.	O
In	O
training	O
,	O
the	O
parameters	O
of	O
the	O
first	O
13	O
convolutional	Method
layers	Method
in	O
the	O
feature	Method
extraction	Method
stage	Method
and	O
the	O
first	O
two	O
FC	Method
layers	O
in	O
the	O
3D	Method
rotation	Method
regression	Method
branch	Method
are	O
initialized	O
with	O
the	O
VGG16	Method
network	Method
trained	O
on	O
ImageNet	Material
.	O
No	O
gradient	O
is	O
back	O
-	O
propagated	O
via	O
the	O
Hough	Method
voting	Method
layer	Method
.	O
Stochastic	Method
Gradient	Method
Descent	Method
(	O
SGD	Method
)	O
with	O
momentum	Method
is	O
used	O
for	O
training	Task
.	O
subsection	O
:	O
Baselines	O
3D	Method
object	Method
coordinate	Method
regression	Method
network	Method
.	O
Since	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
6D	O
pose	Task
estimation	Task
methods	O
mostly	O
rely	O
on	O
regressing	O
image	O
pixels	O
to	O
3D	O
object	O
coordinates	O
,	O
we	O
implement	O
a	O
variation	O
of	O
our	O
network	O
for	O
3D	Task
object	Task
coordinate	Task
regression	Task
for	O
comparison	O
.	O
In	O
this	O
network	O
,	O
instead	O
of	O
regressing	O
to	O
center	O
direction	O
and	O
depth	O
as	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
we	O
regress	O
each	O
pixel	O
to	O
its	O
3D	O
coordinate	O
in	O
the	O
object	O
coordinate	O
system	O
.	O
We	O
can	O
use	O
the	O
same	O
architecture	O
since	O
each	O
pixel	O
still	O
regresses	O
to	O
three	O
variables	O
for	O
each	O
class	O
.	O
Then	O
we	O
remove	O
the	O
3D	Method
rotation	Method
regression	Method
branch	Method
.	O
Using	O
the	O
semantic	Task
labeling	Task
results	O
and	O
3D	Task
object	Task
coordinate	Task
regression	Task
results	O
,	O
the	O
6D	O
pose	O
is	O
recovered	O
using	O
the	O
pre	O
-	O
emptive	O
RANSAC	Method
as	O
in	O
.	O
Pose	Task
refinement	Task
.	O
The	O
6D	O
pose	O
estimated	O
from	O
our	O
network	O
can	O
be	O
refined	O
when	O
depth	O
is	O
available	O
.	O
We	O
use	O
the	O
Iterative	Method
Closest	Method
Point	Method
(	O
ICP	Method
)	O
algorithm	O
to	O
refine	O
the	O
6D	O
pose	O
.	O
Specifically	O
,	O
we	O
employ	O
ICP	Method
with	O
projective	Method
data	Method
association	Method
and	O
a	O
point	Method
-	Method
plane	Method
residual	Method
term	Method
.	O
We	O
render	O
a	O
predicted	O
point	O
cloud	O
given	O
the	O
3D	Method
model	Method
and	O
an	O
estimated	O
pose	O
,	O
and	O
assume	O
that	O
each	O
observed	O
depth	O
value	O
is	O
associated	O
with	O
the	O
predicted	O
depth	O
value	O
at	O
the	O
same	O
pixel	O
location	O
.	O
The	O
residual	O
for	O
each	O
pixel	O
is	O
then	O
the	O
smallest	O
distance	O
from	O
the	O
observed	O
point	O
in	O
3D	O
to	O
the	O
plane	O
defined	O
by	O
the	O
rendered	O
point	O
in	O
3D	O
and	O
its	O
normal	O
.	O
Points	O
with	O
residuals	O
above	O
a	O
specified	O
threshold	O
are	O
rejected	O
and	O
the	O
remaining	O
residuals	O
are	O
minimized	O
using	O
gradient	Method
descent	Method
.	O
Semantic	O
labels	O
from	O
the	O
network	O
are	O
used	O
to	O
crop	O
the	O
observed	O
points	O
from	O
the	O
depth	O
image	O
.	O
Since	O
ICP	Method
is	O
not	O
robust	O
to	O
local	O
minimums	O
,	O
we	O
refinement	O
multiple	O
poses	O
by	O
perturbing	O
the	O
estimated	O
pose	O
from	O
the	O
network	O
,	O
and	O
then	O
select	O
the	O
best	O
refined	O
pose	O
using	O
the	O
alignment	Metric
metric	Metric
proposed	O
in	O
.	O
subsection	O
:	O
Analysis	O
on	O
the	O
Rotation	Task
Regress	Task
Losses	Task
We	O
first	O
conduct	O
experiments	O
to	O
analyze	O
the	O
effect	O
of	O
the	O
two	O
loss	Method
functions	Method
for	O
rotation	Task
regression	Task
on	O
symmetric	O
objects	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
rotation	O
error	O
histograms	O
for	O
two	O
symmetric	O
objects	O
in	O
the	O
YCB	Material
-	Material
Video	Material
dataset	Material
(	O
wood	O
block	O
and	O
large	O
clamp	O
)	O
using	O
the	O
two	O
loss	Method
functions	Method
in	O
training	O
.	O
The	O
rotation	O
errors	O
of	O
the	O
PLoss	Method
for	O
the	O
wood	O
block	O
and	O
the	O
large	O
clamp	O
span	O
from	O
0	O
degree	O
to	O
180	O
degree	O
.	O
The	O
two	O
histograms	O
indicate	O
that	O
the	O
network	O
is	O
confused	O
by	O
the	O
symmetric	O
objects	O
.	O
While	O
the	O
histograms	O
of	O
the	O
SLoss	O
concentrate	O
on	O
the	O
180	O
degree	O
error	O
for	O
the	O
wood	O
block	O
and	O
0	O
degree	O
and	O
180	O
degree	O
for	O
the	O
large	O
clamp	O
,	O
since	O
they	O
are	O
symmetric	O
with	O
respect	O
to	O
180	O
degree	O
rotation	O
around	O
their	O
coordinate	O
axes	O
.	O
subsection	O
:	O
Results	O
on	O
the	O
YCB	Material
-	Material
Video	Material
Dataset	Material
Table	O
[	O
reference	O
]	O
and	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
a	O
)	O
presents	O
detailed	O
evaluation	O
for	O
all	O
the	O
21	O
objects	O
in	O
the	O
YCB	Material
-	Material
Video	Material
dataset	Material
.	O
We	O
show	O
the	O
area	O
under	O
the	O
accuracy	Metric
-	Metric
threshold	Metric
curve	Metric
using	O
both	O
the	O
ADD	Metric
metric	O
and	O
the	O
ADD	Metric
-	O
S	O
metric	O
,	O
where	O
we	O
vary	O
the	O
threshold	O
for	O
the	O
average	Metric
distance	Metric
and	O
then	O
compute	O
the	O
pose	Metric
accuracy	Metric
.	O
The	O
maximum	O
threshold	O
is	O
set	O
to	O
10	O
cm	O
.	O
We	O
can	O
see	O
that	O
i	O
)	O
By	O
only	O
using	O
color	Material
images	Material
,	O
our	O
network	O
significantly	O
outperforms	O
the	O
3D	Method
coordinate	Method
regression	Method
network	Method
combined	O
with	O
the	O
pre	Method
-	Method
emptive	Method
RANSAC	Method
algorithm	Method
for	O
6D	Task
pose	Task
estimation	Task
.	O
When	O
there	O
are	O
errors	O
in	O
the	O
3D	O
coordinate	O
regression	O
results	O
,	O
the	O
estimated	O
6D	O
pose	O
can	O
drift	O
far	O
away	O
from	O
the	O
ground	O
truth	O
pose	O
.	O
While	O
in	O
our	O
network	O
,	O
the	O
center	Method
localization	Method
helps	O
to	O
constrain	O
the	O
3D	Task
translation	Task
estimation	Task
even	O
if	O
the	O
object	O
is	O
occluded	O
.	O
ii	O
)	O
Refining	O
the	O
poses	O
with	O
ICP	Method
significantly	O
improves	O
the	O
performance	O
.	O
PoseCNN	Method
with	O
ICP	Method
achieves	O
superior	O
performance	O
compared	O
to	O
the	O
3D	Method
coordinate	Method
regression	Method
network	Method
when	O
using	O
depth	O
images	O
.	O
The	O
initial	O
pose	O
in	O
ICP	Method
is	O
critical	O
for	O
convergence	Task
.	O
PoseCNN	Method
provides	O
better	O
initial	O
6D	O
poses	O
for	O
ICP	Method
refinement	Method
.	O
iii	O
)	O
We	O
can	O
see	O
that	O
some	O
objects	O
are	O
more	O
difficult	O
to	O
handle	O
such	O
as	O
the	O
tuna	O
fish	O
can	O
that	O
is	O
small	O
and	O
with	O
less	O
texture	O
.	O
The	O
network	O
is	O
also	O
confused	O
by	O
the	O
large	O
clamp	O
and	O
the	O
extra	O
large	O
clamp	O
since	O
they	O
have	O
the	O
same	O
appearance	O
.	O
The	O
3D	Method
coordinate	Method
regression	Method
network	Method
can	O
not	O
handle	O
symmetric	O
objects	O
very	O
well	O
such	O
as	O
the	O
banana	O
and	O
the	O
bowl	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
displays	O
some	O
6D	Task
pose	Task
estimation	Task
results	O
on	O
the	O
YCB	Material
-	Material
Video	Material
dataset	Material
.	O
We	O
can	O
see	O
that	O
the	O
center	Method
prediction	Method
is	O
quite	O
accurate	O
even	O
if	O
the	O
center	O
is	O
occluded	O
by	O
another	O
object	O
.	O
Our	O
network	O
with	O
color	O
only	O
is	O
already	O
able	O
to	O
provide	O
good	O
6D	Task
pose	Task
estimation	Task
.	O
With	O
ICP	Method
refinement	Method
,	O
the	O
accuracy	Metric
of	O
the	O
6D	O
pose	O
is	O
further	O
improved	O
.	O
subsection	O
:	O
Results	O
on	O
the	O
OccludedLINEMOD	Material
Dataset	Material
The	O
OccludedLINEMOD	Material
dataset	Material
is	O
challenging	O
due	O
to	O
significant	O
occlusions	O
between	O
objects	O
.	O
We	O
first	O
conduct	O
experiments	O
using	O
color	O
images	O
only	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
the	O
accuracy	Metric
-	Metric
threshold	Metric
curves	Metric
with	O
reprojection	Metric
error	Metric
for	O
7	O
objects	O
in	O
the	O
dataset	O
,	O
where	O
we	O
compare	O
PoseCNN	Method
with	O
that	O
achieves	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
this	O
dataset	O
using	O
color	O
images	O
as	O
input	O
.	O
Our	O
method	O
significantly	O
outperforms	O
by	O
a	O
large	O
margin	O
,	O
especially	O
when	O
the	O
reprojection	Metric
error	Metric
threshold	Metric
is	O
small	O
.	O
These	O
results	O
show	O
that	O
PoseCNN	Method
is	O
able	O
to	O
correctly	O
localize	O
the	O
target	O
object	O
even	O
under	O
severe	O
occlusions	O
.	O
By	O
refining	O
the	O
poses	O
using	O
depth	O
images	O
in	O
ICP	Method
,	O
our	O
method	O
also	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
using	O
RGB	Material
-	Material
D	Material
data	Material
as	O
input	O
.	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
pose	Task
estimation	Task
accuracy	O
on	O
the	O
OccludedLINEMOD	Material
dataset	Material
.	O
The	O
most	O
improvement	O
comes	O
from	O
the	O
two	O
symmetric	O
objects	O
“	O
Eggbox	O
”	O
and	O
“	O
Glue	Method
”	O
.	O
By	O
using	O
our	O
ShapeMatch	Method
-	Method
Loss	Method
for	O
training	Task
,	O
PoseCNN	Method
is	O
able	O
to	O
correctly	O
estimate	O
the	O
6D	O
pose	O
of	O
the	O
two	O
objects	O
with	O
respect	O
to	O
symmetry	O
.	O
We	O
also	O
present	O
the	O
result	O
of	O
PoseCNN	Method
using	O
color	O
only	O
in	O
Table	O
[	O
reference	O
]	O
.	O
These	O
accuracies	Metric
are	O
much	O
lower	O
since	O
the	O
threshold	O
here	O
is	O
usually	O
smaller	O
than	O
2	O
cm	O
.	O
It	O
is	O
very	O
challenging	O
for	O
color	Method
-	Method
based	Method
methods	Method
to	O
obtain	O
6D	O
poses	O
within	O
such	O
small	O
threshold	O
when	O
there	O
are	O
occlusions	O
between	O
objects	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
two	O
examples	O
of	O
the	O
6D	Task
pose	Task
estimation	Task
results	O
on	O
the	O
OccludedLINEMOD	Material
dataset	Material
.	O
section	O
:	O
CONCLUSIONS	O
In	O
this	O
work	O
,	O
we	O
introduce	O
PoseCNN	Method
,	O
a	O
convolutional	Method
neural	Method
network	Method
for	O
6D	O
object	O
pose	Task
estimation	Task
.	O
PoseCNN	Method
decouples	O
the	O
estimation	Task
of	Task
3D	Task
rotation	Task
and	O
3D	Task
translation	Task
.	O
It	O
estimates	O
the	O
3D	O
translation	O
by	O
localizing	O
the	O
object	O
center	O
and	O
predicting	O
the	O
center	O
distance	O
.	O
By	O
regressing	O
each	O
pixel	O
to	O
a	O
unit	O
vector	O
towards	O
the	O
object	O
center	O
,	O
the	O
center	O
can	O
be	O
estimated	O
robustly	O
independent	O
of	O
scale	O
.	O
More	O
importantly	O
,	O
pixels	O
vote	O
the	O
object	O
center	O
even	O
if	O
it	O
is	O
occluded	O
by	O
other	O
objects	O
.	O
The	O
3D	O
rotation	O
is	O
predicted	O
by	O
regressing	O
to	O
a	O
quaternion	Method
representation	Method
.	O
Two	O
new	O
loss	Method
functions	Method
are	O
introduced	O
for	O
rotation	Task
estimation	Task
,	O
with	O
the	O
ShapeMatch	Method
-	Method
Loss	Method
designed	O
for	O
symmetric	O
objects	O
.	O
As	O
a	O
result	O
,	O
PoseCNN	Method
is	O
able	O
to	O
handle	O
occlusion	O
and	O
symmetric	O
objects	O
in	O
cluttered	O
scenes	O
.	O
We	O
also	O
introduce	O
a	O
large	O
scale	O
video	O
dataset	O
for	O
6D	O
object	O
pose	Task
estimation	Task
.	O
Our	O
results	O
are	O
extremely	O
encouraging	O
in	O
that	O
they	O
indicate	O
that	O
it	O
is	O
feasible	O
to	O
accurately	O
estimate	O
the	O
6D	Task
pose	Task
of	Task
objects	Task
in	Task
cluttered	Task
scenes	Task
using	O
vision	O
data	O
only	O
.	O
This	O
opens	O
the	O
path	O
to	O
using	O
cameras	O
with	O
resolution	O
and	O
field	O
of	O
view	O
that	O
goes	O
far	O
beyond	O
currently	O
used	O
depth	Method
camera	Method
systems	Method
.	O
We	O
note	O
that	O
the	O
SLoss	O
sometimes	O
results	O
in	O
local	O
minimums	O
in	O
the	O
pose	O
space	O
similar	O
to	O
ICP	Method
.	O
It	O
would	O
be	O
interesting	O
to	O
explore	O
more	O
efficient	O
way	O
in	O
handle	O
symmetric	Task
objects	Task
in	O
6D	Task
pose	Task
estimation	Task
in	O
the	O
future	O
.	O
section	O
:	O
ACKNOWLEDGMENTS	O
This	O
work	O
was	O
funded	O
in	O
part	O
by	O
Siemens	O
and	O
by	O
NSF	O
STTR	O
grant	O
63	O
-	O
5197	O
with	O
Lula	O
Robotics	O
.	O
bibliography	O
:	O
References	O
