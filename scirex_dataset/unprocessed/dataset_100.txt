document	O
:	O
Partially	Method
Shuffling	Method
the	O
Training	O
Data	O
to	O
Improve	O
Language	Method
Models	Method
Although	O
SGD	Method
requires	O
shuffling	O
the	O
training	O
data	O
between	O
epochs	O
,	O
currently	O
none	O
of	O
the	O
word	Task
-	Task
level	Task
language	Task
modeling	Task
systems	O
do	O
this	O
.	O
Naively	O
shuffling	O
all	O
sentences	O
in	O
the	O
training	O
data	O
would	O
not	O
permit	O
the	O
model	O
to	O
learn	O
inter	O
-	O
sentence	O
dependencies	O
.	O
Here	O
we	O
present	O
a	O
method	O
that	O
partially	O
shuffles	O
the	O
training	O
data	O
between	O
epochs	O
.	O
This	O
method	O
makes	O
each	O
batch	O
random	O
,	O
while	O
keeping	O
most	O
sentence	O
ordering	O
intact	O
.	O
It	O
achieves	O
new	O
state	O
of	O
the	O
art	O
results	O
on	O
word	Task
-	Task
level	Task
language	Task
modeling	Task
on	O
both	O
the	O
Penn	Material
Treebank	Material
and	O
WikiText	Material
-	Material
2	Material
datasets	O
.	O
section	O
:	O
Background	O
A	O
language	Method
model	Method
is	O
trained	O
to	O
predict	O
word	O
given	O
all	O
previous	O
words	O
.	O
A	O
recurrent	Method
language	Method
model	Method
receives	O
at	O
timestep	O
the	O
th	O
word	O
and	O
the	O
previous	O
hidden	O
state	O
and	O
outputs	O
a	O
prediction	O
of	O
the	O
next	O
word	O
and	O
the	O
next	O
hidden	O
state	O
.	O
The	O
training	O
data	O
for	O
word	Task
-	Task
level	Task
language	Task
modeling	Task
consists	O
of	O
a	O
series	O
of	O
concatenated	O
documents	O
.	O
The	O
sentences	O
from	O
these	O
documents	O
are	O
unshuffled	O
.	O
This	O
lets	O
the	O
model	O
learn	O
long	O
term	O
,	O
multi	O
-	O
sentence	O
dependencies	O
between	O
words	O
.	O
The	O
concatenation	Method
operation	Method
results	O
in	O
a	O
single	O
long	O
sequence	O
of	O
words	O
.	O
The	O
naive	O
way	O
to	O
train	O
a	O
language	Method
model	Method
would	O
be	O
to	O
,	O
at	O
every	O
epoch	O
,	O
use	O
the	O
entire	O
training	O
sequence	O
as	O
the	O
input	O
,	O
and	O
use	O
the	O
same	O
sequence	O
shifted	O
one	O
word	O
to	O
the	O
left	O
as	O
target	O
output	O
.	O
Since	O
the	O
training	O
sequence	O
is	O
too	O
long	O
,	O
this	O
solution	O
is	O
infeasible	O
.	O
To	O
solve	O
this	O
,	O
we	O
set	O
a	O
back	O
propagation	O
through	O
-	O
time	O
length	O
(	O
)	O
,	O
and	O
split	O
the	O
training	O
sequence	O
into	O
sub	O
-	O
sequences	O
of	O
length	O
.	O
In	O
this	O
case	O
,	O
in	O
each	O
epoch	O
the	O
model	O
is	O
first	O
trained	O
on	O
the	O
first	O
sub	O
-	O
sequence	O
,	O
and	O
then	O
on	O
the	O
second	O
one	O
,	O
and	O
so	O
on	O
.	O
While	O
gradients	O
are	O
not	O
passed	O
between	O
different	O
sub	O
-	O
sequences	O
,	O
the	O
last	O
hidden	O
state	O
from	O
sub	O
-	O
sequence	O
becomes	O
the	O
initial	O
hidden	O
state	O
while	O
training	O
the	O
model	O
with	O
sub	O
-	O
sequence	O
.	O
For	O
example	O
,	O
if	O
the	O
training	O
sequence	O
of	O
words	O
is	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
for	O
,	O
the	O
resulting	O
four	O
sub	O
-	O
sequences	O
are	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
]	O
[	O
J	O
K	O
L	O
]	O
Note	O
that	O
we	O
only	O
present	O
the	O
input	O
sub	O
-	O
sequences	O
,	O
as	O
the	O
target	O
output	O
sub	O
-	O
sequences	O
are	O
simply	O
the	O
input	O
sub	O
-	O
sequences	O
shifted	O
one	O
word	O
to	O
the	O
left	O
.	O
This	O
method	O
works	O
,	O
but	O
it	O
does	O
not	O
utilize	O
current	O
GPUs	Method
to	O
their	O
full	O
potential	O
.	O
In	O
order	O
to	O
speed	O
up	O
training	Task
,	O
we	O
batch	O
our	O
training	O
data	O
.	O
We	O
set	O
a	O
batch	O
size	O
,	O
and	O
at	O
every	O
training	O
step	O
we	O
train	O
the	O
model	O
on	O
sub	O
-	O
sequences	O
in	O
parallel	O
.	O
To	O
do	O
this	O
,	O
we	O
first	O
split	O
the	O
training	O
sequence	O
into	O
parts	O
.	O
Continuing	O
the	O
example	O
from	O
above	O
,	O
for	O
,	O
this	O
results	O
in	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
Then	O
,	O
as	O
before	O
,	O
we	O
split	O
each	O
part	O
into	O
sub	O
-	O
sequences	O
of	O
length	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
]	O
[	O
J	O
K	O
L	O
]	O
Then	O
,	O
during	O
the	O
first	O
training	O
step	O
in	O
each	O
epoch	O
we	O
train	O
on	O
:	O
[	O
A	O
B	O
C	O
]	O
[	O
G	O
H	O
I	O
]	O
and	O
during	O
the	O
second	O
training	O
step	O
in	O
each	O
epoch	O
we	O
train	O
on	O
:	O
[	O
D	O
E	O
F	O
]	O
[	O
J	O
K	O
L	O
]	O
Note	O
that	O
at	O
every	O
step	O
,	O
all	O
sub	O
-	O
sequences	O
in	O
the	O
batch	O
are	O
processed	O
in	O
parallel	O
.	O
Before	O
we	O
introduced	O
batching	O
,	O
in	O
each	O
epoch	O
the	O
output	O
for	O
each	O
word	O
in	O
the	O
training	O
sequence	O
was	O
dependant	O
on	O
all	O
previous	O
words	O
.	O
With	O
batching	Method
,	O
the	O
output	O
of	O
the	O
model	O
for	O
each	O
word	O
is	O
only	O
dependant	O
on	O
the	O
previous	O
words	O
in	O
that	O
batch	O
element	O
(	O
or	O
equivalently	O
,	O
row	O
in	O
our	O
example	O
)	O
,	O
and	O
the	O
other	O
words	O
are	O
ignored	O
.	O
In	O
our	O
example	O
,	O
the	O
hidden	O
state	O
that	O
is	O
given	O
when	O
inputting	O
G	O
is	O
the	O
default	O
initial	O
hidden	O
state	O
,	O
and	O
not	O
the	O
one	O
that	O
resulted	O
after	O
the	O
input	O
of	O
F	O
.	O
This	O
is	O
not	O
optimal	O
,	O
but	O
since	O
batching	Method
reduces	O
the	O
training	Metric
time	Metric
by	O
a	O
significant	O
amount	O
,	O
all	O
current	O
models	O
use	O
this	O
method	O
.	O
section	O
:	O
The	O
Partial	Method
Shuffle	Method
Method	Method
While	O
SGD	Method
calls	O
for	O
random	O
batches	O
in	O
each	O
epoch	O
,	O
in	O
existing	O
language	Method
models	Method
,	O
the	O
data	O
is	O
not	O
shuffled	O
between	O
epochs	O
during	O
training	O
.	O
This	O
means	O
that	O
batch	O
in	O
every	O
epoch	O
is	O
made	O
up	O
of	O
the	O
same	O
sub	O
-	O
sequences	O
.	O
The	O
straightforward	O
way	O
to	O
shuffle	O
the	O
data	O
would	O
be	O
to	O
shuffle	O
all	O
sentences	O
in	O
the	O
training	O
sequence	O
between	O
each	O
epoch	O
.	O
This	O
hurts	O
the	O
language	Method
model	Method
â€™s	O
performance	O
,	O
since	O
it	O
does	O
not	O
learn	O
inter	O
-	O
sentence	O
dependencies	O
.	O
Here	O
we	O
present	O
the	O
Partial	Method
Shuffle	Method
method	Method
,	O
which	O
improves	O
the	O
performance	O
of	O
the	O
model	O
.	O
Like	O
before	O
,	O
we	O
first	O
separate	O
the	O
sequence	O
of	O
words	O
into	O
rows	O
.	O
Using	O
the	O
example	O
sequence	O
from	O
above	O
,	O
this	O
would	O
result	O
in	O
(	O
for	O
)	O
:	O
[	O
A	O
B	O
C	O
D	O
E	O
F	O
]	O
[	O
G	O
H	O
I	O
J	O
K	O
L	O
]	O
Then	O
,	O
for	O
each	O
row	O
,	O
we	O
pick	O
a	O
random	O
index	O
between	O
zero	O
and	O
the	O
length	O
of	O
the	O
row	O
and	O
we	O
take	O
the	O
words	O
that	O
are	O
located	O
before	O
this	O
index	O
and	O
move	O
them	O
to	O
the	O
end	O
of	O
the	O
row	O
.	O
So	O
in	O
our	O
example	O
,	O
if	O
the	O
random	O
index	O
for	O
row	O
one	O
was	O
and	O
for	O
row	O
two	O
was	O
this	O
would	O
result	O
in	O
(	O
red	O
marks	O
the	O
words	O
which	O
were	O
moved	O
)	O
:	O
[	O
C	O
D	O
E	O
F	O
A	O
B	O
]	O
[	O
L	O
H	O
J	O
K	O
]	O
Finally	O
,	O
as	O
before	O
,	O
each	O
row	O
(	O
or	O
equivalently	O
,	O
batch	O
element	O
)	O
is	O
divided	O
into	O
back	Method
-	Method
propagation	Method
through	O
time	O
segments	O
.	O
For	O
,	O
this	O
will	O
result	O
in	O
:	O
[	O
C	O
D	O
E	O
]	O
[	O
F	O
A	O
B	O
]	O
[	O
L	O
G	O
H	O
]	O
[	O
I	O
J	O
K	O
]	O
This	O
method	O
randomizes	O
the	O
batches	O
while	O
still	O
keeping	O
most	O
of	O
the	O
word	O
ordering	O
intact	O
.	O
section	O
:	O
Results	O
We	O
evaluate	O
our	O
method	O
on	O
the	O
current	O
state	O
of	O
the	O
art	Method
model	Method
,	O
DOC	Method
,	O
and	O
the	O
previous	O
state	O
of	O
the	O
art	O
model	O
,	O
MoS	Method
,	O
on	O
the	O
Penn	Material
Treebank	Material
and	O
WikiText	Material
-	Material
2	Material
language	O
modeling	O
datasets	O
.	O
For	O
each	O
model	O
,	O
the	O
hyper	O
-	O
parameters	O
(	O
including	O
and	O
)	O
are	O
not	O
modified	O
from	O
their	O
original	O
values	O
.	O
In	O
addition	O
,	O
we	O
present	O
results	O
for	O
finetuned	Method
models	Method
,	O
with	O
and	O
without	O
the	O
Partial	Method
Shuffle	Method
.	O
Our	O
shuffling	Method
method	Method
improves	O
the	O
performance	O
of	O
all	O
models	O
,	O
and	O
achieves	O
new	O
state	O
of	O
the	O
art	O
results	O
on	O
both	O
datasets	O
.	O
Our	O
method	O
does	O
not	O
require	O
any	O
additional	O
parameters	O
or	O
hyper	O
-	O
parameters	O
,	O
and	O
runs	O
in	O
less	O
than	O
th	O
of	O
a	O
second	O
per	O
epoch	O
on	O
the	O
Penn	Material
Treebank	Material
dataset	O
.	O
section	O
:	O
Acknowledgements	O
This	O
note	O
benefited	O
from	O
feedback	O
from	O
Judit	O
Acs	O
,	O
Shimi	O
Salant	O
and	O
Noah	O
A.	O
Smith	O
,	O
which	O
is	O
acknowledged	O
with	O
gratitude	O
.	O
bibliography	O
:	O
References	O
