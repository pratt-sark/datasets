document	O
:	O
Towards	O
Viewpoint	Task
Invariant	Task
3D	Task
Human	Task
Pose	Task
Estimation	Task
We	O
propose	O
a	O
viewpoint	Method
invariant	Method
model	Method
for	O
3D	Task
human	Task
pose	Task
estimation	Task
from	O
a	O
single	O
depth	O
image	O
.	O
To	O
achieve	O
this	O
,	O
our	O
discriminative	Method
model	Method
embeds	O
local	O
regions	O
into	O
a	O
learned	O
viewpoint	O
invariant	O
feature	O
space	O
.	O
Formulated	O
as	O
a	O
multi	Task
-	Task
task	Task
learning	Task
problem	Task
,	O
our	O
model	O
is	O
able	O
to	O
selectively	O
predict	O
partial	O
poses	O
in	O
the	O
presence	O
of	O
noise	O
and	O
occlusion	O
.	O
Our	O
approach	O
leverages	O
a	O
convolutional	Method
and	Method
recurrent	Method
network	Method
architecture	Method
with	O
a	O
top	Method
-	Method
down	Method
error	Method
feedback	Method
mechanism	Method
to	O
self	O
-	O
correct	O
previous	O
pose	O
estimates	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
a	O
previously	O
published	O
depth	O
dataset	O
and	O
a	O
newly	O
collected	O
human	O
pose	O
dataset	O
containing	O
100	O
K	O
annotated	O
depth	O
images	O
from	O
extreme	O
viewpoints	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
achieves	O
competitive	O
performance	O
on	O
frontal	O
views	O
while	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
alternate	O
viewpoints	O
.	O
compatibility	O
=	O
false	O
section	O
:	O
Introduction	O
Depth	Method
sensors	Method
are	O
becoming	O
ubiquitous	O
in	O
applications	O
ranging	O
from	O
security	Task
to	O
robotics	Task
and	O
from	O
entertainment	Task
to	Task
smart	Task
spaces	Task
.	O
While	O
recent	O
advances	O
in	O
pose	Task
estimation	Task
have	O
improved	O
performance	O
on	O
front	Material
and	O
side	O
views	O
,	O
most	O
real	O
-	O
world	O
settings	O
present	O
challenging	O
viewpoints	O
such	O
as	O
top	O
or	O
angled	O
views	O
in	O
retail	O
stores	O
,	O
hospital	O
environments	O
,	O
or	O
airport	O
settings	O
.	O
These	O
viewpoints	O
introduce	O
high	O
levels	O
of	O
self	O
-	O
occlusion	O
making	O
human	Task
pose	Task
estimation	Task
difficult	O
for	O
existing	O
algorithms	O
.	O
Humans	Method
are	O
remarkably	O
robust	O
at	O
predicting	Task
full	Task
rigid	Task
-	Task
body	Task
and	Task
articulated	Task
poses	Task
in	O
these	O
challenging	O
scenarios	O
.	O
However	O
,	O
most	O
work	O
in	O
the	O
human	Task
pose	Task
estimation	Task
literature	Task
has	O
addressed	O
relatively	O
constrained	Task
settings	Task
.	O
There	O
has	O
been	O
a	O
long	O
line	O
of	O
work	O
on	O
generative	Method
pose	Method
models	Method
,	O
where	O
a	O
pose	O
is	O
estimated	O
by	O
constructing	O
a	O
skeleton	O
using	O
templates	O
or	O
priors	O
in	O
a	O
top	O
-	O
down	O
manner	O
.	O
In	O
contrast	O
,	O
discriminative	Method
methods	Method
directly	O
identify	O
individual	O
body	O
parts	O
,	O
labels	O
,	O
or	O
positions	O
and	O
construct	O
the	O
skeleton	O
in	O
a	O
bottom	Method
-	Method
up	Method
approach	Method
.	O
However	O
,	O
recent	O
research	O
in	O
both	O
classes	O
primarily	O
focus	O
on	O
frontal	O
views	O
with	O
few	O
occlusions	O
despite	O
the	O
abundance	O
of	O
occlusion	O
and	O
partial	Task
-	Task
pose	Task
research	Task
in	O
object	Task
detection	Task
.	O
Even	O
modern	O
representation	Method
learning	Method
techniques	Method
address	O
human	Task
pose	Task
estimation	Task
from	O
frontal	O
or	O
side	O
views	O
.	O
While	O
the	O
above	O
methods	O
improve	O
human	Task
pose	Task
estimation	Task
,	O
they	O
fail	O
to	O
address	O
viewpoint	O
variances	O
.	O
In	O
this	O
work	O
we	O
address	O
the	O
problem	O
of	O
viewpoint	Task
invariant	Task
pose	Task
estimation	Task
from	O
single	O
depth	O
images	O
.	O
There	O
are	O
two	O
challenges	O
towards	O
this	O
goal	O
.	O
The	O
first	O
challenge	O
is	O
designing	O
a	O
model	O
that	O
is	O
not	O
only	O
rich	O
enough	O
to	O
reason	O
about	O
3D	O
spatial	O
information	O
but	O
also	O
robust	O
to	O
viewpoint	O
changes	O
.	O
The	O
model	O
must	O
understand	O
both	O
local	O
and	O
global	O
human	O
pose	O
structure	O
.	O
That	O
is	O
,	O
it	O
must	O
fuse	O
techniques	O
from	O
local	Method
part	Method
-	Method
based	Method
discriminative	Method
models	Method
and	O
global	Method
skeleton	Method
-	Method
driven	Method
generative	Method
models	Method
.	O
Additionally	O
,	O
it	O
must	O
be	O
able	O
to	O
reason	O
about	O
3D	O
volumes	O
,	O
geometric	O
,	O
and	O
viewpoint	O
transformations	O
.	O
The	O
second	O
challenge	O
is	O
that	O
existing	O
real	O
-	O
world	O
depth	O
datasets	O
are	O
often	O
small	O
in	O
size	O
,	O
both	O
in	O
terms	O
of	O
number	O
of	O
frames	O
and	O
number	O
of	O
classes	O
.	O
As	O
a	O
result	O
,	O
the	O
use	O
of	O
representation	Method
learning	Method
methods	Method
and	O
viewpoint	Method
transfer	Method
techniques	Method
has	O
been	O
limited	O
.	O
To	O
address	O
these	O
challenges	O
,	O
our	O
contributions	O
are	O
as	O
follows	O
:	O
First	O
,	O
on	O
the	O
technical	O
side	O
,	O
we	O
embed	O
local	O
pose	O
information	O
into	O
a	O
learned	O
,	O
viewpoint	O
invariant	O
feature	O
space	O
.	O
Furthermore	O
,	O
we	O
extend	O
the	O
iterative	Method
error	Method
feedback	Method
model	Method
to	O
model	O
higher	O
-	O
order	O
temporal	O
dependencies	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
To	O
handle	O
occlusions	O
,	O
we	O
formulate	O
our	O
model	O
with	O
a	O
multi	Task
-	Task
task	Task
learning	Task
objective	Task
.	O
Second	O
,	O
we	O
introduce	O
a	O
new	O
dataset	O
of	O
100	O
K	O
depth	O
images	O
with	O
pixel	O
-	O
wise	O
body	O
part	O
labels	O
and	O
3D	O
human	O
joint	O
locations	O
.	O
The	O
dataset	O
consists	O
of	O
extreme	O
cases	O
of	O
viewpoint	O
variance	O
with	O
front	Material
,	O
top	O
,	O
and	O
side	O
views	O
of	O
people	O
performing	O
15	O
actions	O
with	O
occluded	O
body	O
parts	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
an	O
existing	O
public	O
dataset	O
and	O
our	O
newly	O
collected	O
dataset	O
demonstrating	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
viewpoint	Task
invariant	Task
pose	Task
estimation	Task
.	O
section	O
:	O
Related	O
Work	O
RGB	Task
-	Task
Based	Task
Human	Task
Pose	Task
Estimation	Task
.	O
Several	O
methods	O
have	O
been	O
proposed	O
for	O
human	Task
pose	Task
estimation	Task
,	O
including	O
edge	Method
-	Method
based	Method
histograms	Method
of	Method
the	Method
human	Method
-	Method
body	Method
and	Method
silhouette	Method
contours	Method
.	O
More	O
general	O
techniques	O
using	O
pictorial	O
structures	O
and	O
deformable	Method
part	Method
models	Method
,	O
continued	O
to	O
build	O
appearance	Method
models	Method
for	O
each	O
local	O
body	O
part	O
independently	O
.	O
Subsequently	O
,	O
higher	O
-	O
level	O
part	Method
-	Method
based	Method
models	Method
were	O
developed	O
to	O
capture	O
more	O
complex	O
body	O
part	O
relationships	O
and	O
obtain	O
more	O
discriminative	O
templates	O
.	O
These	O
models	O
continued	O
to	O
evolve	O
,	O
attempting	O
to	O
capture	O
even	O
higher	O
-	O
level	O
part	O
features	O
.	O
Convolutional	Method
networks	Method
,	O
a	O
class	O
of	O
representation	Method
learning	Method
methods	Method
,	O
began	O
to	O
exhibit	O
performance	O
gains	O
not	O
only	O
in	O
human	Task
pose	Task
estimation	Task
,	O
but	O
various	O
areas	O
of	O
computer	Task
vision	Task
.	O
Since	O
valid	O
human	O
poses	O
represent	O
a	O
much	O
lower	O
-	O
dimensional	O
manifold	O
in	O
the	O
high	O
-	O
dimensional	O
input	O
space	O
,	O
it	O
is	O
difficult	O
to	O
directly	O
regress	O
from	O
input	O
image	O
to	O
output	O
poses	O
with	O
a	O
convolutional	Method
network	Method
.	O
As	O
a	O
solution	O
to	O
this	O
,	O
researchers	O
framed	O
the	O
problem	O
as	O
a	O
multi	Task
-	Task
task	Task
learning	Task
problem	Task
where	O
human	O
joints	O
must	O
be	O
first	O
detected	O
then	O
precisely	O
localized	O
.	O
Jain	O
et	O
al	O
.	O
enforce	O
global	O
pose	O
consistency	O
with	O
a	O
Markov	Method
random	Method
field	Method
representing	O
human	O
anatomical	O
constraints	O
.	O
Follow	O
up	O
work	O
by	O
Tompson	O
et	O
al	O
.	O
combines	O
a	O
convolutional	Method
network	Method
part	Method
-	Method
detector	Method
with	O
a	O
part	Method
-	Method
based	Method
spatial	Method
model	Method
into	O
a	O
unified	Method
framework	Method
.	O
Because	O
human	Task
pose	Task
estimation	Task
is	O
ultimately	O
a	O
structured	Task
prediction	Task
task	Task
,	O
it	O
is	O
difficult	O
for	O
convolutional	Method
networks	Method
to	O
correctly	O
regress	O
the	O
full	O
pose	O
in	O
a	O
single	O
pass	O
.	O
Recently	O
,	O
iterative	Method
refinement	Method
techniques	Method
have	O
been	O
proposed	O
to	O
address	O
this	O
issue	O
.	O
In	O
,	O
Sun	O
et	O
al	O
.	O
proposed	O
a	O
multi	Method
-	Method
stage	Method
system	Method
of	Method
convolutional	Method
networks	Method
for	O
predicting	Task
facial	Task
point	Task
locations	Task
.	O
Each	O
stage	O
refines	O
the	O
output	O
from	O
the	O
previous	O
stage	O
given	O
a	O
local	O
region	O
of	O
the	O
input	O
.	O
Building	O
on	O
this	O
work	O
,	O
DeepPose	Method
uses	O
a	O
cascade	Method
of	Method
convolutional	Method
networks	Method
for	O
full	Task
-	Task
body	Task
pose	Task
estimation	Task
.	O
In	O
another	O
body	O
of	O
work	O
,	O
instead	O
of	O
predicting	O
absolute	O
human	O
joint	O
locations	O
,	O
Carreira	O
et	O
al	O
.	O
refine	O
pose	O
estimates	O
by	O
predicting	O
error	O
feedback	O
(	O
i.e.	O
corrections	O
)	O
at	O
each	O
iteration	O
.	O
Depth	Task
-	Task
Based	Task
Human	Task
Pose	Task
Estimation	Task
.	O
Both	O
generative	Method
and	Method
discriminative	Method
models	Method
have	O
been	O
proposed	O
.	O
Generative	Method
models	Method
(	O
i.e.	O
top	Method
-	Method
down	Method
approaches	Method
)	O
fit	O
a	O
human	O
body	O
template	O
,	O
with	O
parametric	Method
or	Method
non	Method
-	Method
parametric	Method
methods	Method
,	O
to	O
the	O
input	O
data	O
.	O
Dense	Task
point	Task
clouds	Task
provided	O
by	O
depth	O
sensors	O
motivate	O
the	O
use	O
of	O
iterative	Method
closest	Method
point	Method
algorithms	Method
and	O
database	Method
lookups	Method
.	O
To	O
further	O
constrain	O
the	O
output	O
space	O
similar	O
to	O
RGB	Method
methods	Method
,	O
graphical	Method
models	Method
impose	O
kinematic	O
constraints	O
to	O
improve	O
full	Task
-	Task
body	Task
pose	Task
estimation	Task
.	O
Other	O
methods	O
such	O
as	O
kernel	Method
methods	Method
with	O
kinematic	Method
chain	Method
structures	Method
and	O
template	Method
fitting	Method
with	O
Gaussian	Method
mixture	Method
models	Method
have	O
been	O
proposed	O
.	O
Discriminative	Method
methods	Method
(	O
i.e.	O
bottom	Method
-	Method
up	Method
approaches	Method
)	O
detect	O
instances	O
of	O
body	O
parts	O
instead	O
of	O
fitting	O
a	O
skeleton	O
template	O
.	O
In	O
,	O
Shotton	O
et	O
al	O
.	O
trained	O
a	O
random	Method
forest	Method
classifier	Method
for	O
body	Task
part	Task
segmentation	Task
from	O
a	O
single	O
depth	O
image	O
and	O
used	O
mean	Method
shift	Method
to	O
estimate	O
joint	O
locations	O
.	O
This	O
work	O
inspired	O
an	O
entire	O
line	O
of	O
depth	Task
-	Task
based	Task
pose	Task
estimation	Task
research	O
exploring	O
regression	Method
tree	Method
methods	Method
:	O
Hough	Method
forests	Method
,	O
random	Method
ferns	Method
,	O
and	O
random	Method
tree	Method
walks	Method
have	O
been	O
proposed	O
in	O
recent	O
years	O
.	O
Occlusion	Task
Handling	Task
and	O
Viewpoint	Task
Invariance	Task
.	O
One	O
popular	O
approach	O
to	O
model	O
occlusions	O
is	O
to	O
treat	O
visibility	O
as	O
a	O
binary	O
mask	O
and	O
jointly	O
reason	O
on	O
this	O
mask	O
with	O
the	O
input	O
images	O
.	O
Other	O
approaches	O
such	O
as	O
,	O
include	O
templates	O
for	O
occluded	O
versions	O
of	O
each	O
part	O
.	O
More	O
sophisticated	O
models	O
introduce	O
occlusion	O
priors	O
or	O
semantic	O
information	O
.	O
For	O
rigid	Task
body	Task
pose	Task
estimation	Task
and	O
3D	Task
object	Task
analysis	Task
,	O
several	O
descriptors	O
have	O
been	O
proposed	O
.	O
Given	O
the	O
success	O
of	O
SIFT	Method
,	O
there	O
have	O
been	O
several	O
attempts	O
at	O
embedding	O
rotational	Task
and	Task
translational	Task
invariance	Task
.	O
Other	O
features	O
such	O
as	O
viewpoint	O
invariant	O
3D	O
feature	O
maps	O
,	O
histograms	O
of	O
3D	O
joint	O
locations	O
,	O
multifractal	O
spectrum	O
,	O
volumetric	Method
attention	Method
models	Method
,	O
and	O
volumetric	Method
convolutional	Method
filters	Method
have	O
been	O
proposed	O
for	O
3D	Task
modeling	Task
.	O
Instead	O
of	O
proposing	O
invariant	O
features	O
,	O
Ozuysal	O
et	O
al	O
.	O
trained	O
a	O
classifier	Method
for	O
each	O
viewpoint	O
.	O
Building	O
on	O
the	O
success	O
of	O
representation	Method
learning	Method
from	O
RGB	Method
,	O
discriminative	Task
pose	Task
estimation	Task
from	O
the	O
depth	O
domain	O
,	O
viewpoint	O
invariant	O
features	O
,	O
and	O
occlusion	Method
modeling	Method
,	O
we	O
design	O
a	O
model	O
which	O
achieves	O
viewpoint	Task
invariant	Task
3D	Task
human	Task
pose	Task
estimation	Task
.	O
section	O
:	O
Model	O
Overview	O
.	O
The	O
goal	O
of	O
our	O
model	O
is	O
to	O
achieve	O
viewpoint	Task
invariant	Task
pose	Task
estimation	Task
.	O
The	O
iterative	Method
error	Method
feedback	Method
mechanism	Method
proposed	O
by	O
demonstrates	O
promising	O
results	O
on	O
front	Material
and	O
side	Material
view	Material
RGB	Material
images	Material
.	O
However	O
,	O
a	O
fundamental	O
challenge	O
remains	O
unsolved	O
:	O
how	O
can	O
a	O
model	O
learn	O
to	O
be	O
viewpoint	O
invariant	O
?	O
Our	O
core	O
contribution	O
is	O
as	O
follows	O
:	O
we	O
leverage	O
depth	O
data	O
to	O
embed	O
local	O
patches	O
into	O
a	O
learned	O
viewpoint	O
invariant	O
feature	O
space	O
.	O
As	O
a	O
result	O
,	O
we	O
can	O
train	O
a	O
body	Method
part	Method
detector	Method
to	O
be	O
invariant	O
to	O
viewpoint	O
changes	O
.	O
To	O
provide	O
richer	O
context	O
,	O
we	O
also	O
introduce	O
recurrent	O
connections	O
to	O
enable	O
our	O
model	O
to	O
reason	O
on	O
past	O
actions	O
and	O
guide	O
downstream	Task
global	Task
pose	Task
estimation	Task
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Model	O
Architecture	O
Local	Method
Input	Method
Representation	Method
.	O
One	O
of	O
our	O
goals	O
is	O
to	O
use	O
local	O
body	O
part	O
context	O
to	O
guide	O
downstream	Task
global	Task
pose	Task
prediction	Task
.	O
To	O
achieve	O
this	O
,	O
we	O
propose	O
a	O
two	O
-	O
step	O
process	O
.	O
First	O
,	O
we	O
extract	O
a	O
set	O
of	O
patches	O
from	O
the	O
input	O
depth	O
image	O
where	O
each	O
patch	O
is	O
centered	O
around	O
each	O
predicted	O
body	O
part	O
.	O
By	O
feeding	O
these	O
patches	O
into	O
our	O
model	O
,	O
it	O
can	O
reason	O
on	O
low	O
-	O
level	O
,	O
local	O
part	O
information	O
.	O
We	O
transform	O
these	O
patches	O
into	O
patches	O
called	O
glimpses	O
.	O
A	O
glimpse	O
is	O
a	O
retina	Method
-	Method
like	Method
encoding	Method
of	O
the	O
original	O
input	O
that	O
encodes	O
pixels	O
further	O
from	O
the	O
center	O
with	O
a	O
progressively	O
lower	O
resolution	O
.	O
As	O
a	O
result	O
,	O
the	O
model	O
must	O
focus	O
on	O
specific	O
input	O
regions	O
with	O
high	O
resolution	O
while	O
maintaining	O
some	O
,	O
but	O
not	O
all	O
spatial	O
information	O
.	O
These	O
glimpses	O
are	O
stacked	O
and	O
denoted	O
by	O
where	O
is	O
the	O
number	O
of	O
joints	O
,	O
is	O
the	O
glimpse	O
height	O
,	O
and	O
is	O
the	O
glimpse	O
and	O
width	O
.	O
Glimpses	O
for	O
iteration	O
are	O
generated	O
using	O
the	O
predicted	O
pose	O
from	O
the	O
previous	O
iteration	O
.	O
When	O
,	O
we	O
use	O
the	O
average	O
pose	O
.	O
Learned	O
Viewpoint	Method
Invariant	Method
Embedding	Method
.	O
We	O
embed	O
the	O
input	O
into	O
a	O
learned	O
,	O
viewpoint	O
invariant	O
feature	O
space	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
Since	O
each	O
glimpse	O
is	O
a	O
real	O
world	O
depth	O
map	O
,	O
we	O
can	O
convert	O
each	O
glimpse	O
into	O
a	O
voxel	O
where	O
is	O
the	O
depth	O
of	O
the	O
voxel	O
.	O
We	O
refer	O
to	O
voxel	O
as	O
a	O
volumetric	Method
representation	Method
of	Method
the	Method
depth	Method
map	Method
and	O
not	O
a	O
full	Method
3D	Method
model	Method
.	O
This	O
representation	O
allows	O
us	O
to	O
transform	O
the	O
glimpse	O
in	O
3D	O
thereby	O
simulating	O
occlusions	O
and	O
geometric	O
variations	O
which	O
may	O
be	O
present	O
from	O
other	O
viewpoints	O
.	O
Given	O
the	O
voxel	O
,	O
we	O
now	O
transform	O
it	O
into	O
a	O
viewpoint	O
invariant	O
feature	O
map	O
.	O
We	O
follow	O
in	O
a	O
two	O
-	O
step	O
process	O
:	O
First	O
,	O
we	O
use	O
a	O
localization	Method
network	Method
to	O
estimate	O
a	O
set	O
of	O
3D	O
transformation	O
parameters	O
which	O
will	O
be	O
applied	O
to	O
the	O
voxel	O
.	O
Second	O
,	O
we	O
compute	O
a	O
sampling	O
grid	O
defined	O
as	O
.	O
Each	O
coordinate	O
of	O
the	O
sampling	O
grid	O
,	O
i.e.	O
,	O
defines	O
where	O
we	O
must	O
apply	O
a	O
sampling	Method
kernel	Method
in	O
voxel	O
to	O
compute	O
of	O
the	O
output	O
feature	O
map	O
.	O
However	O
,	O
since	O
and	O
are	O
real	O
-	O
valued	O
,	O
we	O
convolve	O
with	O
a	O
sampling	Method
kernel	Method
,	O
,	O
and	O
define	O
the	O
output	O
feature	O
map	O
:	O
where	O
the	O
kernel	O
is	O
the	O
trilinear	Method
sampling	Method
kernel	Method
.	O
As	O
a	O
final	O
step	O
,	O
we	O
project	O
the	O
viewpoint	O
invariant	O
3D	O
feature	O
map	O
into	O
a	O
viewpoint	O
invariant	O
2D	O
feature	O
map	O
:	O
Notice	O
that	O
Equations	O
(	O
[	O
reference	O
]	O
)	O
and	O
(	O
[	O
reference	O
]	O
)	O
are	O
linear	O
functions	O
applied	O
to	O
the	O
voxel	O
.	O
As	O
a	O
result	O
,	O
upstream	O
gradients	O
can	O
flow	O
smoothly	O
through	O
these	O
mathematical	O
units	O
.	O
The	O
resulting	O
now	O
represents	O
two	O
-	O
dimensional	Method
viewpoint	Method
invariant	Method
representation	Method
of	O
the	O
input	O
glimpse	O
.	O
At	O
this	O
point	O
,	O
is	O
used	O
as	O
input	O
into	O
a	O
convolutional	Method
network	Method
for	O
human	Task
body	Task
part	Task
detection	Task
and	O
error	Task
feedback	Task
prediction	Task
.	O
Convolutional	Method
and	Method
Recurrent	Method
Networks	Method
.	O
As	O
previously	O
mentioned	O
,	O
our	O
goal	O
is	O
to	O
use	O
local	O
input	O
patches	O
to	O
guide	O
downstream	Task
global	Task
pose	Task
predictions	Task
.	O
We	O
stack	O
the	O
viewpoint	O
invariant	O
feature	O
maps	O
for	O
each	O
joint	O
to	O
form	O
a	O
tensor	O
.	O
This	O
tensor	O
is	O
fed	O
to	O
a	O
convolutional	Method
network	Method
.	O
Through	O
the	O
hierarchical	O
receptive	O
fields	O
of	O
the	O
convolutional	Method
network	Method
,	O
the	O
network	O
’s	O
output	O
is	O
a	O
global	O
representation	O
of	O
the	O
human	O
pose	O
.	O
Directly	O
regressing	O
body	O
part	O
positions	O
from	O
the	O
dense	Method
activation	Method
layers	Method
has	O
proven	O
to	O
be	O
difficult	O
due	O
to	O
the	O
highly	O
non	O
-	O
linear	O
mapping	O
present	O
in	O
traditional	O
human	Task
pose	Task
estimation	Task
.	O
Inspired	O
by	O
’s	O
work	O
in	O
the	O
RGB	Task
domain	Task
,	O
we	O
adopt	O
an	O
iterative	Method
refinement	Method
technique	Method
which	O
uses	O
multiple	O
steps	O
to	O
fine	O
-	O
tune	O
the	O
pose	O
by	O
correcting	O
previous	O
pose	O
estimates	O
.	O
In	O
,	O
each	O
refinement	O
step	O
is	O
only	O
indirectly	O
influenced	O
by	O
previous	O
iterations	O
through	O
the	O
accumulation	O
of	O
error	O
feedback	O
.	O
We	O
claim	O
that	O
these	O
refinement	Method
iterations	Method
should	O
have	O
a	O
more	O
direct	O
and	O
shared	O
temporal	Method
representation	Method
.	O
To	O
remedy	O
this	O
,	O
we	O
introduce	O
recurrent	O
connections	O
between	O
each	O
iteration	O
;	O
specifically	O
a	O
long	Method
short	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
module	O
.	O
This	O
enables	O
our	O
model	O
to	O
directly	O
access	O
the	O
underlying	O
hidden	O
network	O
state	O
which	O
generated	O
prior	O
feedback	O
and	O
model	O
higher	O
-	O
order	O
temporal	O
dependencies	O
.	O
subsection	O
:	O
Multi	Task
-	Task
Task	Task
Loss	Task
Our	O
primary	O
goal	O
is	O
to	O
achieve	O
viewpoint	Task
invariance	Task
.	O
In	O
extreme	O
cases	O
such	O
as	O
top	O
views	O
,	O
many	O
human	O
joints	O
are	O
occluded	O
.	O
To	O
be	O
robust	O
to	O
such	O
occlusions	O
,	O
we	O
want	O
our	O
model	O
to	O
reason	O
on	O
the	O
visibility	O
of	O
joints	O
.	O
We	O
formulate	O
the	O
optimization	Method
procedure	Method
as	O
a	O
multi	Task
-	Task
task	Task
problem	Task
consisting	O
of	O
two	O
objectives	O
:	O
(	O
i	O
)	O
a	O
body	Task
-	Task
part	Task
detection	Task
task	Task
,	O
where	O
the	O
goal	O
is	O
to	O
determine	O
whether	O
a	O
body	O
part	O
is	O
visible	O
or	O
occluded	O
in	O
the	O
input	O
and	O
(	O
ii	O
)	O
a	O
pose	Task
regression	Task
task	Task
,	O
where	O
we	O
predict	O
the	O
offsets	O
to	O
the	O
correct	O
real	O
world	O
3D	O
position	O
of	O
visible	O
human	O
body	O
joints	O
.	O
Body	Task
-	Task
Part	Task
Detection	Task
.	O
For	O
body	Task
part	Task
detection	Task
,	O
the	O
goal	O
is	O
to	O
determine	O
whether	O
a	O
particular	O
body	O
part	O
is	O
visible	O
or	O
occluded	O
in	O
the	O
input	O
.	O
This	O
is	O
denoted	O
by	O
the	O
predicted	O
visibility	O
mask	O
which	O
is	O
a	O
binary	O
vector	O
,	O
where	O
is	O
the	O
total	O
number	O
of	O
body	O
joints	O
.	O
The	O
ground	O
truth	O
visibility	O
mask	O
is	O
denoted	O
by	O
.	O
If	O
a	O
body	O
part	O
is	O
predicted	O
to	O
be	O
visible	O
,	O
then	O
,	O
otherwise	O
denotes	O
occlusion	O
.	O
The	O
visibility	O
mask	O
is	O
computed	O
using	O
a	O
softmax	O
over	O
the	O
unnormalized	O
log	O
probabilities	O
generated	O
by	O
the	O
LSTM	Method
.	O
Hence	O
,	O
our	O
objective	O
is	O
to	O
minimize	O
the	O
cross	Metric
-	Metric
entropy	Metric
.	O
The	O
visibility	O
loss	O
for	O
a	O
single	O
example	O
is	O
:	O
Regardless	O
of	O
the	O
ground	O
truth	O
and	O
the	O
predicted	O
visibility	O
mask	O
,	O
the	O
above	O
formulation	O
forces	O
our	O
model	O
to	O
improve	O
its	O
part	Task
detection	Task
.	O
Additionally	O
,	O
it	O
allows	O
for	O
occluded	Task
body	Task
part	Task
recovery	Task
if	O
the	O
ground	O
truth	O
visibility	O
is	O
fixed	O
to	O
.	O
Partial	Metric
Error	Metric
Feedback	Metric
.	O
Ultimately	O
,	O
our	O
goal	O
is	O
to	O
predict	O
the	O
location	O
of	O
the	O
joint	O
corresponding	O
to	O
each	O
visible	O
human	O
body	O
part	O
.	O
To	O
achieve	O
this	O
,	O
we	O
refine	O
our	O
previous	O
pose	Method
prediction	Method
by	O
learning	O
correction	O
offsets	O
(	O
i.e.	O
feedback	O
)	O
denoted	O
by	O
.	O
Furthermore	O
,	O
we	O
only	O
learn	O
correction	O
offsets	O
for	O
joints	O
that	O
are	O
visible	O
.	O
At	O
each	O
time	O
step	O
,	O
a	O
regression	Method
predicts	Method
offsets	Method
which	O
are	O
used	O
to	O
update	O
the	O
current	O
pose	Task
estimate	Task
.	O
Specifically	O
:	O
denote	O
real	O
-	O
world	O
positions	O
of	O
each	O
joint	O
.	O
The	O
loss	O
shown	O
in	O
(	O
[	O
reference	O
]	O
)	O
is	O
motivated	O
by	O
our	O
goal	O
of	O
predicting	Task
partial	Task
poses	Task
.	O
Consider	O
the	O
case	O
of	O
when	O
the	O
right	O
knee	O
is	O
not	O
visible	O
in	O
the	O
input	O
.	O
If	O
our	O
model	O
successfully	O
labels	O
the	O
right	O
knee	O
as	O
occluded	O
,	O
we	O
wish	O
to	O
prevent	O
the	O
error	O
feedback	O
loss	O
from	O
backpropagating	O
through	O
our	O
network	O
.	O
To	O
achieve	O
this	O
,	O
we	O
include	O
the	O
indicator	Method
term	Method
which	O
only	O
backpropagates	O
pose	O
error	O
feedback	O
if	O
a	O
particular	O
joint	O
is	O
visible	O
in	O
the	O
original	O
image	O
.	O
A	O
secondary	O
benefit	O
is	O
that	O
we	O
do	O
not	O
force	O
the	O
regressor	O
to	O
output	O
dummy	O
real	O
values	O
(	O
if	O
a	O
joint	O
is	O
occluded	O
)	O
which	O
may	O
skew	O
the	O
model	O
’s	O
understanding	O
of	O
output	Metric
magnitude	Metric
.	O
Global	Task
Loss	Task
.	O
The	O
resulting	O
objective	O
is	O
the	O
linear	Method
combination	Method
of	O
the	O
error	Method
feedback	Method
cost	Method
function	Method
for	O
all	O
joints	O
and	O
the	O
detection	Method
cost	Method
function	Method
for	O
all	O
body	O
parts	O
:	O
.	O
The	O
mixing	O
parameters	O
and	O
define	O
the	O
relative	O
weight	O
of	O
each	O
sub	Metric
-	Metric
objective	Metric
.	O
subsection	O
:	O
Training	O
and	O
Optimization	Task
We	O
train	O
the	O
full	O
model	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
single	O
step	O
of	O
optimization	Task
.	O
We	O
train	O
the	O
convolutional	Method
and	Method
recurrent	Method
network	Method
from	O
scratch	O
with	O
all	O
weights	O
initialized	O
from	O
a	O
Gaussian	Method
with	Method
.	O
Gradients	O
are	O
computed	O
using	O
and	O
flow	O
through	O
the	O
recurrent	Method
and	Method
convolutional	Method
networks	Method
.	O
We	O
use	O
the	O
Adam	Method
optimizer	Method
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
,	O
,	O
and	O
.	O
An	O
exponential	Method
learning	Method
rate	Method
decay	Method
schedule	Method
is	O
applied	O
with	O
a	O
decay	O
rate	O
of	O
0.99	O
every	O
1	O
,	O
000	O
iterations	O
.	O
section	O
:	O
Datasets	O
We	O
evaluate	O
our	O
model	O
on	O
a	O
publicly	O
available	O
dataset	O
that	O
has	O
been	O
used	O
by	O
recent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
human	Method
pose	Method
methods	Method
.	O
To	O
more	O
rigorously	O
evaluate	O
our	O
model	O
,	O
we	O
also	O
collected	O
a	O
new	O
dataset	O
consisting	O
of	O
varied	O
camera	O
viewpoints	O
.	O
See	O
Figure	O
[	O
reference	O
]	O
for	O
samples	O
.	O
Previous	O
Depth	O
Datasets	O
.	O
We	O
use	O
the	O
Stanford	Material
EVAL	Material
dataset	Material
which	O
consists	O
of	O
9	O
K	O
front	Material
-	O
facing	O
depth	O
images	O
.	O
The	O
dataset	O
contains	O
3	O
people	O
performing	O
8	O
action	O
sequences	O
each	O
.	O
The	O
EVAL	Material
dataset	Material
was	O
recorded	O
using	O
the	O
Microsoft	Method
Kinect	Method
camera	Method
at	O
30	O
fps	O
.	O
Similar	O
to	O
leave	Method
-	Method
one	Method
-	Method
out	Method
cross	Method
validation	Method
,	O
we	O
adopt	O
a	O
leave	Method
-	Method
one	Method
-	Method
out	Method
train	Method
-	Method
test	Method
procedure	Method
.	O
One	O
person	O
is	O
selected	O
as	O
the	O
test	O
set	O
and	O
the	O
other	O
two	O
people	O
are	O
designated	O
as	O
the	O
training	O
set	O
.	O
This	O
is	O
performed	O
three	O
times	O
such	O
that	O
each	O
person	O
is	O
the	O
test	O
set	O
once	O
.	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
[	O
b	O
]	O
0.3	O
Invariant	Material
-	Material
Top	Material
View	Material
Dataset	Material
(	O
ITOP	Material
)	O
.	O
Existing	O
depth	O
datasets	O
for	O
pose	Task
estimation	Task
are	O
often	O
small	O
in	O
size	O
,	O
both	O
in	O
the	O
number	O
of	O
people	O
and	O
number	O
of	O
frames	O
per	O
person	O
.	O
To	O
address	O
these	O
issues	O
,	O
we	O
collected	O
a	O
new	O
dataset	O
consisting	O
of	O
100	O
K	O
real	O
-	O
world	O
depth	O
images	O
from	O
multiple	O
camera	O
viewpoints	O
.	O
Named	O
ITOP	Material
,	O
the	O
dataset	O
consists	O
of	O
20	O
people	O
performing	O
15	O
action	O
sequences	O
each	O
.	O
Each	O
depth	O
image	O
is	O
labeled	O
with	O
real	O
-	O
world	O
3D	O
joint	O
locations	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
respective	O
camera	O
.	O
The	O
dataset	O
consists	O
of	O
two	O
“	O
views	O
,	O
”	O
namely	O
the	O
front	Material
/	O
side	O
view	O
and	O
the	O
top	O
view	O
.	O
The	O
frontal	O
view	O
contains	O
views	O
of	O
each	O
person	O
,	O
although	O
not	O
necessarily	O
uniformly	O
distributed	O
.	O
The	O
top	O
view	O
contains	O
images	O
captured	O
solely	O
from	O
the	O
top	O
(	O
i.e.	O
camera	O
on	O
the	O
ceiling	O
pointed	O
down	O
to	O
the	O
floor	O
)	O
.	O
Data	O
Collection	O
.	O
Two	O
Asus	Method
Xtion	Method
PRO	Method
cameras	Method
were	O
used	O
.	O
One	O
camera	O
was	O
placed	O
on	O
the	O
ceiling	O
facing	O
down	O
while	O
another	O
camera	O
was	O
from	O
a	O
traditional	O
front	Material
-	O
facing	O
viewpoint	O
.	O
To	O
annotate	O
each	O
frame	O
,	O
we	O
used	O
a	O
series	O
of	O
steps	O
that	O
progressively	O
involved	O
more	O
human	O
supervision	O
if	O
necessary	O
.	O
First	O
,	O
3D	O
joints	O
were	O
estimated	O
using	O
from	O
the	O
front	Material
-	O
facing	O
camera	O
.	O
These	O
coordinates	O
were	O
then	O
transformed	O
into	O
the	O
respective	O
world	O
coordinate	O
system	O
of	O
each	O
camera	O
in	O
the	O
system	O
.	O
Second	O
,	O
we	O
used	O
an	O
iterative	Method
ground	Method
truth	Method
error	Method
correction	Method
technique	Method
based	O
on	O
per	Method
-	Method
pixel	Method
labeling	Method
using	O
k	Method
-	Method
nearest	Method
neighbors	Method
and	O
center	Method
of	Method
mass	Method
convergence	Method
.	O
Finally	O
,	O
humans	O
manually	O
validated	O
,	O
corrected	O
,	O
and	O
discarded	O
noisy	O
frames	O
.	O
On	O
average	O
,	O
the	O
human	Method
labeling	Method
procedure	Method
took	O
one	O
second	O
per	O
frame	O
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Evaluation	Metric
Metrics	Metric
We	O
evaluate	O
our	O
model	O
using	O
two	O
metrics	O
.	O
As	O
introduced	O
in	O
,	O
we	O
use	O
the	O
percentage	O
of	O
correct	O
keypoints	O
(	O
PCKh	Method
)	O
with	O
a	O
variable	O
threshold	O
.	O
This	O
metric	O
defines	O
a	O
successful	O
human	Task
joint	Task
localization	Task
if	O
the	O
predicted	O
joint	O
is	O
within	O
50	O
%	O
of	O
the	O
head	O
segment	O
length	O
to	O
the	O
ground	O
truth	O
joint	O
.	O
For	O
summary	O
tables	O
and	O
figures	O
,	O
we	O
use	O
the	O
mean	Metric
average	Metric
precision	Metric
(	O
mAP	Metric
)	O
which	O
is	O
the	O
average	Metric
precision	Metric
for	O
all	O
human	O
body	O
parts	O
.	O
Precision	Metric
is	O
reported	O
for	O
individual	O
body	O
parts	O
.	O
A	O
successful	O
detection	Task
occurs	O
when	O
the	O
predicted	O
joint	O
is	O
less	O
than	O
10	O
cm	O
from	O
the	O
ground	O
truth	O
in	O
3D	O
space	O
.	O
subsection	O
:	O
Implementation	O
Details	O
Our	O
model	O
is	O
implemented	O
in	O
TensorFlow	Method
.	O
We	O
use	O
mini	O
-	O
batches	O
of	O
size	O
10	O
and	O
10	O
refinement	O
steps	O
per	O
batch	O
.	O
We	O
use	O
the	O
VGG	Method
-	Method
16	Method
architecture	Method
for	O
our	O
convolutional	Method
network	Method
but	O
instead	O
modify	O
the	O
first	Method
layer	Method
to	O
accommodate	O
the	O
increased	O
number	O
of	O
input	O
channels	O
.	O
Additionally	O
,	O
we	O
reduce	O
the	O
number	O
of	O
neurons	O
in	O
the	O
dense	O
layers	O
to	O
2048	O
.	O
We	O
remove	O
the	O
final	O
softmax	O
layer	O
and	O
use	O
the	O
second	O
dense	O
layer	O
activations	O
as	O
input	O
into	O
a	O
recurrent	Method
network	Method
.	O
For	O
the	O
recurrent	Method
network	Method
,	O
we	O
use	O
a	O
long	Method
short	Method
term	Method
memory	Method
(	O
LSTM	Method
)	O
module	O
consisting	O
of	O
2048	O
hidden	O
units	O
.	O
The	O
LSTM	Method
hidden	O
state	O
is	O
duplicated	O
and	O
passed	O
to	O
a	O
softmax	Method
layer	Method
and	O
a	O
regression	Method
layer	Method
for	O
loss	Task
computation	Task
and	O
pose	Task
-	Task
error	Task
computation	Task
.	O
The	O
model	O
is	O
trained	O
from	O
scratch	O
.	O
The	O
grid	Method
generator	Method
is	O
a	O
convolutional	Method
network	Method
with	O
four	O
layers	O
.	O
Each	O
layer	O
contains	O
:	O
(	O
i	O
)	O
a	O
convolutional	Method
layer	Method
with	O
32	O
filters	O
of	O
size	O
with	O
stride	O
1	O
and	O
padding	O
1	O
,	O
(	O
ii	O
)	O
a	O
rectified	Method
linear	Method
unit	Method
,	O
(	O
iii	O
)	O
,	O
a	O
max	Method
-	Method
pooling	Method
over	O
a	O
region	O
with	O
stride	O
2	O
.	O
The	O
fourth	O
layer	O
’s	O
output	O
is	O
and	O
is	O
connected	O
to	O
a	O
dense	Method
layer	Method
consisting	O
of	O
12	O
output	O
nodes	O
which	O
defines	O
.	O
The	O
specific	O
3D	O
transformation	O
parameters	O
are	O
defined	O
in	O
.	O
To	O
generate	O
glimpses	O
for	O
the	O
first	O
refinement	Task
iteration	Task
,	O
the	O
mean	O
3D	O
pose	O
from	O
the	O
training	O
set	O
is	O
used	O
.	O
Glimpses	O
are	O
160	O
pixels	O
in	O
height	O
and	O
width	O
and	O
centered	O
at	O
each	O
joint	O
location	O
(	O
in	O
the	O
image	O
plane	O
)	O
.	O
Each	O
glimpse	O
consists	O
of	O
4	O
patches	O
where	O
each	O
patch	O
is	O
quadratically	O
downsampled	O
according	O
to	O
the	O
patch	O
number	O
(	O
i.e.	O
its	O
distance	O
from	O
the	O
glimpse	O
center	O
)	O
.	O
The	O
input	O
to	O
our	O
convolutional	Method
network	Method
is	O
where	O
is	O
the	O
number	O
of	O
body	O
part	O
joints	O
.	O
subsection	O
:	O
Comparison	O
with	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
We	O
compare	O
our	O
model	O
to	O
three	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
:	O
random	Method
forests	Method
,	O
random	Method
tree	Method
walks	Method
(	O
RTW	Method
)	O
,	O
and	O
iterative	Method
error	Method
feedback	Method
(	O
IEF	Method
)	O
.	O
One	O
of	O
our	O
primary	O
goals	O
is	O
to	O
achieve	O
viewpoint	Task
invariance	Task
.	O
To	O
evaluate	O
this	O
,	O
we	O
perform	O
three	O
sets	O
of	O
experiments	O
,	O
progressing	O
in	O
level	O
of	O
difficulty	O
.	O
First	O
,	O
we	O
train	O
and	O
test	O
all	O
models	O
on	O
front	Material
view	Material
images	Material
.	O
This	O
is	O
the	O
classical	O
human	Task
pose	Task
estimation	Task
task	Task
.	O
Second	O
,	O
we	O
train	O
and	O
test	O
all	O
models	O
on	O
top	Material
view	Material
images	Material
.	O
This	O
is	O
similar	O
to	O
the	O
classical	O
pose	Task
estimation	Task
task	Task
but	O
from	O
a	O
different	O
viewpoint	O
.	O
Third	O
,	O
we	O
train	O
on	O
front	Material
view	Material
images	Material
and	O
test	O
on	O
top	Material
view	Material
images	Material
.	O
This	O
is	O
the	O
most	O
difficult	O
experiment	O
and	O
truly	O
tests	O
a	O
model	O
’s	O
ability	O
to	O
learn	O
viewpoint	O
transfer	O
.	O
Baselines	O
.	O
We	O
give	O
a	O
brief	O
overview	O
of	O
the	O
baseline	O
algorithms	O
:	O
1	O
.	O
The	O
random	Method
forest	Method
model	Method
consists	O
of	O
multiple	O
decision	Method
trees	Method
that	O
traverse	O
each	O
pixel	O
to	O
find	O
the	O
body	O
part	O
labels	O
for	O
that	O
pixel	O
.	O
Once	O
pixels	O
are	O
classified	O
into	O
body	O
parts	O
,	O
joint	O
positions	O
are	O
found	O
with	O
mean	Method
shift	Method
.	O
2	O
.	O
Random	Method
tree	Method
walk	Method
(	O
RTW	Method
)	O
trains	O
a	O
regression	Method
tree	Method
to	O
estimate	O
the	O
probability	O
distribution	O
to	O
the	O
direction	O
toward	O
the	O
particular	O
joint	O
,	O
relative	O
to	O
the	O
current	O
position	O
.	O
At	O
test	O
time	O
,	O
the	O
direction	O
for	O
the	O
random	Method
walk	Method
is	O
randomly	O
chosen	O
from	O
a	O
set	O
of	O
representative	O
directions	O
.	O
3	O
.	O
Iterative	Method
error	Method
feedback	Method
(	O
IEF	Method
)	O
is	O
a	O
self	Method
-	Method
correcting	Method
model	Method
used	O
to	O
progressively	O
make	O
changes	O
to	O
an	O
initial	Task
pose	Task
estimation	Task
by	O
using	O
error	O
feedback	O
.	O
[	O
b	O
]	O
0.31	O
[	O
b	O
]	O
0.31	O
[	O
b	O
]	O
0.31	O
Train	O
on	O
front	Material
views	O
,	O
test	O
on	O
front	Material
views	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
average	Metric
precision	Metric
for	O
each	O
joint	O
using	O
a	O
10	O
cm	O
threshold	O
and	O
the	O
overall	O
mean	Metric
average	Metric
precision	Metric
(	O
mAP	Metric
)	O
while	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
PCKh	Method
for	O
all	O
models	O
.	O
IEF	Method
and	O
the	O
random	Method
forest	Method
methods	Method
were	O
not	O
evaluated	O
on	O
the	O
EVAL	Material
dataset	Material
.	O
Random	Method
forest	Method
depends	O
on	O
a	O
per	Method
-	Method
pixel	Method
body	Method
part	Method
labeling	Method
,	O
which	O
is	O
not	O
provided	O
by	O
EVAL	Method
.	O
IEF	Method
was	O
unable	O
to	O
converge	O
to	O
comparable	O
results	O
on	O
the	O
EVAL	Material
dataset	Material
.	O
We	O
discuss	O
the	O
ITOP	Material
results	O
below	O
.	O
For	O
frontal	O
views	O
,	O
RTW	Method
achieves	O
a	O
mAP	Metric
of	O
84.8	O
and	O
80.5	O
for	O
the	O
upper	O
and	O
full	O
body	O
,	O
respectively	O
.	O
Our	O
recurrent	Method
error	Method
feedback	Method
(	O
REF	Method
)	O
model	O
performs	O
similarly	O
to	O
RTW	Method
,	O
achieving	O
a	O
mAP	Metric
of	O
2	O
to	O
3	O
points	O
less	O
.	O
The	O
random	Method
forest	Method
algorithm	Method
achieves	O
the	O
lowest	O
full	O
body	O
mAP	Metric
of	O
65.8	O
.	O
This	O
could	O
be	O
attributed	O
to	O
the	O
limited	O
amount	O
of	O
training	O
data	O
.	O
The	O
original	O
algorithm	O
was	O
trained	O
on	O
900	O
K	O
synthetic	O
depth	O
images	O
.	O
We	O
show	O
qualitative	O
results	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
front	Material
-	O
view	O
ITOP	Material
dataset	O
is	O
shown	O
in	O
columns	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
.	O
Both	O
our	O
model	O
and	O
IEF	Method
make	O
similar	O
mistakes	O
:	O
both	O
models	O
sometimes	O
fail	O
to	O
learn	O
sufficient	O
feedback	O
to	O
converge	O
to	O
the	O
correct	O
body	O
part	O
location	O
.	O
Since	O
we	O
do	O
not	O
impose	O
joint	O
position	O
constraints	O
or	O
enforce	O
skeleton	O
priors	O
,	O
our	O
method	O
incorrectly	O
predicts	O
the	O
elbow	O
location	O
.	O
Train	O
on	O
top	O
view	O
,	O
test	O
on	O
top	O
view	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
examples	O
of	O
qualitative	O
results	O
from	O
frontal	O
and	O
top	O
down	O
views	O
for	O
Shotton	O
et	O
al	O
.	O
and	O
random	Method
tree	Method
walk	Method
(	O
RTW	Method
)	O
.	O
For	O
the	O
top	O
-	O
down	O
view	O
,	O
we	O
show	O
only	O
8	O
joints	O
on	O
the	O
upper	O
body	O
(	O
i.e.	O
head	O
,	O
neck	O
,	O
left	O
shoulder	O
,	O
right	O
shoulder	O
,	O
left	O
elbow	O
,	O
right	O
elbow	O
,	O
left	O
hand	O
,	O
and	O
right	O
hand	O
)	O
as	O
the	O
lower	O
body	O
joints	O
are	O
almost	O
always	O
occluded	O
.	O
RF	Method
and	O
RTW	Method
give	O
reasonable	O
results	O
when	O
all	O
joints	O
are	O
visible	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
a	O
and	O
[	O
reference	O
]	O
c	O
)	O
but	O
do	O
not	O
perform	O
well	O
in	O
the	O
case	O
of	O
occlusion	O
(	O
Figure	O
[	O
reference	O
]	O
b	O
and	O
[	O
reference	O
]	O
d	O
)	O
.	O
For	O
the	O
random	Method
forest	Method
method	Method
,	O
we	O
can	O
see	O
from	O
figure	O
[	O
reference	O
]	O
b	O
that	O
the	O
prediction	O
for	O
the	O
occluded	O
right	O
elbow	O
is	O
topologically	O
invalid	O
though	O
both	O
right	O
shoulder	O
and	O
hand	O
are	O
visible	O
and	O
correctly	O
predicted	O
.	O
This	O
is	O
because	O
the	O
model	O
does	O
n’t	O
take	O
into	O
account	O
the	O
topological	O
information	O
among	O
joints	O
,	O
so	O
it	O
is	O
not	O
able	O
to	O
modify	O
its	O
prediction	O
for	O
one	O
joint	O
base	O
on	O
the	O
predicted	O
positions	O
of	O
neighboring	O
joints	O
.	O
For	O
RTW	Method
,	O
figure	O
[	O
reference	O
]	O
b	O
shows	O
that	O
the	O
predicted	O
position	O
for	O
right	O
hand	O
goes	O
to	O
the	O
right	O
leg	O
.	O
Though	O
legs	O
and	O
hands	O
possess	O
very	O
different	O
depth	O
information	O
,	O
the	O
model	O
mistook	O
the	O
right	O
leg	O
for	O
right	O
hand	O
when	O
the	O
hand	O
is	O
occluded	O
and	O
the	O
leg	O
appears	O
in	O
the	O
common	O
spatial	O
location	O
of	O
a	O
hand	O
.	O
Train	O
on	O
frontal	O
views	O
,	O
test	O
on	O
top	O
views	O
.	O
This	O
is	O
the	O
most	O
difficult	O
task	O
for	O
3D	Task
pose	Task
estimation	Task
algorithms	Task
since	O
the	O
test	O
set	O
contains	O
significant	O
scale	O
and	O
shape	O
differences	O
from	O
the	O
training	O
data	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
RTW	Method
gives	O
the	O
lowest	O
performance	O
as	O
the	O
model	O
relies	O
heavily	O
on	O
topological	O
information	O
.	O
If	O
the	O
prediction	O
for	O
an	O
initial	O
joint	O
fails	O
,	O
error	O
will	O
accumulate	O
onto	O
subsequent	O
joints	O
.	O
Both	O
deep	Method
learning	Method
methods	Method
are	O
able	O
to	O
localize	O
joints	O
despite	O
the	O
viewpoint	O
change	O
.	O
IEF	Method
achieves	O
a	O
47.9	O
detection	Metric
rate	Metric
for	O
the	O
head	O
while	O
our	O
model	O
achieves	O
a	O
55.6	O
detection	Metric
rate	Metric
.	O
This	O
can	O
be	O
attributed	O
to	O
the	O
proximity	O
of	O
upper	O
body	O
joints	O
in	O
both	O
viewpoints	O
.	O
The	O
head	O
,	O
neck	O
,	O
and	O
torso	O
locations	O
are	O
similarly	O
positioned	O
across	O
viewpoints	O
.	O
Runtime	Task
Analysis	Task
.	O
Methods	O
which	O
employ	O
deep	Method
learning	Method
techniques	Method
often	O
require	O
more	O
computation	O
for	O
forward	Task
propagation	Task
compared	O
to	O
non	Method
deep	Method
learning	Method
approaches	Method
.	O
Our	O
model	O
requires	O
1.7	O
seconds	O
per	O
frame	O
(	O
10	O
iterations	O
,	O
forward	O
-	O
pass	O
only	O
)	O
while	O
the	O
random	Method
tree	Method
walk	Method
requires	O
0.1	O
second	O
per	O
frame	O
.	O
While	O
this	O
is	O
dependent	O
on	O
implementation	O
details	O
,	O
it	O
does	O
illustrate	O
the	O
tradeoff	O
between	O
speed	Metric
and	O
performance	O
.	O
subsection	O
:	O
Ablation	Task
Studies	Task
To	O
further	O
gauge	O
the	O
effectiveness	O
of	O
our	O
model	O
,	O
we	O
analyze	O
each	O
component	O
of	O
our	O
model	O
and	O
provide	O
both	O
quantitative	Task
and	Task
qualitative	Task
analyses	Task
.	O
Specifically	O
,	O
we	O
evaluate	O
the	O
effect	O
of	O
error	O
feedback	O
and	O
discuss	O
the	O
relevance	O
of	O
the	O
input	O
glimpse	Method
representation	Method
.	O
Effect	O
of	O
Recurrent	O
Connections	O
.	O
We	O
analyze	O
the	O
effect	O
of	O
recurrent	O
connections	O
compared	O
to	O
regular	Method
iterative	Method
error	Method
feedback	Method
and	O
direct	Method
prediction	Method
.	O
To	O
evaluate	O
iterative	O
feedback	O
,	O
we	O
use	O
our	O
final	O
model	O
but	O
remove	O
the	O
LSTM	Method
module	O
and	O
regress	O
the	O
visibility	O
mask	O
and	O
error	O
feedback	O
using	O
the	O
dense	O
layer	O
activations	O
.	O
Note	O
that	O
we	O
still	O
use	O
a	O
multi	O
-	O
task	O
loss	O
and	O
glimpse	O
inputs	O
.	O
Direct	Task
prediction	Task
does	O
not	O
involve	O
feedback	O
but	O
instead	O
attempts	O
to	O
directly	O
regress	O
correct	O
pose	O
locations	O
in	O
a	O
single	O
pass	O
.	O
Quantitative	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Direct	Task
prediction	Task
,	O
as	O
expected	O
,	O
performs	O
poorly	O
as	O
it	O
is	O
very	O
difficult	O
to	O
regress	O
exact	O
3D	O
joint	O
locations	O
in	O
a	O
single	O
pass	O
.	O
Iterative	Method
-	Method
based	Method
approaches	Method
significantly	O
improve	O
performance	O
by	O
30	O
points	O
.	O
It	O
is	O
clear	O
that	O
recurrent	O
connections	O
improve	O
performance	O
,	O
especially	O
in	O
the	O
top	Task
-	Task
view	Task
case	Task
where	O
recurrent	O
feedback	O
achieves	O
91.4	O
upper	O
body	O
mAP	Metric
while	O
iterative	O
feedback	O
achieves	O
51.7	O
upper	O
body	O
mAP	Metric
.	O
Figure	O
[	O
reference	O
]	O
shows	O
how	O
our	O
model	O
updates	O
the	O
pose	O
over	O
time	O
.	O
Consistent	O
across	O
all	O
images	O
,	O
the	O
first	O
iteration	O
always	O
involves	O
a	O
large	O
,	O
seemingly	O
random	O
transformation	O
of	O
the	O
pose	O
.	O
This	O
can	O
be	O
thought	O
of	O
as	O
the	O
model	O
is	O
“	O
looking	O
around	O
”	O
the	O
initial	O
pose	Method
estimate	Method
.	O
Once	O
the	O
model	O
understands	O
the	O
initial	O
surrounding	O
area	O
,	O
it	O
returns	O
to	O
the	O
human	O
body	O
and	O
begins	O
to	O
fine	O
-	O
tune	O
the	O
pose	Task
prediction	Task
,	O
as	O
shown	O
in	O
iteration	O
10	O
.	O
Figure	O
[	O
reference	O
]	O
b	O
quantitatively	O
illustrates	O
this	O
result	O
.	O
Effect	O
of	O
Glimpses	O
.	O
Our	O
motivation	O
for	O
glimpses	O
is	O
to	O
provide	O
additional	O
local	O
context	O
to	O
our	O
model	O
to	O
guide	O
downstream	Task
,	Task
global	Task
pose	Task
estimation	Task
.	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
evaluate	O
the	O
performance	O
of	O
glimpses	O
vs	O
indicator	O
masks	O
(	O
i.e.	O
heatmaps	O
)	O
.	O
Figure	O
[	O
reference	O
]	O
b	O
shows	O
that	O
glimpses	Method
do	O
provide	O
more	O
context	O
for	O
the	O
global	Task
pose	Task
prediction	Task
task	Task
.	O
As	O
the	O
number	O
of	O
refinement	O
iterations	O
increases	O
,	O
using	O
glimpses	Method
,	O
the	O
localization	Metric
error	Metric
for	O
each	O
joint	O
is	O
less	O
than	O
the	O
error	O
with	O
heatmaps	Method
.	O
By	O
looking	O
at	O
Figure	O
[	O
reference	O
]	O
a	O
,	O
it	O
becomes	O
apparent	O
that	O
heatmaps	O
provide	O
limited	O
spatial	O
information	O
.	O
The	O
indicator	O
mask	O
is	O
a	O
way	O
of	O
encoding	O
two	O
-	O
dimensional	O
body	O
part	O
coordinates	O
but	O
does	O
not	O
explicitly	O
provide	O
local	O
context	O
information	O
.	O
Glimpses	Method
are	O
able	O
to	O
provide	O
such	O
context	O
from	O
the	O
input	O
image	O
.	O
[	O
b	O
]	O
0.59	O
[	O
b	O
]	O
0.38	O
section	O
:	O
Conclusion	O
We	O
introduced	O
a	O
viewpoint	Method
invariant	Method
model	Method
that	O
estimates	O
3D	Task
human	Task
pose	Task
from	O
a	O
single	O
depth	O
image	O
.	O
Our	O
model	O
is	O
formulated	O
as	O
a	O
deep	Method
discriminative	Method
model	Method
that	O
attends	O
to	O
glimpses	O
in	O
the	O
input	O
.	O
Using	O
a	O
multi	Task
-	Task
task	Task
optimization	Task
objective	Task
,	O
our	O
model	O
is	O
able	O
to	O
selectively	O
predict	O
partial	O
poses	O
by	O
using	O
a	O
predicted	O
visibility	O
mask	O
.	O
This	O
enables	O
our	O
model	O
to	O
iteratively	O
improve	O
its	O
pose	Task
estimates	Task
by	O
predicting	O
occlusion	O
and	O
human	O
joint	O
offsets	O
.	O
We	O
showed	O
that	O
our	O
model	O
achieves	O
competitive	O
performance	O
on	O
an	O
existing	O
depth	Task
-	Task
based	Task
pose	Task
estimation	Task
dataset	Task
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
newly	O
collected	O
dataset	O
containing	O
100	O
K	O
annotated	O
depth	O
images	O
from	O
several	O
view	O
points	O
.	O
subsubsection	O
:	O
Acknowledgements	O
.	O
We	O
gratefully	O
acknowledge	O
the	O
Clinical	O
Excellence	O
Research	O
Center	O
(	O
CERC	O
)	O
at	O
Stanford	O
Medicine	O
and	O
thank	O
the	O
Office	O
of	O
Naval	O
Research	O
,	O
Multidisciplinary	O
University	O
Research	O
Initiatives	O
Program	O
(	O
ONR	O
MURI	O
)	O
for	O
their	O
support	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Appendices	O
appendix	O
:	O
Localization	Method
Heatmaps	Method
To	O
further	O
analyze	O
the	O
viewpoint	Task
transfer	Task
task	Task
(	O
train	O
on	O
front	Material
and	O
side	O
views	O
,	O
test	O
on	O
top	O
views	O
)	O
,	O
we	O
visualize	O
the	O
localization	O
heatmap	O
in	O
the	O
figures	O
below	O
.	O
For	O
each	O
body	O
part	O
,	O
we	O
plot	O
the	O
predicted	O
test	O
-	O
set	O
locations	O
with	O
respect	O
to	O
the	O
ground	O
truth	O
.	O
Clusters	O
closer	O
to	O
are	O
better	O
.	O
All	O
axes	O
denote	O
centimeters	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
our	O
model	O
’s	O
outputs	O
for	O
the	O
viewpoint	Task
transfer	Task
task	Task
.	O
For	O
lower	O
body	O
parts	O
,	O
our	O
model	O
makes	O
a	O
systemic	Metric
error	Metric
of	O
predicting	O
joints	O
to	O
be	O
lower	O
(	O
i.e.	O
closer	O
to	O
the	O
ground	O
)	O
than	O
the	O
ground	O
truth	O
.	O
From	O
the	O
top	O
view	O
,	O
the	O
lower	O
body	O
parts	O
are	O
not	O
only	O
further	O
from	O
the	O
camera	O
but	O
they	O
are	O
also	O
often	O
occluded	O
which	O
forces	O
our	O
model	O
to	O
reason	O
based	O
on	O
global	O
pose	O
structure	O
as	O
opposed	O
to	O
fine	O
-	O
tuned	O
local	O
information	O
.	O
For	O
the	O
upper	O
body	O
,	O
most	O
joints	O
are	O
visible	O
which	O
lead	O
to	O
more	O
correct	O
predictions	O
.	O
Below	O
,	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
the	O
differences	O
between	O
the	O
initialization	Method
strategies	Method
of	O
IEF	Method
and	O
our	O
method	O
.	O
Random	Method
tree	Method
walk	Method
tends	O
to	O
perform	O
poorly	O
on	O
the	O
viewpoint	Task
transfer	Task
task	Task
.	O
The	O
heatmaps	O
below	O
show	O
predictions	O
very	O
far	O
from	O
the	O
ground	O
truth	O
.	O
