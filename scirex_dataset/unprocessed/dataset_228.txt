document	O
:	O
Incremental	Method
Learning	Method
in	O
Person	Task
Re	Task
-	Task
Identification	Task
Person	Task
Re	Task
-	Task
Identification	Task
is	O
still	O
a	O
challenging	O
task	O
in	O
Computer	Task
Vision	Task
due	O
to	O
variety	O
of	O
reasons	O
.	O
On	O
the	O
other	O
side	O
,	O
Incremental	Method
Learning	Method
is	O
still	O
an	O
issue	O
since	O
Deep	Method
Learning	Method
models	Method
tend	O
to	O
face	O
the	O
problem	O
of	O
overcatastrophic	Task
forgetting	Task
when	O
trained	O
on	O
subsequent	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
model	O
which	O
can	O
be	O
used	O
for	O
multiple	O
tasks	O
in	O
Person	Task
Re	Task
-	Task
Identification	Task
,	O
provide	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
variety	O
of	O
tasks	O
and	O
still	O
achieve	O
considerable	O
accuracy	Metric
later	O
on	O
.	O
We	O
evaluated	O
our	O
model	O
on	O
two	O
datasets	O
Market	Material
1501	Material
and	O
Duke	Material
MTMC	Material
.	O
Extensive	O
experiments	O
show	O
that	O
this	O
method	O
can	O
achieve	O
Incremental	Method
Learning	Method
in	O
Person	Task
ReID	Task
efficiently	O
as	O
well	O
as	O
for	O
other	O
tasks	O
in	O
computer	Task
vision	Task
as	O
well	O
.	O
The	O
code	O
for	O
this	O
work	O
can	O
be	O
found	O
section	O
:	O
Introduction	O
Deep	Method
neural	Method
networks	Method
have	O
revolutionized	O
the	O
field	O
of	O
computer	Task
vision	Task
.	O
In	O
recent	O
years	O
,	O
a	O
lot	O
of	O
work	O
has	O
been	O
done	O
in	O
Person	Task
Re	Task
-	Task
Identification	Task
,	O
we	O
’	O
ve	O
seen	O
a	O
considerable	O
progress	O
but	O
still	O
we	O
face	O
a	O
lot	O
of	O
challenges	O
in	O
terms	O
of	O
getting	O
accurate	O
predictions	Task
in	O
real	O
life	O
instances	O
.	O
It	O
plays	O
an	O
important	O
role	O
in	O
many	O
areas	O
,	O
surveillance	Task
being	O
one	O
of	O
them	O
.	O
In	O
some	O
sense	O
,	O
it	O
can	O
be	O
compared	O
to	O
other	O
prominent	O
tasks	O
in	O
computer	Task
vision	Task
like	O
Image	Task
Retrieval	Task
or	O
Object	Task
Detection	Task
,	O
where	O
a	O
lot	O
of	O
progress	O
have	O
been	O
made	O
.	O
Moreover	O
,	O
there	O
has	O
been	O
a	O
growing	O
demand	O
of	O
deep	Method
learning	Method
models	Method
that	O
incur	O
low	Metric
computational	Metric
cost	Metric
.	O
Deployment	O
of	O
such	O
models	O
can	O
be	O
cumbersome	O
and	O
may	O
not	O
prove	O
to	O
be	O
much	O
efficient	O
especially	O
if	O
the	O
same	O
task	O
can	O
be	O
carried	O
out	O
with	O
lesser	O
number	O
of	O
parameters	O
.	O
Given	O
a	O
set	O
of	O
images	O
of	O
a	O
person	O
taken	O
from	O
different	O
angles	O
from	O
different	O
camera	O
,	O
our	O
model	O
is	O
required	O
to	O
generate	O
a	O
higher	O
prediction	O
if	O
those	O
images	O
are	O
of	O
the	O
same	O
person	O
and	O
vice	O
versa	O
.	O
The	O
problem	O
is	O
composed	O
by	O
multiple	O
reasons	O
some	O
of	O
which	O
may	O
include	O
background	O
clutter	O
,	O
illumination	O
conditions	O
,	O
occlusion	O
,	O
body	O
pose	O
,	O
orientation	O
of	O
cameras	O
.	O
Numerous	O
methods	O
have	O
been	O
proposed	O
to	O
address	O
some	O
of	O
these	O
issues	O
.	O
So	O
far	O
the	O
models	O
that	O
have	O
been	O
proposed	O
in	O
Person	Task
-	Task
ReID	Task
are	O
good	O
in	O
doing	O
well	O
in	O
particular	O
dataset	O
but	O
when	O
tested	O
on	O
quite	O
dissimilar	O
dataset	O
,	O
they	O
struggle	O
to	O
get	O
just	O
right	O
predictions	O
.	O
Unlike	O
other	O
tasks	O
such	O
as	O
Image	Task
Classification	Task
or	O
Object	Task
Detection	Task
,	O
we	O
are	O
required	O
to	O
have	O
our	O
model	O
perform	O
well	O
on	O
a	O
large	O
number	O
of	O
classes	O
and	O
all	O
these	O
images	O
are	O
not	O
as	O
much	O
distinctive	O
as	O
other	O
objects	O
do	O
which	O
makes	O
it	O
difficult	O
for	O
neural	Method
net	Method
to	O
generate	O
accurate	Task
predictions	Task
.	O
We	O
devise	O
a	O
new	O
method	O
that	O
can	O
be	O
used	O
to	O
create	O
robust	Task
Person	Task
-	Task
ReID	Task
systems	Task
at	O
lower	O
computational	Metric
cost	Metric
that	O
can	O
not	O
only	O
perform	O
well	O
on	O
one	O
task	O
,	O
but	O
if	O
trained	O
properly	O
using	O
our	O
techniques	O
,	O
can	O
be	O
well	O
adapted	O
to	O
other	O
tasks	O
as	O
well	O
.	O
section	O
:	O
Related	O
work	O
For	O
Incremental	Method
Learning	Method
,	O
many	O
research	O
work	O
has	O
been	O
carried	O
out	O
.	O
Our	O
work	O
is	O
slightly	O
inspired	O
from	O
LwF	Method
,	O
which	O
was	O
used	O
for	O
classification	Task
purpose	Task
.	O
They	O
made	O
use	O
of	O
CIFAR10	Material
and	O
SVHN	Material
as	O
the	O
two	O
tasks	O
and	O
then	O
achieved	O
considerable	O
performance	O
.	O
Other	O
closely	O
associated	O
work	O
which	O
builds	O
upon	O
it	O
is	O
SeNA	Method
-	Method
CNN	Method
,	O
wherein	O
they	O
made	O
the	O
architecture	O
a	O
little	O
more	O
complex	O
by	O
introducing	O
more	O
layers	O
in	O
different	O
pipelines	O
instead	O
of	O
just	O
dealing	O
with	O
fully	O
connected	O
layers	O
.	O
Our	O
work	O
is	O
the	O
first	O
one	O
that	O
tries	O
to	O
tackle	O
the	O
problem	O
of	O
Incremental	Method
Learning	Method
in	O
Person	Task
Re	Task
-	Task
Identification	Task
,	O
unlike	O
image	Task
classification	Task
where	O
we	O
have	O
relatively	O
lesser	O
number	O
of	O
classes	O
,	O
the	O
number	O
is	O
way	O
more	O
,	O
and	O
this	O
increases	O
the	O
difficulty	O
level	O
for	O
generating	Task
accurate	Task
predictions	Task
.	O
In	O
defense	O
made	O
use	O
of	O
Triplet	O
Loss	O
to	O
show	O
that	O
it	O
can	O
be	O
used	O
to	O
perform	O
end	Task
to	Task
end	Task
deep	Task
metric	Task
learning	Task
.	O
Some	O
work	O
that	O
has	O
been	O
carried	O
out	O
in	O
this	O
incremental	Task
learning	Task
space	Task
also	O
makes	O
use	O
of	O
Distillation	Method
of	Method
neural	Method
networks	Method
wherein	O
you	O
train	O
a	O
smaller	O
network	O
to	O
produce	O
close	O
predictions	O
to	O
cumbersome	O
models	O
.	O
But	O
to	O
carry	O
out	O
this	O
task	O
,	O
we	O
are	O
also	O
required	O
to	O
train	O
our	O
cumbersome	Method
model	Method
first	O
to	O
be	O
able	O
to	O
train	O
the	O
smaller	O
model	O
which	O
is	O
again	O
a	O
big	O
task	O
.	O
Our	O
proposed	O
method	O
does	O
n’t	O
rely	O
on	O
multiple	Method
models	Method
,	O
or	O
older	O
data	O
that	O
has	O
been	O
used	O
to	O
train	O
the	O
network	O
on	O
earlier	O
task	O
,	O
rather	O
we	O
have	O
multiple	O
pipelines	O
inside	O
one	O
model	O
which	O
aims	O
to	O
resolve	O
this	O
issue	O
.	O
section	O
:	O
Our	O
proposed	O
method	O
We	O
propose	O
a	O
new	O
architecture	O
that	O
’s	O
relatively	O
simple	O
as	O
compared	O
to	O
other	O
proposed	O
methods	O
for	O
achieving	O
Incremental	Method
Learning	Method
along	O
with	O
few	O
techniques	O
that	O
makes	O
convergence	Task
faster	O
and	O
increases	O
model	O
’s	O
accuracy	Metric
.	O
subsection	O
:	O
Overall	O
architecture	O
We	O
use	O
a	O
ResNet50	Method
which	O
has	O
been	O
pretrained	O
on	O
ImageNet	Material
.We	O
remove	O
the	O
last	O
two	O
layers	O
i.e	O
Fully	Method
connected	Method
layer	Method
and	O
Average	Method
Pooling	Method
layer	Method
.	O
We	O
then	O
introduce	O
pipelines	Method
(	O
multiple	O
heads	O
)	O
.The	O
main	O
goal	O
behind	O
keeping	O
ResNet	Method
is	O
to	O
perform	O
the	O
task	O
of	O
feature	Task
extraction	Task
effectively	O
,	O
it	O
acts	O
as	O
a	O
base	Method
model	Method
which	O
contains	O
global	O
common	O
features	O
extracted	O
from	O
the	O
data	O
.	O
Then	O
these	O
pipelines	O
have	O
been	O
introduced	O
to	O
generate	O
task	Task
specific	Task
predictions	Task
.	O
These	O
pipelines	O
can	O
be	O
modified	O
as	O
per	O
use	O
case	O
to	O
better	O
adapt	O
to	O
given	O
task	O
.	O
In	O
our	O
case	O
,	O
since	O
the	O
two	O
tasks	O
were	O
same	O
,	O
we	O
decided	O
to	O
keep	O
them	O
identical	O
.	O
subsection	O
:	O
Multiple	O
Pipelines	O
We	O
introduced	O
two	O
pipelines	O
after	O
ResNet	Method
,	O
one	O
is	O
meant	O
to	O
work	O
on	O
Market1501	O
and	O
the	O
other	O
works	O
on	O
DukeMTMC	Material
.	O
Each	O
pipeline	O
consists	O
of	O
two	O
convolutional	Method
blocks	Method
followed	O
by	O
a	O
Fully	Method
connected	Method
layer	Method
.	O
Each	O
convolutional	Method
block	Method
consists	O
of	O
convolutional	Method
layer	Method
which	O
takes	O
in	O
ni	O
input	O
channels	O
with	O
kernel	O
size	O
1	O
,	O
stride	O
1	O
and	O
outputs	O
ni	O
/	O
2	O
channels	O
.	O
This	O
is	O
followed	O
by	O
Batch	Method
Normalization	Method
and	O
usage	O
of	O
Leaky	Method
ReLU	Method
activation	Method
.	O
Another	O
block	O
takes	O
in	O
ni	O
/	O
2	O
input	O
channels	O
and	O
outputs	O
ni	O
channels	O
with	O
kernel	O
size	O
3	O
and	O
stride	O
kept	O
to	O
1	O
.	O
So	O
in	O
this	O
process	O
dimensionality	O
is	O
not	O
changed	O
.	O
Later	O
the	O
input	O
is	O
then	O
fed	O
to	O
Fully	Method
connected	Method
layer	Method
,	O
to	O
generate	O
the	O
prediction	O
vector	O
depending	O
upon	O
the	O
number	O
of	O
classes	O
we	O
require	O
.	O
Our	O
Experiments	O
showed	O
that	O
addition	O
of	O
residual	O
blocks	O
within	O
pipelines	O
to	O
learn	O
residual	O
mapping	O
rather	O
than	O
underlying	O
mapping	O
did	O
n’t	O
improve	O
model	O
’s	O
performance	O
much	O
due	O
to	O
lesser	O
number	O
of	O
convolutional	O
blocks	O
in	O
pipeline	O
but	O
adding	O
residual	O
connections	O
in	O
case	O
of	O
several	O
layers	O
is	O
bound	O
to	O
help	O
and	O
would	O
also	O
reduce	O
the	O
need	O
for	O
more	O
number	O
of	O
parameters	O
by	O
a	O
greater	O
amount	O
.	O
subsection	O
:	O
Optimizer	O
We	O
tried	O
many	O
optimizers	O
.	O
We	O
initially	O
tried	O
with	O
Adam	Method
,	O
that	O
gave	O
an	O
accuracy	Metric
of	O
74	O
%	O
on	O
Rank	Metric
1	Metric
on	O
Market1501	Material
dataset	Material
,	O
then	O
we	O
introduced	O
weight	Method
decay	Method
that	O
helped	O
us	O
achieve	O
higher	O
accuracy	Metric
.	O
We	O
then	O
tried	O
SGD	Method
with	O
Cyclical	Method
Learning	Method
Rate	Method
scheduler	Method
,	O
we	O
were	O
able	O
to	O
achieve	O
much	O
higher	O
accuracy	Metric
.	O
We	O
saw	O
an	O
increment	O
of	O
more	O
than	O
10	O
%	O
on	O
Rank	Metric
1	Metric
on	O
Market	Material
1501	Material
to	O
reach	O
89.3%.This	O
clearly	O
shows	O
that	O
there	O
were	O
issues	O
with	O
non	Task
convex	Task
optimization	Task
and	O
not	O
enough	O
gradients	O
were	O
being	O
generated	O
to	O
get	O
out	O
out	O
of	O
saddle	O
points	O
.	O
We	O
use	O
the	O
triangular	Method
variant	Method
with	O
default	O
values	O
as	O
suggested	O
.	O
We	O
restricted	O
our	O
batch	O
size	O
to	O
32	O
as	O
it	O
provided	O
the	O
best	O
results	O
.	O
Keeping	O
a	O
higher	O
batch	O
size	O
would	O
lead	O
to	O
less	O
frequent	O
weight	O
updates	O
.	O
Since	O
the	O
learning	Metric
rate	Metric
becomes	O
variable	O
with	O
CLR	Method
,	O
it	O
can	O
take	O
advantage	O
of	O
it	O
’s	O
behaviour	O
of	O
making	O
LR	O
variable	O
wherever	O
necessary	O
in	O
a	O
more	O
effective	O
manner	O
as	O
our	O
experiments	O
have	O
showed	O
.	O
subsection	O
:	O
Using	O
Covariance	Method
loss	Method
for	O
contrastive	Task
feature	Task
learning	Task
We	O
are	O
proposing	O
a	O
new	O
addition	O
to	O
our	O
loss	Method
function	Method
,	O
whose	O
main	O
aim	O
is	O
make	O
positive	O
targets	O
(	O
images	O
of	O
same	O
person	O
,	O
taken	O
with	O
different	O
camera	O
)	O
closer	O
and	O
negative	O
targets	O
(	O
images	O
of	O
different	O
person	O
)	O
far	O
away	O
in	O
embedding	O
space	O
.	O
We	O
take	O
feature	O
maps	O
which	O
we	O
get	O
from	O
second	O
convolution	O
block	O
from	O
both	O
the	O
pipelines	O
during	O
second	O
phase	O
.This	O
is	O
going	O
to	O
optimize	O
embedding	O
space	O
such	O
that	O
data	O
points	O
with	O
same	O
identity	O
are	O
closer	O
to	O
each	O
other	O
than	O
those	O
with	O
different	O
identities	O
.	O
We	O
are	O
required	O
to	O
take	O
feature	O
maps	O
of	O
positive	O
targets	O
and	O
negative	O
targets	O
,	O
we	O
then	O
have	O
to	O
perform	O
the	O
following	O
operation	O
:	O
where	O
P	O
and	O
N	O
denotes	O
positive	O
and	O
negative	O
targets	O
respectively	O
and	O
i	O
corresponding	O
indexes	O
.	O
,	O
and	O
are	O
three	O
hyperparameters	O
.	O
Finding	O
the	O
most	O
optimum	O
value	O
for	O
these	O
hyperparameters	O
is	O
an	O
exhaustive	O
process	O
since	O
we	O
are	O
required	O
to	O
let	O
our	O
model	O
train	O
for	O
considerable	O
amount	O
of	O
epochs	O
.	O
Higher	O
value	O
of	O
introduces	O
fluctuations	O
in	O
overall	O
loss	Metric
function	Metric
since	O
the	O
value	O
changes	O
rapidly	O
and	O
may	O
cause	O
instability	O
.	O
We	O
found	O
that	O
these	O
set	O
of	O
values	O
worked	O
best	O
in	O
our	O
case	O
To	O
perform	O
this	O
type	O
of	O
task	O
we	O
either	O
can	O
keep	O
track	O
of	O
positive	O
targets	O
and	O
negative	O
targets	O
before	O
feeding	O
them	O
to	O
the	O
model	O
or	O
we	O
can	O
create	O
a	O
mask	O
which	O
can	O
indicate	O
which	O
feature	O
maps	O
to	O
choose	O
.	O
We	O
use	O
the	O
second	O
approach	O
.	O
Mask	Method
outputs	O
a	O
consecutive	O
vectors	O
in	O
pair	O
,	O
one	O
for	O
positive	O
targets	O
and	O
the	O
other	O
for	O
negative	O
targets	O
that	O
indicates	O
which	O
feature	O
maps	O
to	O
pick	O
out	O
of	O
multiple	O
maps	O
.	O
Dimension	O
of	O
feature	O
map	O
is	O
.	O
To	O
perform	O
the	O
operation	O
of	O
subtraction	Task
,	O
we	O
flatten	O
the	O
feature	O
map	O
of	O
both	O
positive	O
and	O
negative	O
targets	O
.	O
Then	O
we	O
perform	O
subtraction	Method
,	O
giving	O
us	O
a	O
Tensor	O
of	O
,	O
add	O
another	O
axis	O
which	O
gives	O
us	O
,	O
then	O
require	O
a	O
transpose	O
of	O
this	O
matrix	O
as	O
well	O
,	O
giving	O
us	O
another	O
matrix	O
of	O
dimension	O
.	O
We	O
are	O
required	O
to	O
create	O
co	O
-	O
occurence	O
embedding	O
matrix	O
whose	O
sum	O
of	O
elements	O
is	O
going	O
to	O
give	O
us	O
an	O
indicator	O
to	O
improve	O
model	O
’s	O
predictions	O
.	O
We	O
tried	O
different	O
values	O
of	O
to	O
get	O
the	O
best	O
accuracy	Metric
possible	O
.	O
The	O
value	O
being	O
computed	O
by	O
covariance	Method
loss	Method
introduces	O
fluctuations	O
on	O
overall	Metric
loss	Metric
since	O
cross	Metric
entropy	Metric
reduces	O
approximately	O
monotonically	O
in	O
the	O
initial	O
and	O
mid	O
course	O
of	O
the	O
training	O
,	O
but	O
that	O
’s	O
not	O
the	O
case	O
with	O
covariance	O
loss	O
since	O
weights	O
of	O
feature	O
maps	O
are	O
changing	O
rapidly	O
relatively	O
.	O
So	O
it	O
’s	O
recommended	O
to	O
keep	O
both	O
the	O
loss	O
values	O
in	O
the	O
same	O
range	O
.	O
Therefore	O
the	O
value	O
of	O
plays	O
a	O
great	O
role	O
in	O
determining	O
the	O
overall	O
performance	O
of	O
the	O
model	O
itself	O
.	O
subsection	O
:	O
Training	O
methodology	O
There	O
are	O
few	O
ways	O
to	O
train	O
these	O
pipelines	O
,	O
we	O
divided	O
the	O
training	O
into	O
two	O
phases	O
.	O
In	O
the	O
first	O
phase	O
,	O
our	O
model	O
along	O
with	O
the	O
first	O
pipeline	O
was	O
trained	O
on	O
Market	Material
1501	Material
with	O
the	O
other	O
pipeline	O
kept	O
frozen	O
and	O
predictions	O
being	O
taken	O
from	O
the	O
first	O
pipeline	O
itself	O
.	O
There	O
can	O
also	O
be	O
slight	O
variation	O
after	O
first	O
phase	O
,	O
where	O
in	O
some	O
sections	O
of	O
the	O
model	O
can	O
be	O
kept	O
frozen	O
and	O
other	O
pipelines	O
be	O
trained	O
in	O
a	O
different	O
manner	O
(	O
fully	O
task	O
specific	O
)	O
.	O
In	O
the	O
second	O
phase	O
,	O
we	O
freeze	O
the	O
first	O
pipeline	O
and	O
then	O
train	O
the	O
base	Method
model	Method
along	O
with	O
second	O
pipeline	O
on	O
Duke	Material
MTMC	Material
and	O
take	O
predictions	O
accordingly	O
.	O
Similar	O
procedure	O
can	O
be	O
repeated	O
for	O
n	O
pipelines	O
for	O
n	Task
tasks	Task
as	O
well	O
.	O
subsection	O
:	O
Objective	Metric
Function	Metric
Our	O
loss	Method
function	Method
has	O
two	O
critical	O
components	O
now	O
.	O
We	O
are	O
using	O
cross	O
entropy	O
as	O
our	O
classification	Metric
loss	Metric
along	O
with	O
covariance	Method
loss	Method
.	O
Cross	Metric
Entropy	Metric
is	O
given	O
as	O
:	O
where	O
is	O
the	O
predicted	O
probability	O
value	O
for	O
class	O
i	O
and	O
is	O
the	O
true	O
probability	O
for	O
that	O
class	O
.	O
Our	O
final	O
loss	Metric
is	O
the	O
sum	O
of	O
cross	Metric
entropy	Metric
and	O
covariance	Method
loss	Method
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Datasets	O
We	O
used	O
two	O
datasets	O
for	O
this	O
work	O
.	O
Although	O
other	O
datasets	O
can	O
be	O
used	O
,	O
but	O
as	O
of	O
now	O
these	O
two	O
are	O
more	O
widely	O
used	O
and	O
have	O
the	O
most	O
number	O
of	O
images	O
as	O
compared	O
to	O
other	O
prevalent	O
datasets	O
.	O
Market1501	Method
contains	O
32668	O
images	O
of	O
1501	O
persons	O
split	O
into	O
train	O
/	O
test	O
sets	O
of	O
12	O
,	O
936	O
/	O
19	O
,	O
732	O
.	O
It	O
has	O
bounding	O
boxes	O
from	O
a	O
person	Method
detector	Method
which	O
have	O
been	O
selected	O
based	O
on	O
their	O
intersection	O
-	O
over	O
-	O
union	O
overlap	O
with	O
manually	O
annotated	O
bounding	O
boxes	O
.	O
Duke	Material
MTMC	Material
has	O
16	O
,	O
522	O
training	O
images	O
of	O
702	O
identities	O
,	O
2	O
,	O
228	O
query	O
images	O
of	O
the	O
other	O
702	O
identities	O
and	O
17	O
,	O
661	O
gallery	O
images	O
(	O
702	O
ID	O
+	O
408	O
distractor	O
ID	O
)	O
.	O
Proposed	O
models	O
are	O
bound	O
to	O
perform	O
much	O
better	O
if	O
it	O
’s	O
trained	O
on	O
more	O
data	O
.	O
Considering	O
how	O
deep	O
commonly	O
used	O
models	O
are	O
,	O
these	O
datasets	O
are	O
not	O
large	O
enough	O
to	O
effectively	O
train	O
such	O
number	O
of	O
parameters	O
.	O
So	O
our	O
model	O
is	O
very	O
unlikely	O
to	O
overfit	O
.	O
subsection	O
:	O
Ensembling	O
Ensembling	Method
has	O
often	O
given	O
improved	O
results	O
in	O
various	O
computer	Task
vision	Task
tasks	Task
.	O
This	O
often	O
works	O
really	O
well	O
when	O
predictions	O
are	O
being	O
taken	O
from	O
multiple	O
models	O
.	O
Here	O
we	O
tried	O
ensembling	O
amongst	O
these	O
pipelines	O
.The	O
first	O
phase	O
was	O
performed	O
as	O
usual	O
.	O
The	O
second	O
phase	O
was	O
tried	O
with	O
ensembling	Method
using	O
the	O
mentioned	O
methods	O
.	O
But	O
we	O
decided	O
not	O
to	O
include	O
this	O
in	O
our	O
proposed	O
architecture	O
as	O
the	O
model	O
converged	O
faster	O
relatively	O
and	O
accuracy	Metric
was	O
saturated	O
to	O
a	O
lower	O
max	O
value	O
.	O
Although	O
it	O
may	O
prove	O
to	O
work	O
better	O
if	O
specific	O
set	O
of	O
pipelines	Method
are	O
used	O
to	O
solve	O
a	O
particular	O
task	O
.	O
subsection	O
:	O
Results	O
Since	O
our	O
main	O
goal	O
is	O
bring	O
generalization	Task
into	O
our	O
model	O
and	O
avoid	O
over	O
catastrophic	O
forgetting	O
,	O
we	O
first	O
train	O
the	O
first	O
pipeline	O
,	O
then	O
we	O
evaluated	O
the	O
predictions	O
coming	O
from	O
the	O
last	O
FC	Method
layer	Method
of	O
first	O
pipeline	O
.	O
Then	O
we	O
train	O
the	O
second	O
pipeline	O
,	O
evaluated	O
it	O
.	O
In	O
the	O
last	O
phase	O
,	O
we	O
do	O
n’t	O
do	O
any	O
training	O
and	O
just	O
evaluate	O
it	O
on	O
the	O
first	O
task	O
our	O
model	O
was	O
made	O
to	O
perform	O
.	O
These	O
results	O
are	O
reported	O
after	O
the	O
model	O
has	O
converged	O
.	O
We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	Metric
on	O
both	O
the	O
tasks	O
,	O
and	O
yet	O
achieve	O
considerable	O
accuracy	Metric
on	O
the	O
first	O
task	O
again	O
.	O
section	O
:	O
Effectiveness	O
of	O
proposed	O
method	O
Our	O
work	O
indicates	O
that	O
we	O
now	O
have	O
a	O
simple	O
method	O
that	O
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
when	O
trained	O
on	O
Person	Task
Re	Task
-	Task
Identification	Task
tasks	O
and	O
yet	O
achieve	O
considerable	O
accuracy	Metric
on	O
older	O
tasks	O
without	O
losing	O
much	O
information	O
and	O
does	O
n’t	O
rely	O
on	O
older	O
data	O
after	O
it	O
has	O
been	O
used	O
for	O
training	O
it	O
.	O
This	O
is	O
a	O
big	O
step	O
because	O
very	O
often	O
in	O
real	O
world	O
,	O
we	O
do	O
n’t	O
have	O
access	O
to	O
old	O
data	O
and	O
this	O
would	O
reduce	O
the	O
robustness	O
of	O
our	O
model	O
otherwise	O
.	O
Our	O
architecture	O
and	O
discussed	O
methods	O
can	O
be	O
applied	O
to	O
other	O
computer	Task
vision	Task
tasks	Task
as	O
well	O
.	O
This	O
method	O
is	O
bound	O
to	O
work	O
with	O
tasks	O
which	O
have	O
less	O
variations	O
in	O
domain	O
.	O
For	O
tasks	O
that	O
are	O
similar	O
,	O
it	O
seems	O
to	O
outperform	O
other	O
commonly	O
used	O
methods	O
of	O
training	Task
.	O
section	O
:	O
Conclusion	O
In	O
this	O
paper	O
,	O
we	O
have	O
shown	O
that	O
we	O
can	O
achieve	O
incremental	Task
learning	Task
in	O
Person	Task
ReID	Task
tasks	Task
with	O
simpler	O
methods	O
yet	O
achieving	O
state	O
-	O
of	O
-	O
the	O
art	O
results	O
.	O
We	O
also	O
propose	O
a	O
new	O
loss	Method
that	O
can	O
be	O
used	O
to	O
bring	O
positive	O
targets	O
closer	O
and	O
vice	O
versa	O
in	O
embedding	O
space	O
.	O
We	O
hope	O
that	O
our	O
work	O
would	O
be	O
build	O
upon	O
by	O
ReID	O
community	O
to	O
build	O
more	O
better	O
and	O
robust	O
incremental	Method
learning	Method
systems	Method
that	O
can	O
be	O
further	O
adapted	O
to	O
other	O
domains	O
as	O
well	O
thus	O
increasing	O
real	O
life	O
usage	O
of	O
such	O
systems	O
.	O
bibliography	O
:	O
References	O
