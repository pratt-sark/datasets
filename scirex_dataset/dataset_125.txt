document O
: O
Robust Task
Multilingual Task
Part Task
- Task
of Task
- Task
Speech Task
Tagging Task
via O
Adversarial Method
Training Method
{ O
michihiro.yasunaga O
, O
jungo.kasai O
, O
dragomir.radev O
} O
@yale.edu O
Adversarial Method
training Method
( O
AT Method
) O
is O
a O
powerful O
regularization Method
method Method
for O
neural Method
networks Method
, O
aiming O
to O
achieve O
robustness O
to O
input O
perturbations O
. O
Yet O
, O
the O
specific O
effects O
of O
the O
robustness Metric
obtained O
from O
AT Method
are O
still O
unclear O
in O
the O
context O
of O
natural Task
language Task
processing Task
. O
In O
this O
paper O
, O
we O
propose O
and O
analyze O
a O
neural O
POS Method
tagging O
model O
that O
exploits O
AT Method
. O
In O
our O
experiments O
on O
the O
Penn Material
Treebank Material
WSJ Material
corpus Material
and O
the O
Universal Material
Dependencies Material
( O
UD Material
) O
dataset O
( O
27 O
languages O
) O
, O
we O
find O
that O
AT Method
not O
only O
improves O
the O
overall O
tagging Metric
accuracy Metric
, O
but O
also O
1 O
) O
prevents O
over O
- O
fitting O
well O
in O
low O
resource O
languages O
and O
2 O
) O
boosts O
tagging Metric
accuracy Metric
for O
rare O
/ O
unseen O
words O
. O
We O
also O
demonstrate O
that O
3 O
) O
the O
improved O
tagging Metric
performance O
by O
AT Method
contributes O
to O
the O
downstream Task
task Task
of O
dependency O
parsing Task
, O
and O
that O
4 O
) O
AT Method
helps O
the O
model O
to O
learn O
cleaner Method
word Method
representations Method
. O
5 O
) O
The O
proposed O
AT Method
model O
is O
generally O
effective O
in O
different O
sequence Task
labeling Task
tasks Task
. O
These O
positive O
results O
motivate O
further O
use O
of O
AT Method
for O
natural Task
language Task
tasks Task
. O
section O
: O
Introduction O
Recently O
, O
neural Method
network Method
- Method
based Method
approaches Method
have O
become O
popular O
in O
many O
natural Task
language Task
processing Task
( O
NLP Task
) O
tasks O
including O
tagging Task
, O
parsing Task
, O
and O
translation Task
. O
However O
, O
it O
has O
been O
shown O
that O
neural Method
networks Method
tend O
to O
be O
locally O
unstable O
and O
even O
tiny O
perturbations O
to O
the O
original O
inputs O
can O
mislead O
the O
models O
. O
Such O
maliciously O
perturbed O
inputs O
are O
called O
adversarial O
examples O
. O
Adversarial Method
training Method
aims O
to O
improve O
the O
robustness O
of O
a O
model O
to O
input O
perturbations O
by O
training O
on O
both O
unmodified O
examples O
and O
adversarial O
examples O
. O
Previous O
work O
on O
image Task
recognition Task
has O
demonstrated O
the O
enhanced O
robustness Metric
of O
their O
models O
to O
unseen O
images O
via O
adversarial Method
training Method
and O
has O
provided O
theoretical O
explanations O
of O
the O
regularization O
effects O
. O
Despite O
its O
potential O
as O
a O
powerful O
regularizer Method
, O
adversarial Method
training Method
( O
AT Method
) O
has O
yet O
to O
be O
explored O
extensively O
in O
natural Task
language Task
tasks Task
. O
Recently O
, O
miyato2017adv O
applied O
AT Method
on O
text Task
classification Task
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
accuracy Metric
. O
Yet O
, O
the O
specific O
effects O
of O
the O
robustness Metric
obtained O
from O
AT Method
are O
still O
unclear O
in O
the O
context O
of O
NLP Task
. O
For O
example O
, O
research O
studies O
have O
yet O
to O
answer O
questions O
such O
as O
1 O
) O
how O
can O
we O
interpret O
perturbations O
or O
robustness Metric
on O
natural O
language O
inputs O
? O
2 O
) O
how O
are O
they O
related O
to O
linguistic O
factors O
like O
vocabulary O
statistics O
? O
3 O
) O
are O
the O
effects O
of O
AT Method
language O
- O
dependent O
? O
Answering O
such O
questions O
is O
crucial O
to O
understand O
and O
motivate O
the O
application O
of O
adversarial Method
training Method
on O
natural Task
language Task
tasks Task
. O
In O
this O
paper O
, O
spotlighting O
a O
well O
- O
studied O
core O
problem O
of O
NLP Task
, O
we O
propose O
and O
carefully O
analyze O
a O
neural O
part Method
- Method
of Method
- Method
speech Method
( O
POS Method
) O
tagging O
model O
that O
exploits O
adversarial Method
training Method
. O
With O
a O
BiLSTM Method
- O
CRF Method
model O
as O
our O
baseline Method
POS Method
tagger O
, O
we O
apply O
adversarial Method
training Method
by O
considering O
perturbations O
to O
input O
word O
/ O
character O
embeddings O
. O
In O
order O
to O
demystify O
the O
effects O
of O
adversarial Method
training Method
in O
the O
context O
of O
NLP Task
, O
we O
conduct O
POS Task
tagging Task
experiments O
on O
multiple O
languages O
using O
the O
Penn Material
Treebank Material
WSJ Material
corpus Material
( O
Englsih O
) O
and O
the O
Universal Material
Dependencies Material
dataset O
( O
27 O
languages O
) O
, O
with O
thorough O
analyses O
of O
the O
following O
points O
: O
Effects O
on O
different O
target O
languages O
Vocabulary O
statistics O
and O
tagging Metric
accuracy Metric
Influence O
on O
downstream Task
tasks Task
Representation Task
learning Task
of Task
words Task
In O
our O
experiments O
, O
we O
find O
that O
our O
adversarial Method
training Method
model Method
consistently O
outperforms O
the O
baseline Method
POS Method
tagger O
, O
and O
even O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
22 O
languages O
. O
Furthermore O
, O
our O
analyses O
reveal O
the O
following O
insights O
into O
adversarial Method
training Method
in O
the O
context O
of O
NLP Task
: O
The O
regularization O
effects O
of O
adversarial Method
training Method
( O
AT Method
) O
are O
general O
across O
different O
languages O
. O
AT Method
can O
prevent O
overfitting O
especially O
well O
when O
training O
examples O
are O
scarce O
, O
providing O
an O
effective O
tool O
to O
process O
low O
resource O
languages O
. O
AT Method
can O
boost O
the O
tagging Task
performance O
for O
rare O
/ O
unseen O
words O
and O
increase O
the O
sentence Metric
- Metric
level Metric
accuracy Metric
. O
This O
positively O
affects O
the O
performance O
of O
down Task
- Task
stream Task
tasks Task
such O
as O
dependency O
parsing Task
, O
where O
low O
sentence O
- O
level O
POS Method
accuracy O
can O
be O
a O
bottleneck O
. O
AT Method
helps O
the O
network O
learn O
cleaner O
word O
embeddings O
, O
showing O
stronger O
correlations O
with O
their O
POS Method
tags O
. O
We O
argue O
that O
the O
effects O
of O
AT Method
can O
be O
interpreted O
from O
the O
perspective O
of O
natural O
language O
. O
Finally O
, O
we O
demonstrate O
that O
the O
proposed O
AT Method
model O
is O
generally O
effective O
across O
different O
sequence Task
labeling Task
tasks Task
. O
This O
work O
therefore O
provides O
a O
strong O
motivation O
and O
basis O
for O
utilizing O
adversarial Method
training Method
in O
NLP Task
tasks O
. O
section O
: O
Related O
Work O
subsection O
: O
POS Method
Tagging O
Part Task
- Task
of Task
- Task
speech Task
( O
POS Method
) O
tagging O
is O
a O
fundamental O
NLP Task
task O
that O
facilitates O
downstream Task
tasks Task
such O
as O
syntactic O
parsing Task
. O
While O
current O
state O
- O
of O
- O
the O
- O
art O
POS Method
taggers O
yield O
accuracy Metric
over O
97.5 O
% O
on O
PTB Material
- Material
WSJ Material
, O
there O
still O
remain O
issues O
. O
The O
per Metric
token Metric
accuracy Metric
metric Metric
is O
easy O
since O
taggers Method
can O
easily O
assign O
correct O
POS Method
tags O
to O
highly O
unambiguous O
tokens O
, O
such O
as O
punctuation O
. O
Sentence Metric
- Metric
level Metric
accuracy Metric
serves O
as O
a O
more O
realistic O
metric O
for O
POS Method
taggers O
but O
it O
still O
remains O
low O
. O
Another O
problem O
with O
current O
POS Method
taggers O
is O
that O
their O
accuracy Metric
deteriorates O
drastically O
on O
low O
resource O
languages O
and O
rare O
words O
. O
In O
this O
work O
, O
we O
demonstrate O
that O
adversarial Method
training Method
( O
AT Method
) O
can O
mitigate O
these O
issues O
. O
It O
is O
empirically O
shown O
that O
POS Task
tagging Task
performance O
can O
greatly O
affect O
downstream Task
tasks Task
such O
as O
dependency O
parsing Task
. O
In O
this O
work O
, O
we O
also O
demonstrate O
that O
the O
improvements O
obtained O
from O
our O
AT Method
POS Method
tagger O
actually O
contribute O
to O
dependency O
parsing Task
. O
Nonetheless O
, O
parsing Task
with O
gold O
POS Method
tags O
still O
yields O
better O
results O
, O
bolstering O
the O
view O
that O
POS Task
tagging Task
is O
an O
essential O
task O
in O
NLP Task
that O
needs O
further O
development O
. O
subsection O
: O
Adversarial Method
Training Method
The O
concept O
of O
adversarial Method
training Method
was O
originally O
introduced O
in O
the O
context O
of O
image Task
classification Task
to O
improve O
the O
robustness Metric
of O
a O
model O
by O
training O
on O
input O
images O
with O
malicious O
perturbations O
. O
Previous O
work O
has O
provided O
a O
theoretical O
framework O
to O
understand O
adversarial O
examples O
and O
the O
regularization O
effects O
of O
adversarial Method
training Method
( O
AT Method
) O
in O
image Task
recognition Task
. O
Recently O
, O
miyato2017adv O
applied O
AT Method
to O
a O
natural Task
language Task
task Task
( O
text Task
classification Task
) O
by O
extending O
the O
concept O
of O
adversarial Method
perturbations Method
to O
word O
embeddings O
. O
Wu2017adv O
further O
explored O
the O
possibility O
of O
AT Method
in O
relation Task
extraction Task
. O
Both O
report O
improved O
performance O
on O
their O
tasks O
via O
AT Method
, O
but O
the O
specific O
effects O
of O
AT Method
have O
yet O
to O
be O
analyzed O
. O
In O
our O
work O
, O
we O
aim O
to O
address O
this O
issue O
by O
providing O
detailed O
analyses O
on O
the O
effects O
of O
AT Method
from O
the O
perspective O
of O
NLP Task
, O
such O
as O
different O
languages O
, O
vocabulary O
statistics O
, O
word O
embedding O
distribution O
, O
and O
aim O
to O
motivate O
future O
research O
that O
exploits O
AT Method
in O
NLP Task
tasks O
. O
AT Method
is O
related O
to O
other O
regularization Method
methods Method
that O
add O
noise O
to O
data O
such O
as O
dropout Method
and O
its O
variant O
for O
NLP Task
tasks O
, O
word Task
dropout Task
. O
xie17_data_noising O
discuss O
various O
data Method
noising Method
techniques Method
for O
language Task
modeling Task
. O
While O
these O
methods O
produce O
random O
noise O
, O
AT Method
generates O
perturbations O
that O
the O
current O
model O
is O
particularly O
vulnerable O
to O
, O
and O
thus O
is O
claimed O
to O
be O
effective O
. O
It O
should O
be O
noted O
that O
while O
related O
in O
name O
, O
adversarial Method
training Method
( O
AT Method
) O
differs O
from O
Generative Method
Adversarial Method
Networks Method
( O
GANs Method
) O
. O
GANs Method
have O
already O
been O
applied O
to O
NLP Task
tasks O
such O
as O
dialogue Task
generation Task
and O
transfer Task
learning Task
. O
Adversarial Method
training Method
also O
differs O
from O
adversarial Method
evaluation Method
, O
recently O
proposed O
for O
reading Task
comprehension Task
tasks Task
. O
section O
: O
Method O
In O
this O
section O
, O
we O
introduce O
our O
baseline Method
POS Method
tagging O
model O
and O
explain O
how O
we O
implement O
adversarial Method
training Method
on O
top O
. O
subsection O
: O
Baseline O
POS Method
Tagging O
Model O
Following O
the O
recent O
top O
- O
performing O
models O
for O
sequence Task
labeling Task
tasks Task
, O
we O
employ O
a O
Bi Method
- Method
directional Method
LSTM Method
- O
CRF Method
model O
as O
our O
baseline Method
( O
see O
Figure O
[ O
reference O
] O
for O
an O
illustration O
) O
. O
paragraph O
: O
Character O
- O
level O
BiLSTM Method
. O
Prior O
work O
has O
shown O
that O
incorporating O
character Method
- Method
level Method
representations Method
of Method
words Method
can O
boost O
POS Metric
tagging Metric
accuracy Metric
by O
capturing O
morphological O
information O
present O
in O
each O
language O
. O
Major O
neural Method
character Method
- Method
level Method
models Method
include O
the O
character Method
- Method
level Method
CNN Method
and O
( Method
Bi Method
) Method
LSTM Method
. O
A O
Bi Method
- Method
directional Method
LSTM Method
( O
BiLSTM Method
) O
processes O
each O
sequence O
both O
forward O
and O
backward O
to O
capture O
sequential O
information O
, O
while O
preventing O
the O
vanishing Task
/ Task
exploding Task
gradient Task
problem Task
. O
We O
observed O
that O
the O
character O
- O
level O
BiLSTM Method
outperformed O
the O
CNN Method
by O
0.1 O
% O
on O
the O
PTB Material
- Material
WSJ Material
development Material
set Material
, O
and O
hence O
in O
all O
of O
our O
experiments O
we O
use O
the O
character O
- O
level O
BiLSTM Method
. O
Specifically O
, O
we O
generate O
a O
character Method
- Method
level Method
representation Method
for O
each O
word O
by O
feeding O
its O
character O
embeddings O
into O
the O
BiLSTM Method
and O
obtaining O
the O
concatenated O
final O
states O
. O
paragraph O
: O
Word O
- O
level O
BiLSTM Method
. O
Each O
word O
in O
a O
sentence O
is O
represented O
by O
concatenating O
its O
word Method
embedding Method
and O
its O
character Method
- Method
level Method
representation Method
. O
They O
are O
fed O
into O
another O
level O
of O
BiLSTM Method
( O
word O
- O
level O
BiLSTM Method
) O
to O
process O
the O
entire O
sentence O
. O
paragraph O
: O
CRF Method
. O
In O
sequence Task
labeling Task
tasks Task
it O
is O
beneficial O
to O
consider O
the O
correlations O
between O
neighboring O
labels O
and O
jointly O
decode O
the O
best O
chain O
of O
labels O
for O
a O
given O
sentence O
. O
With O
this O
motivation O
, O
we O
apply O
a O
conditional Method
random Method
field Method
( O
CRF Method
) O
on O
top O
of O
the O
word O
- O
level O
BiLSTM Method
to O
perform O
POS Method
tag O
inference O
with O
global Task
normalization Task
, O
addressing O
the O
“ O
label Task
bias Task
” Task
problem Task
. O
Specifically O
, O
given O
an O
input O
sentence O
, O
we O
pass O
the O
output O
sequence O
of O
the O
word O
- O
level O
BiLSTM Method
to O
a O
first O
- O
order O
chain O
CRF Method
to O
compute O
the O
conditional O
probability O
of O
the O
target O
label O
sequence O
: O
where O
represents O
all O
of O
the O
model O
parameters O
( O
in O
the O
BiLSTMs Method
and O
CRF Method
) O
, O
and O
denote O
the O
input O
embeddings O
and O
the O
target O
POS Method
tag O
sequence O
, O
respectively O
, O
for O
the O
given O
sentence O
. O
For O
training Task
, O
we O
minimize O
the O
negative Metric
log Metric
- Metric
likelihood Metric
( O
loss Metric
function Metric
) O
with O
respect O
to O
the O
model O
parameters O
. O
Decoding Task
searches O
for O
the O
POS Method
tag O
sequence O
with O
the O
highest O
conditional O
probability O
using O
the O
Viterbi Method
algorithm Method
. O
For O
more O
detail O
about O
the O
BiLSTM Method
- O
CRF Method
formulation O
, O
refer O
to O
ma O
- O
hovy:2016:P16 O
- O
1 O
. O
subsection O
: O
Adversarial Method
Training Method
Adversarial Method
training Method
is O
a O
powerful O
regularization Method
method Method
, O
primarily O
explored O
in O
image Task
recognition Task
to O
improve O
the O
robustness Metric
of O
classifiers Method
to O
input O
perturbations O
. O
Given O
a O
classifier Method
, O
we O
first O
generate O
input O
examples O
that O
are O
very O
close O
to O
original O
inputs O
( O
so O
should O
yield O
the O
same O
labels O
) O
yet O
are O
likely O
to O
be O
misclassified O
by O
the O
current O
model O
. O
Specifically O
, O
these O
adversarial O
examples O
are O
generated O
by O
adding O
small O
perturbations O
to O
the O
inputs O
in O
the O
direction O
that O
significantly O
increases O
the O
loss Metric
function Metric
of O
the O
classifier Method
( O
worst O
- O
case O
perturbations O
) O
. O
Then O
, O
the O
classifier Method
is O
trained O
on O
the O
mixture O
of O
clean O
examples O
and O
adversarial O
examples O
to O
improve O
the O
stability O
to O
input O
perturbations O
. O
In O
this O
work O
, O
we O
incorporate O
adversarial Method
training Method
into O
our O
baseline Method
POS Method
tagger O
, O
aiming O
to O
achieve O
better O
regularization O
effects O
and O
to O
provide O
their O
interpretations O
in O
the O
context O
of O
NLP Task
. O
paragraph O
: O
Generating O
adversarial O
examples O
. O
Adversarial Method
training Method
( O
AT Method
) O
considers O
continuous O
perturbations O
to O
inputs O
, O
so O
we O
define O
perturbations O
at O
the O
level O
of O
dense O
word O
/ O
character O
embeddings O
rather O
than O
one Method
- Method
hot Method
vector Method
representations Method
, O
similarly O
to O
miyato2017adv O
. O
Specifically O
, O
given O
an O
input O
sentence O
, O
we O
consider O
the O
concatenation O
of O
all O
the O
word O
/ O
character O
embeddings O
in O
the O
sentence O
: O
. O
To O
prepare O
an O
adversarial O
example O
, O
we O
aim O
to O
generate O
the O
worst O
- O
case O
perturbation O
of O
a O
small O
bounded O
norm O
that O
maximizes O
the O
loss Metric
function Metric
of O
the O
current O
model O
: O
where O
is O
the O
current O
value O
of O
the O
model O
parameters O
, O
treated O
as O
a O
constant O
, O
and O
y O
denotes O
the O
target O
labels O
. O
Since O
the O
exact O
computation O
of O
such O
η O
is O
intractable O
in O
complex Task
neural Task
networks Task
, O
we O
employ O
the O
Fast Method
Gradient Method
Method Method
i.e. O
first Method
order Method
approximation Method
to O
obtain O
an O
approximate O
worst Metric
- Metric
case Metric
perturbation Metric
of Metric
norm Metric
, O
by O
a O
single O
gradient Method
computation Method
: O
is O
a O
hyperparameter O
to O
be O
determined O
in O
the O
development O
dataset O
. O
Note O
that O
the O
perturbation O
η O
is O
generated O
in O
the O
direction O
that O
significantly O
increases O
the O
loss O
. O
We O
find O
such O
η O
against O
the O
current O
model O
parameterized O
by O
, O
at O
each O
training O
step O
, O
and O
construct O
an O
adversarial O
example O
by O
However O
, O
if O
we O
do O
not O
restrict O
the O
norm O
of O
word O
/ O
character O
embeddings O
, O
the O
model O
could O
trivially O
learn O
embeddings O
of O
large O
norms O
to O
make O
the O
perturbations O
insignificant O
. O
To O
prevent O
this O
issue O
, O
we O
normalize O
word O
/ O
character O
embeddings O
so O
that O
they O
have O
mean O
0 O
and O
variance O
1 O
for O
every O
entry O
, O
as O
in O
miyato2017adv O
. O
The O
normalization Task
is O
performed O
every O
time O
we O
feed O
input O
embeddings O
into O
the O
LSTMs Method
and O
generate O
adversarial O
examples O
. O
To O
ensure O
a O
fair O
comparison O
, O
we O
also O
normalize O
input O
embeddings O
in O
our O
baseline Method
model O
. O
While O
miyato2017adv O
set O
the O
norm O
of O
a O
perturbation O
( O
Eq O
[ O
reference O
] O
) O
to O
be O
a O
fixed O
value O
for O
all O
input O
sentences O
, O
to O
generate O
adversarial O
examples O
for O
an O
entire O
sentence O
of O
a O
variable O
length O
and O
to O
include O
character O
embeddings O
besides O
word O
embeddings O
, O
we O
make O
the O
perturbation O
size O
adaptive O
to O
the O
dimension O
of O
the O
concatenated O
input O
embedding O
. O
We O
set O
to O
be O
( O
i.e. O
, O
proportional O
to O
D O
) O
, O
as O
the O
expected O
squared O
norm O
of O
s O
after O
the O
embedding Method
normalization Method
is O
. O
The O
scaling O
factor O
is O
selected O
from O
0.001 O
, O
0.005 O
, O
0.01 O
, O
0.05 O
, O
0.1 O
based O
on O
the O
development O
performance O
in O
each O
treebank O
. O
We O
used O
0.01 O
for O
PTB Material
- Material
WSJ Material
and O
UD Material
- O
Spanish O
, O
and O
0.05 O
for O
the O
rest O
. O
Note O
that O
would O
generate O
no O
noise O
( O
identical O
to O
the O
baseline Method
) O
; O
if O
, O
the O
generated O
adversarial Method
perturbation Method
would O
have O
a O
norm O
comparable O
to O
the O
original O
embedding O
, O
which O
could O
change O
the O
semantics O
of O
the O
input O
sentence O
. O
Hence O
, O
the O
optimal O
perturbation O
scale O
should O
lie O
in O
between O
and O
be O
small O
enough O
to O
preserve O
the O
semantics O
of O
the O
original O
input O
. O
paragraph O
: O
Adversarial Method
training Method
. O
At O
each O
training O
step O
, O
we O
generate O
adversarial O
examples O
against O
the O
current O
model O
, O
and O
train O
on O
the O
mixture O
of O
clean O
examples O
and O
adversarial O
examples O
to O
achieve O
robustness O
to O
input O
perturbations O
. O
To O
this O
end O
, O
we O
define O
the O
loss Method
function Method
for O
adversarial Method
training Method
as O
: O
where O
, O
represent O
the O
loss O
from O
a O
clean O
example O
and O
the O
loss O
from O
its O
adversarial O
example O
, O
respectively O
, O
and O
determines O
the O
weighting O
between O
them O
. O
We O
used O
in O
all O
our O
experiments O
. O
This O
objective O
function O
can O
be O
optimized O
with O
respect O
to O
the O
model O
parameters O
θ O
, O
in O
the O
same O
manner O
as O
the O
baseline Method
model O
. O
section O
: O
Experiments O
To O
fully O
analyze O
the O
effects O
of O
adversarial Method
training Method
, O
we O
train O
and O
evaluate O
our O
baseline Method
/ O
adversarial Method
POS Method
tagging Method
models Method
on O
both O
a O
standard O
English O
dataset O
and O
a O
multilingual O
dataset O
. O
subsection O
: O
Datasets O
As O
a O
standard O
English O
dataset O
, O
we O
use O
the O
Wall O
Street O
Journal O
( O
WSJ O
) O
portion O
of O
the O
Penn Material
Treebank Material
( O
PTB Material
) O
, O
containing O
45 O
different O
POS Method
tags O
. O
We O
adopt O
the O
standard O
split O
: O
sections O
0 O
- O
18 O
for O
training O
, O
19 O
- O
21 O
for O
development Task
and O
22 O
- O
24 O
for O
testing O
. O
For O
multilingual O
POS Method
tagging O
experiments O
, O
to O
compare O
with O
prior O
work O
, O
we O
use O
treebanks O
from O
Universal Material
Dependencies Material
( O
UD Material
) O
v1.2 O
( O
17 O
POS Method
) O
with O
the O
given O
data O
splits O
. O
We O
experiment O
on O
languages O
for O
which O
pre O
- O
trained O
Polyglot Method
word Method
embeddings Method
are O
available O
, O
resulting O
in O
27 O
languages O
listed O
in O
Table O
[ O
reference O
] O
. O
We O
regard O
languages O
with O
less O
than O
60k O
tokens O
of O
training O
data O
as O
low O
- O
resource O
( O
Table O
[ O
reference O
] O
, O
bottom O
) O
, O
as O
in O
plank2016multilingual O
. O
subsection O
: O
Training O
& O
Evaluation Metric
Details O
paragraph O
: O
Model O
settings O
. O
We O
initialize O
word O
embeddings O
with O
100 O
- O
dimensional O
GloVe O
for O
English O
, O
and O
with O
64 Method
- Method
dimensional Method
Polyglot Method
for O
other O
languages O
. O
We O
use O
30 O
- O
dimensional O
character O
embeddings O
, O
and O
set O
the O
state O
sizes O
of O
character O
/ O
word O
- O
level O
BiLSTM Method
to O
be O
50 O
, O
200 O
for O
English O
, O
50 O
, O
100 O
for O
low O
resource O
languages O
, O
and O
50 O
, O
150 O
for O
other O
languages O
. O
The O
model O
parameters O
and O
character O
embeddings O
are O
randomly O
initialized O
, O
as O
in O
ma O
- O
hovy:2016:P16 O
- O
1 O
. O
We O
apply O
dropout Method
to O
input O
embeddings O
and O
BiLSTM Method
outputs O
for O
both O
baseline Method
and O
adversarial Method
training Method
, O
with O
dropout Metric
rate Metric
0.5 Metric
. O
paragraph O
: O
Optimization Task
. O
We O
train O
the O
model O
parameters O
and O
word O
/ O
character O
embeddings O
by O
the O
mini Method
- Method
batch Method
stochastic Method
gradient Method
descent Method
( O
SGD Method
) O
with O
batch O
size O
10 O
, O
momentum O
0.9 O
, O
initial O
learning Metric
rate Metric
0.01 O
and O
decay Metric
rate Metric
0.05 Metric
. O
We O
also O
use O
a O
gradient Method
clipping Method
of Method
5.0 Method
. O
The O
models O
are O
trained O
with O
early O
stopping O
based O
on O
the O
development O
performance O
. O
paragraph O
: O
Evaluation O
. O
We O
evaluate O
per Metric
token Metric
tagging Metric
accuracy Metric
on O
test O
sets O
. O
We O
repeat O
the O
experiment O
three O
times O
and O
report O
the O
statistical Metric
significance Metric
. O
subsection O
: O
Results O
paragraph O
: O
PTB Material
- Material
WSJ Material
dataset Material
. O
Table O
[ O
reference O
] O
shows O
the O
POS Task
tagging Task
results O
. O
As O
expected O
, O
our O
baseline Method
( O
BiLSTM Method
- O
CRF Method
) O
model O
( O
accuracy Metric
97.54 O
% O
) O
performs O
on O
par O
with O
other O
state O
- O
of O
- O
the O
- O
art O
systems O
. O
Built O
upon O
this O
baseline Method
, O
our O
adversarial Method
training Method
( O
AT Method
) O
model O
reaches O
accuracy Metric
97.58 O
% O
thanks O
to O
its O
regularization O
power O
, O
outperforming O
recent O
POS Method
taggers O
except O
wang:2015 O
. O
The O
improvement O
over O
the O
baseline Method
is O
statistically O
significant O
, O
with O
- O
value O
0.05 O
on O
the O
- O
test O
. O
We O
provide O
additional O
analysis O
on O
this O
result O
in O
later O
sections O
. O
paragraph O
: O
Multilingual O
dataset O
( O
UD Material
) O
. O
Experimental O
results O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
Our O
AT Method
model O
shows O
clear O
advantages O
over O
the O
baseline Method
in O
all O
of O
the O
27 O
languages O
( O
average O
improvement O
0.25 O
% O
; O
see O
the O
two O
shaded O
columns O
) O
. O
Considering O
that O
our O
baseline Method
( O
BiLSTM Method
- O
CRF Method
) O
is O
already O
a O
top O
performing O
model O
for O
POS Task
tagging Task
, O
these O
improvements O
made O
by O
AT Method
are O
substantial O
. O
The O
improvements O
are O
also O
statistically O
significant O
for O
all O
the O
languages O
, O
with O
- O
value O
0.05 O
on O
the O
- O
test O
, O
suggesting O
that O
the O
regularization Method
by O
AT Method
is O
generally O
effective O
across O
different O
languages O
. O
Moreover O
, O
our O
AT Method
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
on O
nearly O
all O
of O
the O
languages O
, O
except O
the O
five O
where O
plank2016multilingual O
’s O
multi Task
- Task
task Task
BiLSTM Task
yielded O
better O
results O
. O
Among O
the O
five O
, O
most O
languages O
are O
morphologically O
rich O
( O
) O
. O
We O
suspect O
that O
their O
joint O
training O
of O
word Task
rarity Task
may O
be O
of O
particular O
help O
in O
processing O
morphologically O
complex O
words O
. O
English O
( O
WSJ O
) O
Word O
Frequency O
French O
( O
UD Material
) O
Word O
Frequency O
Additionally O
, O
we O
see O
that O
our O
AT Method
model O
achieves O
notably O
large O
improvements O
over O
the O
baseline Method
in O
resource O
- O
poor O
languages O
( O
the O
bottom O
of O
Table O
[ O
reference O
] O
) O
, O
with O
average O
improvement O
0.35 O
% O
, O
as O
compared O
to O
that O
for O
resource O
- O
rich O
languages O
, O
0.20 O
% O
. O
To O
further O
visualize O
the O
regularization O
effects O
, O
we O
present O
the O
learning O
curves O
for O
three O
representative O
languages O
, O
English O
( O
WSJ O
) O
, O
French O
( O
UD Material
- O
fr O
) O
and O
Romanian O
( O
UD Material
- O
ro O
, O
low O
- O
resource O
) O
, O
based O
on O
the O
development Metric
loss Metric
( O
see O
Figure O
[ O
reference O
] O
) O
. O
For O
all O
the O
three O
languages O
, O
we O
can O
observe O
that O
the O
AT Method
model O
( O
red O
solid O
line O
) O
prevents O
overfitting O
better O
than O
the O
baseline Method
( O
black O
dotted O
line O
) O
, O
and O
this O
advantage O
is O
more O
significant O
in O
low O
resource O
languages O
. O
For O
example O
, O
in O
Romanian O
, O
the O
baseline Method
model O
starts O
to O
increase O
development O
loss O
after O
1 O
, O
000 O
iterations O
even O
with O
dropout Method
, O
whereas O
the O
AT Method
model O
keeps O
improving O
until O
2 O
, O
500 O
iterations O
, O
achieving O
notably O
lower O
development Metric
loss Metric
( O
0.4 O
down O
) O
. O
These O
results O
illustrate O
that O
AT Method
can O
prevent O
overfitting O
especially O
well O
on O
small O
datasets O
and O
can O
augment O
the O
regularization O
power O
beyond O
dropout Method
. O
AT Method
can O
also O
be O
viewed O
as O
an O
effective O
means O
of O
data Task
augmentation Task
, O
where O
we O
generate O
and O
train O
with O
new O
examples O
the O
current O
model O
is O
particularly O
vulnerable O
to O
at O
every O
time O
step O
, O
enhancing O
the O
robustness O
of O
the O
model O
. O
AT Method
can O
therefore O
be O
a O
promising O
tool O
to O
process O
low O
resource O
languages O
. O
section O
: O
Analysis O
English O
( O
WSJ O
) O
Word O
Frequency O
French O
( O
UD Material
) O
Word O
Frequency O
In O
the O
previous O
sections O
, O
we O
demonstrated O
the O
regularization Method
power Method
of O
adversarial Method
training Method
( O
AT Method
) O
on O
different O
languages O
, O
based O
on O
the O
overall O
POS Method
tagging O
performance O
and O
learning Metric
curves Metric
. O
In O
this O
section O
, O
we O
conduct O
further O
analyses O
on O
the O
robustness O
of O
AT Method
from O
NLP Task
specific O
aspects O
such O
as O
word Task
statistics Task
, O
sequence Method
modeling Method
, O
downstream Task
tasks Task
, O
and O
word Method
representation Method
learning Method
. O
We O
find O
that O
AT Method
can O
boost O
tagging Metric
accuracy Metric
on O
rare O
words O
and O
neighbors O
of O
unseen O
words O
( O
§ O
[ O
reference O
] O
) O
. O
Furthermore O
, O
this O
robustness O
against O
rare O
/ O
unseen O
words O
leads O
to O
better O
sentence Metric
- Metric
level Metric
accuracy Metric
and O
downstream O
dependency O
parsing Task
( O
§ O
[ O
reference O
] O
) O
. O
We O
illustrate O
these O
findings O
using O
two O
major O
languages O
, O
English O
( O
WSJ O
) O
and O
French O
( O
UD Material
) O
, O
which O
have O
substantially O
large O
training O
and O
testing O
data O
to O
discuss O
vocabulary O
statistics O
and O
sentence Metric
- Metric
level Metric
performance Metric
. O
Finally O
, O
we O
study O
the O
effects O
of O
AT Method
on O
word Task
representation Task
learning Task
( O
§ O
[ O
reference O
] O
) O
, O
and O
the O
applicability O
of O
AT Method
to O
different O
sequential Task
tasks Task
( O
§ O
[ O
reference O
] O
) O
. O
subsection O
: O
Word Method
- Method
level Method
Analysis Method
Poor O
tagging Metric
accuracy Metric
on O
rare O
/ O
unseen O
words O
is O
one O
of O
the O
bottlenecks O
in O
current O
POS Method
taggers O
. O
Aiming O
to O
reveal O
the O
effects O
of O
AT Method
on O
rare O
/ O
unseen O
words O
, O
we O
analyze O
tagging Task
performance O
at O
the O
word O
level O
, O
considering O
vocabulary O
statistics O
. O
paragraph O
: O
Word O
frequency O
. O
To O
define O
rare O
/ O
unseen O
words O
, O
we O
consider O
each O
word O
’s O
frequency O
of O
occurrence O
in O
the O
training O
set O
. O
We O
categorize O
all O
words O
in O
the O
test O
set O
based O
on O
this O
frequency O
and O
study O
the O
test O
tagging Metric
accuracy Metric
for O
each O
group O
( O
see O
Table O
[ O
reference O
] O
) O
. O
In O
both O
languages O
, O
the O
AT Method
model O
achieves O
large O
improvements O
over O
the O
baseline Method
on O
rare O
words O
( O
e.g. O
, O
frequency O
1 O
- O
10 O
in O
training O
) O
, O
as O
opposed O
to O
more O
frequent O
words O
. O
This O
result O
again O
corroborates O
the O
data Metric
augmentation Metric
power Metric
of O
AT Method
under O
small O
training O
examples O
. O
On O
the O
other O
hand O
, O
we O
did O
not O
observe O
meaningful O
improvements O
on O
unseen O
words O
( O
frequency O
0 O
in O
training O
) O
. O
A O
possible O
explanation O
is O
that O
AT Method
can O
facilitate O
the O
learning O
of O
words O
with O
at O
least O
a O
few O
occurrences O
in O
training O
( O
rare O
words O
) O
, O
but O
is O
not O
particularly O
effective O
in O
inferring O
the O
POS Method
tags O
of O
words O
for O
which O
no O
training O
examples O
are O
given O
( O
unseen O
words O
) O
. O
paragraph O
: O
Neighboring O
words O
. O
One O
important O
characteristic O
of O
natural Task
language Task
tasks Task
is O
the O
sequential O
nature O
of O
inputs O
( O
i.e. O
, O
sequence O
of O
words O
) O
, O
where O
each O
word O
influences O
the O
function O
of O
its O
neighboring O
words O
. O
Since O
our O
model O
uses O
BiLSTM Method
- O
CRF Method
for O
that O
reason O
, O
we O
also O
study O
the O
tagging Task
performance O
on O
the O
neighbors O
of O
rare O
/ O
unseen O
words O
, O
and O
analyze O
the O
effects O
of O
AT Method
with O
the O
sequence Method
model Method
in O
mind O
. O
In O
Table O
[ O
reference O
] O
, O
we O
cluster O
all O
words O
in O
the O
test O
set O
based O
on O
their O
frequency O
in O
training O
again O
, O
and O
consider O
the O
tagging Metric
accuracy Metric
on O
the O
neighbors O
( O
left O
and O
right O
) O
of O
these O
words O
in O
the O
test O
text O
. O
We O
observe O
that O
AT Method
tends O
to O
achieve O
large O
improvements O
over O
the O
baseline Method
on O
the O
neighbors O
of O
unseen O
words O
( O
training O
frequency O
0 O
) O
, O
while O
the O
improvements O
on O
the O
neighbors O
of O
more O
frequent O
words O
remain O
moderate O
. O
Our O
AT Method
model O
thus O
exhibits O
strong O
stability O
to O
uncertain O
neighbors O
, O
as O
compared O
to O
the O
baseline Method
. O
We O
suspect O
that O
because O
we O
generate O
adversarial O
examples O
against O
entire O
input O
sentences O
, O
training O
with O
adversarial O
examples O
makes O
the O
model O
more O
robust O
not O
only O
to O
perturbations O
in O
each O
word O
but O
also O
to O
perturbations O
in O
its O
neighboring O
words O
, O
leading O
to O
greater O
stability O
to O
uncertain O
neighbors O
. O
subsection O
: O
Sentence O
- O
level O
& O
Downstream Task
Analysis Task
English O
( O
WSJ O
) O
Sentence O
- O
Stanford Method
Parser Method
Parsey Metric
McParseface Metric
level Metric
Acc Metric
. O
( O
w O
/ O
gold O
tags O
) O
French O
( O
UD Material
) O
Sentence O
- O
Parsey O
Universal O
level O
Acc O
. O
( O
w O
/ O
gold O
tags O
) O
In O
the O
word Task
- Task
level Task
analysis Task
, O
we O
showed O
that O
AT Method
can O
boost O
tagging Metric
accuracy Metric
on O
rare O
words O
and O
the O
neighbors O
of O
unseen O
words O
, O
enhancing O
overall O
robustness Metric
on O
rare O
/ O
unseen O
words O
. O
In O
this O
section O
, O
we O
discuss O
the O
benefit O
of O
our O
improved O
POS Method
tagger O
in O
a O
major O
downstream Task
task Task
, O
dependency O
parsing Task
. O
Most O
of O
the O
recent O
state O
- O
of O
- O
the O
- O
art O
dependency Method
parsers Method
take O
predicted O
POS Method
tags O
as O
input O
( O
e.g. O
chen2014fast O
, O
andor2016globally O
, O
DozatManning17 O
) O
. O
dozat O
- O
qi O
- O
manning:2017:K17 O
- O
3 O
empirically O
show O
that O
their O
dependency Method
parser Method
gains O
significant O
improvements O
by O
using O
POS Method
tags O
predicted O
by O
a O
Bi O
- O
LSTM O
POS Method
tagger O
, O
while O
POS Method
tags O
predicted O
by O
the O
UDPipe Method
tagger Method
do O
not O
contribute O
to O
parsing Task
performance O
as O
much O
. O
This O
observation O
illustrates O
that O
POS Task
tagging Task
performance O
has O
a O
great O
influence O
on O
dependency O
parsing Task
, O
motivating O
the O
hypothesis O
that O
the O
POS Task
tagging Task
improvements O
gained O
from O
our O
adversarial Method
training Method
help O
dependency O
parsing Task
. O
To O
test O
the O
hypothesis O
, O
we O
consider O
three O
settings O
in O
dependency O
parsing Task
of O
English O
and O
French O
: O
using O
POS Method
tags O
predicted O
by O
the O
baseline Method
model O
, O
using O
POS Method
tags O
predicted O
by O
the O
AT Method
model O
, O
and O
using O
gold O
POS Method
tags O
. O
For O
English O
( O
PTB Material
- Material
WSJ Material
) O
, O
we O
first O
convert O
the O
treebank O
into O
Stanford O
Dependencies O
( O
SD Method
) O
using O
Stanford Method
CoreNLP Method
( O
ver O
3.8.0 O
) O
, O
and O
then O
apply O
two O
well O
- O
known O
dependency Method
parsers Method
: O
Stanford Method
Parser Method
( O
ver Method
3.5.0 Method
) O
and O
Parsey Method
McParseface Method
( O
SyntaxNet Method
) O
. O
For O
French O
( O
UD Material
) O
, O
we O
use O
Parsey O
Universal O
from O
SyntaxNet O
. O
The O
three O
parsers Method
are O
all O
publicly O
available O
and O
pre O
- O
trained O
on O
corresponding O
treebanks O
. O
Table O
[ O
reference O
] O
shows O
the O
results O
of O
the O
experiments O
. O
We O
can O
observe O
improvements O
in O
both O
languages O
by O
using O
the O
POS Method
tags O
predicted O
by O
our O
AT Method
POS Method
tagger O
. O
As O
Manning:2011:from97to100 O
points O
out O
, O
when O
predicted O
POS Method
tags O
are O
used O
for O
downstream O
dependency O
parsing Task
, O
a O
single O
bad O
mistake O
in O
a O
sentence O
can O
greatly O
damage O
the O
usefulness O
of O
the O
POS Method
tagger O
. O
The O
robustness O
of O
our O
AT Method
POS Method
tagger O
against O
rare O
/ O
unseen O
words O
helps O
to O
mitigate O
such O
an O
issue O
. O
This O
advantage O
can O
also O
be O
observed O
from O
the O
AT Method
POS Method
tagger O
’s O
notably O
higher O
sentence Metric
- Metric
level Metric
accuracy Metric
than O
the O
baseline Method
( O
see O
Table O
[ O
reference O
] O
left O
) O
. O
Nonetheless O
, O
gold O
POS Method
tags O
still O
yield O
better O
parsing Task
results O
as O
compared O
to O
the O
baseline Method
/ O
AT Method
POS Method
taggers O
, O
supporting O
the O
claim O
that O
POS Task
tagging Task
needs O
further O
improvement O
for O
downstream Task
tasks Task
. O
subsection O
: O
Effects O
on O
Representation Task
Learning Task
English O
( O
WSJ O
) O
( O
GloVe O
) O
French O
( O
UD Material
) O
( O
polyglot O
) O
English O
( O
WSJ O
) O
Perturbation Metric
scale Metric
Avg Metric
. O
cluster Metric
tightness Metric
Next O
, O
we O
perform O
an O
analysis O
on O
representation Task
learning Task
of Task
words Task
( O
word Task
embeddings Task
) O
for O
the O
English Material
( Material
PTB Material
- Material
WSJ Material
) O
and O
French O
( O
UD Material
) O
experiments O
. O
We O
hypothesize O
that O
adversarial Method
training Method
( O
AT Method
) O
helps O
to O
learn O
better O
word O
embeddings O
so O
that O
the O
POS Method
tag O
prediction O
of O
a O
word O
can O
not O
be O
influenced O
by O
a O
small O
perturbation O
in O
the O
input O
embedding O
. O
To O
verify O
this O
hypothesis O
, O
we O
cluster O
all O
words O
in O
the O
test O
set O
based O
on O
their O
correct O
POS Method
tags O
and O
evaluate O
the O
tightness O
of O
the O
word O
vector O
distribution O
within O
each O
cluster O
. O
We O
compare O
this O
clustering Metric
quality Metric
among O
the O
three O
settings O
: O
1 O
) O
beginning O
( O
initialized O
with O
GloVe Method
or O
Polyglot Method
) O
, O
2 O
) O
after O
baseline Method
training O
( O
50 O
epochs O
) O
, O
and O
3 O
) O
after O
adversarial Method
training Method
( O
50 O
epochs O
) O
, O
to O
study O
the O
effects O
of O
AT Method
on O
word Task
representation Task
learning Task
. O
For O
evaluating O
the O
tightness Task
of Task
word Task
vector Task
distribution Task
, O
we O
employ O
the O
cosine Metric
similarity Metric
metric Metric
, O
which O
is O
widely O
used O
as O
a O
measure O
of O
the O
closeness O
between O
two O
word O
vectors O
( O
e.g. O
, O
mikolov2013distributed O
; O
pennington O
- O
socher O
- O
manning:2014:EMNLP2014 O
) O
. O
To O
measure O
the O
tightness O
of O
each O
cluster O
, O
we O
compute O
the O
cosine O
similarity O
for O
every O
pair O
of O
words O
within O
, O
and O
then O
take O
the O
average O
. O
We O
also O
report O
the O
average O
tightness O
across O
all O
the O
clusters O
. O
The O
evaluation O
results O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
We O
report O
the O
tightness Metric
scores Metric
for O
the O
four O
major O
clusters O
: O
noun O
, O
verb O
, O
adjective O
, O
and O
adverb O
( O
from O
left O
to O
right O
) O
. O
As O
can O
be O
seen O
from O
the O
table O
, O
for O
both O
languages O
, O
adversarial Method
training Method
( O
AT Method
) O
results O
in O
cleaner O
word O
embedding O
distributions O
than O
the O
baseline Method
, O
with O
a O
higher O
cosine Metric
similarity Metric
within O
each O
POS Method
cluster O
, O
and O
with O
a O
clear O
advantage O
in O
the O
average O
tightness O
across O
all O
the O
clusters O
. O
In O
other O
words O
, O
the O
learned O
word O
vectors O
show O
stronger O
correlations O
with O
their O
POS Method
tags O
. O
This O
result O
confirms O
that O
training O
with O
adversarial O
examples O
can O
help O
to O
learn O
cleaner O
word O
embeddings O
so O
that O
the O
meaning O
/ O
grammatical O
function O
of O
a O
word O
can O
not O
be O
altered O
by O
a O
small O
perturbation O
in O
its O
embedding O
. O
This O
analysis O
provides O
a O
means O
to O
interpret O
the O
robustness Metric
to O
input O
perturbations O
, O
from O
the O
perspective O
of O
NLP Task
. O
paragraph O
: O
Relation O
with O
perturbation Metric
size Metric
. O
We O
also O
study O
how O
the O
size O
of O
added O
perturbations O
influences O
word Method
representation Method
learning Method
in O
adversarial Method
training Method
. O
Recall O
that O
we O
set O
the O
norm O
of O
a O
perturbation O
to O
be O
, O
where O
is O
the O
dimension O
of O
the O
concatenated O
input O
embeddings O
( O
see O
§ O
[ O
reference O
] O
) O
. O
For O
instance O
, O
would O
produce O
no O
noise O
; O
would O
generate O
a O
perturbation O
of O
a O
norm O
equivalent O
to O
the O
original O
word O
embeddings O
. O
We O
hypothesize O
that O
AT Method
facilitates O
word Task
representation Task
learning Task
when O
is O
small O
enough O
to O
preserve O
the O
semantics O
of O
input O
words O
, O
but O
can O
hinder O
the O
learning Task
when O
is O
too O
large O
. O
To O
test O
the O
hypothesis O
, O
we O
repeat O
the O
clustering Task
evaluation Task
for O
word O
embeddings O
trained O
with O
varied O
perturbation O
scale O
: O
0 O
, O
0.001 O
, O
0.01 O
, O
0.05 O
, O
0.1 O
, O
0.5 O
( O
see O
Table O
[ O
reference O
] O
) O
. O
We O
observe O
that O
the O
quality O
of O
learned O
word O
embedding O
distribution O
keeps O
improving O
as O
goes O
up O
from O
0 O
to O
0.1 O
, O
but O
starts O
to O
drop O
around O
. O
We O
also O
find O
that O
this O
optimal O
in O
word Task
embedding Task
learning Task
( O
i.e. O
, O
0.1 O
) O
is O
larger O
than O
the O
which O
yielded O
the O
best O
tagging Task
performance O
on O
development O
sets O
( O
i.e. O
, O
0.01 O
or O
0.05 O
) O
. O
A O
possible O
explanation O
is O
that O
while O
word O
embeddings O
can O
adapt O
to O
relatively O
large O
( O
e.g. O
, O
0.1 O
) O
during O
training Task
, O
as O
adversarial Method
perturbations Method
are O
generated O
at O
the O
embedding O
level O
, O
such O
could O
change O
the O
semantics O
of O
the O
input O
from O
the O
current O
tagging Method
model Method
’s O
perspective O
and O
hinder O
the O
training Task
of Task
tagging Task
. O
subsection O
: O
Other O
Sequence Task
Labeling Task
Tasks Task
Finally O
, O
to O
further O
confirm O
the O
applicability O
of O
AT Method
, O
we O
experiment O
with O
our O
BiLSTM Method
- O
CRF Method
AT Method
model O
in O
different O
sequence Task
labeling Task
tasks Task
: O
chunking Task
and O
named Task
entity Task
recognition Task
( O
NER Task
) O
. O
Chunking Method
can O
be O
performed O
as O
a O
sequence Task
labeling Task
task Task
that O
assigns O
a O
chunking O
tag O
( O
B O
- O
NP O
, O
I O
- O
VP O
, O
etc O
. O
) O
to O
each O
word O
. O
We O
conduct O
experiments O
on O
the O
CoNLL O
2000 O
shared O
task O
with O
the O
standard O
data O
split O
: O
PTB Material
- Material
WSJ Material
Sections O
15 O
- O
18 O
for O
training O
and O
20 O
for O
testing O
. O
We O
use O
Section O
19 O
as O
the O
development O
set O
and O
employ O
the O
IOBES Method
tagging Method
scheme Method
, O
following O
HashimotoXTS16 O
. O
NER Task
aims O
to O
assign O
an O
entity O
type O
to O
each O
word O
, O
such O
as O
person O
, O
location O
, O
organization O
, O
and O
misc O
. O
We O
conduct O
experiments O
on O
the O
CoNLL O
- O
2003 O
( O
English O
) O
shared O
task O
, O
adopting O
the O
IOBES Method
tagging Method
scheme Method
as O
in O
. O
The O
results O
are O
summarized O
in O
Table O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
AT Method
enhanced O
F1 Metric
score Metric
from O
the O
baseline Method
BiLSTM Method
- O
CRF Method
model O
’s O
95.18 O
to O
95.25 O
for O
chunking Task
, O
and O
from O
91.22 O
to O
91.56 O
for O
NER Task
, O
also O
significantly O
outperforming O
ma Method
- O
hovy:2016:P16 O
- O
1 O
. O
These O
improvements O
made O
by O
AT Method
are O
bigger O
than O
that O
for O
English O
POS Method
tagging O
, O
most O
likely O
due O
to O
the O
larger O
room O
for O
improvement O
in O
chunking Task
and O
NER Task
. O
The O
improvements O
are O
again O
statistically O
significant O
, O
with O
- O
value O
0.05 O
on O
the O
- O
test O
. O
The O
experimental O
results O
suggest O
that O
the O
proposed O
adversarial Method
training Method
scheme Method
is O
generally O
effective O
across O
different O
sequence Task
labeling Task
tasks Task
. O
Our O
BiLSTM Method
- O
CRF Method
AT Method
model O
did O
not O
reach O
the O
performance O
by O
HashimotoXTS16 O
’s O
multi Method
- Method
task Method
model Method
and O
peters2017semi O
’s O
state O
- O
of O
- O
the O
- O
art O
system O
that O
incorporates O
pretrained Method
language Method
models Method
. O
It O
would O
be O
interesting O
future O
work O
to O
combine O
the O
strengths O
of O
these O
joint Method
models Method
( O
e.g. O
, O
syntactic O
and O
semantic O
aids O
) O
and O
adversarial Method
training Method
( O
e.g. O
, O
robustness Metric
) O
. O
section O
: O
Conclusion O
We O
proposed O
and O
carefully O
analyzed O
a O
POS Method
tagging O
model O
that O
exploits O
adversarial Method
training Method
( O
AT Method
) O
. O
In O
our O
multilingual O
experiments O
, O
we O
find O
that O
AT Method
achieves O
substantial O
improvements O
on O
all O
the O
languages O
tested O
, O
especially O
on O
low O
resource O
ones O
. O
AT Method
also O
enhances O
the O
robustness Metric
to O
rare O
/ O
unseen O
words O
and O
sentence Metric
- Metric
level Metric
accuracy Metric
, O
alleviating O
the O
major O
issues O
of O
current O
POS Method
taggers O
, O
and O
contributing O
to O
the O
downstream Task
task Task
, O
dependency O
parsing Task
. O
Furthermore O
, O
our O
analyses O
on O
different O
languages O
, O
word O
/ O
neighbor O
statistics O
and O
word Task
representation Task
learning Task
reveal O
the O
effects O
of O
AT Method
from O
the O
perspective O
of O
NLP Task
. O
The O
proposed O
AT Method
model O
is O
applicable O
to O
general Task
sequence Task
labeling Task
tasks Task
. O
This O
work O
therefore O
provides O
a O
strong O
basis O
and O
motivation O
for O
utilizing O
AT Method
in O
natural Task
language Task
tasks Task
. O
section O
: O
Acknowledgements O
We O
would O
like O
to O
thank O
Rui O
Zhang O
, O
Jonathan O
Kummerfeld O
, O
Yutaro O
Yamada O
, O
as O
well O
as O
all O
the O
anonymous O
reviewers O
for O
their O
helpful O
feedback O
and O
suggestions O
on O
this O
work O
. O
bibliography O
: O
References O
