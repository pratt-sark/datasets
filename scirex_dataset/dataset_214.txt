document O
: O
StarGAN Method
: O
Unified Method
Generative Method
Adversarial Method
Networks Method
for O
Multi Task
- Task
Domain Task
Image Task
- Task
to Task
- Task
Image Task
Translation Task
Recent O
studies O
have O
shown O
remarkable O
success O
in O
image Task
- Task
to Task
- Task
image Task
translation Task
for O
two O
domains O
. O
However O
, O
existing O
approaches O
have O
limited O
scalability O
and O
robustness Metric
in O
handling O
more O
than O
two O
domains O
, O
since O
different O
models O
should O
be O
built O
independently O
for O
every O
pair O
of O
image O
domains O
. O
To O
address O
this O
limitation O
, O
we O
propose O
StarGAN Method
, O
a O
novel O
and O
scalable O
approach O
that O
can O
perform O
image Task
- Task
to Task
- Task
image Task
translations Task
for O
multiple O
domains O
using O
only O
a O
single O
model O
. O
Such O
a O
unified Method
model Method
architecture Method
of O
StarGAN Method
allows O
simultaneous O
training O
of O
multiple O
datasets O
with O
different O
domains O
within O
a O
single O
network O
. O
This O
leads O
to O
StarGAN Method
’s O
superior O
quality O
of O
translated Metric
images Metric
compared O
to O
existing O
models O
as O
well O
as O
the O
novel O
capability O
of O
flexibly O
translating O
an O
input O
image O
to O
any O
desired O
target O
domain O
. O
We O
empirically O
demonstrate O
the O
effectiveness O
of O
our O
approach O
on O
a O
facial Task
attribute Task
transfer Task
and O
a O
facial Task
expression Task
synthesis Task
tasks Task
. O
section O
: O
Introduction O
The O
task O
of O
image Task
- Task
to Task
- Task
image Task
translation Task
is O
to O
change O
a O
particular O
aspect O
of O
a O
given O
image O
to O
another O
, O
e.g. O
, O
changing O
the O
facial O
expression O
of O
a O
person O
from O
smiling O
to O
frowning O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
. O
This O
task O
has O
experienced O
significant O
improvements O
following O
the O
introduction O
of O
generative Method
adversarial Method
networks Method
( O
GANs Method
) O
, O
with O
results O
ranging O
from O
changing O
hair O
color O
, O
reconstructing O
photos O
from O
edge O
maps O
, O
and O
changing O
the O
seasons O
of O
scenery O
images O
. O
Given O
training O
data O
from O
two O
different O
domains O
, O
these O
models O
learn O
to O
translate O
images O
from O
one O
domain O
to O
the O
other O
. O
We O
denote O
the O
terms O
attribute O
as O
a O
meaningful O
feature O
inherent O
in O
an O
image O
such O
as O
hair O
color O
, O
gender O
or O
age O
, O
and O
attribute O
value O
as O
a O
particular O
value O
of O
an O
attribute O
, O
e.g. O
, O
black O
/ O
blond O
/ O
brown O
for O
hair O
color O
or O
male O
/ O
female O
for O
gender O
. O
We O
further O
denote O
domain O
as O
a O
set O
of O
images O
sharing O
the O
same O
attribute O
value O
. O
For O
example O
, O
images O
of O
women O
can O
represent O
one O
domain O
while O
those O
of O
men O
represent O
another O
. O
Several O
image O
datasets O
come O
with O
a O
number O
of O
labeled O
attributes O
. O
For O
instance O
, O
the O
CelebA Material
dataset O
contains O
40 O
labels O
related O
to O
facial O
attributes O
such O
as O
hair O
color O
, O
gender O
, O
and O
age O
, O
and O
the O
RaFD Material
dataset Material
has O
8 O
labels O
for O
facial O
expressions O
such O
as O
‘ O
happy O
’ O
, O
‘ O
angry O
’ O
and O
‘ O
sad O
’ O
. O
These O
settings O
enable O
us O
to O
perform O
more O
interesting O
tasks O
, O
namely O
multi Task
- Task
domain Task
image Task
- Task
to Task
- Task
image Task
translation Task
, O
where O
we O
change O
images O
according O
to O
attributes O
from O
multiple O
domains O
. O
The O
first O
five O
columns O
in O
Fig O
. O
[ O
reference O
] O
show O
how O
a O
CelebA Material
image O
can O
be O
translated O
according O
to O
any O
of O
the O
four O
domains O
, O
‘ O
blond O
hair O
’ O
, O
‘ O
gender O
’ O
, O
‘ O
aged O
’ O
, O
and O
‘ O
pale O
skin O
’ O
. O
We O
can O
further O
extend O
to O
training O
multiple O
domains O
from O
different O
datasets O
, O
such O
as O
jointly O
training O
CelebA Material
and O
RaFD Material
images O
to O
change O
a O
CelebA Material
image O
’s O
facial O
expression O
using O
features O
learned O
by O
training O
on O
RaFD Material
, O
as O
in O
the O
rightmost O
columns O
of O
Fig O
. O
[ O
reference O
] O
. O
However O
, O
existing O
models O
are O
both O
inefficient O
and O
ineffective O
in O
such O
multi Task
- Task
domain Task
image Task
translation Task
tasks Task
. O
Their O
inefficiency O
results O
from O
the O
fact O
that O
in O
order O
to O
learn O
all O
mappings O
among O
domains O
, O
generators O
have O
to O
be O
trained O
. O
Fig O
. O
[ O
reference O
] O
( O
a O
) O
illustrates O
how O
twelve O
distinct O
generator Method
networks Method
have O
to O
be O
trained O
to O
translate O
images O
among O
four O
different O
domains O
. O
Meanwhile O
, O
they O
are O
ineffective O
that O
even O
though O
there O
exist O
global O
features O
that O
can O
be O
learned O
from O
images O
of O
all O
domains O
such O
as O
face O
shapes O
, O
each O
generator O
can O
not O
fully O
utilize O
the O
entire O
training O
data O
and O
only O
can O
learn O
from O
two O
domains O
out O
of O
. O
Failure O
to O
fully O
utilize O
training O
data O
is O
likely O
to O
limit O
the O
quality O
of O
generated O
images O
. O
Furthermore O
, O
they O
are O
incapable O
of O
jointly O
training O
domains O
from O
different O
datasets O
because O
each O
dataset O
is O
partially O
labeled O
, O
which O
we O
further O
discuss O
in O
Section O
[ O
reference O
] O
. O
As O
a O
solution O
to O
such O
problems O
we O
propose O
StarGAN Method
, O
a O
novel O
and O
scalable O
approach O
capable O
of O
learning Task
mappings Task
among O
multiple O
domains O
. O
As O
demonstrated O
in O
Fig O
. O
[ O
reference O
] O
( O
b O
) O
, O
our O
model O
takes O
in O
training O
data O
of O
multiple O
domains O
, O
and O
learns O
the O
mappings O
between O
all O
available O
domains O
using O
only O
a O
single O
generator Method
. O
The O
idea O
is O
simple O
. O
Instead O
of O
learning O
a O
fixed O
translation O
( O
e.g. O
, O
black O
- O
to O
- O
blond O
hair O
) O
, O
our O
generator O
takes O
in O
as O
inputs O
both O
image O
and O
domain O
information O
, O
and O
learns O
to O
flexibly O
translate O
the O
image O
into O
the O
corresponding O
domain O
. O
We O
use O
a O
label O
( O
e.g. O
, O
binary O
or O
one O
- O
hot O
vector O
) O
to O
represent O
domain O
information O
. O
During O
training O
, O
we O
randomly O
generate O
a O
target O
domain O
label O
and O
train O
the O
model O
to O
flexibly O
translate O
an O
input O
image O
into O
the O
target O
domain O
. O
By O
doing O
so O
, O
we O
can O
control O
the O
domain O
label O
and O
translate O
the O
image O
into O
any O
desired O
domain O
at O
testing O
phase O
. O
We O
also O
introduce O
a O
simple O
but O
effective O
approach O
that O
enables O
joint Task
training Task
between O
domains O
of O
different O
datasets O
by O
adding O
a O
mask O
vector O
to O
the O
domain O
label O
. O
Our O
proposed O
method O
ensures O
that O
the O
model O
can O
ignore O
unknown O
labels O
and O
focus O
on O
the O
label O
provided O
by O
a O
particular O
dataset O
. O
In O
this O
manner O
, O
our O
model O
can O
perform O
well O
on O
tasks O
such O
as O
synthesizing O
facial O
expressions O
of O
CelebA Material
images O
using O
features O
learned O
from O
RaFD Material
, O
as O
shown O
in O
the O
rightmost O
columns O
of O
Fig O
. O
[ O
reference O
] O
. O
As O
far O
as O
our O
knowledge O
goes O
, O
our O
work O
is O
the O
first O
to O
successfully O
perform O
multi Task
- Task
domain Task
image Task
translation Task
across O
different O
datasets O
. O
Overall O
, O
our O
contributions O
are O
as O
follows O
: O
We O
propose O
StarGAN Method
, O
a O
novel O
generative Method
adversarial Method
network Method
that O
learns O
the O
mappings O
among O
multiple O
domains O
using O
only O
a O
single O
generator Method
and O
a O
discriminator Method
, O
training O
effectively O
from O
images O
of O
all O
domains O
. O
We O
demonstrate O
how O
we O
can O
successfully O
learn O
multi Task
- Task
domain Task
image Task
translation Task
between O
multiple O
datasets O
by O
utilizing O
a O
mask Method
vector Method
method Method
that O
enables O
StarGAN Method
to O
control O
all O
available O
domain O
labels O
. O
We O
provide O
both O
qualitative O
and O
quantitative O
results O
on O
facial Task
attribute Task
transfer Task
and Task
facial Task
expression Task
synthesis Task
tasks Task
using O
StarGAN Method
, O
showing O
its O
superiority O
over O
baseline Method
models Method
. O
section O
: O
Related O
Work O
Generative Method
Adversarial Method
Networks Method
. O
Generative Method
adversarial Method
networks Method
( O
GANs Method
) O
have O
shown O
remarkable O
results O
in O
various O
computer Task
vision Task
tasks Task
such O
as O
image Task
generation Task
, O
image Task
translation Task
, O
super Task
- Task
resolution Task
imaging Task
, O
and O
face Task
image Task
synthesis Task
. O
A O
typical O
GAN Method
model Method
consists O
of O
two O
modules O
: O
a O
discriminator Method
and O
a O
generator Method
. O
The O
discriminator Method
learns O
to O
distinguish O
between O
real O
and O
fake O
samples O
, O
while O
the O
generator Method
learns O
to O
generate O
fake O
samples O
that O
are O
indistinguishable O
from O
real O
samples O
. O
Our O
approach O
also O
leverages O
the O
adversarial O
loss O
to O
make O
the O
generated O
images O
as O
realistic O
as O
possible O
. O
Conditional Method
GANs Method
. O
GAN Method
- Method
based Method
conditional Method
image Method
generation Method
has O
also O
been O
actively O
studied O
. O
Prior O
studies O
have O
provided O
both O
the O
discriminator Method
and Method
generator Method
with O
class O
information O
in O
order O
to O
generate O
samples O
conditioned O
on O
the O
class O
. O
Other O
recent O
approaches O
focused O
on O
generating O
particular O
images O
highly O
relevant O
to O
a O
given O
text O
description O
. O
The O
idea O
of O
conditional Task
image Task
generation Task
has O
also O
been O
successfully O
applied O
to O
domain Task
transfer Task
, O
super Task
- Task
resolution Task
imaging Task
, O
and O
photo Task
editing Task
. O
In O
this O
paper O
, O
we O
propose O
a O
scalable Method
GAN Method
framework Method
that O
can O
flexibly O
steer O
the O
image Task
translation Task
to O
various O
target O
domains O
, O
by O
providing O
conditional O
domain O
information O
. O
Image Task
- Task
to Task
- Task
Image Task
Translation Task
. O
Recent O
work O
have O
achieved O
impressive O
results O
in O
image Task
- Task
to Task
- Task
image Task
translation Task
. O
For O
instance O
, O
pix2pix Method
learns O
this O
task O
in O
a O
supervised Method
manner Method
using O
cGANs Method
. O
It O
combines O
an O
adversarial Method
loss Method
with O
a O
L1 Method
loss Method
, O
thus O
requires O
paired O
data O
samples O
. O
To O
alleviate O
the O
problem O
of O
obtaining O
data Task
pairs Task
, O
unpaired O
image Method
- Method
to Method
- Method
image Method
translation Method
frameworks Method
have O
been O
proposed O
. O
UNIT Method
combines O
variational Method
autoencoders Method
( O
VAEs Method
) O
with O
CoGAN Method
, O
a O
GAN Method
framework Method
where O
two O
generators O
share O
weights O
to O
learn O
the O
joint O
distribution O
of O
images O
in O
cross O
domains O
. O
CycleGAN O
and O
DiscoGAN Method
preserve O
key O
attributes O
between O
the O
input O
and O
the O
translated O
image O
by O
utilizing O
a O
cycle O
consistency O
loss O
. O
However O
, O
all O
these O
frameworks O
are O
only O
capable O
of O
learning O
the O
relations O
between O
two O
different O
domains O
at O
a O
time O
. O
Their O
approaches O
have O
limited O
scalability O
in O
handling O
multiple O
domains O
since O
different O
models O
should O
be O
trained O
for O
each O
pair O
of O
domains O
. O
Unlike O
the O
aforementioned O
approaches O
, O
our O
framework O
can O
learn O
the O
relations O
among O
multiple O
domains O
using O
only O
a O
single O
model O
. O
section O
: O
Star Method
Generative Method
Adversarial Method
Networks Method
We O
first O
describe O
our O
proposed O
StarGAN Method
, O
a O
framework O
to O
address O
multi Task
- Task
domain Task
image Task
- Task
to Task
- Task
image Task
translation Task
within O
a O
single O
dataset O
. O
Then O
, O
we O
discuss O
how O
StarGAN Method
incorporates O
multiple O
datasets O
containing O
different O
label O
sets O
to O
flexibly O
perform O
image Task
translations Task
using O
any O
of O
these O
labels O
. O
subsection O
: O
Multi Task
- Task
Domain Task
Image Task
- Task
to Task
- Task
Image Task
Translation Task
Our O
goal O
is O
to O
train O
a O
single O
generator Method
that O
learns O
mappings O
among O
multiple O
domains O
. O
To O
achieve O
this O
, O
we O
train O
to O
translate O
an O
input O
image O
into O
an O
output O
image O
conditioned O
on O
the O
target O
domain O
label O
, O
. O
We O
randomly O
generate O
the O
target O
domain O
label O
so O
that O
learns O
to O
flexibly O
translate O
the O
input O
image O
. O
We O
also O
introduce O
an O
auxiliary Method
classifier Method
that O
allows O
a O
single O
discriminator Method
to O
control O
multiple O
domains O
. O
That O
is O
, O
our O
discriminator Method
produces O
probability O
distributions O
over O
both O
sources O
and O
domain O
labels O
, O
. O
Fig O
. O
[ O
reference O
] O
illustrates O
the O
training O
process O
of O
our O
proposed O
approach O
. O
Adversarial O
Loss O
. O
To O
make O
the O
generated O
images O
indistinguishable O
from O
real O
images O
, O
we O
adopt O
an O
adversarial Method
loss Method
where O
generates O
an O
image O
conditioned O
on O
both O
the O
input O
image O
and O
the O
target O
domain O
label O
, O
while O
tries O
to O
distinguish O
between O
real O
and O
fake O
images O
. O
In O
this O
paper O
, O
we O
refer O
to O
the O
term O
as O
a O
probability O
distribution O
over O
sources O
given O
by O
. O
The O
generator Method
tries O
to O
minimize O
this O
objective O
, O
while O
the O
discriminator Method
tries O
to O
maximize O
it O
. O
Domain Task
Classification Task
Loss Task
. O
For O
a O
given O
input O
image O
and O
a O
target O
domain O
label O
, O
our O
goal O
is O
to O
translate O
into O
an O
output O
image O
, O
which O
is O
properly O
classified O
to O
the O
target O
domain O
. O
To O
achieve O
this O
condition O
, O
we O
add O
an O
auxiliary Method
classifier Method
on O
top O
of O
and O
impose O
the O
domain O
classification O
loss O
when O
optimizing O
both O
and O
. O
That O
is O
, O
we O
decompose O
the O
objective O
into O
two O
terms O
: O
a O
domain Method
classification Method
loss Method
of O
real O
images O
used O
to O
optimize O
, O
and O
a O
domain Task
classification Task
loss Task
of O
fake O
images O
used O
to O
optimize O
. O
In O
detail O
, O
the O
former O
is O
defined O
as O
where O
the O
term O
represents O
a O
probability O
distribution O
over O
domain O
labels O
computed O
by O
. O
By O
minimizing O
this O
objective O
, O
learns O
to O
classify O
a O
real O
image O
to O
its O
corresponding O
original O
domain O
. O
We O
assume O
that O
the O
input O
image O
and O
domain O
label O
pair O
is O
given O
by O
the O
training O
data O
. O
On O
the O
other O
hand O
, O
the O
loss Method
function Method
for O
the O
domain Task
classification Task
of Task
fake Task
images Task
is O
defined O
as O
In O
other O
words O
, O
tries O
to O
minimize O
this O
objective O
to O
generate O
images O
that O
can O
be O
classified O
as O
the O
target O
domain O
. O
Reconstruction Metric
Loss Metric
. O
By O
minimizing O
the O
adversarial O
and O
classification O
losses O
, O
is O
trained O
to O
generate O
images O
that O
are O
realistic O
and O
classified O
to O
its O
correct O
target O
domain O
. O
However O
, O
minimizing O
the O
losses O
( O
Eqs O
. O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
) O
does O
not O
guarantee O
that O
translated O
images O
preserve O
the O
content O
of O
its O
input O
images O
while O
changing O
only O
the O
domain O
- O
related O
part O
of O
the O
inputs O
. O
To O
alleviate O
this O
problem O
, O
we O
apply O
a O
cycle Method
consistency Method
loss Method
to O
the O
generator Method
, O
defined O
as O
where O
takes O
in O
the O
translated O
image O
and O
the O
original O
domain O
label O
as O
input O
and O
tries O
to O
reconstruct O
the O
original O
image O
. O
We O
adopt O
the O
L1 O
norm O
as O
our O
reconstruction O
loss O
. O
Note O
that O
we O
use O
a O
single O
generator Method
twice Method
, O
first O
to O
translate O
an O
original O
image O
into O
an O
image O
in O
the O
target O
domain O
and O
then O
to O
reconstruct O
the O
original O
image O
from O
the O
translated O
image O
. O
Full O
Objective O
. O
Finally O
, O
the O
objective O
functions O
to O
optimize O
and O
are O
written O
, O
respectively O
, O
as O
where O
and O
are O
hyper O
- O
parameters O
that O
control O
the O
relative O
importance O
of O
domain Task
classification Task
and O
reconstruction O
losses O
, O
respectively O
, O
compared O
to O
the O
adversarial Method
loss Method
. O
We O
use O
and O
in O
all O
of O
our O
experiments O
. O
subsection O
: O
Training O
with O
Multiple O
Datasets O
An O
important O
advantage O
of O
StarGAN Method
is O
that O
it O
simultaneously O
incorporates O
multiple O
datasets O
containing O
different O
types O
of O
labels O
, O
so O
that O
StarGAN Method
can O
control O
all O
the O
labels O
at O
the O
test O
phase O
. O
An O
issue O
when O
learning O
from O
multiple O
datasets O
, O
however O
, O
is O
that O
the O
label O
information O
is O
only O
partially O
known O
to O
each O
dataset O
. O
In O
the O
case O
of O
CelebA Material
and O
RaFD Material
, O
while O
the O
former O
contains O
labels O
for O
attributes O
such O
as O
hair O
color O
and O
gender O
, O
it O
does O
not O
have O
any O
labels O
for O
facial O
expressions O
such O
as O
‘ O
happy O
’ O
and O
‘ O
angry O
’ O
, O
and O
vice O
versa O
for O
the O
latter O
. O
This O
is O
problematic O
because O
the O
complete O
information O
on O
the O
label O
vector O
is O
required O
when O
reconstructing O
the O
input O
image O
from O
the O
translated O
image O
( O
See O
Eq O
. O
( O
[ O
reference O
] O
) O
) O
. O
Mask O
Vector O
. O
To O
alleviate O
this O
problem O
, O
we O
introduce O
a O
mask Method
vector Method
that O
allows O
StarGAN Method
to O
ignore O
unspecified O
labels O
and O
focus O
on O
the O
explicitly O
known O
label O
provided O
by O
a O
particular O
dataset O
. O
In O
StarGAN Method
, O
we O
use O
an O
- O
dimensional O
one O
- O
hot O
vector O
to O
represent O
, O
with O
being O
the O
number O
of O
datasets O
. O
In O
addition O
, O
we O
define O
a O
unified O
version O
of O
the O
label O
as O
a O
vector O
where O
refers O
to O
concatenation O
, O
and O
represents O
a O
vector O
for O
the O
labels O
of O
the O
- O
th O
dataset O
. O
The O
vector O
of O
the O
known O
label O
can O
be O
represented O
as O
either O
a O
binary O
vector O
for O
binary O
attributes O
or O
a O
one O
- O
hot O
vector O
for O
categorical O
attributes O
. O
For O
the O
remaining O
unknown O
labels O
we O
simply O
assign O
zero O
values O
. O
In O
our O
experiments O
, O
we O
utilize O
the O
CelebA Material
and O
RaFD Material
datasets Material
, O
where O
is O
two O
. O
Training Method
Strategy Method
. O
When O
training O
StarGAN Method
with O
multiple O
datasets O
, O
we O
use O
the O
domain O
label O
defined O
in O
Eq O
. O
( O
[ O
reference O
] O
) O
as O
input O
to O
the O
generator O
. O
By O
doing O
so O
, O
the O
generator Method
learns O
to O
ignore O
the O
unspecified O
labels O
, O
which O
are O
zero O
vectors O
, O
and O
focus O
on O
the O
explicitly O
given O
label O
. O
The O
structure O
of O
the O
generator O
is O
exactly O
the O
same O
as O
in O
training O
with O
a O
single O
dataset O
, O
except O
for O
the O
dimension O
of O
the O
input O
label O
. O
On O
the O
other O
hand O
, O
we O
extend O
the O
auxiliary Method
classifier Method
of O
the O
discriminator Method
to O
generate O
probability O
distributions O
over O
labels O
for O
all O
datasets O
. O
Then O
, O
we O
train O
the O
model O
in O
a O
multi Task
- Task
task Task
learning Task
setting Task
, O
where O
the O
discriminator Method
tries O
to O
minimize O
only O
the O
classification Metric
error Metric
associated O
to O
the O
known O
label O
. O
For O
example O
, O
when O
training O
with O
images O
in O
CelebA Material
, O
the O
discriminator Method
minimizes O
only O
classification Metric
errors Metric
for O
labels O
related O
to O
CelebA Material
attributes O
, O
and O
not O
facial O
expressions O
related O
to O
RaFD Material
. O
Under O
these O
settings O
, O
by O
alternating O
between O
CelebA Material
and O
RaFD Material
the O
discriminator Method
learns O
all O
of O
the O
discriminative O
features O
for O
both O
datasets O
, O
and O
the O
generator Method
learns O
to O
control O
all O
the O
labels O
in O
both O
datasets O
. O
section O
: O
Implementation O
Improved O
GAN Method
Training Method
. O
To O
stabilize O
the O
training Task
process Task
and O
generate O
higher O
quality O
images O
, O
we O
replace O
Eq O
. O
( O
[ O
reference O
] O
) O
with O
Wasserstein O
GAN O
objective O
with O
gradient O
penalty O
defined O
as O
where O
is O
sampled O
uniformly O
along O
a O
straight O
line O
between O
a O
pair O
of O
a O
real O
and O
a O
generated O
images O
. O
We O
use O
for O
all O
experiments O
. O
Network Method
Architecture Method
. O
Adapted O
from O
CycleGAN Method
, O
StarGAN Method
has O
the O
generator Method
network Method
composed O
of O
two O
convolutional Method
layers Method
with O
the O
stride O
size O
of O
two O
for O
downsampling O
, O
six O
residual O
blocks O
, O
and O
two O
transposed Method
convolutional Method
layers Method
with O
the O
stride O
size O
of O
two O
for O
upsampling O
. O
We O
use O
instance Method
normalization Method
for O
the O
generator Method
but O
no O
normalization Method
for O
the O
discriminator Method
. O
We O
leverage O
PatchGANs Method
for O
the O
discriminator Method
network Method
, O
which O
classifies O
whether O
local O
image O
patches O
are O
real O
or O
fake O
. O
See O
the O
appendix O
( O
Section O
[ O
reference O
] O
) O
for O
more O
details O
about O
the O
network Method
architecture Method
. O
section O
: O
Experiments O
In O
this O
section O
, O
we O
first O
compare O
StarGAN Method
against O
recent O
methods O
on O
facial Task
attribute Task
transfer Task
by O
conducting O
user O
studies O
. O
Next O
, O
we O
perform O
a O
classification Task
experiment O
on O
facial Task
expression Task
synthesis Task
. O
Lastly O
, O
we O
demonstrate O
empirical O
results O
that O
StarGAN Method
can O
learn O
image Task
- Task
to Task
- Task
image Task
translation Task
from O
multiple O
datasets O
. O
All O
our O
experiments O
were O
conducted O
by O
using O
the O
model O
output O
from O
unseen O
images O
during O
the O
training O
phase O
. O
subsection O
: O
Baseline O
Models O
As O
our O
baseline Method
models Method
, O
we O
adopt O
DIAT Method
and Method
CycleGAN Method
, O
both O
of O
which O
performs O
image Task
- Task
to Task
- Task
image Task
translation Task
between O
two O
different O
domains O
. O
For O
comparison O
, O
we O
trained O
these O
models O
multiple O
times O
for O
every O
pair O
of O
two O
different O
domains O
. O
We O
also O
adopt O
IcGAN Method
as O
a O
baseline O
which O
can O
perform O
attribute Task
transfer Task
using O
a O
cGAN Method
. O
DIAT Method
uses O
an O
adversarial O
loss O
to O
learn O
the O
mapping O
from O
to O
, O
where O
and O
are O
face O
images O
in O
two O
different O
domains O
and O
, O
respectively O
. O
This O
method O
has O
a O
regularization O
term O
on O
the O
mapping O
as O
to O
preserve O
identity O
features O
of O
the O
source O
image O
, O
where O
is O
a O
feature Method
extractor Method
pretrained O
on O
a O
face Task
recognition Task
task Task
. O
CycleGAN Method
also O
uses O
an O
adversarial O
loss O
to O
learn O
the O
mapping O
between O
two O
different O
domains O
and O
. O
This O
method O
regularizes O
the O
mapping O
via O
cycle O
consistency O
losses O
, O
and O
. O
This O
method O
requires O
two O
generators O
and O
discriminators Method
for O
each O
pair O
of O
two O
different O
domains O
. O
IcGAN Method
combines O
an O
encoder Method
with O
a O
cGAN Method
model Method
. O
cGAN Method
learns O
the O
mapping Method
that O
generates O
an O
image O
conditioned O
on O
both O
the O
latent O
vector O
and O
the O
conditional O
vector O
. O
In O
addition O
, O
IcGAN Method
introduces O
an O
encoder Method
to O
learn O
the O
inverse O
mappings O
of O
cGAN O
, O
and O
. O
This O
allows O
IcGAN Method
to O
synthesis O
images O
by O
only O
changing O
the O
conditional O
vector O
and O
preserving O
the O
latent O
vector O
. O
subsection O
: O
Datasets O
CelebA. Material
The O
CelebFaces Material
Attributes Material
( O
CelebA Material
) O
dataset O
contains O
202 O
, O
599 O
face O
images O
of O
celebrities O
, O
each O
annotated O
with O
40 O
binary O
attributes O
. O
We O
crop O
the O
initial O
size O
images O
to O
, O
then O
resize O
them O
as O
. O
We O
randomly O
select O
2 O
, O
000 O
images O
as O
test O
set O
and O
use O
all O
remaining O
images O
for O
training O
data O
. O
We O
construct O
seven O
domains O
using O
the O
following O
attributes O
: O
hair O
color O
( O
black O
, O
blond O
, O
brown O
) O
, O
gender O
( O
male O
/ O
female O
) O
, O
and O
age O
( O
young O
/ O
old O
) O
. O
RaFD Material
. O
The O
Radboud Material
Faces Material
Database Material
( O
RaFD Material
) O
consists O
of O
4 O
, O
824 O
images O
collected O
from O
67 O
participants O
. O
Each O
participant O
makes O
eight O
facial O
expressions O
in O
three O
different O
gaze O
directions O
, O
which O
are O
captured O
from O
three O
different O
angles O
. O
We O
crop O
the O
images O
to O
, O
where O
the O
faces O
are O
centered O
, O
and O
then O
resize O
them O
to O
. O
subsection O
: O
Training O
All O
models O
are O
trained O
using O
Adam Method
with O
and O
. O
For O
data Task
augmentation Task
we O
flip O
the O
images O
horizontally O
with O
a O
probability O
of O
0.5 O
. O
We O
perform O
one O
generator Method
update Method
after O
five O
discriminator Method
updates Method
as O
in O
. O
The O
batch O
size O
is O
set O
to O
16 O
for O
all O
experiments O
. O
For O
experiments O
on O
CelebA Material
, O
we O
train O
all O
models O
with O
a O
learning Metric
rate Metric
of O
0.0001 O
for O
the O
first O
10 O
epochs O
and O
linearly O
decay O
the O
learning Metric
rate Metric
to O
0 O
over O
the O
next O
10 O
epochs O
. O
To O
compensate O
for O
the O
lack O
of O
data O
, O
when O
training O
with O
RaFD Material
we O
train O
all O
models O
for O
100 O
epochs O
with O
a O
learning Metric
rate Metric
of O
0.0001 O
and O
apply O
the O
same O
decaying Method
strategy Method
over O
the O
next O
100 O
epochs O
. O
Training Task
takes O
about O
one O
day O
on O
a O
single O
NVIDIA Method
Tesla Method
M40 Method
GPU Method
. O
subsection O
: O
Experimental O
Results O
on O
CelebA Material
We O
first O
compare O
our O
proposed O
method O
to O
the O
baseline Method
models Method
on O
a O
single Task
and Task
multi Task
- Task
attribute Task
transfer Task
tasks Task
. O
We O
train O
the O
cross Method
- Method
domain Method
models Method
such O
as O
DIAT Method
and O
CycleGAN Method
multiple O
times O
considering O
all O
possible O
attribute O
value O
pairs O
. O
In O
the O
case O
of O
DIAT Task
and O
CycleGAN Task
, O
we O
perform O
multi O
- O
step O
translations O
to O
synthesize O
multiple O
attributes O
( O
e.g. O
transferring O
a O
gender O
attribute O
after O
changing O
a O
hair O
color O
) O
. O
Qualitative Task
evaluation Task
. O
Fig O
. O
[ O
reference O
] O
shows O
the O
facial Task
attribute Task
transfer Task
results O
on O
CelebA. Material
We O
observed O
that O
our O
method O
provides O
a O
higher O
visual Metric
quality Metric
of Metric
translation Metric
results O
on O
test O
data O
compared O
to O
the O
cross Method
- Method
domain Method
models Method
. O
One O
possible O
reason O
is O
the O
regularization O
effect O
of O
StarGAN Method
through O
a O
multi Method
- Method
task Method
learning Method
framework Method
. O
In O
other O
words O
, O
rather O
than O
training O
a O
model O
to O
perform O
a O
fixed O
translation O
( O
e.g. O
, O
brown O
- O
to O
- O
blond O
hair O
) O
, O
which O
is O
prone O
to O
overfitting O
, O
we O
train O
our O
model O
to O
flexibly O
translate O
images O
according O
to O
the O
labels O
of O
the O
target O
domain O
. O
This O
allows O
our O
model O
to O
learn O
reliable O
features O
universally O
applicable O
to O
multiple O
domains O
of O
images O
with O
different O
facial O
attribute O
values O
. O
Furthermore O
, O
compared O
to O
IcGAN Method
, O
our O
model O
demonstrates O
an O
advantage O
in O
preserving O
the O
facial O
identity O
feature O
of O
an O
input O
. O
We O
conjecture O
that O
this O
is O
because O
our O
method O
maintains O
the O
spatial O
information O
by O
using O
activation O
maps O
from O
the O
convolutional Method
layer Method
as O
latent Method
representation Method
, O
rather O
than O
just O
a O
low O
- O
dimensional O
latent O
vector O
as O
in O
IcGAN Method
. O
Quantitative Metric
evaluation Metric
protocol Metric
. O
For O
quantitative O
evaluations O
, O
we O
performed O
two O
user O
studies O
in O
a O
survey O
format O
using O
Amazon Method
Mechanical Method
Turk Method
( O
AMT Method
) O
to O
assess O
single Task
and Task
multiple Task
attribute Task
transfer Task
tasks Task
. O
Given O
an O
input O
image O
, O
the O
Turkers O
were O
instructed O
to O
choose O
the O
best O
generated O
image O
based O
on O
perceptual Metric
realism Metric
, O
quality O
of O
transfer Task
in O
attribute O
( O
s O
) O
, O
and O
preservation O
of O
a O
figure O
’s O
original O
identity O
. O
The O
options O
were O
four O
randomly O
shuffled O
images O
generated O
from O
four O
different O
methods O
. O
The O
generated O
images O
in O
one O
study O
have O
a O
single O
attribute O
transfer O
in O
either O
hair O
color O
( O
black O
, O
blond O
, O
brown O
) O
, O
gender O
, O
or O
age O
. O
In O
another O
study O
, O
the O
generated O
images O
involve O
a O
combination O
of O
attribute O
transfers O
. O
Each O
Turker O
was O
asked O
30 O
to O
40 O
questions O
with O
a O
few O
simple O
yet O
logical O
questions O
for O
validating O
human O
effort O
. O
The O
number O
of O
validated O
Turkers O
in O
each O
user O
study O
is O
146 O
and O
100 O
in O
single O
and O
multiple O
transfer Task
tasks Task
, O
respectively O
. O
Quantitative O
results O
. O
Tables O
[ O
reference O
] O
and O
[ O
reference O
] O
show O
the O
results O
of O
our O
AMT Method
experiment Method
on O
single Task
- Task
and Task
multi Task
- Task
attribute Task
transfer Task
tasks Task
, O
respectively O
. O
StarGAN Method
obtained O
the O
majority O
of O
votes O
for O
best O
transferring O
attributes O
in O
all O
cases O
. O
In O
the O
case O
of O
gender O
changes O
in O
Table O
[ O
reference O
] O
, O
the O
voting O
difference O
between O
our O
model O
and O
other O
models O
was O
marginal O
, O
e.g. O
, O
39.1 O
% O
for O
StarGAN Method
vs. O
31.4 O
% O
for O
DIAT Method
. O
However O
, O
in O
multi O
- O
attribute O
changes O
, O
e.g. O
, O
the O
‘ O
G O
+ O
A O
’ O
case O
in O
Table O
[ O
reference O
] O
, O
the O
performance O
difference O
becomes O
significant O
, O
e.g. O
, O
49.8 O
% O
for O
StarGAN Method
vs. O
20.3 O
% O
for O
IcGAN Method
) O
, O
clearly O
showing O
the O
advantages O
of O
StarGAN Method
in O
more O
complicated O
, O
multi Task
- Task
attribute Task
transfer Task
tasks Task
. O
This O
is O
because O
unlike O
the O
other O
methods O
, O
StarGAN Method
can O
handle O
image Task
translation Task
involving O
multiple O
attribute O
changes O
by O
randomly O
generating O
a O
target O
domain O
label O
in O
the O
training O
phase O
. O
subsection O
: O
Experimental O
Results O
on O
RaFD Material
We O
next O
train O
our O
model O
on O
the O
RaFD Material
dataset Material
to O
learn O
the O
task O
of O
synthesizing Task
facial Task
expressions Task
. O
To O
compare O
StarGAN Method
and O
baseline Method
models Method
, O
we O
fix O
the O
input O
domain O
as O
the O
‘ O
neutral O
’ O
expression O
, O
but O
the O
target O
domain O
varies O
among O
the O
seven O
remaining O
expressions O
. O
Qualitative Task
evaluation Task
. O
As O
seen O
in O
Fig O
. O
[ O
reference O
] O
, O
StarGAN Method
clearly O
generates O
the O
most O
natural O
- O
looking O
expressions O
while O
properly O
maintaining O
the O
personal O
identity O
and O
facial O
features O
of O
the O
input O
. O
While O
DIAT Method
and O
CycleGAN Method
mostly O
preserve O
the O
identity O
of O
the O
input O
, O
many O
of O
their O
results O
are O
shown O
blurry O
and O
do O
not O
maintain O
the O
degree O
of O
sharpness Metric
as O
seen O
in O
the O
input O
. O
IcGAN Method
even O
fails O
to O
preserve O
the O
personal O
identity O
in O
the O
image O
by O
generating O
male O
images O
. O
We O
believe O
that O
the O
superiority O
of O
StarGAN Method
in O
the O
image Metric
quality Metric
is O
due O
to O
its O
implicit O
data Method
augmentation Method
effect O
from O
a O
multi Task
- Task
task Task
learning Task
setting Task
. O
RaFD Material
images O
contain O
a O
relatively O
small O
size O
of O
samples O
, O
e.g. O
, O
500 O
images O
per O
domain O
. O
When O
trained O
on O
two O
domains O
, O
DIAT Method
and O
CycleGAN Method
can O
only O
use O
1 O
, O
000 O
training O
images O
at O
a O
time O
, O
but O
StarGAN Method
can O
use O
4 O
, O
000 O
images O
in O
total O
from O
all O
the O
available O
domains O
for O
its O
training O
. O
This O
allows O
StarGAN Method
to O
properly O
learn O
how O
to O
maintain O
the O
quality Metric
and O
sharpness Metric
of O
the O
generated O
output O
. O
Quantitative Task
evaluation Task
. O
For O
a O
quantitative O
evaluation O
, O
we O
compute O
the O
classification Metric
error Metric
of O
a O
facial O
expression O
on O
synthesized O
images O
. O
We O
trained O
a O
facial Method
expression Method
classifier Method
on O
the O
RaFD Material
dataset Material
( O
90% O
/ O
10 O
% O
splitting O
for O
training O
and O
test O
sets O
) O
using O
a O
ResNet Method
- Method
18 Method
architecture Method
, O
resulting O
in O
a O
near O
- O
perfect O
accuracy Metric
of O
99.55 O
% O
. O
We O
then O
trained O
each O
of O
image Method
translation Method
models Method
using O
the O
same O
training O
set O
and O
performed O
image Task
translation Task
on O
the O
same O
, O
unseen O
test O
set O
. O
Finally O
, O
we O
classified O
the O
expression O
of O
these O
translated O
images O
using O
the O
above O
- O
mentioned O
classifier Method
. O
As O
can O
be O
seen O
in O
Table O
[ O
reference O
] O
, O
our O
model O
achieves O
the O
lowest O
classification Metric
error Metric
, O
indicating O
that O
our O
model O
produces O
the O
most O
realistic O
facial O
expressions O
among O
all O
the O
methods O
compared O
. O
Another O
important O
advantage O
of O
our O
model O
is O
the O
scalability O
in O
terms O
of O
the O
number O
of O
parameters O
required O
. O
The O
last O
column O
in O
Table O
[ O
reference O
] O
shows O
that O
the O
number O
of O
parameters O
required O
to O
learn O
all O
translations O
by O
StarGAN Method
is O
seven O
times O
smaller O
than O
that O
of O
DIAT Method
and O
fourteen O
times O
smaller O
than O
that O
of O
CycleGAN Method
. O
This O
is O
because O
StarGAN Method
requires O
only O
a O
single O
generator Method
and Method
discriminator Method
pair Method
, O
regardless O
of O
the O
number O
of O
domains O
, O
while O
in O
the O
case O
of O
cross Method
- Method
domain Method
models Method
such O
as O
CycleGAN Method
, O
a O
completely O
different O
model O
should O
be O
trained O
for O
each O
source O
- O
target O
domain O
pair O
. O
subsection O
: O
Experimental O
Results O
on O
CelebA Material
+ O
RaFD Material
Finally O
, O
we O
empirically O
demonstrate O
that O
our O
model O
can O
learn O
not O
only O
from O
multiple O
domains O
within O
a O
single O
dataset O
, O
but O
also O
from O
multiple O
datasets O
. O
We O
train O
our O
model O
jointly O
on O
the O
CelebA Material
and O
RaFD Material
datasets O
using O
the O
mask O
vector O
( O
see O
Section O
[ O
reference O
] O
) O
. O
To O
distinguish O
between O
the O
model O
trained O
only O
on O
RaFD Material
and O
the O
model O
trained O
on O
both O
CelebA Material
and O
RaFD Material
, O
we O
denote O
the O
former O
as O
StarGAN Method
- Method
SNG Method
( O
single O
) O
and O
the O
latter O
as O
StarGAN Method
- Method
JNT Method
( O
joint O
) O
. O
Effects O
of O
joint Method
training Method
. O
Fig O
. O
[ O
reference O
] O
shows O
qualitative O
comparisons O
between O
StarGAN Method
- Method
SNG Method
and O
StarGAN Method
- Method
JNT Method
, O
where O
the O
task O
is O
to O
synthesize O
facial O
expressions O
of O
images O
in O
CelebA. Material
StarGAN Method
- Method
JNT Method
exhibits O
emotional O
expressions O
with O
high O
visual Metric
quality Metric
, O
while O
StarGAN Method
- Method
SNG Method
generates O
reasonable O
but O
blurry O
images O
with O
gray O
backgrounds O
. O
This O
difference O
is O
due O
to O
the O
fact O
that O
StarGAN Method
- Method
JNT Method
learns O
to O
translate O
CelebA Material
images O
during O
training O
but O
not O
StarGAN Method
- Method
SNG Method
. O
In O
other O
words O
, O
StarGAN Method
- Method
JNT Method
can O
leverage O
both O
datasets O
to O
improve O
shared Task
low Task
- Task
level Task
tasks Task
such O
facial Task
keypoint Task
detection Task
and O
segmentation Task
. O
By O
utilizing O
both O
CelebA Material
and O
RaFD Material
, O
StarGAN Method
- Method
JNT Method
can O
improve O
these O
low Task
- Task
level Task
tasks Task
, O
which O
is O
beneficial O
to O
learning Task
facial Task
expression Task
synthesis Task
. O
Learned O
role O
of O
mask O
vector O
. O
In O
this O
experiment O
, O
we O
gave O
a O
one O
- O
hot O
vector O
by O
setting O
the O
dimension O
of O
a O
particular O
facial O
expression O
( O
available O
from O
the O
second O
dataset O
, O
RaFD Material
) O
to O
one O
. O
In O
this O
case O
, O
since O
the O
label O
associated O
with O
the O
second O
data O
set O
is O
explicitly O
given O
, O
the O
proper O
mask O
vector O
would O
be O
. O
Fig O
. O
[ O
reference O
] O
shows O
the O
case O
where O
this O
proper O
mask O
vector O
was O
given O
and O
the O
opposite O
case O
where O
a O
wrong O
mask O
vector O
of O
was O
given O
. O
When O
the O
wrong O
mask O
vector O
was O
used O
, O
StarGAN Method
- Method
JNT Method
fails O
to O
synthesize O
facial O
expressions O
, O
and O
it O
manipulates O
the O
age O
of O
the O
input O
image O
. O
This O
is O
because O
the O
model O
ignores O
the O
facial O
expression O
label O
as O
unknown O
and O
treats O
the O
facial O
attribute O
label O
as O
valid O
by O
the O
mask O
vector O
. O
Note O
that O
since O
one O
of O
the O
facial O
attributes O
is O
‘ O
young O
’ O
, O
the O
model O
translates O
the O
image O
from O
young O
to O
old O
when O
it O
takes O
in O
a O
zero O
vector O
as O
input O
. O
From O
this O
behavior O
, O
we O
can O
confirm O
that O
StarGAN Method
properly O
learned O
the O
intended O
role O
of O
a O
mask O
vector O
in O
image Task
- Task
to Task
- Task
image Task
translations Task
when O
involving O
all O
the O
labels O
from O
multiple O
datasets O
altogether O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
StarGAN Method
, O
a O
scalable Task
image Task
- Task
to Task
- Task
image Task
translation Task
model Task
among O
multiple O
domains O
using O
a O
single O
generator Method
and O
a O
discriminator Method
. O
Besides O
the O
advantages O
in O
scalability Metric
, O
StarGAN Method
generated O
images O
of O
higher O
visual Metric
quality Metric
compared O
to O
existing O
methods O
, O
owing O
to O
the O
generalization O
capability O
behind O
the O
multi Task
- Task
task Task
learning Task
setting Task
. O
In O
addition O
, O
the O
use O
of O
the O
proposed O
simple O
mask Method
vector Method
enables O
StarGAN Method
to O
utilize O
multiple O
datasets O
with O
different O
sets O
of O
domain O
labels O
, O
thus O
handling O
all O
available O
labels O
from O
them O
. O
We O
hope O
our O
work O
to O
enable O
users O
to O
develop O
interesting O
image Task
translation Task
applications Task
across O
multiple O
domains O
. O
Acknowledgements O
. O
This O
work O
was O
mainly O
done O
while O
the O
first O
author O
did O
a O
research O
internship O
at O
Clova O
AI O
Research O
, O
NAVER O
. O
We O
thank O
all O
the O
researchers O
at O
NAVER O
, O
especially O
Donghyun O
Kwak O
, O
for O
insightful O
discussions O
. O
This O
work O
was O
partially O
supported O
by O
the O
National O
Research O
Foundation O
of O
Korea O
( O
NRF O
) O
grant O
funded O
by O
the O
Korean O
government O
( O
MSIP O
) O
( O
No O
. O
NRF2016R1C1B2015924 O
) O
. O
Jaegul O
Choo O
is O
the O
corresponding O
author O
. O
bibliography O
: O
References O
section O
: O
Appendix O
subsection O
: O
Training O
with O
Multiple O
Datasets O
Fig O
. O
[ O
reference O
] O
shows O
an O
overview O
of O
StarGAN Method
when O
learning Task
from O
both O
the O
CelebA Material
and O
RaFD Material
datasets Material
. O
As O
can O
be O
seen O
at O
the O
top O
of O
the O
figure O
, O
the O
label O
for O
CelebA Material
contains O
binary O
attributes O
( O
Black O
, O
Blond O
, O
Brown O
, O
Male O
, O
and O
Young O
) O
, O
while O
the O
label O
for O
RaFD Material
provides O
information O
on O
categorical O
attributes O
( O
Angry O
, O
Fearful O
, O
Happy O
, O
Sad O
, O
and O
Disgusted O
) O
. O
The O
mask O
vector O
is O
a O
two O
- O
dimensional O
one O
- O
hot O
vector O
which O
indicates O
whether O
the O
CelebA Material
or O
RaFD Material
label O
is O
valid O
. O
subsection O
: O
Network Method
Architecture Method
The O
network Method
architectures Method
of O
StarGAN Method
are O
shown O
in O
Table O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
For O
the O
generator Method
network Method
, O
we O
use O
instance Method
normalization Method
in O
all O
layers O
except O
the O
last O
output O
layer O
. O
For O
the O
discriminator Method
network Method
, O
we O
use O
Leaky Method
ReLU Method
with O
a O
negative O
slope O
of O
0.01 O
. O
There O
are O
some O
notations O
; O
: O
the O
number O
of O
domain O
, O
: O
the O
dimension O
of O
domain O
labels O
( O
when O
training O
with O
both O
the O
CelebA Material
and O
RaFD Material
datasets O
, O
otherwise O
same O
as O
) O
, O
N O
: O
the O
number O
of O
output O
channels O
, O
K O
: O
kernel O
size O
, O
S O
: O
stride O
size O
, O
P O
: O
padding O
size O
, O
IN O
: O
instance Method
normalization Method
. O
subsection O
: O
Additional O
Qualitative O
Results O
Figs O
. O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
show O
additional O
images O
with O
resolutions O
generated O
by O
StarGAN Method
. O
All O
images O
were O
generated O
by O
a O
single O
generator Method
trained O
on O
both O
the O
CelebA Material
and O
RaFD Material
datasets Material
. O
We O
trained O
StarGAN Method
on O
a O
single O
NVIDIA Method
Pascal Method
M40 Method
GPU Method
for O
seven O
days O
. O
