document	O
:	O
Non	Task
-	Task
Autoregressive	Task
Neural	Task
Machine	Task
Translation	Task
Existing	O
approaches	O
to	O
neural	Task
machine	Task
translation	Task
condition	O
each	O
output	O
word	O
on	O
previously	O
generated	O
outputs	O
.	O
We	O
introduce	O
a	O
model	O
that	O
avoids	O
this	O
autoregressive	Method
property	O
and	O
produces	O
its	O
outputs	O
in	O
parallel	O
,	O
allowing	O
an	O
order	O
of	O
magnitude	O
lower	O
latency	Metric
during	O
inference	Task
.	O
Through	O
knowledge	Method
distillation	Method
,	O
the	O
use	O
of	O
input	O
token	O
fertilities	O
as	O
a	O
latent	O
variable	O
,	O
and	O
policy	Method
gradient	Method
fine	Method
-	Method
tuning	Method
,	O
we	O
achieve	O
this	O
at	O
a	O
cost	O
of	O
as	O
little	O
as	O
2.0	O
BLEU	Metric
points	Metric
relative	O
to	O
the	O
autoregressive	Method
Transformer	Method
network	Method
used	O
as	O
a	O
teacher	O
.	O
We	O
demonstrate	O
substantial	O
cumulative	O
improvements	O
associated	O
with	O
each	O
of	O
the	O
three	O
aspects	O
of	O
our	O
training	Method
strategy	Method
,	O
and	O
validate	O
our	O
approach	O
on	O
IWSLT	Material
2016	O
English	Material
–	Material
German	Material
and	O
two	O
WMT	Material
language	Material
pairs	Material
.	O
By	O
sampling	O
fertilities	O
in	O
parallel	O
at	O
inference	O
time	O
,	O
our	O
non	O
-	O
autoregressive	Method
model	O
achieves	O
near	O
-	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
of	O
29.8	O
BLEU	Metric
on	O
WMT	Material
2016	Material
English	Material
–	Material
Romanian	Material
.	O
section	O
:	O
Introduction	O
Neural	Method
network	Method
based	Method
models	Method
outperform	O
traditional	O
statistical	Method
models	Method
for	O
machine	Task
translation	Task
(	O
MT	Task
)	O
bahdanau2014neural	O
,	O
luong2015effective	O
.	O
However	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	Method
models	Method
are	O
much	O
slower	O
than	O
statistical	Method
MT	Method
approaches	Method
at	O
inference	O
time	O
wu2016google	O
.	O
Both	O
model	Method
families	Method
use	O
autoregressive	Method
decoders	O
that	O
operate	O
one	O
step	O
at	O
a	O
time	O
:	O
they	O
generate	O
each	O
token	O
conditioned	O
on	O
the	O
sequence	O
of	O
tokens	O
previously	O
generated	O
.	O
This	O
process	O
is	O
not	O
parallelizable	O
,	O
and	O
,	O
in	O
the	O
case	O
of	O
neural	Method
MT	Method
models	Method
,	O
it	O
is	O
particularly	O
slow	O
because	O
a	O
computationally	Method
intensive	Method
neural	Method
network	Method
is	O
used	O
to	O
generate	O
each	O
token	O
.	O
While	O
several	O
recently	O
proposed	O
models	O
avoid	O
recurrence	O
at	O
train	O
time	O
by	O
leveraging	O
convolutions	Method
kalchbrenner2016neural	O
,	O
gehring2017convolutional	O
,	O
kaiser2017depthwise	O
or	O
self	Method
-	Method
attention	Method
vaswani2017attention	O
as	O
more	O
-	O
parallelizable	O
alternatives	O
to	O
recurrent	Method
neural	Method
networks	Method
(	O
RNNs	Method
)	O
,	O
use	O
of	O
autoregressive	Method
decoding	O
makes	O
it	O
impossible	O
to	O
take	O
full	O
advantage	O
of	O
parallelism	O
during	O
inference	Task
.	O
We	O
introduce	O
a	O
non	O
-	O
autoregressive	Method
translation	Method
model	Method
based	O
on	O
the	O
Transformer	Method
network	Method
vaswani2017attention	O
.	O
We	O
modify	O
the	O
encoder	Method
of	O
the	O
original	O
Transformer	Method
network	Method
by	O
adding	O
a	O
module	O
that	O
predicts	O
fertilities	O
,	O
sequences	O
of	O
numbers	O
that	O
form	O
an	O
important	O
component	O
of	O
many	O
traditional	O
machine	Method
translation	Method
models	Method
brown1993mathematics	O
.	O
These	O
fertilities	O
are	O
supervised	O
during	O
training	O
and	O
provide	O
the	O
decoder	Method
at	O
inference	O
time	O
with	O
a	O
globally	O
consistent	O
plan	O
on	O
which	O
to	O
condition	O
its	O
simultaneously	O
computed	O
outputs	O
.	O
section	O
:	O
Background	O
subsection	O
:	O
Autoregressive	Task
Neural	Task
Machine	Task
Translation	Task
Given	O
a	O
source	O
sentence	O
,	O
a	O
neural	O
machine	O
translation	Method
model	Method
factors	O
the	O
distribution	O
over	O
possible	O
output	O
sentences	O
into	O
a	O
chain	O
of	O
conditional	O
probabilities	O
with	O
a	O
left	O
-	O
to	O
-	O
right	O
causal	O
structure	O
:	O
where	O
the	O
special	O
tokens	O
(	O
e.g.	O
)	O
and	O
(	O
e.g.	O
)	O
are	O
used	O
to	O
represent	O
the	O
beginning	O
and	O
end	O
of	O
all	O
target	O
sentences	O
.	O
These	O
conditional	O
probabilities	O
are	O
parameterized	O
using	O
a	O
neural	Method
network	Method
.	O
Typically	O
,	O
an	O
encoder	Method
-	Method
decoder	Method
architecture	Method
sutskever2014sequence	O
with	O
a	O
unidirectional	Method
RNN	Method
-	Method
based	Method
decoder	Method
is	O
used	O
to	O
capture	O
the	O
causal	O
structure	O
of	O
the	O
output	O
distribution	O
.	O
paragraph	O
:	O
Maximum	Method
Likelihood	Method
training	Method
Choosing	O
to	O
factorize	O
the	O
machine	O
translation	O
output	O
distribution	O
autoregressively	O
enables	O
straightforward	O
maximum	Method
likelihood	Method
training	Method
with	O
a	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
applied	O
at	O
each	O
decoding	O
step	O
:	O
This	O
loss	O
provides	O
direct	O
supervision	O
for	O
each	O
conditional	Method
probability	Method
prediction	Method
.	O
paragraph	O
:	O
Autoregressive	Method
NMT	Method
without	Method
RNNs	Method
Since	O
the	O
entire	O
target	O
translation	O
is	O
known	O
at	O
training	O
time	O
,	O
the	O
calculation	O
of	O
later	O
conditional	O
probabilities	O
(	O
and	O
their	O
corresponding	O
losses	O
)	O
does	O
not	O
depend	O
on	O
the	O
output	O
words	O
chosen	O
during	O
earlier	O
decoding	O
steps	O
.	O
Even	O
though	O
decoding	Task
must	O
remain	O
entirely	O
sequential	O
during	O
inference	Task
,	O
models	O
can	O
take	O
advantage	O
of	O
this	O
parallelism	O
during	O
training	O
.	O
One	O
such	O
approach	O
replaces	O
recurrent	Method
layers	Method
in	O
the	O
decoder	Method
with	O
masked	Method
convolution	Method
layers	Method
kalchbrenner2016neural	O
,	O
gehring2017convolutional	Method
that	O
provide	O
the	O
causal	O
structure	O
required	O
by	O
the	O
autoregressive	Method
factorization	Method
.	O
A	O
recently	O
introduced	O
option	O
which	O
reduces	O
sequential	Task
computation	Task
still	O
further	O
is	O
to	O
construct	O
the	O
decoder	O
layers	O
out	O
of	O
self	Method
-	Method
attention	Method
computations	O
that	O
have	O
been	O
causally	O
masked	O
in	O
an	O
analogous	O
way	O
.	O
The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Transformer	Method
network	Method
takes	O
this	O
approach	O
,	O
which	O
allows	O
information	O
to	O
flow	O
in	O
the	O
decoder	O
across	O
arbitrarily	O
long	O
distances	O
in	O
a	O
constant	O
number	O
of	O
operations	O
,	O
asymptotically	O
fewer	O
than	O
required	O
by	O
convolutional	Method
architectures	Method
vaswani2017attention	O
.	O
subsection	O
:	O
Non	Method
-	Method
Autoregressive	Method
Decoding	Method
paragraph	O
:	O
Pros	O
and	O
cons	O
of	O
autoregressive	Method
decoding	O
The	O
autoregressive	Method
factorization	Method
used	O
by	O
conventional	O
NMT	Method
models	Method
has	O
several	O
benefits	O
.	O
It	O
corresponds	O
to	O
the	O
word	O
-	O
by	O
-	O
word	O
nature	O
of	O
human	Task
language	Task
production	Task
and	O
effectively	O
captures	O
the	O
distribution	O
of	O
real	O
translations	O
.	O
Autoregressive	Method
models	Method
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
large	O
-	O
scale	O
corpora	O
and	O
are	O
easy	O
to	O
train	O
,	O
while	O
beam	Method
search	Method
provides	O
an	O
effective	O
local	Method
search	Method
method	Method
for	O
finding	O
approximately	O
-	O
optimal	O
output	O
translations	O
.	O
But	O
there	O
are	O
also	O
drawbacks	O
.	O
As	O
the	O
individual	O
steps	O
of	O
the	O
decoder	Method
must	O
be	O
run	O
sequentially	O
rather	O
than	O
in	O
parallel	O
,	O
autoregressive	Method
decoding	O
prevents	O
architectures	O
like	O
the	O
Transformer	O
from	O
fully	O
realizing	O
their	O
train	Metric
-	Metric
time	Metric
performance	Metric
advantage	O
during	O
inference	Task
.	O
Meanwhile	O
,	O
beam	Method
search	Method
suffers	O
from	O
diminishing	O
returns	O
with	O
respect	O
to	O
beam	O
size	O
koehn2017six	O
and	O
exhibits	O
limited	O
search	O
parallelism	O
because	O
it	O
introduces	O
computational	O
dependence	O
between	O
beams	O
.	O
paragraph	O
:	O
Towards	O
non	O
-	O
autoregressive	Method
decoding	O
A	O
naïve	O
solution	O
is	O
to	O
remove	O
the	O
autoregressive	Method
connection	O
directly	O
from	O
an	O
existing	O
encoder	Method
-	Method
decoder	Method
model	Method
.	O
Assuming	O
that	O
the	O
target	O
sequence	O
length	O
can	O
be	O
modeled	O
with	O
a	O
separate	O
conditional	O
distribution	O
,	O
this	O
becomes	O
This	O
model	O
still	O
has	O
an	O
explicit	O
likelihood	O
function	O
,	O
and	O
it	O
can	O
still	O
be	O
trained	O
using	O
independent	O
cross	O
-	O
entropy	O
losses	O
on	O
each	O
output	O
distribution	O
.	O
Now	O
,	O
however	O
,	O
these	O
distributions	O
can	O
be	O
computed	O
in	O
parallel	O
at	O
inference	O
time	O
.	O
subsection	O
:	O
The	O
Multimodality	Task
Problem	Task
However	O
,	O
this	O
naïve	O
approach	O
does	O
not	O
yield	O
good	O
results	O
,	O
because	O
such	O
a	O
model	O
exhibits	O
complete	O
conditional	O
independence	O
.	O
Each	O
token	O
’s	O
distribution	O
depends	O
only	O
on	O
the	O
source	O
sentence	O
.	O
This	O
makes	O
it	O
a	O
poor	O
approximation	O
to	O
the	O
true	O
target	O
distribution	O
,	O
which	O
exhibits	O
strong	O
correlation	O
across	O
time	O
.	O
Intuitively	O
,	O
such	O
a	O
decoder	O
is	O
akin	O
to	O
a	O
panel	O
of	O
human	O
translators	O
each	O
asked	O
to	O
provide	O
a	O
single	O
word	O
of	O
a	O
translation	O
independently	O
of	O
the	O
words	O
their	O
colleagues	O
choose	O
.	O
In	O
particular	O
,	O
consider	O
an	O
English	O
source	O
sentence	O
like	O
“	O
Thank	O
you	O
.	O
”	O
This	O
can	O
be	O
accurately	O
translated	O
into	O
German	O
as	O
any	O
one	O
of	O
“	O
Danke	O
.	O
”	O
,	O
“	O
Danke	O
schön	O
.	O
”	O
,	O
or	O
“	O
Vielen	O
Dank	O
.	O
”	O
,	O
all	O
of	O
which	O
may	O
occur	O
in	O
a	O
given	O
training	O
corpus	O
.	O
This	O
target	O
distribution	O
can	O
not	O
be	O
represented	O
as	O
a	O
product	O
of	O
independent	Method
probability	Method
distributions	Method
for	O
each	O
of	O
the	O
first	O
,	O
second	O
,	O
and	O
third	O
words	O
,	O
because	O
a	O
conditionally	Method
independent	Method
distribution	Method
can	O
not	O
allow	O
“	O
Danke	O
schön	O
.	O
”	O
and	O
“	O
Vielen	O
Dank	O
.	O
”	O
without	O
also	O
licensing	O
“	O
Danke	O
Dank	O
.	O
”	O
and	O
“	O
Vielen	O
schön	O
.	O
”	O
The	O
conditional	Method
independence	Method
assumption	Method
prevents	O
a	O
model	O
from	O
properly	O
capturing	O
the	O
highly	O
multimodal	O
distribution	O
of	O
target	O
translations	O
.	O
We	O
call	O
this	O
the	O
“	O
multimodality	Task
problem	Task
”	O
and	O
introduce	O
both	O
a	O
modified	Method
model	Method
and	O
new	O
training	Method
techniques	Method
to	O
tackle	O
this	O
issue	O
.	O
section	O
:	O
The	O
Non	Method
-	Method
Autoregressive	Method
Transformer	Method
(	O
NAT	Method
)	O
We	O
introduce	O
a	O
novel	O
NMT	Method
model	Method
—	O
the	O
Non	Method
-	Method
Autoregressive	Method
Transformer	Method
(	O
NAT	Method
)	O
—that	O
can	O
produce	O
an	O
entire	O
output	O
translation	O
in	O
parallel	O
.	O
As	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
the	O
model	O
is	O
composed	O
of	O
the	O
following	O
four	O
modules	O
:	O
an	O
encoder	Method
stack	Method
,	O
a	O
decoder	Method
stack	Method
,	O
a	O
newly	O
added	O
fertility	Method
predictor	Method
(	O
details	O
in	O
[	O
reference	O
]	O
)	O
,	O
and	O
a	O
translation	Method
predictor	Method
for	O
token	Task
decoding	Task
.	O
subsection	O
:	O
Encoder	Method
Stack	Method
Similar	O
to	O
the	O
autoregressive	Method
Transformer	O
,	O
both	O
the	O
encoder	Method
and	Method
decoder	Method
stacks	Method
are	O
composed	O
entirely	O
of	O
feed	Method
-	Method
forward	Method
networks	Method
(	O
MLPs	Method
)	O
and	O
multi	Method
-	Method
head	Method
attention	Method
modules	Method
.	O
Since	O
no	O
RNNs	Method
are	O
used	O
,	O
there	O
is	O
no	O
inherent	O
requirement	O
for	O
sequential	Task
execution	Task
,	O
making	O
non	O
-	O
autoregressive	Method
decoding	O
possible	O
.	O
For	O
our	O
proposed	O
NAT	Method
,	O
the	O
encoder	Method
stays	O
unchanged	O
from	O
the	O
original	O
Transformer	Method
network	Method
.	O
subsection	O
:	O
Decoder	Method
Stack	Method
In	O
order	O
to	O
translate	O
non	O
-	O
autoregressively	O
and	O
parallelize	O
the	O
decoding	Task
process	Task
,	O
we	O
modify	O
the	O
decoder	Method
stack	Method
as	O
follows	O
.	O
paragraph	O
:	O
Decoder	O
Inputs	O
Before	O
decoding	Task
starts	O
,	O
the	O
NAT	Method
needs	O
to	O
know	O
how	O
long	O
the	O
target	O
sentence	O
will	O
be	O
in	O
order	O
to	O
generate	O
all	O
words	O
in	O
parallel	O
.	O
More	O
crucially	O
,	O
we	O
can	O
not	O
use	O
time	O
-	O
shifted	O
target	O
outputs	O
(	O
during	O
training	O
)	O
or	O
previously	O
predicted	O
outputs	O
(	O
during	O
inference	Task
)	O
as	O
the	O
inputs	O
to	O
the	O
first	O
decoder	Method
layer	Method
.	O
Omitting	O
inputs	O
to	O
the	O
first	O
decoder	Method
layer	Method
entirely	O
,	O
or	O
using	O
only	O
positional	O
embeddings	O
,	O
resulted	O
in	O
very	O
poor	O
performance	O
.	O
Instead	O
,	O
we	O
initialize	O
the	O
decoding	Method
process	Method
using	O
copied	O
source	O
inputs	O
from	O
the	O
encoder	O
side	O
.	O
As	O
the	O
source	O
and	O
target	O
sentences	O
are	O
often	O
of	O
different	O
lengths	O
,	O
we	O
propose	O
two	O
methods	O
:	O
Copy	O
source	O
inputs	O
uniformly	O
:	O
Each	O
decoder	O
input	O
is	O
a	O
copy	O
of	O
the	O
-	O
th	O
encoder	O
input	O
.	O
This	O
is	O
equivalent	O
to	O
“	O
scanning	O
”	O
source	O
inputs	O
from	O
left	O
to	O
right	O
with	O
a	O
constant	O
“	O
speed	O
,	O
”	O
and	O
results	O
in	O
a	O
decoding	Method
process	Method
that	O
is	O
deterministic	O
given	O
a	O
(	O
predicted	O
)	O
target	O
length	O
.	O
Copy	O
source	O
inputs	O
using	O
fertilities	O
:	O
A	O
more	O
powerful	O
way	O
,	O
depicted	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
and	O
discussed	O
in	O
more	O
detail	O
below	O
,	O
is	O
to	O
copy	O
each	O
encoder	O
input	O
as	O
a	O
decoder	O
input	O
zero	O
or	O
more	O
times	O
,	O
with	O
the	O
number	O
of	O
times	O
each	O
input	O
is	O
copied	O
referred	O
to	O
as	O
that	O
input	O
word	O
’s	O
“	O
fertility	O
.	O
”	O
In	O
this	O
case	O
the	O
source	O
inputs	O
are	O
scanned	O
from	O
left	O
to	O
right	O
at	O
a	O
“	O
speed	O
”	O
that	O
varies	O
inversely	O
with	O
the	O
fertility	O
of	O
each	O
input	O
;	O
the	O
decoding	Method
process	Method
is	O
now	O
conditioned	O
on	O
the	O
sequence	O
of	O
fertilities	O
,	O
while	O
the	O
resulting	O
output	O
length	O
is	O
determined	O
by	O
the	O
sum	O
of	O
all	O
fertility	O
values	O
.	O
paragraph	O
:	O
Non	O
-	O
causal	O
self	Method
-	Method
attention	Method
Without	O
the	O
constraint	O
of	O
an	O
autoregressive	Method
factorization	Method
of	O
the	O
output	O
distribution	O
,	O
we	O
no	O
longer	O
need	O
to	O
prevent	O
earlier	O
decoding	O
steps	O
from	O
accessing	O
information	O
from	O
later	O
steps	O
.	O
Thus	O
we	O
can	O
avoid	O
the	O
causal	O
mask	O
used	O
in	O
the	O
self	Method
-	Method
attention	Method
module	Method
of	O
the	O
conventional	O
Transformer	Method
’s	Method
decoder	Method
.	O
Instead	O
,	O
we	O
mask	O
out	O
each	O
query	O
position	O
only	O
from	O
attending	O
to	O
itself	O
,	O
which	O
we	O
found	O
to	O
improve	O
decoder	Task
performance	O
relative	O
to	O
unmasked	O
self	Method
-	Method
attention	Method
.	O
paragraph	O
:	O
Positional	O
attention	O
We	O
also	O
include	O
an	O
additional	O
positional	Method
attention	Method
module	Method
in	O
each	O
decoder	Method
layer	Method
,	O
which	O
is	O
a	O
multi	Method
-	Method
head	Method
attention	Method
module	Method
with	O
the	O
same	O
general	Method
attention	Method
mechanism	Method
used	O
in	O
other	O
parts	O
of	O
the	O
Transformer	Method
network	Method
,	O
i.e.	O
where	O
is	O
the	O
model	O
hidden	O
size	O
,	O
but	O
with	O
the	O
positional	Method
encoding	Method
as	O
both	O
query	O
and	O
key	O
and	O
the	O
decoder	O
states	O
as	O
the	O
value	O
.	O
This	O
incorporates	O
positional	O
information	O
directly	O
into	O
the	O
attention	Method
process	Method
and	O
provides	O
a	O
stronger	O
positional	O
signal	O
than	O
the	O
embedding	Method
layer	Method
alone	O
.	O
We	O
also	O
hypothesize	O
that	O
this	O
additional	O
information	O
improves	O
the	O
decoder	O
’s	O
ability	O
to	O
perform	O
local	Task
reordering	Task
.	O
subsection	O
:	O
Modeling	Task
Fertility	Task
to	O
Tackle	O
the	O
Multimodality	Task
Problem	Task
The	O
multimodality	Task
problem	Task
can	O
be	O
attacked	O
by	O
introducing	O
a	O
latent	O
variable	O
to	O
directly	O
model	O
the	O
nondeterminism	O
in	O
the	O
translation	Task
process	Task
:	O
we	O
first	O
sample	O
from	O
a	O
prior	O
distribution	O
and	O
then	O
condition	O
on	O
to	O
non	O
-	O
autoregressively	Method
generate	O
a	O
translation	O
.	O
One	O
way	O
to	O
interpret	O
this	O
latent	O
variable	O
is	O
as	O
a	O
sentence	O
-	O
level	O
“	O
plan	O
”	O
akin	O
to	O
those	O
discussed	O
in	O
the	O
language	Task
production	Task
literature	Task
martin2010planning	O
.	O
There	O
are	O
several	O
desirable	O
properties	O
for	O
this	O
latent	O
variable	O
:	O
It	O
should	O
be	O
simple	O
to	O
infer	O
a	O
value	O
for	O
the	O
latent	O
variable	O
given	O
a	O
particular	O
input	O
-	O
output	O
pair	O
,	O
as	O
this	O
is	O
needed	O
to	O
train	O
the	O
model	O
end	O
-	O
to	O
-	O
end	O
.	O
Adding	O
to	O
the	O
conditioning	O
context	O
should	O
account	O
as	O
much	O
as	O
possible	O
for	O
the	O
correlations	O
across	O
time	O
between	O
different	O
outputs	O
,	O
so	O
that	O
the	O
remaining	O
marginal	O
probabilities	O
at	O
each	O
output	O
location	O
are	O
as	O
close	O
as	O
possible	O
to	O
satisfying	O
conditional	O
independence	O
.	O
It	O
should	O
not	O
account	O
for	O
the	O
variation	O
in	O
output	O
translations	O
so	O
directly	O
that	O
becomes	O
trivial	O
to	O
learn	O
,	O
since	O
that	O
is	O
the	O
function	O
our	O
decoder	Method
neural	Method
network	Method
will	O
approximate	O
.	O
The	O
factorization	O
by	O
length	O
introduced	O
in	O
Eq	O
.	O
[	O
reference	O
]	O
provides	O
a	O
very	O
weak	O
example	O
of	O
a	O
latent	Method
variable	Method
model	Method
,	O
satisfying	O
the	O
first	O
and	O
third	O
property	O
but	O
not	O
the	O
first	O
.	O
We	O
propose	O
the	O
use	O
of	O
fertilities	O
instead	O
.	O
These	O
are	O
integers	O
for	O
each	O
word	O
in	O
the	O
source	O
sentence	O
that	O
correspond	O
to	O
the	O
number	O
of	O
words	O
in	O
the	O
target	O
sentence	O
that	O
can	O
be	O
aligned	O
to	O
that	O
source	O
word	O
using	O
a	O
hard	Method
alignment	Method
algorithm	Method
like	O
IBM	Method
Model	Method
2	O
brown1993mathematics	O
.	O
One	O
of	O
the	O
most	O
important	O
properties	O
of	O
the	O
proposed	O
NAT	Method
is	O
that	O
it	O
naturally	O
introduces	O
an	O
informative	O
latent	O
variable	O
when	O
we	O
choose	O
to	O
copy	O
the	O
encoder	O
inputs	O
based	O
on	O
predicted	O
fertilities	O
.	O
More	O
precisely	O
,	O
given	O
a	O
source	O
sentence	O
,	O
the	O
conditional	O
probability	O
of	O
a	O
target	O
translation	O
is	O
:	O
where	O
is	O
the	O
set	O
of	O
all	O
fertility	O
sequences	O
—	O
one	O
fertility	O
value	O
per	O
source	O
word	O
—	O
that	O
sum	O
to	O
the	O
length	O
of	O
and	O
denotes	O
the	O
token	O
repeated	O
times	O
.	O
paragraph	O
:	O
Fertility	Task
prediction	Task
As	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
we	O
model	O
the	O
fertility	O
at	O
each	O
position	O
independently	O
using	O
a	O
one	Method
-	Method
layer	Method
neural	Method
network	Method
with	O
a	O
softmax	Method
classifier	Method
(	O
in	O
our	O
experiments	O
)	O
on	O
top	O
of	O
the	O
output	O
of	O
the	O
last	O
encoder	Method
layer	Method
.	O
This	O
models	O
the	O
way	O
that	O
fertility	O
values	O
are	O
a	O
property	O
of	O
each	O
input	O
word	O
but	O
depend	O
on	O
information	O
and	O
context	O
from	O
the	O
entire	O
sentence	O
.	O
paragraph	O
:	O
Benefits	O
of	O
fertility	O
Fertilities	Method
possess	O
all	O
three	O
of	O
the	O
properties	O
listed	O
earlier	O
as	O
desired	O
of	O
a	O
latent	O
variable	O
for	O
non	O
-	O
autoregressive	Method
machine	O
translation	O
:	O
An	O
external	Method
aligner	Method
provides	O
a	O
simple	O
and	O
fast	O
approximate	Method
inference	Method
model	Method
that	O
effectively	O
reduces	O
the	O
unsupervised	Task
training	Task
problem	Task
to	O
two	O
supervised	Task
ones	Task
.	O
Using	O
fertilities	O
as	O
a	O
latent	O
variable	O
makes	O
significant	O
progress	O
towards	O
solving	O
the	O
multimodality	Task
problem	Task
by	O
providing	O
a	O
natural	O
factorization	O
of	O
the	O
output	O
space	O
.	O
Given	O
a	O
source	O
sentence	O
,	O
restricting	O
the	O
output	O
distribution	O
to	O
those	O
target	O
sentences	O
consistent	O
with	O
a	O
particular	O
fertility	O
sequence	O
dramatically	O
reduces	O
the	O
mode	O
space	O
.	O
Furthermore	O
,	O
the	O
global	O
choice	O
of	O
mode	O
is	O
factored	O
into	O
a	O
set	O
of	O
local	O
mode	O
choices	O
:	O
namely	O
,	O
how	O
to	O
translate	O
each	O
input	O
word	O
.	O
These	O
local	O
mode	O
choices	O
can	O
be	O
effectively	O
supervised	O
because	O
the	O
fertilities	O
provide	O
a	O
fixed	O
“	O
scaffold	O
.	O
”	O
Including	O
both	O
fertilities	O
and	O
reordering	O
in	O
the	O
latent	O
variable	O
would	O
provide	O
complete	O
alignment	O
statistics	O
.	O
This	O
would	O
make	O
the	O
decoding	O
function	O
trivially	O
easy	O
to	O
approximate	O
given	O
the	O
latent	O
variable	O
and	O
force	O
all	O
of	O
the	O
modeling	O
complexity	O
into	O
the	O
encoder	Method
.	O
Using	O
fertilities	Method
alone	O
allows	O
the	O
decoder	O
to	O
take	O
some	O
of	O
this	O
burden	O
off	O
of	O
the	O
encoder	O
.	O
Our	O
use	O
of	O
fertilities	O
as	O
a	O
latent	O
variable	O
also	O
means	O
that	O
there	O
is	O
no	O
need	O
to	O
have	O
a	O
separate	O
means	O
of	O
explicitly	O
modeling	O
the	O
length	O
of	O
the	O
translation	O
,	O
which	O
is	O
simply	O
the	O
sum	O
of	O
fertilities	O
.	O
And	O
fertilities	O
provide	O
a	O
powerful	O
way	O
to	O
condition	O
the	O
decoding	Method
process	Method
,	O
allowing	O
the	O
model	O
to	O
generate	O
diverse	O
translations	O
by	O
sampling	O
over	O
the	O
fertility	O
space	O
.	O
subsection	O
:	O
Translation	Method
Predictor	Method
and	O
the	O
Decoding	Task
Process	Task
At	O
inference	O
time	O
,	O
the	O
model	O
can	O
identify	O
the	O
translation	O
with	O
the	O
highest	O
conditional	O
probability	O
(	O
see	O
Eq	O
.	O
[	O
reference	O
]	O
)	O
by	O
marginalizing	O
over	O
all	O
possible	O
latent	O
fertility	O
sequences	O
.	O
Given	O
a	O
fertility	O
sequence	O
,	O
however	O
,	O
identifying	O
the	O
optimal	Task
translation	Task
only	O
requires	O
independently	O
maximizing	O
the	O
local	O
probability	O
for	O
each	O
output	O
position	O
.	O
We	O
define	O
to	O
represent	O
the	O
optimal	Task
translation	Task
given	O
a	O
source	O
sentence	O
and	O
a	O
sequence	O
of	O
fertility	O
values	O
.	O
But	O
searching	O
and	O
marginalizing	O
over	O
the	O
whole	O
fertility	O
space	O
is	O
still	O
intractable	O
.	O
We	O
propose	O
three	O
heuristic	Method
decoding	Method
algorithms	Method
to	O
reduce	O
the	O
search	O
space	O
of	O
the	O
NAT	Method
model	O
:	O
paragraph	O
:	O
Argmax	Method
decoding	Method
Since	O
the	O
fertility	O
sequence	O
is	O
also	O
modeled	O
with	O
a	O
conditionally	Method
independent	Method
factorization	Method
,	O
we	O
can	O
simply	O
estimate	O
the	O
best	O
translation	O
by	O
choosing	O
the	O
highest	O
-	O
probability	O
fertility	O
for	O
each	O
input	O
word	O
:	O
paragraph	O
:	O
Average	Method
decoding	Method
We	O
can	O
also	O
estimate	O
each	O
fertility	O
as	O
the	O
expectation	O
of	O
its	O
corresponding	O
softmax	O
distribution	O
:	O
paragraph	O
:	O
Noisy	Method
parallel	Method
decoding	Method
(	O
NPD	Method
)	O
A	O
more	O
accurate	O
approximation	O
of	O
the	O
true	O
optimum	O
of	O
the	O
target	O
distribution	O
,	O
inspired	O
by	O
cho2016noisy	O
,	O
is	O
to	O
draw	O
samples	O
from	O
the	O
fertility	O
space	O
and	O
compute	O
the	O
best	O
translation	O
for	O
each	O
fertility	O
sequence	O
.	O
We	O
can	O
then	O
use	O
the	O
autoregressive	Method
teacher	O
to	O
identify	O
the	O
best	O
overall	O
translation	O
:	O
Note	O
that	O
,	O
when	O
using	O
an	O
autoregressive	Method
model	O
as	O
a	O
scoring	Method
function	Method
for	O
a	O
set	O
of	O
decoded	O
translations	O
,	O
it	O
can	O
run	O
as	O
fast	O
as	O
it	O
does	O
at	O
train	O
time	O
because	O
it	O
can	O
be	O
provided	O
with	O
all	O
decoder	O
inputs	O
in	O
parallel	O
.	O
NPD	Method
is	O
a	O
stochastic	Method
search	Method
method	Method
,	O
and	O
it	O
also	O
increases	O
the	O
computational	Metric
resources	Metric
required	O
linearly	O
by	O
the	O
sample	O
size	O
.	O
However	O
,	O
because	O
all	O
the	O
search	O
samples	O
can	O
be	O
computed	O
and	O
scored	O
entirely	O
independently	O
,	O
the	O
process	O
only	O
doubles	O
the	O
latency	Metric
compared	O
to	O
computing	O
a	O
single	O
translation	O
if	O
sufficient	O
parallelism	O
is	O
available	O
.	O
section	O
:	O
Training	O
The	O
proposed	O
NAT	Method
contains	O
a	O
discrete	O
sequential	O
latent	O
variable	O
,	O
whose	O
conditional	O
posterior	O
distribution	O
we	O
can	O
approximate	O
using	O
a	O
proposal	Method
distribution	Method
.	O
This	O
provides	O
a	O
variational	O
bound	O
for	O
the	O
overall	O
maximum	Metric
likelihood	Metric
loss	Metric
:	O
We	O
choose	O
a	O
proposal	Method
distribution	Method
defined	O
by	O
a	O
separate	O
,	O
fixed	Method
fertility	Method
model	Method
.	O
Possible	O
options	O
include	O
the	O
output	O
of	O
an	O
external	Method
aligner	Method
,	O
which	O
produces	O
a	O
deterministic	O
sequence	O
of	O
integer	O
fertilities	O
for	O
each	O
(	O
source	O
,	O
target	O
)	O
pair	O
in	O
a	O
training	O
corpus	O
,	O
or	O
fertilities	O
computed	O
from	O
the	O
attention	O
weights	O
used	O
in	O
our	O
fixed	Method
autoregressive	Method
teacher	Method
model	Method
.	O
This	O
simplifies	O
the	O
inference	Task
process	Task
considerably	O
,	O
as	O
the	O
expectation	O
over	O
is	O
deterministic	O
.	O
The	O
resulting	O
loss	O
function	O
,	O
consisting	O
of	O
the	O
two	O
bracketed	O
terms	O
in	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
allows	O
us	O
to	O
train	O
the	O
entire	O
model	O
in	O
a	O
supervised	Method
fashion	Method
,	O
using	O
the	O
inferred	O
fertilities	O
to	O
simultaneously	O
train	O
the	O
translation	Method
model	Method
and	O
supervise	O
the	O
fertility	Method
neural	Method
network	Method
model	Method
.	O
subsection	O
:	O
Sequence	Method
-	Method
Level	Method
Knowledge	Method
Distillation	Method
While	O
the	O
latent	Method
fertility	Method
model	Method
substantially	O
improves	O
the	O
ability	O
of	O
the	O
non	O
-	O
autoregressive	Method
output	O
distribution	O
to	O
approximate	O
the	O
multimodal	O
target	O
distribution	O
,	O
it	O
does	O
not	O
completely	O
solve	O
the	O
problem	O
of	O
nondeterminism	O
in	O
the	O
training	O
data	O
.	O
In	O
many	O
cases	O
,	O
there	O
are	O
multiple	O
correct	O
translations	O
consistent	O
with	O
a	O
single	O
sequence	O
of	O
fertilities	O
—	O
for	O
instance	O
,	O
both	O
“	O
Danke	O
schön	O
.	O
”	O
and	O
“	O
Vielen	O
dank	O
.	O
”	O
are	O
consistent	O
with	O
the	O
English	Material
input	Material
“	O
Thank	O
you	O
.	O
”	O
and	O
the	O
fertility	O
sequence	O
,	O
because	O
“	O
you	O
”	O
is	O
not	O
directly	O
translated	O
in	O
either	O
German	Material
sentence	Material
.	O
Thus	O
we	O
additionally	O
apply	O
sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
kim2016sequence	O
to	O
construct	O
a	O
new	O
corpus	O
by	O
training	O
an	O
autoregressive	Method
machine	O
translation	Method
model	Method
,	O
known	O
as	O
the	O
teacher	Method
,	O
on	O
an	O
existing	O
training	O
corpus	O
,	O
then	O
using	O
that	O
model	O
’s	O
greedy	O
outputs	O
as	O
the	O
targets	O
for	O
training	O
the	O
non	Method
-	Method
autoregressive	Method
student	Method
.	O
The	O
resulting	O
targets	O
are	O
less	O
noisy	O
and	O
more	O
deterministic	O
,	O
as	O
the	O
trained	O
model	O
will	O
consistently	O
translate	O
a	O
sentence	O
like	O
“	O
Thank	O
you	O
.	O
”	O
into	O
the	O
same	O
German	O
translation	O
every	O
time	O
;	O
on	O
the	O
other	O
hand	O
,	O
they	O
are	O
also	O
lower	O
in	O
quality	O
than	O
the	O
original	O
dataset	O
.	O
subsection	O
:	O
Fine	Task
-	Task
Tuning	Task
Our	O
supervised	Method
fertility	Method
model	Method
enables	O
a	O
decomposition	O
of	O
the	O
overall	O
maximum	O
likelihood	O
loss	O
into	O
translation	O
and	O
fertility	O
terms	O
,	O
but	O
it	O
has	O
some	O
drawbacks	O
compared	O
to	O
variational	Method
training	Method
.	O
In	O
particular	O
,	O
it	O
heavily	O
relies	O
on	O
the	O
deterministic	Method
,	Method
approximate	Method
inference	Method
model	Method
provided	O
by	O
the	O
external	Method
alignment	Method
system	Method
,	O
while	O
it	O
would	O
be	O
desirable	O
to	O
train	O
the	O
entire	O
model	O
,	O
including	O
the	O
fertility	Method
predictor	Method
,	O
end	O
to	O
end	O
.	O
Thus	O
we	O
propose	O
a	O
fine	Method
-	Method
tuning	Method
step	Method
after	O
training	O
the	O
NAT	Method
to	O
convergence	O
.	O
We	O
introduce	O
an	O
additional	O
loss	O
term	O
consisting	O
of	O
the	O
reverse	O
K	O
-	O
L	O
divergence	O
with	O
the	O
teacher	O
output	O
distribution	O
,	O
a	O
form	O
of	O
word	Method
-	Method
level	Method
knowledge	Method
distillation	Method
:	O
where	O
.	O
Such	O
a	O
loss	O
is	O
more	O
favorable	O
towards	O
highly	O
peaked	O
student	O
output	O
distributions	O
than	O
a	O
standard	O
cross	Metric
-	Metric
entropy	Metric
error	Metric
would	O
be	O
.	O
Then	O
we	O
train	O
the	O
whole	O
model	O
jointly	O
with	O
a	O
weighted	O
sum	O
of	O
the	O
original	O
distillation	O
loss	O
and	O
two	O
such	O
terms	O
,	O
one	O
an	O
expectation	O
over	O
the	O
predicted	O
fertility	O
distribution	O
,	O
normalized	O
with	O
a	O
baseline	O
,	O
and	O
the	O
other	O
based	O
on	O
the	O
external	Method
fertility	Method
inference	Method
model	Method
:	O
where	O
is	O
the	O
average	O
fertility	O
computed	O
by	O
Eq	O
.	O
[	O
reference	O
]	O
.	O
The	O
gradient	O
with	O
respect	O
to	O
the	O
non	O
-	O
differentiable	O
term	O
can	O
be	O
estimated	O
with	O
REINFORCE	Method
williams1992simple	O
,	O
while	O
the	O
term	O
can	O
be	O
trained	O
using	O
ordinary	Method
backpropagation	Method
.	O
section	O
:	O
Experiments	O
subsection	O
:	O
Experimental	O
Settings	O
paragraph	O
:	O
Dataset	O
We	O
evaluate	O
the	O
proposed	O
NAT	Method
on	O
three	O
widely	O
used	O
public	O
machine	O
translation	O
corpora	O
:	O
IWSLT16	O
En	Material
–	Material
De	Material
,	O
WMT14	O
En	Material
–	Material
De	Material
,	O
and	O
WMT16	Material
En	Material
–	Material
Ro	Material
.	O
We	O
use	O
IWSLT	Material
—	O
which	O
is	O
smaller	O
than	O
the	O
other	O
two	O
datasets	O
—	O
as	O
the	O
development	O
dataset	O
for	O
ablation	O
experiments	O
,	O
and	O
additionally	O
train	O
and	O
test	O
our	O
primary	O
models	O
on	O
both	O
directions	O
of	O
both	O
WMT	Material
datasets	Material
.	O
All	O
the	O
data	O
are	O
tokenized	O
and	O
segmented	O
into	O
subword	O
symbols	O
using	O
byte	Method
-	Method
pair	Method
encoding	Method
(	O
BPE	Method
)	O
sennrich2015neural	O
to	O
restrict	O
the	O
size	O
of	O
the	O
vocabulary	O
.	O
For	O
both	O
WMT	Material
datasets	Material
,	O
we	O
use	O
shared	O
BPE	O
vocabulary	O
and	O
additionally	O
share	O
encoder	Method
and	Method
decoder	Method
word	Method
embeddings	Method
;	O
for	O
IWSLT	Material
,	O
we	O
use	O
separate	O
English	O
and	O
German	O
vocabulary	O
and	O
embeddings	O
.	O
paragraph	O
:	O
Teacher	O
Sequence	Method
-	Method
level	Method
knowledge	Method
distillation	Method
is	O
applied	O
to	O
alleviate	O
multimodality	O
in	O
the	O
training	O
dataset	O
,	O
using	O
autoregressive	Method
models	O
as	O
the	O
teachers	O
.	O
The	O
same	O
teacher	Method
model	Method
used	O
for	O
distillation	Task
is	O
also	O
used	O
as	O
a	O
scoring	Method
function	Method
for	O
fine	Task
-	Task
tuning	Task
and	O
noisy	Method
parallel	Method
decoding	Method
.	O
To	O
enable	O
a	O
fair	O
comparison	O
,	O
and	O
benefit	O
from	O
its	O
high	O
translation	Metric
quality	Metric
,	O
we	O
implemented	O
the	O
autoregressive	Method
teachers	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Transformer	Method
architecture	Method
.	O
In	O
addition	O
,	O
we	O
use	O
the	O
same	O
sizes	O
and	O
hyperparameters	O
for	O
each	O
student	O
and	O
its	O
respective	O
teacher	O
,	O
with	O
the	O
exception	O
of	O
the	O
newly	O
added	O
positional	Method
self	Method
-	Method
attention	Method
and	O
fertility	Method
prediction	Method
modules	Method
.	O
paragraph	O
:	O
Preparation	O
for	O
knowledge	Method
distillation	Method
We	O
first	O
train	O
all	O
teacher	Method
models	Method
using	O
maximum	Method
likelihood	Method
,	O
then	O
freeze	O
their	O
parameters	O
.	O
To	O
avoid	O
the	O
redundancy	O
of	O
running	O
fixed	Method
teacher	Method
models	Method
repeatedly	O
on	O
the	O
same	O
data	O
,	O
we	O
decode	O
the	O
entire	O
training	O
set	O
once	O
using	O
each	O
teacher	O
to	O
create	O
a	O
new	O
training	O
dataset	O
for	O
its	O
respective	O
student	O
.	O
paragraph	O
:	O
Encoder	Method
initialization	Method
We	O
find	O
it	O
helpful	O
to	O
initialize	O
the	O
weights	O
in	O
the	O
NAT	Method
student	O
’s	O
encoder	O
with	O
the	O
encoder	O
weights	O
from	O
its	O
teacher	O
,	O
as	O
the	O
autoregressive	Method
and	O
non	Method
-	Method
autoregressive	Method
models	Method
share	O
the	O
same	O
encoder	O
input	O
and	O
architecture	O
.	O
paragraph	O
:	O
Fertility	Task
supervision	Task
during	O
training	O
As	O
described	O
above	O
,	O
we	O
supervise	O
the	O
fertility	O
predictions	O
at	O
train	O
time	O
by	O
using	O
a	O
fixed	Method
aligner	Method
as	O
a	O
fertility	Method
inference	Method
function	Method
.	O
We	O
use	O
the	O
fast_alignhttps:	O
//	O
github.com	O
/	O
clab	O
/	O
fast_align	O
implementation	O
of	O
IBM	Method
Model	Method
2	Method
for	O
this	O
purpose	O
,	O
with	O
default	O
parameters	O
dyer2013simple	O
.	O
paragraph	O
:	O
Hyperparameters	O
For	O
experiments	O
on	O
WMT	Material
datasets	Material
,	O
we	O
use	O
the	O
hyperparameter	O
settings	O
of	O
the	O
base	Method
Transformer	Method
model	Method
described	O
in	O
vaswani2017attention	O
,	O
though	O
without	O
label	Method
smoothing	Method
.	O
As	O
IWSLT	Material
is	O
a	O
smaller	O
corpus	O
,	O
and	O
to	O
reduce	O
training	O
time	O
,	O
we	O
use	O
a	O
set	O
of	O
smaller	O
hyperparameters	O
(	O
,	O
and	O
)	O
for	O
all	O
experiments	O
on	O
that	O
dataset	O
.	O
For	O
fine	Task
-	Task
tuning	Task
we	O
use	O
.	O
paragraph	O
:	O
Evaluation	Metric
metrics	Metric
We	O
evaluate	O
using	O
tokenized	O
and	O
cased	O
BLEU	Metric
scores	Metric
papineni2002bleu	O
.	O
paragraph	O
:	O
Implementation	O
We	O
have	O
open	O
-	O
sourced	O
our	O
PyTorch	Method
implementation	Method
of	O
the	O
NAT	Method
.	O
subsection	O
:	O
Results	O
Across	O
the	O
three	O
datasets	O
we	O
used	O
,	O
the	O
NAT	Method
performs	O
between	O
2	O
-	O
5	O
BLEU	Metric
points	Metric
worse	O
than	O
its	O
autoregressive	Method
teacher	O
,	O
with	O
part	O
or	O
all	O
of	O
this	O
gap	O
addressed	O
by	O
the	O
use	O
of	O
noisy	Method
parallel	Method
decoding	Method
.	O
In	O
the	O
case	O
of	O
WMT16	Material
English	O
–	O
Romanian	O
,	O
NPD	Method
improves	O
the	O
performance	O
of	O
our	O
non	O
-	O
autoregressive	Method
model	O
to	O
within	O
0.2	O
BLEU	Metric
points	Metric
of	O
the	O
previous	O
overall	O
state	O
of	O
the	O
art	O
gehring2017convolutional	O
.	O
Comparing	O
latencies	O
on	O
the	O
development	O
model	O
shows	O
a	O
speedup	O
of	O
more	O
than	O
a	O
factor	O
of	O
10	O
over	O
greedy	O
autoregressive	Method
decoding	O
,	O
or	O
a	O
factor	O
of	O
15	O
over	O
beam	Method
search	Method
.	O
Latencies	O
for	O
decoding	Task
with	O
NPD	Method
,	O
regardless	O
of	O
sample	O
size	O
,	O
could	O
be	O
reduced	O
to	O
about	O
80ms	O
by	O
parallelizing	O
across	O
multiple	O
GPUs	O
because	O
each	O
sample	O
can	O
be	O
generated	O
,	O
then	O
scored	O
,	O
independently	O
from	O
the	O
others	O
.	O
subsection	O
:	O
Ablation	Task
Study	Task
We	O
also	O
conduct	O
an	O
extensive	O
ablation	Task
study	Task
with	O
the	O
proposed	O
NAT	Method
on	O
the	O
IWSLT	Material
dataset	O
.	O
First	O
,	O
we	O
note	O
that	O
the	O
model	O
fails	O
to	O
train	O
when	O
provided	O
with	O
only	O
positional	O
embeddings	O
as	O
input	O
to	O
the	O
decoder	Method
.	O
Second	O
,	O
we	O
see	O
that	O
training	O
on	O
the	O
distillation	O
corpus	O
rather	O
than	O
the	O
ground	O
truth	O
provides	O
a	O
fairly	O
consistent	O
improvement	O
of	O
around	O
5	O
BLEU	Metric
points	Metric
.	O
Third	O
,	O
switching	O
from	O
uniform	O
copying	O
of	O
source	O
inputs	O
to	O
fertility	Method
-	Method
based	Method
copying	Method
improves	O
performance	O
by	O
four	O
BLEU	Metric
points	Metric
when	O
using	O
ground	Metric
-	Metric
truth	Metric
training	Metric
or	O
two	O
when	O
using	O
distillation	Method
.	O
Fine	Method
-	Method
tuning	Method
does	O
not	O
converge	O
with	O
reinforcement	Method
learning	Method
alone	O
,	O
or	O
with	O
the	O
term	O
alone	O
,	O
but	O
use	O
of	O
all	O
three	O
fine	Method
-	Method
tuning	Method
terms	Method
together	O
leads	O
to	O
an	O
improvement	O
of	O
around	O
1.5	O
BLEU	Metric
points	Metric
.	O
Training	O
the	O
student	Method
model	Method
from	O
a	O
distillation	O
corpus	O
produced	O
using	O
beam	Method
search	Method
is	O
similar	O
to	O
training	O
from	O
the	O
greedily	O
-	O
distilled	O
corpus	O
.	O
We	O
include	O
two	O
examples	O
of	O
translations	O
from	O
the	O
IWSLT	Material
development	O
set	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
Instances	O
of	O
repeated	O
words	O
or	O
phrases	O
,	O
highlighted	O
in	O
gray	O
,	O
are	O
most	O
prevalent	O
in	O
the	O
non	O
-	O
autoregressive	Method
output	O
for	O
the	O
relatively	O
complex	O
first	O
example	O
sentence	O
.	O
Two	O
pairs	O
of	O
repeated	O
words	O
in	O
the	O
first	O
example	O
,	O
as	O
well	O
as	O
a	O
pair	O
in	O
the	O
second	O
,	O
are	O
not	O
present	O
in	O
the	O
versions	O
with	O
noisy	Method
parallel	Method
decoding	Method
,	O
suggesting	O
that	O
NPD	Method
scoring	O
using	O
the	O
teacher	Method
model	Method
can	O
filter	O
out	O
such	O
mistakes	O
.	O
The	O
translations	O
produced	O
by	O
the	O
NAT	Method
with	O
NPD	Method
,	O
while	O
of	O
a	O
similar	O
quality	O
to	O
those	O
produced	O
by	O
the	O
autoregressive	Method
model	O
,	O
are	O
also	O
noticeably	O
more	O
literal	O
.	O
We	O
also	O
show	O
an	O
example	O
of	O
the	O
noisy	Method
parallel	Method
decoding	Method
process	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
demonstrating	O
the	O
diversity	O
of	O
translations	O
that	O
can	O
be	O
found	O
by	O
sampling	O
from	O
the	O
fertility	O
space	O
.	O
section	O
:	O
Conclusion	O
We	O
introduce	O
a	O
latent	Method
variable	Method
model	Method
for	O
non	O
-	O
autoregressive	Method
machine	O
translation	O
that	O
enables	O
a	O
decoder	Method
based	O
on	O
vaswani2017attention	O
to	O
take	O
full	O
advantage	O
of	O
its	O
exceptional	O
degree	O
of	O
internal	O
parallelism	O
even	O
at	O
inference	O
time	O
.	O
As	O
a	O
result	O
,	O
we	O
measure	O
translation	Metric
latencies	Metric
of	O
one	O
-	O
tenth	O
that	O
of	O
an	O
equal	O
-	O
sized	O
autoregressive	Method
model	O
,	O
while	O
maintaining	O
competitive	O
BLEU	Metric
scores	Metric
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Schematic	O
and	O
Analysis	O
