document	O
:	O
Teaching	Task
Machines	Task
to	O
Read	O
and	O
Comprehend	O
Teaching	Task
machines	Task
to	O
read	O
natural	O
language	O
documents	O
remains	O
an	O
elusive	O
challenge	O
.	O
Machine	Task
reading	Task
systems	Task
can	O
be	O
tested	O
on	O
their	O
ability	O
to	O
answer	O
questions	O
posed	O
on	O
the	O
contents	O
of	O
documents	O
that	O
they	O
have	O
seen	O
,	O
but	O
until	O
now	O
large	O
scale	O
training	O
and	O
test	O
datasets	O
have	O
been	O
missing	O
for	O
this	O
type	O
of	O
evaluation	Task
.	O
In	O
this	O
work	O
we	O
define	O
a	O
new	O
methodology	O
that	O
resolves	O
this	O
bottleneck	O
and	O
provides	O
large	Task
scale	Task
supervised	Task
reading	Task
comprehension	Task
data	Task
.	O
This	O
allows	O
us	O
to	O
develop	O
a	O
class	O
of	O
attention	Method
based	Method
deep	Method
neural	Method
networks	Method
that	O
learn	O
to	O
read	O
real	O
documents	O
and	O
answer	O
complex	O
questions	O
with	O
minimal	O
prior	O
knowledge	O
of	O
language	O
structure	O
.	O
figuret	O
section	O
:	O
Introduction	O
Progress	O
on	O
the	O
path	O
from	O
shallow	Method
bag	Method
-	Method
of	Method
-	Method
words	Method
information	Method
retrieval	Method
algorithms	Method
to	O
machines	O
capable	O
of	O
reading	O
and	O
understanding	Task
documents	Task
has	O
been	O
slow	O
.	O
Traditional	O
approaches	O
to	O
machine	Task
reading	Task
and	O
comprehension	O
have	O
been	O
based	O
on	O
either	O
hand	Method
engineered	Method
grammars	Method
,	O
or	O
information	Method
extraction	Method
methods	Method
of	O
detecting	Task
predicate	Task
argument	Task
triples	Task
that	O
can	O
later	O
be	O
queried	O
as	O
a	O
relational	O
database	O
.	O
Supervised	Method
machine	Method
learning	Method
approaches	Method
have	O
largely	O
been	O
absent	O
from	O
this	O
space	O
due	O
to	O
both	O
the	O
lack	O
of	O
large	O
scale	O
training	O
datasets	O
,	O
and	O
the	O
difficulty	O
in	O
structuring	O
statistical	Method
models	Method
flexible	O
enough	O
to	O
learn	O
to	O
exploit	O
document	O
structure	O
.	O
While	O
obtaining	O
supervised	O
natural	O
language	O
reading	O
comprehension	O
data	O
has	O
proved	O
difficult	O
,	O
some	O
researchers	O
have	O
explored	O
generating	O
synthetic	O
narratives	O
and	O
queries	O
.	O
Such	O
approaches	O
allow	O
the	O
generation	O
of	O
almost	O
unlimited	O
amounts	O
of	O
supervised	O
data	O
and	O
enable	O
researchers	O
to	O
isolate	O
the	O
performance	O
of	O
their	O
algorithms	O
on	O
individual	O
simulated	O
phenomena	O
.	O
Work	O
on	O
such	O
data	O
has	O
shown	O
that	O
neural	Method
network	Method
based	Method
models	Method
hold	O
promise	O
for	O
modelling	O
reading	Task
comprehension	Task
,	O
something	O
that	O
we	O
will	O
build	O
upon	O
here	O
.	O
Historically	O
,	O
however	O
,	O
many	O
similar	O
approaches	O
in	O
Computational	Task
Linguistics	Task
have	O
failed	O
to	O
manage	O
the	O
transition	O
from	O
synthetic	O
data	O
to	O
real	O
environments	O
,	O
as	O
such	O
closed	O
worlds	O
inevitably	O
fail	O
to	O
capture	O
the	O
complexity	O
,	O
richness	O
,	O
and	O
noise	O
of	O
natural	O
language	O
.	O
In	O
this	O
work	O
we	O
seek	O
to	O
directly	O
address	O
the	O
lack	O
of	O
real	O
natural	O
language	O
training	O
data	O
by	O
introducing	O
a	O
novel	O
approach	O
to	O
building	O
a	O
supervised	Task
reading	Task
comprehension	Task
data	Task
set	Task
.	O
We	O
observe	O
that	O
summary	O
and	O
paraphrase	O
sentences	O
,	O
with	O
their	O
associated	O
documents	O
,	O
can	O
be	O
readily	O
converted	O
to	O
context	O
–	O
query	O
–	O
answer	O
triples	O
using	O
simple	O
entity	Method
detection	Method
and	O
anonymisation	Method
algorithms	Method
.	O
Using	O
this	O
approach	O
we	O
have	O
collected	O
two	O
new	O
corpora	O
of	O
roughly	O
a	O
million	O
news	O
stories	O
with	O
associated	O
queries	O
from	O
the	O
CNN	Material
and	Material
Daily	Material
Mail	Material
websites	Material
.	O
We	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
new	O
corpora	O
by	O
building	O
novel	O
deep	Method
learning	Method
models	Method
for	O
reading	Task
comprehension	Task
.	O
These	O
models	O
draw	O
on	O
recent	O
developments	O
for	O
incorporating	O
attention	Method
mechanisms	Method
into	O
recurrent	Method
neural	Method
network	Method
architectures	Method
.	O
This	O
allows	O
a	O
model	O
to	O
focus	O
on	O
the	O
aspects	O
of	O
a	O
document	O
that	O
it	O
believes	O
will	O
help	O
it	O
answer	O
a	O
question	O
,	O
and	O
also	O
allows	O
us	O
to	O
visualises	O
its	O
inference	Method
process	Method
.	O
We	O
compare	O
these	O
neural	Method
models	Method
to	O
a	O
range	O
of	O
baselines	O
and	O
heuristic	Metric
benchmarks	Metric
based	O
upon	O
a	O
traditional	O
frame	Method
semantic	Method
analysis	Method
provided	O
by	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
natural	Method
language	Method
processing	Method
(	O
NLP	Method
)	O
pipeline	O
.	O
Our	O
results	O
indicate	O
that	O
the	O
neural	Method
models	Method
achieve	O
a	O
higher	O
accuracy	Metric
,	O
and	O
do	O
so	O
without	O
any	O
specific	O
encoding	O
of	O
the	O
document	O
or	O
query	O
structure	O
.	O
section	O
:	O
Supervised	O
training	O
data	O
for	O
reading	Task
comprehension	Task
The	O
reading	Task
comprehension	Task
task	Task
naturally	O
lends	O
itself	O
to	O
a	O
formulation	O
as	O
a	O
supervised	Task
learning	Task
problem	Task
.	O
Specifically	O
we	O
seek	O
to	O
estimate	O
the	O
conditional	O
probability	O
,	O
where	O
is	O
a	O
context	O
document	O
,	O
a	O
query	O
relating	O
to	O
that	O
document	O
,	O
and	O
the	O
answer	O
to	O
that	O
query	O
.	O
For	O
a	O
focused	O
evaluation	O
we	O
wish	O
to	O
be	O
able	O
to	O
exclude	O
additional	O
information	O
,	O
such	O
as	O
world	O
knowledge	O
gained	O
from	O
co	O
-	O
occurrence	O
statistics	O
,	O
in	O
order	O
to	O
test	O
a	O
model	O
’s	O
core	O
capability	O
to	O
detect	O
and	O
understand	O
the	O
linguistic	O
relationships	O
between	O
entities	O
in	O
the	O
context	O
document	O
.	O
Such	O
an	O
approach	O
requires	O
a	O
large	O
training	O
corpus	O
of	O
document	O
–	O
query	Task
–	Task
answer	Task
triples	Task
and	O
until	O
now	O
such	O
corpora	O
have	O
been	O
limited	O
to	O
hundreds	O
of	O
examples	O
and	O
thus	O
mostly	O
of	O
use	O
only	O
for	O
testing	O
.	O
This	O
limitation	O
has	O
meant	O
that	O
most	O
work	O
in	O
this	O
area	O
has	O
taken	O
the	O
form	O
of	O
unsupervised	Method
approaches	Method
which	O
use	O
templates	O
or	O
syntactic	Method
/	Method
semantic	Method
analysers	Method
to	O
extract	O
relation	O
tuples	O
from	O
the	O
document	O
to	O
form	O
a	O
knowledge	Method
graph	Method
that	O
can	O
be	O
queried	O
.	O
Here	O
we	O
propose	O
a	O
methodology	O
for	O
creating	O
real	Task
-	Task
world	Task
,	Task
large	Task
scale	Task
supervised	Task
training	Task
data	Task
for	O
learning	Task
reading	Task
comprehension	Task
models	Task
.	O
Inspired	O
by	O
work	O
in	O
summarisation	Task
,	O
we	O
create	O
two	O
machine	Task
reading	Task
corpora	O
by	O
exploiting	O
online	O
newspaper	O
articles	O
and	O
their	O
matching	O
summaries	O
.	O
We	O
have	O
collected	O
93k	O
articles	O
from	O
the	O
CNN	Material
and	O
220k	O
articles	O
from	O
the	O
Daily	Material
Mail	Material
websites	Material
.	O
Both	O
news	O
providers	O
supplement	O
their	O
articles	O
with	O
a	O
number	O
of	O
bullet	O
points	O
,	O
summarising	O
aspects	O
of	O
the	O
information	O
contained	O
in	O
the	O
article	O
.	O
Of	O
key	O
importance	O
is	O
that	O
these	O
summary	O
points	O
are	O
abstractive	O
and	O
do	O
not	O
simply	O
copy	O
sentences	O
from	O
the	O
documents	O
.	O
We	O
construct	O
a	O
corpus	O
of	O
document	O
–	O
query	O
–	O
answer	O
triples	O
by	O
turning	O
these	O
bullet	O
points	O
into	O
Cloze	O
style	O
questions	O
by	O
replacing	O
one	O
entity	O
at	O
a	O
time	O
with	O
a	O
placeholder	O
.	O
This	O
results	O
in	O
a	O
combined	O
corpus	O
of	O
roughly	O
1	O
M	O
data	O
points	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
Code	O
to	O
replicate	O
our	O
datasets	O
—	O
and	O
to	O
apply	O
this	O
method	O
to	O
other	O
sources	O
—	O
is	O
available	O
online	O
.	O
Corpus	O
statistics	O
.	O
Articles	O
were	O
collected	O
starting	O
in	O
April	O
2007	O
for	O
CNN	Material
and	O
June	O
2010	O
for	O
the	O
Daily	Material
Mail	Material
,	O
both	O
until	O
the	O
end	O
of	O
April	O
2015	O
.	O
Validation	O
data	O
is	O
from	O
March	O
,	O
test	O
data	O
from	O
April	O
2015	O
.	O
Articles	O
of	O
over	O
2000	O
tokens	O
and	O
queries	O
whose	O
answer	O
entity	O
did	O
not	O
appear	O
in	O
the	O
context	O
were	O
filtered	O
out	O
.	O
Percentage	O
of	O
time	O
that	O
the	O
correct	O
answer	O
is	O
contained	O
in	O
the	O
top	O
most	O
frequent	O
entities	O
in	O
a	O
given	O
document	O
.	O
subsection	O
:	O
Entity	Task
replacement	Task
and	O
permutation	Task
Note	O
that	O
the	O
focus	O
of	O
this	O
paper	O
is	O
to	O
provide	O
a	O
corpus	O
for	O
evaluating	O
a	O
model	O
’s	O
ability	O
to	O
read	O
and	O
comprehend	O
a	O
single	O
document	O
,	O
not	O
world	O
knowledge	O
or	O
co	O
-	O
occurrence	O
.	O
To	O
understand	O
that	O
distinction	O
consider	O
for	O
instance	O
the	O
following	O
Cloze	O
form	O
queries	O
(	O
created	O
from	O
headlines	O
in	O
the	O
Daily	Material
Mail	Material
validation	Material
set	Material
)	O
:	O
An	O
ngram	Method
language	Method
model	Method
trained	O
on	O
the	O
Daily	Material
Mail	Material
would	O
easily	O
correctly	O
predict	O
that	O
(	O
X	O
=	O
cancer	O
)	O
,	O
regardless	O
of	O
the	O
contents	O
of	O
the	O
context	O
document	O
,	O
simply	O
because	O
this	O
is	O
a	O
very	O
frequently	O
cured	O
entity	O
in	O
the	O
Daily	Material
Mail	Material
corpus	Material
.	O
To	O
prevent	O
such	O
degenerate	O
solutions	O
and	O
create	O
a	O
focused	Task
task	Task
we	O
anonymise	O
and	O
randomise	O
our	O
corpora	O
with	O
the	O
following	O
procedure	O
,	O
Compare	O
the	O
original	O
and	O
anonymised	O
version	O
of	O
the	O
example	O
in	O
Table	O
[	O
reference	O
]	O
.	O
Clearly	O
a	O
human	O
reader	O
can	O
answer	O
both	O
queries	O
correctly	O
.	O
However	O
in	O
the	O
anonymised	O
setup	O
the	O
context	O
document	O
is	O
required	O
for	O
answering	O
the	O
query	O
,	O
whereas	O
the	O
original	O
version	O
could	O
also	O
be	O
answered	O
by	O
someone	O
with	O
the	O
requisite	O
background	O
knowledge	O
.	O
Therefore	O
,	O
following	O
this	O
procedure	O
,	O
the	O
only	O
remaining	O
strategy	O
for	O
answering	Task
questions	Task
is	O
to	O
do	O
so	O
by	O
exploiting	O
the	O
context	O
presented	O
with	O
each	O
question	O
.	O
Thus	O
performance	O
on	O
our	O
two	O
corpora	O
truly	O
measures	O
reading	Metric
comprehension	Metric
capability	Metric
.	O
Naturally	O
a	O
production	Task
system	Task
would	O
benefit	O
from	O
using	O
all	O
available	O
information	O
sources	O
,	O
such	O
as	O
clues	O
through	O
language	O
and	O
co	O
-	O
occurrence	O
statistics	O
.	O
Table	O
[	O
reference	O
]	O
gives	O
an	O
indication	O
of	O
the	O
difficulty	O
of	O
the	O
task	O
,	O
showing	O
how	O
frequent	O
the	O
correct	O
answer	O
is	O
contained	O
in	O
the	O
top	O
entity	O
markers	O
in	O
a	O
given	O
document	O
.	O
Note	O
that	O
our	O
models	O
do	O
n’t	O
distinguish	O
between	O
entity	O
markers	O
and	O
regular	O
words	O
.	O
This	O
makes	O
the	O
task	O
harder	O
and	O
the	O
models	O
more	O
general	O
.	O
section	O
:	O
Models	O
So	O
far	O
we	O
have	O
motivated	O
the	O
need	O
for	O
better	O
datasets	O
and	O
tasks	O
to	O
evaluate	O
the	O
capabilities	O
of	O
machine	Task
reading	Task
models	O
.	O
We	O
proceed	O
by	O
describing	O
a	O
number	O
of	O
baselines	O
,	O
benchmarks	O
and	O
new	O
models	O
to	O
evaluate	O
against	O
this	O
paradigm	O
.	O
We	O
define	O
two	O
simple	O
baselines	O
,	O
the	O
majority	Method
baseline	Method
(	O
maximum	O
frequency	O
)	O
picks	O
the	O
entity	O
most	O
frequently	O
observed	O
in	O
the	O
context	O
document	O
,	O
whereas	O
the	O
exclusive	O
majority	O
(	O
exclusive	O
frequency	O
)	O
chooses	O
the	O
entity	O
most	O
frequently	O
observed	O
in	O
the	O
context	O
but	O
not	O
observed	O
in	O
the	O
query	O
.	O
The	O
idea	O
behind	O
this	O
exclusion	O
is	O
that	O
the	O
placeholder	O
is	O
unlikely	O
to	O
be	O
mentioned	O
twice	O
in	O
a	O
single	O
Cloze	O
form	O
query	O
.	O
subsection	O
:	O
Symbolic	Method
Matching	Method
Models	Method
Traditionally	O
,	O
a	O
pipeline	O
of	O
NLP	Method
models	O
has	O
been	O
used	O
for	O
attempting	O
question	Task
answering	Task
,	O
that	O
is	O
models	O
that	O
make	O
heavy	O
use	O
of	O
linguistic	O
annotation	O
,	O
structured	O
world	O
knowledge	O
and	O
semantic	Method
parsing	Method
and	O
similar	O
NLP	Method
pipeline	O
outputs	O
.	O
Building	O
on	O
these	O
approaches	O
,	O
we	O
define	O
a	O
number	O
of	O
NLP	Method
-	O
centric	O
models	O
for	O
our	O
machine	Task
reading	Task
task	O
.	O
paragraph	O
:	O
Frame	Method
-	Method
Semantic	Method
Parsing	Method
Frame	Method
-	Method
semantic	Method
parsing	Method
attempts	O
to	O
identify	O
predicates	O
and	O
their	O
arguments	O
,	O
allowing	O
models	O
access	O
to	O
information	O
about	O
“	O
who	O
did	O
what	O
to	O
whom	O
”	O
.	O
Naturally	O
this	O
kind	O
of	O
annotation	O
lends	O
itself	O
to	O
being	O
exploited	O
for	O
question	Task
answering	Task
.	O
We	O
develop	O
a	O
benchmark	O
that	O
makes	O
use	O
of	O
frame	O
-	O
semantic	O
annotations	O
which	O
we	O
obtained	O
by	O
parsing	O
our	O
model	O
with	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
frame	Method
-	Method
semantic	Method
parser	Method
.	O
As	O
the	O
parser	Method
makes	O
extensive	O
use	O
of	O
linguistic	O
information	O
we	O
run	O
these	O
benchmarks	O
on	O
the	O
unanonymised	O
version	O
of	O
our	O
corpora	O
.	O
There	O
is	O
no	O
significant	O
advantage	O
in	O
this	O
as	O
the	O
frame	Method
-	Method
semantic	Method
approach	Method
used	O
here	O
does	O
not	O
possess	O
the	O
capability	O
to	O
generalise	O
through	O
a	O
language	Method
model	Method
beyond	O
exploiting	O
one	O
during	O
the	O
parsing	Task
phase	Task
.	O
Thus	O
,	O
the	O
key	O
objective	O
of	O
evaluating	O
machine	Task
comprehension	Task
abilities	Task
is	O
maintained	O
.	O
Extracting	O
entity	O
-	O
predicate	O
triples	O
—	O
denoted	O
as	O
—from	O
both	O
the	O
query	O
and	O
context	O
document	O
,	O
we	O
attempt	O
to	O
resolve	O
queries	O
using	O
a	O
number	O
of	O
rules	O
with	O
an	O
increasing	O
recall	Metric
/	Metric
precision	Metric
trade	O
-	O
off	O
as	O
follows	O
(	O
Table	O
[	O
reference	O
]	O
)	O
.	O
For	O
reasons	O
of	O
clarity	O
,	O
we	O
pretend	O
that	O
all	O
PropBank	O
triples	O
are	O
of	O
the	O
form	O
.	O
In	O
practice	O
,	O
we	O
take	O
the	O
argument	O
numberings	O
of	O
the	O
parser	Method
into	O
account	O
and	O
only	O
compare	O
like	O
with	O
like	O
,	O
except	O
in	O
cases	O
such	O
as	O
the	O
permuted	O
frame	O
rule	O
,	O
where	O
ordering	O
is	O
relaxed	O
.	O
In	O
the	O
case	O
of	O
multiple	O
possible	O
answers	O
from	O
a	O
single	O
rule	O
,	O
we	O
randomly	O
choose	O
one	O
.	O
paragraph	O
:	O
Word	Metric
Distance	Metric
Benchmark	Metric
We	O
consider	O
another	O
baseline	O
that	O
relies	O
on	O
word	Method
distance	Method
measurements	Method
.	O
Here	O
,	O
we	O
align	O
the	O
placeholder	O
of	O
the	O
Cloze	O
form	O
question	O
with	O
each	O
possible	O
entity	O
in	O
the	O
context	O
document	O
and	O
calculate	O
a	O
distance	Metric
measure	Metric
between	O
the	O
question	O
and	O
the	O
context	O
around	O
the	O
aligned	O
entity	O
.	O
This	O
score	O
is	O
calculated	O
by	O
summing	O
the	O
distances	O
of	O
every	O
word	O
in	O
to	O
their	O
nearest	O
aligned	O
word	O
in	O
,	O
where	O
alignment	Task
is	O
defined	O
by	O
matching	O
words	O
either	O
directly	O
or	O
as	O
aligned	O
by	O
the	O
coreference	Method
system	Method
.	O
We	O
tune	O
the	O
maximum	O
penalty	O
per	O
word	O
(	O
)	O
on	O
the	O
validation	O
data	O
.	O
subsection	O
:	O
Neural	Method
Network	Method
Models	Method
Neural	Method
networks	Method
have	O
successfully	O
been	O
applied	O
to	O
a	O
range	O
of	O
tasks	O
in	O
NLP	Method
.	O
This	O
includes	O
classification	Task
tasks	Task
such	O
as	O
sentiment	Task
analysis	Task
or	O
POS	Task
tagging	Task
,	O
as	O
well	O
as	O
generative	Task
problems	Task
such	O
as	O
language	Method
modelling	Method
or	O
machine	Task
translation	Task
.	O
We	O
propose	O
three	O
neural	Method
models	Method
for	O
estimating	O
the	O
probability	Task
of	Task
word	Task
type	Task
from	O
document	Task
answering	Task
query	Task
:	O
where	O
is	O
the	O
vocabulary	O
,	O
and	O
indexes	O
row	O
of	O
weight	O
matrix	O
and	O
through	O
a	O
slight	O
abuse	O
of	O
notation	O
word	O
types	O
double	O
as	O
indexes	O
.	O
Note	O
that	O
we	O
do	O
not	O
privilege	O
entities	O
or	O
variables	O
,	O
the	O
model	O
must	O
learn	O
to	O
differentiate	O
these	O
in	O
the	O
input	O
sequence	O
.	O
The	O
function	O
returns	O
a	O
vector	Method
embedding	Method
of	O
a	O
document	O
and	O
query	O
pair	O
.	O
paragraph	O
:	O
The	O
Deep	O
LSTM	Method
Reader	O
Long	Method
short	Method
-	Method
term	Method
memory	Method
(	O
LSTM	Method
,	O
)	O
networks	O
have	O
recently	O
seen	O
considerable	O
success	O
in	O
tasks	O
such	O
as	O
machine	Task
translation	Task
and	O
language	Task
modelling	Task
.	O
When	O
used	O
for	O
translation	Task
,	O
Deep	Method
LSTMs	Method
have	O
shown	O
a	O
remarkable	O
ability	O
to	O
embed	O
long	O
sequences	O
into	O
a	O
vector	Method
representation	Method
which	O
contains	O
enough	O
information	O
to	O
generate	O
a	O
full	O
translation	O
in	O
another	O
language	O
.	O
Our	O
first	O
neural	Method
model	Method
for	O
reading	Task
comprehension	Task
tests	O
the	O
ability	O
of	O
Deep	O
LSTM	Method
encoders	O
to	O
handle	O
significantly	O
longer	O
sequences	O
.	O
We	O
feed	O
our	O
documents	O
one	O
word	O
at	O
a	O
time	O
into	O
a	O
Deep	O
LSTM	Method
encoder	O
,	O
after	O
a	O
delimiter	O
we	O
then	O
also	O
feed	O
the	O
query	O
into	O
the	O
encoder	O
.	O
Alternatively	O
we	O
also	O
experiment	O
with	O
processing	O
the	O
query	O
then	O
the	O
document	O
.	O
The	O
result	O
is	O
that	O
this	O
model	O
processes	O
each	O
document	O
query	O
pair	O
as	O
a	O
single	O
long	O
sequence	O
.	O
Given	O
the	O
embedded	O
document	O
and	O
query	O
the	O
network	O
predicts	O
which	O
token	O
in	O
the	O
document	O
answers	O
the	O
query	O
.	O
We	O
employ	O
a	O
Deep	O
LSTM	Method
cell	O
with	O
skip	O
connections	O
from	O
each	O
input	O
to	O
every	O
hidden	O
layer	O
,	O
and	O
from	O
every	O
hidden	O
layer	O
to	O
the	O
output	O
:	O
where	O
indicates	O
vector	O
concatenation	O
is	O
the	O
hidden	O
state	O
for	O
layer	O
at	O
time	O
,	O
and	O
,	O
,	O
are	O
the	O
input	O
,	O
forget	O
,	O
and	O
output	O
gates	O
respectively	O
.	O
Thus	O
our	O
Deep	O
LSTM	Method
Reader	O
is	O
defined	O
by	O
with	O
input	O
the	O
concatenation	O
of	O
and	O
separated	O
by	O
the	O
delimiter	O
.	O
paragraph	O
:	O
The	O
Attentive	Method
Reader	Method
[	O
b	O
]	O
0.49	O
[	O
b	O
]	O
0.49	O
[	O
b	O
]	O
1.0	O
The	O
Deep	O
LSTM	Method
Reader	O
must	O
propagate	O
dependencies	O
over	O
long	O
distances	O
in	O
order	O
to	O
connect	O
queries	O
to	O
their	O
answers	O
.	O
The	O
fixed	O
width	O
hidden	O
vector	O
forms	O
a	O
bottleneck	O
for	O
this	O
information	Task
flow	Task
that	O
we	O
propose	O
to	O
circumvent	O
using	O
an	O
attention	Method
mechanism	Method
inspired	O
by	O
recent	O
results	O
in	O
translation	Task
and	Task
image	Task
recognition	Task
.	O
This	O
attention	Method
model	Method
first	O
encodes	O
the	O
document	O
and	O
the	O
query	O
using	O
separate	O
bidirectional	Method
single	Method
layer	Method
LSTMs	Method
.	O
We	O
denote	O
the	O
outputs	O
of	O
the	O
forward	Method
and	Method
backward	Method
LSTMs	Method
as	O
and	O
respectively	O
.	O
The	O
encoding	O
of	O
a	O
query	O
of	O
length	O
is	O
formed	O
by	O
the	O
concatenation	O
of	O
the	O
final	O
forward	O
and	O
backward	O
outputs	O
,	O
For	O
the	O
document	O
the	O
composite	O
output	O
for	O
each	O
token	O
at	O
position	O
is	O
,	O
The	O
representation	O
of	O
the	O
document	O
is	O
formed	O
by	O
a	O
weighted	O
sum	O
of	O
these	O
output	O
vectors	O
.	O
These	O
weights	O
are	O
interpreted	O
as	O
the	O
degree	O
to	O
which	O
the	O
network	O
attends	O
to	O
a	O
particular	O
token	O
in	O
the	O
document	O
when	O
answering	O
the	O
query	O
:	O
where	O
we	O
are	O
interpreting	O
as	O
a	O
matrix	O
with	O
each	O
column	O
being	O
the	O
composite	O
representation	O
of	O
document	O
token	O
.	O
The	O
variable	O
is	O
the	O
normalised	O
attention	O
at	O
token	O
.	O
Given	O
this	O
attention	O
score	O
the	O
embedding	O
of	O
the	O
document	O
is	O
computed	O
as	O
the	O
weighted	O
sum	O
of	O
the	O
token	Method
embeddings	Method
.	O
The	O
model	O
is	O
completed	O
with	O
the	O
definition	O
of	O
the	O
joint	Task
document	Task
and	Task
query	Task
embedding	Task
via	O
a	O
non	Method
-	Method
linear	Method
combination	Method
:	O
The	O
Attentive	Method
Reader	Method
can	O
be	O
viewed	O
as	O
a	O
generalisation	O
of	O
the	O
application	O
of	O
Memory	Method
Networks	Method
to	O
question	Task
answering	Task
.	O
That	O
model	O
employs	O
an	O
attention	Method
mechanism	Method
at	O
the	O
sentence	O
level	O
where	O
each	O
sentence	O
is	O
represented	O
by	O
a	O
bag	Method
of	Method
embeddings	Method
.	O
The	O
Attentive	Method
Reader	Method
employs	O
a	O
finer	O
grained	Method
token	Method
level	Method
attention	Method
mechanism	Method
where	O
the	O
tokens	O
are	O
embedded	O
given	O
their	O
entire	O
future	O
and	O
past	O
context	O
in	O
the	O
input	O
document	O
.	O
paragraph	O
:	O
The	O
Impatient	Method
Reader	Method
The	O
Attentive	Method
Reader	Method
is	O
able	O
to	O
focus	O
on	O
the	O
passages	O
of	O
a	O
context	O
document	O
that	O
are	O
most	O
likely	O
to	O
inform	O
the	O
answer	O
to	O
the	O
query	O
.	O
We	O
can	O
go	O
further	O
by	O
equipping	O
the	O
model	O
with	O
the	O
ability	O
to	O
reread	O
from	O
the	O
document	O
as	O
each	O
query	O
token	O
is	O
read	O
.	O
At	O
each	O
token	O
of	O
the	O
query	O
the	O
model	O
computes	O
a	O
document	Method
representation	Method
vector	Method
using	O
the	O
bidirectional	Method
embedding	Method
:	O
The	O
result	O
is	O
an	O
attention	Method
mechanism	Method
that	O
allows	O
the	O
model	O
to	O
recurrently	O
accumulate	O
information	O
from	O
the	O
document	O
as	O
it	O
sees	O
each	O
query	O
token	O
,	O
ultimately	O
outputting	O
a	O
final	O
joint	Method
document	Method
query	Method
representation	Method
for	O
the	O
answer	Task
prediction	Task
,	O
section	O
:	O
Empirical	O
Evaluation	O
Having	O
described	O
a	O
number	O
of	O
models	O
in	O
the	O
previous	O
section	O
,	O
we	O
next	O
evaluate	O
these	O
models	O
on	O
our	O
reading	O
comprehension	O
corpora	O
.	O
Our	O
hypothesis	O
is	O
that	O
neural	Method
models	Method
should	O
in	O
principle	O
be	O
well	O
suited	O
for	O
this	O
task	O
.	O
However	O
,	O
we	O
argued	O
that	O
simple	O
recurrent	Method
models	Method
such	O
as	O
the	O
LSTM	Method
probably	O
have	O
insufficient	O
expressive	O
power	O
for	O
solving	O
tasks	O
that	O
require	O
complex	O
inference	Task
.	O
We	O
expect	O
that	O
the	O
attention	Method
-	Method
based	Method
models	Method
would	O
therefore	O
outperform	O
the	O
pure	O
LSTM	Method
-	O
based	O
approaches	O
.	O
Considering	O
the	O
second	O
dimension	O
of	O
our	O
investigation	O
,	O
the	O
comparison	O
of	O
traditional	O
versus	O
neural	Method
approaches	Method
to	O
NLP	Method
,	O
we	O
do	O
not	O
have	O
a	O
strong	O
prior	O
favouring	O
one	O
approach	O
over	O
the	O
other	O
.	O
While	O
numerous	O
publications	O
in	O
the	O
past	O
few	O
years	O
have	O
demonstrated	O
neural	Method
models	Method
outperforming	O
classical	O
methods	O
,	O
it	O
remains	O
unclear	O
how	O
much	O
of	O
that	O
is	O
a	O
side	O
-	O
effect	O
of	O
the	O
language	Method
modelling	Method
capabilities	Method
intrinsic	O
to	O
any	O
neural	Method
model	Method
for	O
NLP	Method
.	O
The	O
entity	Task
anonymisation	Task
and	Task
permutation	Task
aspect	Task
of	O
the	O
task	O
presented	O
here	O
may	O
end	O
up	O
levelling	O
the	O
playing	O
field	O
in	O
that	O
regard	O
,	O
favouring	O
models	O
capable	O
of	O
dealing	O
with	O
syntax	O
rather	O
than	O
just	O
semantics	O
.	O
With	O
these	O
considerations	O
in	O
mind	O
,	O
the	O
experimental	O
part	O
of	O
this	O
paper	O
is	O
designed	O
with	O
a	O
three	O
-	O
fold	O
aim	O
.	O
First	O
,	O
we	O
want	O
to	O
establish	O
the	O
difficulty	O
of	O
our	O
machine	Task
reading	Task
task	O
by	O
applying	O
a	O
wide	O
range	O
of	O
models	O
to	O
it	O
.	O
Second	O
,	O
we	O
compare	O
the	O
performance	O
of	O
parse	Method
-	Method
based	Method
methods	Method
versus	O
that	O
of	O
neural	Method
models	Method
.	O
Third	O
,	O
within	O
the	O
group	O
of	O
neural	Method
models	Method
examined	O
,	O
we	O
want	O
to	O
determine	O
what	O
each	O
component	O
contributes	O
to	O
the	O
end	O
performance	O
;	O
that	O
is	O
,	O
we	O
want	O
to	O
analyse	O
the	O
extent	O
to	O
which	O
an	O
LSTM	Method
can	O
solve	O
this	O
task	O
,	O
and	O
to	O
what	O
extent	O
various	O
attention	Method
mechanisms	Method
impact	O
performance	O
.	O
All	O
model	O
hyperparameters	O
were	O
tuned	O
on	O
the	O
respective	O
validation	O
sets	O
of	O
the	O
two	O
corpora	O
.	O
Our	O
experimental	O
results	O
are	O
in	O
Table	O
[	O
reference	O
]	O
,	O
with	O
the	O
Attentive	O
and	O
Impatient	O
Readers	O
performing	O
best	O
across	O
both	O
datasets	O
.	O
Accuracy	Metric
of	O
all	O
the	O
models	O
and	O
benchmarks	O
on	O
the	O
CNN	Material
and	Material
Daily	Material
Mail	Material
datasets	Material
.	O
The	O
Uniform	O
Reader	O
baseline	O
sets	O
all	O
of	O
the	O
⁢m	O
(	O
t	O
)	O
parameters	O
to	O
be	O
equal	O
.	O
[	O
-	O
0.3em	O
]	O
Precision@Recall	Metric
for	O
the	O
attention	Method
models	Method
on	O
the	O
CNN	Material
validation	Material
data	Material
.	O
paragraph	O
:	O
Frame	Metric
-	Metric
semantic	Metric
benchmark	Metric
While	O
the	O
one	Method
frame	Method
-	Method
semantic	Method
model	Method
proposed	O
in	O
this	O
paper	O
is	O
clearly	O
a	O
simplification	O
of	O
what	O
could	O
be	O
achieved	O
with	O
annotations	O
from	O
an	O
NLP	Method
pipeline	O
,	O
it	O
does	O
highlight	O
the	O
difficulty	O
of	O
the	O
task	O
when	O
approached	O
from	O
a	O
symbolic	O
NLP	Method
perspective	O
.	O
Two	O
issues	O
stand	O
out	O
when	O
analysing	O
the	O
results	O
in	O
detail	O
.	O
First	O
,	O
the	O
frame	Method
-	Method
semantic	Method
pipeline	Method
has	O
a	O
poor	O
degree	O
of	O
coverage	O
with	O
many	O
relations	O
not	O
being	O
picked	O
up	O
by	O
our	O
PropBank	Method
parser	Method
as	O
they	O
do	O
not	O
adhere	O
to	O
the	O
default	O
predicate	O
-	O
argument	O
structure	O
.	O
This	O
effect	O
is	O
exacerbated	O
by	O
the	O
type	O
of	O
language	O
used	O
in	O
the	O
highlights	O
that	O
form	O
the	O
basis	O
of	O
our	O
datasets	O
.	O
The	O
second	O
issue	O
is	O
that	O
the	O
frame	Method
-	Method
semantic	Method
approach	Method
does	O
not	O
trivially	O
scale	O
to	O
situations	O
where	O
several	O
sentences	O
,	O
and	O
thus	O
frames	O
,	O
are	O
required	O
to	O
answer	O
a	O
query	O
.	O
This	O
was	O
true	O
for	O
the	O
majority	O
of	O
queries	O
in	O
the	O
dataset	O
.	O
paragraph	O
:	O
Word	Metric
distance	Metric
benchmark	Metric
More	O
surprising	O
perhaps	O
is	O
the	O
relatively	O
strong	O
performance	O
of	O
the	O
word	Metric
distance	Metric
benchmark	Metric
,	O
particularly	O
relative	O
to	O
the	O
frame	Task
-	Task
semantic	Task
benchmark	Task
,	O
which	O
we	O
had	O
expected	O
to	O
perform	O
better	O
.	O
Here	O
,	O
again	O
,	O
the	O
nature	O
of	O
the	O
datasets	O
used	O
can	O
explain	O
aspects	O
of	O
this	O
result	O
.	O
Where	O
the	O
frame	Method
-	Method
semantic	Method
model	Method
suffered	O
due	O
to	O
the	O
language	O
used	O
in	O
the	O
highlights	O
,	O
the	O
word	Method
distance	Method
model	Method
benefited	O
.	O
Particularly	O
in	O
the	O
case	O
of	O
the	O
Daily	Material
Mail	Material
dataset	Material
,	O
highlights	O
frequently	O
have	O
significant	O
lexical	O
overlap	O
with	O
passages	O
in	O
the	O
accompanying	O
article	O
,	O
which	O
makes	O
it	O
easy	O
for	O
the	O
word	Metric
distance	Metric
benchmark	Metric
.	O
For	O
instance	O
the	O
query	O
“	O
Tom	O
Hanks	O
is	O
friends	O
with	O
X	O
’s	O
manager	O
,	O
Scooter	O
Brown	O
”	O
has	O
the	O
phrase	O
“	O
…	O
turns	O
out	O
he	O
is	O
good	O
friends	O
with	O
Scooter	O
Brown	O
,	O
manager	O
for	O
Carly	O
Rae	O
Jepson	O
”	O
in	O
the	O
context	O
.	O
The	O
word	Method
distance	Method
benchmark	Method
correctly	O
aligns	O
these	O
two	O
while	O
the	O
frame	Method
-	Method
semantic	Method
approach	Method
fails	O
to	O
pickup	O
the	O
friendship	O
or	O
management	O
relations	O
when	O
parsing	O
the	O
query	O
.	O
We	O
expect	O
that	O
on	O
other	O
types	O
of	O
machine	Task
reading	Task
data	O
where	O
questions	O
rather	O
than	O
Cloze	O
queries	O
are	O
used	O
this	O
particular	O
model	O
would	O
perform	O
significantly	O
worse	O
.	O
paragraph	O
:	O
Neural	Method
models	Method
Within	O
the	O
group	O
of	O
neural	Method
models	Method
explored	O
here	O
,	O
the	O
results	O
paint	O
a	O
clear	O
picture	O
with	O
the	O
Impatient	O
and	O
the	O
Attentive	O
Readers	O
outperforming	O
all	O
other	O
models	O
.	O
This	O
is	O
consistent	O
with	O
our	O
hypothesis	O
that	O
attention	O
is	O
a	O
key	O
ingredient	O
for	O
machine	Task
reading	Task
and	O
question	Task
answering	Task
due	O
to	O
the	O
need	O
to	O
propagate	O
information	O
over	O
long	O
distances	O
.	O
The	O
Deep	O
LSTM	Method
Reader	O
performs	O
surprisingly	O
well	O
,	O
once	O
again	O
demonstrating	O
that	O
this	O
simple	O
sequential	Method
architecture	Method
can	O
do	O
a	O
reasonable	O
job	O
of	O
learning	O
to	O
abstract	O
long	O
sequences	O
,	O
even	O
when	O
they	O
are	O
up	O
to	O
two	O
thousand	O
tokens	O
in	O
length	O
.	O
However	O
this	O
model	O
does	O
fail	O
to	O
match	O
the	O
performance	O
of	O
the	O
attention	Method
based	Method
models	Method
,	O
even	O
though	O
these	O
only	O
use	O
single	Method
layer	Method
LSTMs	Method
.	O
The	O
poor	O
results	O
of	O
the	O
Uniform	O
Reader	O
support	O
our	O
hypothesis	O
of	O
the	O
significance	O
of	O
the	O
attention	Method
mechanism	Method
in	O
the	O
Attentive	Method
model	Method
’s	O
performance	O
as	O
the	O
only	O
difference	O
between	O
these	O
models	O
is	O
that	O
the	O
attention	O
variables	O
are	O
ignored	O
in	O
the	O
Uniform	Method
Reader	Method
.	O
The	O
precision@recall	Metric
statistics	Metric
in	O
Figure	O
[	O
reference	O
]	O
again	O
highlight	O
the	O
strength	O
of	O
the	O
attentive	Method
approach	Method
.	O
We	O
can	O
visualise	O
the	O
attention	Method
mechanism	Method
as	O
a	O
heatmap	Method
over	O
a	O
context	O
document	O
to	O
gain	O
further	O
insight	O
into	O
the	O
models	O
’	O
performance	O
.	O
The	O
highlighted	O
words	O
show	O
which	O
tokens	O
in	O
the	O
document	O
were	O
attended	O
to	O
by	O
the	O
model	O
.	O
In	O
addition	O
we	O
must	O
also	O
take	O
into	O
account	O
that	O
the	O
vectors	O
at	O
each	O
token	O
integrate	O
long	O
range	O
contextual	O
information	O
via	O
the	O
bidirectional	O
LSTM	Method
encoders	O
.	O
Figure	O
[	O
reference	O
]	O
depicts	O
heat	O
maps	O
for	O
two	O
queries	O
that	O
were	O
correctly	O
answered	O
by	O
the	O
Attentive	Method
Reader	Method
.	O
In	O
both	O
cases	O
confidently	O
arriving	O
at	O
the	O
correct	O
answer	O
requires	O
the	O
model	O
to	O
perform	O
both	O
significant	O
lexical	Method
generalsiation	Method
,	O
e.g.	O
‘	O
killed	O
’	O
‘	O
deceased	O
’	O
,	O
and	O
co	Task
-	Task
reference	Task
or	O
anaphora	Task
resolution	Task
,	O
e.g.	O
‘	O
ent119	O
was	O
killed	O
’	O
‘	O
he	O
was	O
identified	O
.	O
’	O
However	O
it	O
is	O
also	O
clear	O
that	O
the	O
model	O
is	O
able	O
to	O
integrate	O
these	O
signals	O
with	O
rough	O
heuristic	O
indicators	O
such	O
as	O
the	O
proximity	O
of	O
query	O
words	O
to	O
the	O
candidate	O
answer	O
.	O
section	O
:	O
Conclusion	O
The	O
supervised	Method
paradigm	Method
for	O
training	O
machine	Task
reading	Task
and	O
comprehension	O
models	O
provides	O
a	O
promising	O
avenue	O
for	O
making	O
progress	O
on	O
the	O
path	O
to	O
building	O
full	Task
natural	Task
language	Task
understanding	Task
systems	Task
.	O
We	O
have	O
demonstrated	O
a	O
methodology	O
for	O
obtaining	O
a	O
large	O
number	O
of	O
document	Task
-	Task
query	Task
-	Task
answer	Task
triples	Task
and	O
shown	O
that	O
recurrent	Method
and	Method
attention	Method
based	Method
neural	Method
networks	Method
provide	O
an	O
effective	O
modelling	Method
framework	Method
for	O
this	O
task	O
.	O
Our	O
analysis	O
indicates	O
that	O
the	O
Attentive	O
and	O
Impatient	O
Readers	O
are	O
able	O
to	O
propagate	O
and	O
integrate	O
semantic	O
information	O
over	O
long	O
distances	O
.	O
In	O
particular	O
we	O
believe	O
that	O
the	O
incorporation	O
of	O
an	O
attention	Method
mechanism	Method
is	O
the	O
key	O
contributor	O
to	O
these	O
results	O
.	O
The	O
attention	Method
mechanism	Method
that	O
we	O
have	O
employed	O
is	O
just	O
one	O
instantiation	O
of	O
a	O
very	O
general	O
idea	O
which	O
can	O
be	O
further	O
exploited	O
.	O
However	O
,	O
the	O
incorporation	O
of	O
world	O
knowledge	O
and	O
multi	Task
-	Task
document	Task
queries	Task
will	O
also	O
require	O
the	O
development	O
of	O
attention	Method
and	Method
embedding	Method
mechanisms	Method
whose	O
complexity	Metric
to	O
query	O
does	O
not	O
scale	O
linearly	O
with	O
the	O
data	O
set	O
size	O
.	O
There	O
are	O
still	O
many	O
queries	O
requiring	O
complex	O
inference	Task
and	O
long	Task
range	Task
reference	Task
resolution	Task
that	O
our	O
models	O
are	O
not	O
yet	O
able	O
to	O
answer	O
.	O
As	O
such	O
our	O
data	O
provides	O
a	O
scalable	O
challenge	O
that	O
should	O
support	O
NLP	Method
research	O
into	O
the	O
future	O
.	O
Further	O
,	O
significantly	O
bigger	O
training	O
data	O
sets	O
can	O
be	O
acquired	O
using	O
the	O
techniques	O
we	O
have	O
described	O
,	O
undoubtedly	O
allowing	O
us	O
to	O
train	O
more	O
expressive	O
and	O
accurate	O
models	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Model	Method
hyperparameters	Method
The	O
precise	O
hyperparameters	O
used	O
for	O
the	O
various	O
attentive	Method
models	Method
are	O
as	O
in	O
Table	O
[	O
reference	O
]	O
.	O
All	O
models	O
were	O
trained	O
using	O
asynchronous	Method
RmsProp	Method
with	O
a	O
momentum	O
of	O
and	O
a	O
decay	O
of	O
.	O
appendix	O
:	O
Performance	O
across	O
document	Metric
length	Metric
To	O
understand	O
how	O
the	O
model	O
performance	O
depends	O
on	O
the	O
size	O
of	O
the	O
context	O
,	O
we	O
plot	O
performance	O
versus	O
document	O
lengths	O
in	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
The	O
first	O
figure	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
plots	O
a	O
sliding	O
window	O
of	O
performance	O
across	O
document	O
length	O
,	O
showing	O
that	O
performance	O
of	O
the	O
attentive	Method
models	Method
degrades	O
slightly	O
as	O
documents	O
increase	O
in	O
length	O
.	O
The	O
second	O
figure	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
shows	O
the	O
cumulative	O
performance	O
with	O
documents	O
up	O
to	O
length	O
,	O
showing	O
that	O
while	O
the	O
length	O
does	O
impact	O
the	O
models	O
’	O
performance	O
,	O
that	O
effect	O
becomes	O
negligible	O
after	O
reaching	O
a	O
length	O
of	O
~500	O
tokens	O
.	O
Precision@Document	Metric
Length	Metric
for	O
the	O
attention	Method
models	Method
on	O
the	O
CNN	Material
validation	Material
data	Material
.	O
The	O
chart	O
shows	O
the	O
precision	Metric
for	O
each	O
decile	O
in	O
document	O
lengths	O
across	O
the	O
corpus	O
as	O
well	O
as	O
the	O
precision	Metric
for	O
the	O
5	O
%	O
longest	O
articles	O
.	O
Aggregated	Metric
precision	Metric
for	O
documents	O
up	O
to	O
a	O
certain	O
lengths	O
.	O
The	O
points	O
mark	O
the	O
decile	O
in	O
document	O
lengths	O
across	O
the	O
corpus	O
.	O
appendix	O
:	O
Additional	O
Heatmap	Method
Analysis	Method
We	O
expand	O
on	O
the	O
analysis	O
of	O
the	O
attention	Method
mechanism	Method
presented	O
in	O
the	O
paper	O
by	O
including	O
visualisations	Method
for	O
additional	O
queries	O
from	O
the	O
CNN	Material
validation	Material
dataset	Material
below	O
.	O
We	O
consider	O
examples	O
from	O
the	O
Attentive	Method
Reader	Method
as	O
well	O
as	O
the	O
Impatient	Method
Reader	Method
in	O
this	O
appendix	O
.	O
subsection	O
:	O
Attentive	Method
Reader	Method
paragraph	O
:	O
Positive	O
Instances	O
Figure	O
[	O
reference	O
]	O
shows	O
two	O
positive	O
examples	O
from	O
the	O
CNN	Material
validation	Material
set	Material
that	O
require	O
reasonable	O
levels	O
of	O
lexical	O
generalisation	O
and	O
co	O
-	O
reference	O
in	O
order	O
to	O
be	O
answered	O
.	O
The	O
first	O
query	O
in	O
Figure	O
[	O
reference	O
]	O
contains	O
strong	O
lexical	O
cues	O
through	O
the	O
quote	O
,	O
but	O
requires	O
identifying	O
the	O
entity	O
quoted	O
,	O
which	O
is	O
non	O
-	O
trivial	O
in	O
the	O
context	O
document	O
.	O
The	O
final	O
positive	O
example	O
(	O
also	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
demonstrates	O
the	O
fearlessness	O
of	O
our	O
model	O
.	O
paragraph	O
:	O
Negative	O
Instances	O
Figures	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
show	O
examples	O
of	O
queries	O
where	O
the	O
Attentive	Method
Reader	Method
fails	O
to	O
select	O
the	O
correct	O
answer	O
.	O
The	O
two	O
examples	O
in	O
Figure	O
[	O
reference	O
]	O
highlight	O
a	O
fairly	O
common	O
phenomenon	O
in	O
the	O
data	O
,	O
namely	O
ambiguous	O
queries	O
,	O
where	O
—	O
at	O
least	O
following	O
the	O
anonymisation	Method
process	Method
—	O
multiple	O
entities	O
are	O
plausible	O
answers	O
even	O
when	O
evaluated	O
manually	O
.	O
Note	O
that	O
in	O
both	O
cases	O
the	O
query	O
searches	O
for	O
an	O
entity	O
marker	O
that	O
describes	O
a	O
geographic	O
location	O
,	O
preceded	O
by	O
the	O
word	O
“	O
in	O
”	O
.	O
Here	O
it	O
is	O
unclear	O
whether	O
the	O
placeholder	O
refers	O
to	O
a	O
part	O
of	O
town	O
,	O
town	O
,	O
region	O
or	O
country	O
.	O
Figure	O
[	O
reference	O
]	O
contains	O
two	O
additional	O
negative	O
cases	O
.	O
The	O
first	O
failure	O
is	O
caused	O
by	O
the	O
co	Method
-	Method
reference	Method
entity	Method
selection	Method
process	Method
.	O
The	O
correct	O
entity	O
,	O
ent15	O
,	O
and	O
the	O
predicted	O
one	O
,	O
ent81	O
,	O
both	O
refer	O
to	O
the	O
same	O
person	O
,	O
but	O
not	O
being	O
clustered	O
together	O
.	O
Arguably	O
this	O
is	O
a	O
difficult	O
clustering	O
as	O
one	O
entity	O
refers	O
to	O
“	O
Kate	O
Middleton	O
”	O
and	O
the	O
other	O
to	O
“	O
The	O
Duchess	O
of	O
Cambridge	O
”	O
.	O
The	O
right	O
example	O
shows	O
a	O
situation	O
in	O
which	O
the	O
model	O
fails	O
as	O
it	O
perhaps	O
gets	O
too	O
little	O
information	O
from	O
the	O
short	O
query	O
and	O
then	O
selects	O
the	O
wrong	O
cue	O
with	O
the	O
term	O
“	O
claims	O
”	O
near	O
the	O
wrongly	O
identified	O
entity	O
ent1	O
(	O
correct	O
:	O
ent74	O
)	O
.	O
subsection	O
:	O
Impatient	Method
Reader	Method
To	O
give	O
a	O
better	O
intuition	O
for	O
the	O
behaviour	O
of	O
the	O
Impatient	Method
Reader	Method
,	O
we	O
use	O
a	O
similar	O
visualisation	Method
technique	Method
as	O
before	O
.	O
However	O
,	O
this	O
time	O
around	O
we	O
highlight	O
the	O
attention	O
at	O
every	O
time	O
step	O
as	O
the	O
model	O
updates	O
its	O
focus	O
while	O
moving	O
through	O
a	O
given	O
query	O
.	O
Figures	O
[	O
reference	O
]	O
–	O
[	O
reference	O
]	O
shows	O
how	O
the	O
attention	O
of	O
the	O
Impatient	Method
Reader	Method
changes	O
and	O
becomes	O
increasingly	O
more	O
accurate	O
as	O
the	O
model	O
considers	O
larger	O
parts	O
of	O
the	O
query	O
.	O
Note	O
how	O
the	O
attention	O
is	O
distributed	O
fairly	O
arbitraty	O
at	O
first	O
,	O
slowly	O
focussing	O
on	O
the	O
correct	O
entity	O
ent5	O
only	O
once	O
the	O
question	O
has	O
sufficiently	O
been	O
parsed	O
.	O
