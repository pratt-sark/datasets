We	O
present	O
a	O
novel	O
neural	Method
network	Method
for	O
processing	Task
sequences	Task
.	O
The	O
ByteNet	Method
is	O
a	O
one	Method
-	Method
dimensional	Method
convolutional	Method
neural	Method
network	Method
that	O
is	O
composed	O
of	O
two	O
parts	O
,	O
one	O
to	O
encode	O
the	O
source	O
sequence	O
and	O
the	O
other	O
to	O
decode	O
the	O
target	O
sequence	O
.	O
The	O
two	O
network	O
parts	O
are	O
connected	O
by	O
stacking	O
the	O
decoder	Method
on	O
top	O
of	O
the	O
encoder	Method
and	O
preserving	O
the	O
temporal	O
resolution	O
of	O
the	O
sequences	O
.	O
To	O
address	O
the	O
differing	O
lengths	O
of	O
the	O
source	O
and	O
the	O
target	O
,	O
we	O
introduce	O
an	O
efficient	O
mechanism	O
by	O
which	O
the	O
decoder	Method
is	O
dynamically	O
unfolded	O
over	O
the	O
representation	O
of	O
the	O
encoder	Method
.	O
The	O
ByteNet	Method
uses	O
dilation	O
in	O
the	O
convolutional	Method
layers	Method
to	O
increase	O
its	O
receptive	O
field	O
.	O
The	O
resulting	O
network	O
has	O
two	O
core	O
properties	O
:	O
it	O
runs	O
in	O
time	O
that	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
sequences	O
and	O
it	O
sidesteps	O
the	O
need	O
for	O
excessive	O
memorization	O
.	O
The	O
ByteNet	Method
decoder	O
attains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
character	Task
-	Task
level	Task
language	O
modelling	O
and	O
outperforms	O
the	O
previous	O
best	O
results	O
obtained	O
with	O
recurrent	Method
networks	Method
.	O
The	O
ByteNet	Method
also	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
on	O
the	O
English	Task
-	Task
to	Task
-	Task
German	Task
WMT	Task
translation	Task
task	Task
,	O
surpassing	O
comparable	O
neural	Method
translation	Method
models	Method
that	O
are	O
based	O
on	O
recurrent	Method
networks	Method
with	O
attentional	Method
pooling	Method
and	O
run	O
in	O
quadratic	O
time	O
.	O
We	O
find	O
that	O
the	O
latent	O
alignment	O
structure	O
contained	O
in	O
the	O
representations	O
reflects	O
the	O
expected	O
alignment	O
between	O
the	O
tokens	O
.	O
NeuralMachineTranslationinLinearTime	Method
section	O
:	O
Introduction	O
In	O
neural	Task
language	Task
modelling	Task
,	O
a	O
neural	Method
network	Method
estimates	O
a	O
distribution	O
over	O
sequences	O
of	O
words	O
or	O
characters	O
that	O
belong	O
to	O
a	O
given	O
language	O
.	O
In	O
neural	Task
machine	Task
translation	Task
,	O
the	O
network	O
estimates	O
a	O
distribution	O
over	O
sequences	O
in	O
the	O
target	O
language	O
conditioned	O
on	O
a	O
given	O
sequence	O
in	O
the	O
source	O
language	O
.	O
The	O
network	O
can	O
be	O
thought	O
of	O
as	O
composed	O
of	O
two	O
parts	O
:	O
a	O
source	Method
network	Method
(	O
the	O
encoder	Method
)	O
that	O
encodes	O
the	O
source	O
sequence	O
into	O
a	O
representation	O
and	O
a	O
target	Method
network	Method
(	O
the	O
decoder	Method
)	O
that	O
uses	O
the	O
representation	O
of	O
the	O
source	Method
encoder	Method
to	O
generate	O
the	O
target	O
sequence	O
.	O
Recurrent	Method
neural	Method
networks	Method
(	O
RNN	Method
)	O
are	O
powerful	O
sequence	Method
models	Method
and	O
are	O
widely	O
used	O
in	O
language	Task
modelling	Task
,	O
yet	O
they	O
have	O
a	O
potential	O
drawback	O
.	O
RNNs	Method
have	O
an	O
inherently	O
serial	O
structure	O
that	O
prevents	O
them	O
from	O
being	O
run	O
in	O
parallel	O
along	O
the	O
sequence	O
length	O
during	O
training	O
and	O
evaluation	Task
.	O
Forward	O
and	O
backward	O
signals	O
in	O
a	O
RNN	Method
also	O
need	O
to	O
traverse	O
the	O
full	O
distance	O
of	O
the	O
serial	O
path	O
to	O
reach	O
from	O
one	O
token	O
in	O
the	O
sequence	O
to	O
another	O
.	O
The	O
larger	O
the	O
distance	O
,	O
the	O
harder	O
it	O
is	O
to	O
learn	O
the	O
dependencies	O
between	O
the	O
tokens	O
.	O
A	O
number	O
of	O
neural	Method
architectures	Method
have	O
been	O
proposed	O
for	O
modelling	Task
translation	Task
,	O
such	O
as	O
encoder	Method
-	Method
decoder	Method
networks	Method
,	O
networks	O
with	O
attentional	Method
pooling	Method
and	O
two	Method
-	Method
dimensional	Method
networks	Method
.	O
Despite	O
the	O
generally	O
good	O
performance	O
,	O
the	O
proposed	O
models	O
either	O
have	O
running	Metric
time	Metric
that	O
is	O
super	O
-	O
linear	O
in	O
the	O
length	O
of	O
the	O
source	O
and	O
target	O
sequences	O
,	O
or	O
they	O
process	O
the	O
source	O
sequence	O
into	O
a	O
constant	Method
size	Method
representation	Method
,	O
burdening	O
the	O
model	O
with	O
a	O
memorization	Method
step	Method
.	O
Both	O
of	O
these	O
drawbacks	O
grow	O
more	O
severe	O
as	O
the	O
length	O
of	O
the	O
sequences	O
increases	O
.	O
We	O
present	O
a	O
family	O
of	O
encoder	Method
-	Method
decoder	Method
neural	Method
networks	Method
that	O
are	O
characterized	O
by	O
two	O
architectural	Method
mechanisms	Method
aimed	O
to	O
address	O
the	O
drawbacks	O
of	O
the	O
conventional	O
approaches	O
mentioned	O
above	O
.	O
The	O
first	O
mechanism	O
involves	O
the	O
stacking	O
of	O
the	O
decoder	Method
on	O
top	O
of	O
the	O
representation	O
of	O
the	O
encoder	O
in	O
a	O
manner	O
that	O
preserves	O
the	O
temporal	O
resolution	O
of	O
the	O
sequences	O
;	O
this	O
is	O
in	O
contrast	O
with	O
architectures	O
that	O
encode	O
the	O
source	O
into	O
a	O
fixed	Method
-	Method
size	Method
representation	Method
.	O
The	O
second	O
mechanism	O
is	O
the	O
dynamic	Method
unfolding	Method
mechanism	Method
that	O
allows	O
the	O
network	O
to	O
process	O
in	O
a	O
simple	O
and	O
efficient	O
way	O
source	O
and	O
target	O
sequences	O
of	O
different	O
lengths	O
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
ByteNet	Method
is	O
the	O
instance	O
within	O
this	O
family	O
of	O
models	O
that	O
uses	O
one	Method
-	Method
dimensional	Method
convolutional	Method
neural	Method
networks	Method
(	O
CNN	Method
)	O
of	O
fixed	O
depth	O
for	O
both	O
the	O
encoder	Method
and	O
the	O
decoder	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
two	O
CNNs	Method
use	O
increasing	O
factors	O
of	O
dilation	O
to	O
rapidly	O
grow	O
the	O
receptive	O
fields	O
;	O
a	O
similar	O
technique	O
is	O
also	O
used	O
in	O
.	O
The	O
convolutions	Method
in	O
the	O
decoder	Method
CNN	Method
are	O
masked	O
to	O
prevent	O
the	O
network	O
from	O
seeing	O
future	O
tokens	O
in	O
the	O
target	O
sequence	O
.	O
The	O
network	O
has	O
beneficial	O
computational	Metric
and	Metric
learning	Metric
properties	Metric
.	O
From	O
a	O
computational	O
perspective	O
,	O
the	O
network	O
has	O
a	O
running	Metric
time	Metric
that	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
source	O
and	O
target	O
sequences	O
(	O
up	O
to	O
a	O
constant	O
where	O
is	O
the	O
size	O
of	O
the	O
desired	O
dependency	O
field	O
)	O
.	O
The	O
computation	O
in	O
the	O
encoder	Method
during	O
training	Task
and	O
decoding	Task
and	O
in	O
the	O
decoder	Task
during	O
training	Task
can	O
also	O
be	O
run	O
efficiently	O
in	O
parallel	O
along	O
the	O
sequences	O
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
.	O
From	O
a	O
learning	Method
perspective	Method
,	O
the	O
representation	O
of	O
the	O
source	O
sequence	O
in	O
the	O
ByteNet	Method
is	O
resolution	O
preserving	O
;	O
the	O
representation	O
sidesteps	O
the	O
need	O
for	O
memorization	O
and	O
allows	O
for	O
maximal	O
bandwidth	O
between	O
encoder	O
and	O
decoder	Method
.	O
In	O
addition	O
,	O
the	O
distance	O
traversed	O
by	O
forward	O
and	O
backward	O
signals	O
between	O
any	O
input	O
and	O
output	O
tokens	O
corresponds	O
to	O
the	O
fixed	O
depth	O
of	O
the	O
networks	O
and	O
is	O
largely	O
independent	O
of	O
the	O
distance	O
between	O
the	O
tokens	O
.	O
Dependencies	O
over	O
large	O
distances	O
are	O
connected	O
by	O
short	O
paths	O
and	O
can	O
be	O
learnt	O
more	O
easily	O
.	O
We	O
apply	O
the	O
ByteNet	Method
model	O
to	O
strings	O
of	O
characters	O
for	O
character	Task
-	Task
level	Task
language	O
modelling	O
and	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
.	O
We	O
evaluate	O
the	O
decoder	Method
network	Method
on	O
the	O
Hutter	Task
Prize	Task
Wikipedia	Task
task	Task
where	O
it	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
of	O
1.31	O
bits	O
/	O
character	O
.	O
We	O
further	O
evaluate	O
the	O
encoder	Method
-	Method
decoder	Method
network	Method
on	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
on	O
the	O
English	Material
-	Material
to	Material
-	Material
German	Material
WMT	Material
benchmark	Material
where	O
it	O
achieves	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
BLEU	Metric
score	Metric
of	O
22.85	O
(	O
0.380	O
bits	O
/	O
character	O
)	O
and	O
25.53	O
(	O
0.389	O
bits	O
/	O
character	O
)	O
on	O
the	O
2014	O
and	O
2015	O
test	O
sets	O
,	O
respectively	O
.	O
On	O
the	O
character	Task
-	Task
level	Task
machine	O
translation	O
task	O
,	O
ByteNet	Method
betters	O
a	O
comparable	O
version	O
of	O
GNMT	Method
that	O
is	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
.	O
These	O
results	O
show	O
that	O
deep	Method
CNNs	Method
are	O
simple	O
,	O
scalable	O
and	O
effective	O
architectures	O
for	O
challenging	O
linguistic	Task
processing	Task
tasks	Task
.	O
The	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
Section	O
2	O
lays	O
out	O
the	O
background	O
and	O
some	O
desiderata	O
for	O
neural	Method
architectures	Method
underlying	O
translation	Method
models	Method
.	O
Section	O
3	O
defines	O
the	O
proposed	O
family	O
of	O
architectures	O
and	O
the	O
specific	O
convolutional	Method
instance	Method
(	O
ByteNet	Method
)	O
used	O
in	O
the	O
experiments	O
.	O
Section	O
4	O
analyses	O
ByteNet	Method
as	O
well	O
as	O
existing	O
neural	Method
translation	Method
models	Method
based	O
on	O
the	O
desiderata	O
set	O
out	O
in	O
Section	O
2	O
.	O
Section	O
5	O
reports	O
the	O
experiments	O
on	O
language	Task
modelling	Task
and	O
Section	O
6	O
reports	O
the	O
experiments	O
on	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
.	O
section	O
:	O
Neural	Method
Translation	Method
Model	Method
Given	O
a	O
string	O
from	O
a	O
source	O
language	O
,	O
a	O
neural	Method
translation	Method
model	Method
estimates	O
a	O
distribution	O
over	O
strings	O
of	O
a	O
target	O
language	O
.	O
The	O
distribution	O
indicates	O
the	O
probability	O
of	O
a	O
string	O
being	O
a	O
translation	O
of	O
.	O
A	O
product	O
of	O
conditionals	O
over	O
the	O
tokens	O
in	O
the	O
target	O
leads	O
to	O
a	O
tractable	O
formulation	O
of	O
the	O
distribution	O
:	O
Each	O
conditional	O
factor	O
expresses	O
complex	O
and	O
long	O
-	O
range	O
dependencies	O
among	O
the	O
source	O
and	O
target	O
tokens	O
.	O
The	O
strings	O
are	O
usually	O
sentences	O
of	O
the	O
respective	O
languages	O
;	O
the	O
tokens	O
are	O
words	O
or	O
,	O
as	O
in	O
the	O
our	O
case	O
,	O
characters	O
.	O
The	O
network	O
that	O
models	O
is	O
composed	O
of	O
two	O
parts	O
:	O
a	O
source	Method
network	Method
(	O
the	O
encoder	Method
)	O
that	O
processes	O
the	O
source	O
string	O
into	O
a	O
representation	O
and	O
a	O
target	Method
network	Method
(	O
the	O
decoder	Method
)	O
that	O
uses	O
the	O
source	Method
representation	Method
to	O
generate	O
the	O
target	O
string	O
.	O
The	O
decoder	Method
functions	O
as	O
a	O
language	Method
model	Method
for	O
the	O
target	O
language	O
.	O
A	O
neural	Method
translation	Method
model	Method
has	O
some	O
basic	O
properties	O
.	O
The	O
decoder	Method
is	O
autoregressive	O
in	O
the	O
target	O
tokens	O
and	O
the	O
model	O
is	O
sensitive	O
to	O
the	O
ordering	O
of	O
the	O
tokens	O
in	O
the	O
source	O
and	O
target	O
strings	O
.	O
It	O
is	O
also	O
useful	O
for	O
the	O
model	O
to	O
be	O
able	O
to	O
assign	O
a	O
non	O
-	O
zero	O
probability	O
to	O
any	O
string	O
in	O
the	O
target	O
language	O
and	O
retain	O
an	O
open	O
vocabulary	O
.	O
subsection	O
:	O
Desiderata	O
Beyond	O
these	O
basic	O
properties	O
the	O
definition	O
of	O
a	O
neural	Method
translation	Method
model	Method
does	O
not	O
determine	O
a	O
unique	O
neural	Method
architecture	Method
,	O
so	O
we	O
aim	O
at	O
identifying	O
some	O
desiderata	O
.	O
First	O
,	O
the	O
running	Metric
time	Metric
of	O
the	O
network	O
should	O
be	O
linear	O
in	O
the	O
length	O
of	O
the	O
source	O
and	O
target	O
strings	O
.	O
This	O
ensures	O
that	O
the	O
model	O
is	O
scalable	O
to	O
longer	O
strings	O
,	O
which	O
is	O
the	O
case	O
when	O
using	O
characters	O
as	O
tokens	O
.	O
The	O
use	O
of	O
operations	O
that	O
run	O
in	O
parallel	O
along	O
the	O
sequence	O
length	O
can	O
also	O
be	O
beneficial	O
for	O
reducing	O
computation	Metric
time	Metric
.	O
Second	O
,	O
the	O
size	O
of	O
the	O
source	Method
representation	Method
should	O
be	O
linear	O
in	O
the	O
length	O
of	O
the	O
source	O
string	O
,	O
i.e.	O
it	O
should	O
be	O
resolution	O
preserving	O
,	O
and	O
not	O
have	O
constant	O
size	O
.	O
This	O
is	O
to	O
avoid	O
burdening	O
the	O
model	O
with	O
an	O
additional	O
memorization	O
step	O
before	O
translation	Task
.	O
In	O
more	O
general	O
terms	O
,	O
the	O
size	O
of	O
a	O
representation	O
should	O
be	O
proportional	O
to	O
the	O
amount	O
of	O
information	O
it	O
represents	O
or	O
predicts	O
.	O
Third	O
,	O
the	O
path	O
traversed	O
by	O
forward	O
and	O
backward	O
signals	O
in	O
the	O
network	O
(	O
between	O
input	O
and	O
ouput	O
tokens	O
)	O
should	O
be	O
short	O
.	O
Shorter	O
paths	O
whose	O
length	O
is	O
largely	O
decoupled	O
from	O
the	O
sequence	O
distance	O
between	O
the	O
two	O
tokens	O
have	O
the	O
potential	O
to	O
better	O
propagate	O
the	O
signals	O
and	O
to	O
let	O
the	O
network	O
learn	O
long	O
-	O
range	O
dependencies	O
more	O
easily	O
.	O
section	O
:	O
ByteNet	Method
We	O
aim	O
at	O
building	O
neural	Method
language	Method
and	Method
translation	Method
models	Method
that	O
capture	O
the	O
desiderata	O
set	O
out	O
in	O
Sect	O
.	O
[	O
reference	O
]	O
.	O
The	O
proposed	O
ByteNet	Method
architecture	O
is	O
composed	O
of	O
a	O
decoder	Method
that	O
is	O
stacked	O
on	O
an	O
encoder	Method
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
and	O
generates	O
variable	O
-	O
length	O
outputs	O
via	O
dynamic	Method
unfolding	Method
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
decoder	Method
is	O
a	O
language	Method
model	Method
that	O
is	O
formed	O
of	O
one	Method
-	Method
dimensional	Method
convolutional	Method
layers	Method
that	O
are	O
masked	O
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
and	O
use	O
dilation	Method
(	O
Sect	O
.	O
[	O
reference	O
]	O
)	O
.	O
The	O
encoder	Method
processes	O
the	O
source	O
string	O
into	O
a	O
representation	O
and	O
is	O
formed	O
of	O
one	Method
-	Method
dimensional	Method
convolutional	Method
layers	Method
that	O
use	O
dilation	Method
but	O
are	O
not	O
masked	O
.	O
Figure	O
[	O
reference	O
]	O
depicts	O
the	O
two	O
networks	O
and	O
their	O
combination	O
.	O
subsection	O
:	O
Encoder	Method
-	Method
Decoder	Method
Stacking	Method
A	O
notable	O
feature	O
of	O
the	O
proposed	O
family	O
of	O
architectures	O
is	O
the	O
way	O
the	O
encoder	Method
and	O
the	O
decoder	Method
are	O
connected	O
.	O
To	O
maximize	O
the	O
representational	O
bandwidth	O
between	O
the	O
encoder	Method
and	O
the	O
decoder	O
,	O
we	O
place	O
the	O
decoder	Method
on	O
top	O
of	O
the	O
representation	O
computed	O
by	O
the	O
encoder	O
.	O
This	O
is	O
in	O
contrast	O
to	O
models	O
that	O
compress	O
the	O
source	Method
representation	Method
into	O
a	O
fixed	O
-	O
size	O
vector	O
or	O
that	O
pool	O
over	O
the	O
source	Method
representation	Method
with	O
a	O
mechanism	O
such	O
as	O
attentional	Method
pooling	Method
.	O
subsection	O
:	O
Dynamic	Method
Unfolding	Method
An	O
encoder	Method
and	O
a	O
decoder	Method
network	Method
that	O
process	O
sequences	O
of	O
different	O
lengths	O
can	O
not	O
be	O
directly	O
connected	O
due	O
to	O
the	O
different	O
sizes	O
of	O
the	O
computed	O
representations	O
.	O
We	O
circumvent	O
this	O
issue	O
via	O
a	O
mechanism	O
which	O
we	O
call	O
dynamic	Method
unfolding	Method
,	O
which	O
works	O
as	O
follows	O
.	O
Given	O
source	O
and	O
target	O
sequences	O
and	O
with	O
respective	O
lengths	O
and	O
,	O
one	O
first	O
chooses	O
a	O
sufficiently	O
tight	O
upper	O
bound	O
on	O
the	O
target	O
length	O
as	O
a	O
linear	O
function	O
of	O
the	O
source	O
length	O
:	O
The	O
tight	O
upper	O
bound	O
is	O
chosen	O
in	O
such	O
a	O
way	O
that	O
,	O
on	O
the	O
one	O
hand	O
,	O
it	O
is	O
greater	O
than	O
the	O
actual	O
length	O
in	O
almost	O
all	O
cases	O
and	O
,	O
on	O
the	O
other	O
hand	O
,	O
it	O
does	O
not	O
increase	O
excessively	O
the	O
amount	O
of	O
computation	O
that	O
is	O
required	O
.	O
Once	O
a	O
linear	O
relationship	O
is	O
chosen	O
,	O
one	O
designs	O
the	O
source	Method
encoder	Method
so	O
that	O
,	O
given	O
a	O
source	O
sequence	O
of	O
length	O
,	O
the	O
encoder	O
outputs	O
a	O
representation	O
of	O
the	O
established	O
length	O
.	O
In	O
our	O
case	O
,	O
we	O
let	O
and	O
when	O
translating	O
from	O
English	Material
into	O
German	Material
,	O
as	O
German	Material
sentences	Material
tend	O
to	O
be	O
somewhat	O
longer	O
than	O
their	O
English	O
counterparts	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
)	O
.	O
In	O
this	O
manner	O
the	O
representation	O
produced	O
by	O
the	O
encoder	Method
can	O
be	O
efficiently	O
computed	O
,	O
while	O
maintaining	O
high	O
bandwidth	Metric
and	O
being	O
resolution	Metric
-	Metric
preserving	Metric
.	O
Once	O
the	O
encoder	Method
representation	Method
is	O
computed	O
,	O
we	O
let	O
the	O
decoder	Method
unfold	O
step	O
-	O
by	O
-	O
step	O
over	O
the	O
encoder	Method
representation	Method
until	O
the	O
decoder	O
itself	O
outputs	O
an	O
end	O
-	O
of	O
-	O
sequence	O
symbol	O
;	O
the	O
unfolding	Method
process	Method
may	O
freely	O
proceed	O
beyond	O
the	O
estimated	O
length	O
of	O
the	O
encoder	Method
representation	Method
.	O
Figure	O
[	O
reference	O
]	O
gives	O
an	O
example	O
of	O
dynamic	Task
unfolding	Task
.	O
subsection	O
:	O
Input	O
Embedding	Method
Tensor	Method
Given	O
the	O
target	O
sequence	O
the	O
ByteNet	Method
decoder	O
embeds	O
each	O
of	O
the	O
first	O
tokens	O
via	O
a	O
look	O
-	O
up	O
table	O
(	O
the	O
tokens	O
serve	O
as	O
targets	O
for	O
the	O
predictions	O
)	O
.	O
The	O
resulting	O
embeddings	O
are	O
concatenated	O
into	O
a	O
tensor	O
of	O
size	O
where	O
is	O
the	O
number	O
of	O
inner	O
channels	O
in	O
the	O
network	O
.	O
subsection	O
:	O
Masked	Method
One	Method
-	Method
dimensional	Method
Convolutions	Method
The	O
decoder	Method
applies	O
masked	Method
one	Method
-	Method
dimensional	Method
convolutions	Method
to	O
the	O
input	O
embedding	O
tensor	O
that	O
have	O
a	O
masked	O
kernel	O
of	O
size	O
.	O
The	O
masking	Method
ensures	O
that	O
information	O
from	O
future	O
tokens	O
does	O
not	O
affect	O
the	O
prediction	O
of	O
the	O
current	O
token	O
.	O
The	O
operation	O
can	O
be	O
implemented	O
either	O
by	O
zeroing	O
out	O
some	O
of	O
the	O
weights	O
of	O
a	O
wider	O
kernel	O
of	O
size	O
or	O
by	O
padding	O
the	O
input	O
map	O
.	O
subsection	O
:	O
Dilation	Task
The	O
masked	Method
convolutions	Method
use	O
dilation	Method
to	O
increase	O
the	O
receptive	O
field	O
of	O
the	O
target	O
network	O
.	O
Dilation	Method
makes	O
the	O
receptive	O
field	O
grow	O
exponentially	O
in	O
terms	O
of	O
the	O
depth	O
of	O
the	O
networks	O
,	O
as	O
opposed	O
to	O
linearly	O
.	O
We	O
use	O
a	O
dilation	Method
scheme	Method
whereby	O
the	O
dilation	O
rates	O
are	O
doubled	O
every	O
layer	O
up	O
to	O
a	O
maximum	O
rate	O
(	O
for	O
our	O
experiments	O
)	O
.	O
The	O
scheme	O
is	O
repeated	O
multiple	O
times	O
in	O
the	O
network	O
always	O
starting	O
from	O
a	O
dilation	Metric
rate	Metric
of	O
1	O
.	O
subsection	O
:	O
Residual	O
Blocks	O
Each	O
layer	O
is	O
wrapped	O
in	O
a	O
residual	O
block	O
that	O
contains	O
additional	O
convolutional	Method
layers	Method
with	O
filters	O
of	O
size	O
.	O
We	O
adopt	O
two	O
variants	O
of	O
the	O
residual	Method
blocks	Method
:	O
one	O
with	O
ReLUs	Method
,	O
which	O
is	O
used	O
in	O
the	O
machine	Task
translation	Task
experiments	Task
,	O
and	O
one	O
with	O
Multiplicative	O
Units	O
,	O
which	O
is	O
used	O
in	O
the	O
language	Task
modelling	Task
experiments	Task
.	O
Figure	O
[	O
reference	O
]	O
diagrams	O
the	O
two	O
variants	O
of	O
the	O
blocks	O
.	O
In	O
both	O
cases	O
,	O
we	O
use	O
layer	Method
normalization	Method
before	O
the	O
activation	Method
function	Method
,	O
as	O
it	O
is	O
well	O
suited	O
to	O
sequence	Task
processing	Task
where	O
computing	O
the	O
activation	O
statistics	O
over	O
the	O
following	O
future	O
tokens	O
(	O
as	O
would	O
be	O
done	O
by	O
batch	Method
normalization	Method
)	O
must	O
be	O
avoided	O
.	O
After	O
a	O
series	O
of	O
residual	O
blocks	O
of	O
increased	O
dilation	O
,	O
the	O
network	O
applies	O
one	O
more	O
convolution	Method
and	O
ReLU	Method
followed	O
by	O
a	O
convolution	Method
and	O
a	O
final	O
softmax	Method
layer	Method
.	O
section	O
:	O
Model	O
Comparison	O
In	O
this	O
section	O
we	O
analyze	O
the	O
properties	O
of	O
various	O
previously	O
introduced	O
neural	Method
translation	Method
models	Method
as	O
well	O
as	O
the	O
ByteNet	Method
family	O
of	O
models	O
.	O
For	O
the	O
sake	O
of	O
a	O
more	O
complete	O
analysis	O
,	O
we	O
include	O
two	O
recurrent	O
ByteNet	Method
variants	O
(	O
which	O
we	O
do	O
not	O
evaluate	O
in	O
the	O
experiments	O
)	O
.	O
subsection	O
:	O
Recurrent	Method
ByteNets	Method
The	O
ByteNet	Method
is	O
composed	O
of	O
two	O
stacked	Method
encoder	Method
and	Method
decoder	Method
networks	Method
where	O
the	O
decoder	Method
network	Method
dynamically	O
adapts	O
to	O
the	O
output	O
length	O
.	O
This	O
way	O
of	O
combining	O
the	O
networks	O
is	O
not	O
tied	O
to	O
the	O
networks	O
being	O
strictly	O
convolutional	Method
.	O
We	O
may	O
consider	O
two	O
variants	O
of	O
the	O
ByteNet	Method
that	O
use	O
recurrent	Method
networks	Method
for	O
one	O
or	O
both	O
of	O
the	O
networks	O
(	O
see	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
The	O
first	O
variant	O
replaces	O
the	O
convolutional	Method
decoder	Method
with	O
a	O
recurrent	Method
one	Method
that	O
is	O
similarly	O
stacked	O
and	O
dynamically	O
unfolded	O
.	O
The	O
second	O
variant	O
also	O
replaces	O
the	O
convolutional	Method
encoder	Method
with	O
a	O
recurrent	Method
encoder	Method
,	O
e.g.	O
a	O
bidirectional	Method
RNN	Method
.	O
The	O
target	O
RNN	O
is	O
then	O
placed	O
on	O
top	O
of	O
the	O
source	O
RNN	O
.	O
Considering	O
the	O
latter	O
Recurrent	Method
ByteNet	Method
,	O
we	O
can	O
see	O
that	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
network	Method
is	O
a	O
Recurrent	Method
ByteNet	Method
where	O
all	O
connections	O
between	O
source	O
and	O
target	O
–	O
except	O
for	O
the	O
first	O
one	O
that	O
connects	O
and	O
–	O
have	O
been	O
severed	O
.	O
The	O
Recurrent	Method
ByteNet	Method
is	O
a	O
generalization	Method
of	Method
the	Method
RNN	Method
Enc	Method
-	Method
Dec	Method
and	O
,	O
modulo	O
the	O
type	O
of	O
weight	Method
-	Method
sharing	Method
scheme	Method
,	O
so	O
is	O
the	O
convolutional	O
ByteNet	Method
.	O
subsection	O
:	O
Comparison	O
of	O
Properties	O
In	O
our	O
comparison	O
we	O
consider	O
the	O
following	O
neural	Method
translation	Method
models	Method
:	O
the	O
Recurrent	Method
Continuous	Method
Translation	Method
Model	Method
(	O
RCTM	Method
)	O
1	O
and	O
2	O
;	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
;	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
Att	Method
with	O
the	O
attentional	Method
pooling	Method
mechanism	Method
of	O
which	O
there	O
are	O
a	O
few	O
variations	O
;	O
the	O
Grid	Method
LSTM	Method
translation	Method
model	Method
that	O
uses	O
a	O
multi	Method
-	Method
dimensional	Method
architecture	Method
;	O
the	O
Extended	Method
Neural	Method
GPU	Method
model	Method
that	O
has	O
a	O
convolutional	Method
RNN	Method
architecture	Method
;	O
the	O
ByteNet	Method
and	O
the	O
two	O
Recurrent	O
ByteNet	Method
variants	O
.	O
Our	O
comparison	O
criteria	O
reflect	O
the	O
desiderata	O
set	O
out	O
in	O
Sect	O
.	O
[	O
reference	O
]	O
.	O
We	O
separate	O
the	O
first	O
(	O
computation	Metric
time	Metric
)	O
desideratum	O
into	O
three	O
columns	O
.	O
The	O
first	O
column	O
indicates	O
the	O
time	Metric
complexity	Metric
of	O
the	O
network	O
as	O
a	O
function	O
of	O
the	O
length	O
of	O
the	O
sequences	O
and	O
is	O
denoted	O
by	O
Time	O
.	O
The	O
other	O
two	O
columns	O
NetS	O
and	O
NetT	O
indicate	O
,	O
respectively	O
,	O
whether	O
the	O
source	O
and	O
the	O
target	Method
network	Method
use	O
a	O
convolutional	Method
structure	Method
(	O
CNN	Method
)	O
or	O
a	O
recurrent	Method
one	Method
(	O
RNN	Method
)	O
;	O
a	O
CNN	Method
structure	Method
has	O
the	O
advantage	O
that	O
it	O
can	O
be	O
run	O
in	O
parallel	O
along	O
the	O
length	O
of	O
the	O
sequence	O
.	O
The	O
second	O
(	O
resolution	Task
preservation	Task
)	O
desideratum	O
corresponds	O
to	O
the	O
RP	O
column	O
,	O
which	O
indicates	O
whether	O
the	O
source	Method
representation	Method
in	O
the	O
network	O
is	O
resolution	O
preserving	O
.	O
Finally	O
,	O
the	O
third	O
desideratum	O
(	O
short	O
forward	O
and	O
backward	O
flow	O
paths	O
)	O
is	O
reflected	O
by	O
two	O
columns	O
.	O
The	O
PathS	O
column	O
corresponds	O
to	O
the	O
length	O
in	O
layer	O
steps	O
of	O
the	O
shortest	O
path	O
between	O
a	O
source	O
token	O
and	O
any	O
output	O
target	O
token	O
.	O
Similarly	O
,	O
the	O
PathT	O
column	O
corresponds	O
to	O
the	O
length	O
of	O
the	O
shortest	O
path	O
between	O
an	O
input	O
target	O
token	O
and	O
any	O
output	O
target	O
token	O
.	O
Shorter	O
paths	O
lead	O
to	O
better	O
forward	O
and	O
backward	O
signal	O
propagation	O
.	O
Table	O
[	O
reference	O
]	O
summarizes	O
the	O
properties	O
of	O
the	O
models	O
.	O
The	O
ByteNet	Method
,	O
the	O
Recurrent	Method
ByteNets	Method
and	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
are	O
the	O
only	O
networks	O
that	O
have	O
linear	Metric
running	Metric
time	Metric
(	O
up	O
to	O
the	O
constant	O
)	O
.	O
The	O
RNN	Method
Enc	Method
-	Method
Dec	Method
,	O
however	O
,	O
does	O
not	O
preserve	O
the	O
source	O
sequence	O
resolution	O
,	O
a	O
feature	O
that	O
aggravates	O
learning	Task
for	O
long	O
sequences	O
such	O
as	O
those	O
that	O
appear	O
in	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
.	O
The	O
RCTM	Method
2	O
,	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
Att	Method
,	O
the	O
Grid	Method
LSTM	Method
and	O
the	O
Extended	Method
Neural	Method
GPU	Method
do	O
preserve	O
the	O
resolution	O
,	O
but	O
at	O
a	O
cost	O
of	O
a	O
quadratic	Metric
running	Metric
time	Metric
.	O
The	O
ByteNet	Method
stands	O
out	O
also	O
for	O
its	O
Path	O
properties	O
.	O
The	O
dilated	O
structure	O
of	O
the	O
convolutions	Method
connects	O
any	O
two	O
source	O
or	O
target	O
tokens	O
in	O
the	O
sequences	O
by	O
way	O
of	O
a	O
small	O
number	O
of	O
network	Method
layers	Method
corresponding	O
to	O
the	O
depth	O
of	O
the	O
source	O
or	O
target	O
networks	O
.	O
For	O
character	Task
sequences	Task
where	O
learning	O
long	O
-	O
range	O
dependencies	O
is	O
important	O
,	O
paths	O
that	O
are	O
sub	O
-	O
linear	O
in	O
the	O
distance	O
are	O
advantageous	O
.	O
section	O
:	O
Character	Task
Prediction	Task
We	O
first	O
evaluate	O
the	O
ByteNet	Method
Decoder	O
separately	O
on	O
a	O
character	Task
-	Task
level	Task
language	O
modelling	O
benchmark	O
.	O
We	O
use	O
the	O
Hutter	Material
Prize	Material
version	Material
of	O
the	O
Wikipedia	Material
dataset	Material
and	O
follow	O
the	O
standard	O
split	O
where	O
the	O
first	O
90	O
million	O
bytes	O
are	O
used	O
for	O
training	O
,	O
the	O
next	O
5	O
million	O
bytes	O
are	O
used	O
for	O
validation	Task
and	O
the	O
last	O
5	O
million	O
bytes	O
are	O
used	O
for	O
testing	O
.	O
The	O
total	O
number	O
of	O
characters	O
in	O
the	O
vocabulary	O
is	O
205	O
.	O
The	O
ByteNet	Method
Decoder	O
that	O
we	O
use	O
for	O
the	O
result	O
has	O
30	O
residual	O
blocks	O
split	O
into	O
six	O
sets	O
of	O
five	O
blocks	O
each	O
;	O
for	O
the	O
five	O
blocks	O
in	O
each	O
set	O
the	O
dilation	Metric
rates	Metric
are	O
,	O
respectively	O
,	O
and	O
.	O
The	O
masked	Method
kernel	Method
has	O
size	O
3	O
.	O
This	O
gives	O
a	O
receptive	O
field	O
of	O
315	O
characters	O
.	O
The	O
number	O
of	O
hidden	O
units	O
is	O
512	O
.	O
For	O
this	O
task	O
we	O
use	O
residual	O
multiplicative	O
blocks	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
Right	O
)	O
.	O
For	O
the	O
optimization	Task
we	O
use	O
Adam	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
and	O
a	O
weight	O
decay	O
term	O
of	O
.	O
We	O
apply	O
dropout	Method
to	O
the	O
last	O
ReLU	O
layer	O
before	O
the	O
softmax	O
dropping	O
units	O
with	O
a	O
probability	O
of	O
0.1	O
.	O
We	O
do	O
not	O
reduce	O
the	O
learning	Metric
rate	Metric
during	O
training	O
.	O
At	O
each	O
step	O
we	O
sample	O
a	O
batch	O
of	O
sequences	O
of	O
500	O
characters	O
each	O
,	O
use	O
the	O
first	O
100	O
characters	O
as	O
the	O
minimum	O
context	O
and	O
predict	O
the	O
latter	O
400	O
characters	O
.	O
Table	O
[	O
reference	O
]	O
lists	O
recent	O
results	O
of	O
various	O
neural	Method
sequence	Method
models	Method
on	O
the	O
Wikipedia	Material
dataset	Material
.	O
All	O
the	O
results	O
except	O
for	O
the	O
ByteNet	Method
result	O
are	O
obtained	O
using	O
some	O
variant	O
of	O
the	O
LSTM	Method
recurrent	Method
neural	Method
network	Method
.	O
The	O
ByteNet	Method
decoder	O
achieves	O
1.31	O
bits	O
/	O
character	O
on	O
the	O
test	O
set	O
.	O
section	O
:	O
Character	Task
-	Task
Level	Task
Machine	Task
Translation	Task
We	O
evaluate	O
the	O
full	O
ByteNet	Method
on	O
the	O
WMT	Task
English	Task
to	Task
German	Task
translation	Task
task	Task
.	O
We	O
use	O
NewsTest	Material
2013	Material
for	O
validation	Task
and	O
NewsTest	Material
2014	Material
and	O
2015	O
for	O
testing	O
.	O
The	O
English	Material
and	Material
German	Material
strings	Material
are	O
encoded	O
as	O
sequences	O
of	O
characters	O
;	O
no	O
explicit	O
segmentation	O
into	O
words	O
or	O
morphemes	O
is	O
applied	O
to	O
the	O
strings	O
.	O
The	O
outputs	O
of	O
the	O
network	O
are	O
strings	O
of	O
characters	O
in	O
the	O
target	O
language	O
.	O
We	O
keep	O
323	O
characters	O
in	O
the	O
German	Material
vocabulary	Material
and	O
296	O
in	O
the	O
English	Material
vocabulary	Material
.	O
The	O
ByteNet	Method
used	O
in	O
the	O
experiments	O
has	O
30	O
residual	O
blocks	O
in	O
the	O
encoder	O
and	O
30	O
residual	O
blocks	O
in	O
the	O
decoder	O
.	O
As	O
in	O
the	O
ByteNet	Method
Decoder	O
,	O
the	O
residual	O
blocks	O
are	O
arranged	O
in	O
sets	O
of	O
five	O
with	O
corresponding	O
dilation	Metric
rates	Metric
of	O
and	O
.	O
For	O
this	O
task	O
we	O
use	O
the	O
residual	O
blocks	O
with	O
ReLUs	O
(	O
Fig	O
.	O
[	O
reference	O
]	O
Left	O
)	O
.	O
The	O
number	O
of	O
hidden	O
units	O
is	O
800	O
.	O
The	O
size	O
of	O
the	O
kernel	O
in	O
the	O
source	O
network	O
is	O
,	O
whereas	O
the	O
size	O
of	O
the	O
masked	O
kernel	O
in	O
the	O
target	O
network	O
is	O
.	O
For	O
the	O
optimization	Task
we	O
use	O
Adam	Method
with	O
a	O
learning	Metric
rate	Metric
of	O
.	O
Each	O
sentence	O
is	O
padded	O
with	O
special	O
characters	O
to	O
the	O
nearest	O
greater	O
multiple	O
of	O
50	O
;	O
20	O
%	O
of	O
further	O
padding	O
is	O
applied	O
to	O
each	O
source	O
sentence	O
as	O
a	O
part	O
of	O
dynamic	O
unfolding	O
(	O
eq	O
.	O
[	O
reference	O
]	O
)	O
.	O
Each	O
pair	O
of	O
sentences	O
is	O
mapped	O
to	O
a	O
bucket	O
based	O
on	O
the	O
pair	O
of	O
padded	O
lengths	O
for	O
efficient	O
batching	O
during	O
training	Task
.	O
We	O
use	O
vanilla	Method
beam	Method
search	Method
according	O
to	O
the	O
total	O
likelihood	O
of	O
the	O
generated	O
candidate	O
and	O
accept	O
only	O
candidates	O
which	O
end	O
in	O
a	O
end	O
-	O
of	O
-	O
sentence	O
token	O
.	O
We	O
use	O
a	O
beam	O
of	O
size	O
12	O
.	O
We	O
do	O
not	O
use	O
length	Method
normalization	Method
,	O
nor	O
do	O
we	O
keep	O
score	O
of	O
which	O
parts	O
of	O
the	O
source	O
sentence	O
have	O
been	O
translated	O
.	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
contain	O
the	O
results	O
of	O
the	O
experiments	O
.	O
On	O
NewsTest	Material
2014	Material
the	O
ByteNet	Method
achieves	O
the	O
highest	O
performance	O
in	O
character	Task
-	Task
level	Task
and	O
subword	Task
-	Task
level	Task
neural	Task
machine	Task
translation	Task
,	O
and	O
compared	O
to	O
the	O
word	Method
-	Method
level	Method
systems	Method
it	O
is	O
second	O
only	O
to	O
the	O
version	O
of	O
GNMT	Method
that	O
uses	O
word	O
-	O
pieces	O
.	O
On	O
NewsTest	Material
2015	Material
,	O
to	O
our	O
knowledge	O
,	O
ByteNet	Method
achieves	O
the	O
best	O
published	O
results	O
to	O
date	O
.	O
Table	O
[	O
reference	O
]	O
contains	O
some	O
of	O
the	O
unaltered	O
generated	O
translations	O
from	O
the	O
ByteNet	Method
that	O
highlight	O
reordering	O
and	O
other	O
phenomena	O
such	O
as	O
transliteration	Task
.	O
The	O
character	Task
-	Task
level	Task
aspect	O
of	O
the	O
model	O
makes	O
post	Task
-	Task
processing	Task
unnecessary	O
in	O
principle	O
.	O
We	O
further	O
visualize	O
the	O
sensitivity	O
of	O
the	O
ByteNet	Method
’s	O
predictions	O
to	O
specific	O
source	O
and	O
target	O
inputs	O
using	O
gradient	Method
-	Method
based	Method
visualization	Method
.	O
Figure	O
[	O
reference	O
]	O
represents	O
a	O
heatmap	O
of	O
the	O
magnitude	O
of	O
the	O
gradients	O
of	O
the	O
generated	O
outputs	O
with	O
respect	O
to	O
the	O
source	O
and	O
target	O
inputs	O
.	O
For	O
visual	O
clarity	O
,	O
we	O
sum	O
the	O
gradients	O
for	O
all	O
the	O
characters	O
that	O
make	O
up	O
each	O
word	O
and	O
normalize	O
the	O
values	O
along	O
each	O
column	O
.	O
In	O
contrast	O
with	O
the	O
attentional	Method
pooling	Method
mechanism	Method
,	O
this	O
general	O
technique	O
allows	O
us	O
to	O
inspect	O
not	O
just	O
dependencies	O
of	O
the	O
outputs	O
on	O
the	O
source	O
inputs	O
,	O
but	O
also	O
dependencies	O
of	O
the	O
outputs	O
on	O
previous	O
target	O
inputs	O
,	O
or	O
on	O
any	O
other	O
neural	Method
network	Method
layers	Method
.	O
section	O
:	O
Conclusion	O
We	O
have	O
introduced	O
the	O
ByteNet	Method
,	O
a	O
neural	Method
translation	Method
model	Method
that	O
has	O
linear	O
running	O
time	O
,	O
decouples	O
translation	Task
from	O
memorization	Method
and	O
has	O
short	O
signal	O
propagation	O
paths	O
for	O
tokens	O
in	O
sequences	O
.	O
We	O
have	O
shown	O
that	O
the	O
ByteNet	Method
decoder	O
is	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
character	Task
-	Task
level	Task
language	O
model	O
based	O
on	O
a	O
convolutional	Method
neural	Method
network	Method
that	O
outperforms	O
recurrent	Method
neural	Method
language	Method
models	Method
.	O
We	O
have	O
also	O
shown	O
that	O
the	O
ByteNet	Method
generalizes	O
the	O
RNN	Method
Enc	Method
-	Method
Dec	Method
architecture	Method
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
character	Task
-	Task
to	Task
-	Task
character	Task
machine	Task
translation	Task
and	O
excellent	O
results	O
in	O
general	O
,	O
while	O
maintaining	O
linear	Metric
running	Metric
time	Metric
complexity	Metric
.	O
We	O
have	O
revealed	O
the	O
latent	O
structure	O
learnt	O
by	O
the	O
ByteNet	Method
and	O
found	O
it	O
to	O
mirror	O
the	O
expected	O
alignment	O
between	O
the	O
tokens	O
in	O
the	O
sentences	O
.	O
bibliography	O
:	O
References	O
