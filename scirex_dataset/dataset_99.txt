document O
: O
OverFeat Method
: O
Integrated Task
Recognition Task
, Task
Localization Task
and Task
Detection Task
using O
Convolutional Method
Networks Method
We O
present O
an O
integrated O
framework O
for O
using O
Convolutional Method
Networks Method
for O
classification Task
, Task
localization Task
and Task
detection Task
. O
We O
show O
how O
a O
multiscale Method
and Method
sliding Method
window Method
approach Method
can O
be O
efficiently O
implemented O
within O
a O
ConvNet Method
. O
We O
also O
introduce O
a O
novel O
deep Method
learning Method
approach Method
to O
localization Task
by O
learning O
to O
predict Task
object Task
boundaries Task
. O
Bounding O
boxes O
are O
then O
accumulated O
rather O
than O
suppressed O
in O
order O
to O
increase O
detection Metric
confidence Metric
. O
We O
show O
that O
different O
tasks O
can O
be O
learned O
simultaneously O
using O
a O
single O
shared Method
network Method
. O
This O
integrated O
framework O
is O
the O
winner O
of O
the O
localization Task
task Task
of O
the O
ImageNet Task
Large Task
Scale Task
Visual Task
Recognition Task
Challenge Task
2013 Task
( O
ILSVRC2013 Task
) O
and O
obtained O
very O
competitive O
results O
for O
the O
detection Task
and Task
classifications Task
tasks Task
. O
In O
post O
- O
competition O
work O
, O
we O
establish O
a O
new O
state O
of O
the O
art O
for O
the O
detection Task
task Task
. O
Finally O
, O
we O
release O
a O
feature Method
extractor Method
from O
our O
best O
model O
called O
OverFeat Method
. O
section O
: O
Introduction O
Recognizing Task
the Task
category Task
of Task
the Task
dominant Task
object Task
in O
an O
image O
is O
a O
tasks O
to O
which O
Convolutional Method
Networks Method
( O
ConvNets Method
) O
have O
been O
applied O
for O
many O
years O
, O
whether O
the O
objects O
were O
handwritten O
characters O
, O
house O
numbers O
, O
textureless O
toys O
, O
traffic O
signs O
, O
objects O
from O
the O
Caltech O
- O
101 O
dataset O
, O
or O
objects O
from O
the O
1000 Material
- Material
category Material
ImageNet Material
dataset Material
. O
The O
accuracy Metric
of O
ConvNets Method
on O
small O
datasets O
such O
as O
Caltech O
- O
101 O
, O
while O
decent O
, O
has O
not O
been O
record O
- O
breaking O
. O
However O
, O
the O
advent O
of O
larger O
datasets O
has O
enabled O
ConvNets Method
to O
significantly O
advance O
the O
state O
of O
the O
art O
on O
datasets O
such O
as O
the O
1000 Material
- Material
category Material
ImageNet Material
. O
The O
main O
advantage O
of O
ConvNets Method
for O
many O
such O
tasks O
is O
that O
the O
entire O
system O
is O
trained O
end O
to O
end O
, O
from O
raw O
pixels O
to O
ultimate O
categories O
, O
thereby O
alleviating O
the O
requirement O
to O
manually O
design O
a O
suitable O
feature Method
extractor Method
. O
The O
main O
disadvantage O
is O
their O
ravenous O
appetite O
for O
labeled O
training O
samples O
. O
The O
main O
point O
of O
this O
paper O
is O
to O
show O
that O
training O
a O
convolutional Method
network Method
to O
simultaneously O
classify O
, O
locate Task
and Task
detect Task
objects Task
in Task
images Task
can O
boost O
the O
classification Metric
accuracy Metric
and O
the O
detection Metric
and Metric
localization Metric
accuracy Metric
of O
all O
tasks O
. O
The O
paper O
proposes O
a O
new O
integrated O
approach O
to O
object Task
detection Task
, O
recognition Task
, O
and O
localization Task
with O
a O
single O
ConvNet Method
. O
We O
also O
introduce O
a O
novel O
method O
for O
localization Task
and Task
detection Task
by O
accumulating Task
predicted Task
bounding Task
boxes Task
. O
We O
suggest O
that O
by O
combining O
many O
localization Task
predictions Task
, O
detection Task
can O
be O
performed O
without O
training O
on O
background O
samples O
and O
that O
it O
is O
possible O
to O
avoid O
the O
time O
- O
consuming O
and O
complicated O
bootstrapping Method
training Method
passes Method
. O
Not O
training O
on O
background O
also O
lets O
the O
network O
focus O
solely O
on O
positive O
classes O
for O
higher O
accuracy Metric
. O
Experiments O
are O
conducted O
on O
the O
ImageNet Material
ILSVRC Material
2012 Material
and Material
2013 Material
datasets Material
and O
establish O
state O
of O
the O
art O
results O
on O
the O
ILSVRC Task
2013 Task
localization Task
and Task
detection Task
tasks Task
. O
While O
images O
from O
the O
ImageNet Material
classification Material
dataset Material
are O
largely O
chosen O
to O
contain O
a O
roughly O
- O
centered O
object O
that O
fills O
much O
of O
the O
image O
, O
objects O
of O
interest O
sometimes O
vary O
significantly O
in O
size O
and O
position O
within O
the O
image O
. O
The O
first O
idea O
in O
addressing O
this O
is O
to O
apply O
a O
ConvNet Method
at O
multiple O
locations O
in O
the O
image O
, O
in O
a O
sliding Method
window Method
fashion Method
, O
and O
over O
multiple O
scales O
. O
Even O
with O
this O
, O
however O
, O
many O
viewing O
windows O
may O
contain O
a O
perfectly O
identifiable O
portion O
of O
the O
object O
( O
say O
, O
the O
head O
of O
a O
dog O
) O
, O
but O
not O
the O
entire O
object O
, O
nor O
even O
the O
center O
of O
the O
object O
. O
This O
leads O
to O
decent O
classification Task
but O
poor O
localization Task
and O
detection Task
. O
Thus O
, O
the O
second O
idea O
is O
to O
train O
the O
system O
to O
not O
only O
produce O
a O
distribution O
over O
categories O
for O
each O
window O
, O
but O
also O
to O
produce O
a O
prediction O
of O
the O
location O
and O
size O
of O
the O
bounding O
box O
containing O
the O
object O
relative O
to O
the O
window O
. O
The O
third O
idea O
is O
to O
accumulate O
the O
evidence O
for O
each O
category O
at O
each O
location O
and O
size O
. O
Many O
authors O
have O
proposed O
to O
use O
ConvNets Method
for O
detection Task
and Task
localization Task
with O
a O
sliding O
window O
over O
multiple O
scales O
, O
going O
back O
to O
the O
early O
1990 O
’s O
for O
multi Task
- Task
character Task
strings Task
, O
faces O
, O
and O
hands O
. O
More O
recently O
, O
ConvNets Method
have O
been O
shown O
to O
yield O
state O
of O
the O
art O
performance O
on O
text Task
detection Task
in O
natural O
images O
, O
face Task
detection Task
and O
pedestrian Task
detection Task
. O
Several O
authors O
have O
also O
proposed O
to O
train O
ConvNets Method
to O
directly O
predict O
the O
instantiation O
parameters O
of O
the O
objects O
to O
be O
located O
, O
such O
as O
the O
position O
relative O
to O
the O
viewing O
window O
, O
or O
the O
pose O
of O
the O
object O
. O
For O
example O
Osadchy O
et O
al O
. O
describe O
a O
ConvNet Method
for O
simultaneous O
face Task
detection Task
and O
pose O
estimation O
. O
Faces O
are O
represented O
by O
a O
3D O
manifold O
in O
the O
nine O
- O
dimensional O
output O
space O
. O
Positions O
on O
the O
manifold O
indicate O
the O
pose O
( O
pitch O
, O
yaw O
, O
and O
roll O
) O
. O
When O
the O
training O
image O
is O
a O
face O
, O
the O
network O
is O
trained O
to O
produce O
a O
point O
on O
the O
manifold O
at O
the O
location O
of O
the O
known O
pose O
. O
If O
the O
image O
is O
not O
a O
face O
, O
the O
output O
is O
pushed O
away O
from O
the O
manifold O
. O
At O
test O
time O
, O
the O
distance O
to O
the O
manifold O
indicate O
whether O
the O
image O
contains O
a O
face O
, O
and O
the O
position O
of O
the O
closest O
point O
on O
the O
manifold O
indicates O
pose O
. O
Taylor O
et O
al O
. O
use O
a O
ConvNet Method
to O
estimate O
the O
location O
of O
body O
parts O
( O
hands O
, O
head O
, O
etc O
) O
so O
as O
to O
derive O
the O
human O
body O
pose O
. O
They O
use O
a O
metric Metric
learning Metric
criterion Metric
to O
train O
the O
network O
to O
produce O
points O
on O
a O
body O
pose O
manifold O
. O
Hinton O
et O
al O
. O
have O
also O
proposed O
to O
train O
networks O
to O
compute O
explicit O
instantiation O
parameters O
of O
features O
as O
part O
of O
a O
recognition Task
process O
. O
Other O
authors O
have O
proposed O
to O
perform O
object Task
localization Task
via O
ConvNet Method
- Method
based Method
segmentation Method
. O
The O
simplest O
approach O
consists O
in O
training O
the O
ConvNet Method
to O
classify O
the O
central O
pixel O
( O
or O
voxel O
for O
volumetric O
images O
) O
of O
its O
viewing O
window O
as O
a O
boundary O
between O
regions O
or O
not O
. O
But O
when O
the O
regions O
must O
be O
categorized O
, O
it O
is O
preferable O
to O
perform O
semantic Task
segmentation Task
. O
The O
main O
idea O
is O
to O
train O
the O
ConvNet Method
to O
classify O
the O
central O
pixel O
of O
the O
viewing O
window O
with O
the O
category O
of O
the O
object O
it O
belongs O
to O
, O
using O
the O
window O
as O
context O
for O
the O
decision O
. O
Applications O
range O
from O
biological Task
image Task
analysis Task
, O
to O
obstacle Task
tagging Task
for O
mobile Task
robots Task
to O
tagging Task
of Task
photos Task
. O
The O
advantage O
of O
this O
approach O
is O
that O
the O
bounding O
contours O
need O
not O
be O
rectangles O
, O
and O
the O
regions O
need O
not O
be O
well O
- O
circumscribed O
objects O
. O
The O
disadvantage O
is O
that O
it O
requires O
dense O
pixel O
- O
level O
labels O
for O
training Task
. O
This O
segmentation Method
pre Method
- Method
processing Method
or O
object Method
proposal Method
step Method
has O
recently O
gained O
popularity O
in O
traditional O
computer Task
vision Task
to O
reduce O
the O
search O
space O
of O
position O
, O
scale O
and O
aspect O
ratio O
for O
detection Task
. O
Hence O
an O
expensive O
classification Method
method Method
can O
be O
applied O
at O
the O
optimal O
location O
in O
the O
search O
space O
, O
thus O
increasing O
recognition Task
accuracy O
. O
Additionally O
, O
suggest O
that O
these O
methods O
improve O
accuracy Metric
by O
drastically O
reducing O
unlikely O
object O
regions O
, O
hence O
reducing O
potential O
false O
positives O
. O
Our O
dense Method
sliding Method
window Method
method Method
, O
however O
, O
is O
able O
to O
outperform O
object Method
proposal Method
methods Method
on O
the O
ILSVRC13 Material
detection Material
dataset Material
. O
Krizhevsky O
et O
al O
. O
[ O
] O
recently O
demonstrated O
impressive O
classification Task
performance O
using O
a O
large O
ConvNet Method
. O
The O
authors O
also O
entered O
the O
ImageNet Material
2012 Material
competition Material
, O
winning O
both O
the O
classification Task
and Task
localization Task
challenges Task
. O
Although O
they O
demonstrated O
an O
impressive O
localization Task
performance O
, O
there O
has O
been O
no O
published O
work O
describing O
how O
their O
approach O
. O
Our O
paper O
is O
thus O
the O
first O
to O
provide O
a O
clear O
explanation O
how O
ConvNets Method
can O
be O
used O
for O
localization Task
and Task
detection Task
for O
ImageNet Material
data Material
. O
In O
this O
paper O
we O
use O
the O
terms O
localization Task
and Task
detection Task
in O
a O
way O
that O
is O
consistent O
with O
their O
use O
in O
the O
ImageNet Material
2013 Material
competition Material
, O
namely O
that O
the O
only O
difference O
is O
the O
evaluation Metric
criterion Metric
used O
and O
both O
involve O
predicting O
the O
bounding O
box O
for O
each O
object O
in O
the O
image O
. O
section O
: O
Vision Task
Tasks Task
In O
this O
paper O
, O
we O
explore O
three O
computer Task
vision Task
tasks Task
in O
increasing O
order O
of O
difficulty O
: O
( O
i O
) O
classification Task
, O
( O
ii O
) O
localization Task
, O
and O
( O
iii O
) O
detection Task
. O
Each O
task O
is O
a O
sub O
- O
task O
of O
the O
next O
. O
While O
all O
tasks O
are O
adressed O
using O
a O
single O
framework O
and O
a O
shared Method
feature Method
learning Method
base Method
, O
we O
will O
describe O
them O
separately O
in O
the O
following O
sections O
. O
Throughout O
the O
paper O
, O
we O
report O
results O
on O
the O
2013 O
ImageNet Task
Large Task
Scale Task
Visual Task
Recognition Task
Challenge Task
( O
ILSVRC2013 Task
) O
. O
In O
the O
classification Task
task Task
of O
this O
challenge O
, O
each O
image O
is O
assigned O
a O
single O
label O
corresponding O
to O
the O
main O
object O
in O
the O
image O
. O
Five O
guesses O
are O
allowed O
to O
find O
the O
correct O
answer O
( O
this O
is O
because O
images O
can O
also O
contain O
multiple O
unlabeled O
objects O
) O
. O
The O
localization Task
task Task
is O
similar O
in O
that O
5 O
guesses O
are O
allowed O
per O
image O
, O
but O
in O
addition O
, O
a O
bounding O
box O
for O
the O
predicted O
object O
must O
be O
returned O
with O
each O
guess O
. O
To O
be O
considered O
correct O
, O
the O
predicted O
box O
must O
match O
the O
groundtruth O
by O
at O
least O
50 O
% O
( O
using O
the O
PASCAL Metric
criterion Metric
of Metric
union Metric
over Metric
intersection Metric
) O
, O
as O
well O
as O
be O
labeled O
with O
the O
correct O
class O
( O
i.e. O
each O
prediction O
is O
a O
label O
and O
bounding O
box O
that O
are O
associated O
together O
) O
. O
The O
detection Task
task Task
differs O
from O
localization Task
in O
that O
there O
can O
be O
any O
number O
of O
objects O
in O
each O
image O
( O
including O
zero O
) O
, O
and O
false O
positives O
are O
penalized O
by O
the O
mean Metric
average Metric
precision Metric
( O
mAP Metric
) Metric
measure Metric
. O
The O
localization Task
task Task
is O
a O
convenient O
intermediate O
step O
between O
classification Task
and O
detection Task
, O
and O
allows O
us O
to O
evaluate O
our O
localization Method
method Method
independently O
of O
challenges O
specific O
to O
detection Task
( O
such O
as O
learning O
a O
background O
class O
) O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
show O
examples O
of O
images O
with O
our O
localization Task
/ Task
detection Task
predictions Task
as O
well O
as O
corresponding O
groundtruth O
. O
Note O
that O
classification Task
and O
localization Task
share O
the O
same O
dataset O
, O
while O
detection Task
also O
has O
additional O
data O
where O
objects O
can O
be O
smaller O
. O
The O
detection Material
data Material
also O
contain O
a O
set O
of O
images O
where O
certain O
objects O
are O
absent O
. O
This O
can O
be O
used O
for O
bootstrapping Task
, O
but O
we O
have O
not O
made O
use O
of O
it O
in O
this O
work O
. O
section O
: O
Classification Task
Our O
classification Method
architecture Method
is O
similar O
to O
the O
best O
ILSVRC12 Method
architecture Method
by O
Krizhevsky O
et O
al O
. O
[ O
] O
. O
However O
, O
we O
improve O
on O
the O
network Method
design Method
and O
the O
inference Task
step Task
. O
Because O
of O
time O
constraints O
, O
some O
of O
the O
training O
features O
in O
Krizhevsky Method
’s Method
model Method
were O
not O
explored O
, O
and O
so O
we O
expect O
our O
results O
can O
be O
improved O
even O
further O
. O
These O
are O
discussed O
in O
the O
future O
work O
section O
[ O
reference O
] O
subsection O
: O
Model O
Design O
and O
Training O
We O
train O
the O
network O
on O
the O
ImageNet Material
2012 Material
training Material
set Material
( O
1.2 O
million O
images O
and O
classes O
) O
. O
Our O
model O
uses O
the O
same O
fixed Method
input Method
size Method
approach Method
proposed O
by O
Krizhevsky O
et O
al O
. O
[ O
] O
during O
training O
but O
turns O
to O
multi O
- O
scale O
for O
classification Task
as O
described O
in O
the O
next O
section O
. O
Each O
image O
is O
downsampled O
so O
that O
the O
smallest O
dimension O
is O
256 O
pixels O
. O
We O
then O
extract O
5 O
random O
crops O
( O
and O
their O
horizontal O
flips O
) O
of O
size O
221x221 O
pixels O
and O
present O
these O
to O
the O
network O
in O
mini O
- O
batches O
of O
size O
128 O
. O
The O
weights O
in O
the O
network O
are O
initialized O
randomly O
with O
. O
They O
are O
then O
updated O
by O
stochastic Method
gradient Method
descent Method
, O
accompanied O
by O
momentum O
term O
of O
and O
an O
weight Method
decay Method
of O
. O
The O
learning Metric
rate Metric
is O
initially O
and O
is O
successively O
decreased O
by O
a O
factor O
of O
after O
epochs O
. O
DropOut Method
with O
a O
rate O
of O
is O
employed O
on O
the O
fully O
connected O
layers O
( O
6th O
and O
7th O
) O
in O
the O
classifier Method
. O
We O
detail O
the O
architecture O
sizes O
in O
tables O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Note O
that O
during O
training Task
, O
we O
treat O
this O
architecture O
as O
non O
- O
spatial O
( O
output O
maps O
of O
size O
1x1 O
) O
, O
as O
opposed O
to O
the O
inference Method
step Method
, O
which O
produces O
spatial O
outputs O
. O
Layers O
1 O
- O
5 O
are O
similar O
to O
Krizhevsky O
et O
al O
. O
[ O
] O
, O
using O
rectification Method
( O
“ O
relu Method
” Method
) O
non Method
- Method
linearities Method
and O
max Method
pooling Method
, O
but O
with O
the O
following O
differences O
: O
( O
i O
) O
no O
contrast O
normalization O
is O
used O
; O
( O
ii O
) O
pooling O
regions O
are O
non O
- O
overlapping O
and O
( O
iii O
) O
our O
model O
has O
larger O
1st O
and O
2nd O
layer O
feature O
maps O
, O
thanks O
to O
a O
smaller O
stride O
( O
2 O
instead O
of O
4 O
) O
. O
A O
larger O
stride O
is O
beneficial O
for O
speed O
but O
will O
hurt O
accuracy Metric
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
show O
the O
filter O
coefficients O
from O
the O
first O
two O
convolutional Method
layers Method
. O
The O
first O
layer O
filters O
capture O
orientated O
edges O
, O
patterns O
and O
blobs O
. O
In O
the O
second O
layer O
, O
the O
filters O
have O
a O
variety O
of O
forms O
, O
some O
diffuse O
, O
others O
with O
strong O
line O
structures O
or O
oriented O
edges O
. O
subsection O
: O
Feature Method
Extractor Method
Along O
with O
this O
paper O
, O
we O
release O
a O
feature Method
extractor Method
named O
“ O
OverFeat Method
” O
in O
order O
to O
provide O
powerful O
features O
for O
computer Task
vision Task
research Task
. O
Two O
models O
are O
provided O
, O
a O
fast O
and O
accurate O
one O
. O
Each O
architecture O
is O
described O
in O
tables O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
We O
also O
compare O
their O
sizes O
in O
Table O
[ O
reference O
] O
in O
terms O
of O
parameters O
and O
connections O
. O
The O
accurate O
model O
is O
more O
accurate O
than O
the O
fast O
one O
( O
14.18 O
% O
classification Metric
error Metric
as O
opposed O
to O
16.39 O
% O
in O
Table O
[ O
reference O
] O
) O
, O
however O
it O
requires O
nearly O
twice O
as O
many O
connections O
. O
Using O
a O
committee O
of O
7 O
accurate O
models O
reaches O
13.6 O
% O
classification Metric
error Metric
as O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
subsection O
: O
Multi Task
- Task
Scale Task
Classification Task
In O
, O
multi Method
- Method
view Method
voting Method
is O
used O
to O
boost O
performance O
: O
a O
fixed O
set O
of O
10 O
views O
( O
4 O
corners O
and O
center O
, O
with O
horizontal O
flip O
) O
is O
averaged O
. O
However O
, O
this O
approach O
can O
ignore O
many O
regions O
of O
the O
image O
, O
and O
is O
computationally O
redundant O
when O
views O
overlap O
. O
Additionally O
, O
it O
is O
only O
applied O
at O
a O
single O
scale O
, O
which O
may O
not O
be O
the O
scale O
at O
which O
the O
ConvNet Method
will O
respond O
with O
optimal O
confidence O
. O
Instead O
, O
we O
explore O
the O
entire O
image O
by O
densely O
running O
the O
network O
at O
each O
location O
and O
at O
multiple O
scales O
. O
While O
the O
sliding Method
window Method
approach Method
may O
be O
computationally O
prohibitive O
for O
certain O
types O
of O
model O
, O
it O
is O
inherently O
efficient O
in O
the O
case O
of O
ConvNets Task
( O
see O
section O
[ O
reference O
] O
) O
. O
This O
approach O
yields O
significantly O
more O
views O
for O
voting Task
, O
which O
increases O
robustness Metric
while O
remaining O
efficient O
. O
The O
result O
of O
convolving O
a O
ConvNet Method
on O
an O
image O
of O
arbitrary O
size O
is O
a O
spatial O
map O
of O
- O
dimensional O
vectors O
at O
each O
scale O
. O
However O
, O
the O
total O
subsampling Metric
ratio Metric
in O
the O
network O
described O
above O
is O
2x3x2x3 O
, O
or O
36 O
. O
Hence O
when O
applied O
densely O
, O
this O
architecture O
can O
only O
produce O
a O
classification O
vector O
every O
36 O
pixels O
in O
the O
input O
dimension O
along O
each O
axis O
. O
This O
coarse O
distribution O
of O
outputs O
decreases O
performance O
compared O
to O
the O
10 Method
- Method
view Method
scheme Method
because O
the O
network O
windows O
are O
not O
well O
aligned O
with O
the O
objects O
in O
the O
images O
. O
The O
better O
aligned O
the O
network O
window O
and O
the O
object O
, O
the O
strongest O
the O
confidence O
of O
the O
network O
response O
. O
To O
circumvent O
this O
problem O
, O
we O
take O
an O
approach O
similar O
to O
that O
introduced O
by O
Giusti O
et O
al O
. O
[ O
] O
, O
and O
apply O
the O
last O
subsampling Method
operation Method
at O
every O
offset O
. O
This O
removes O
the O
loss O
of O
resolution O
from O
this O
layer O
, O
yielding O
a O
total O
subsampling Metric
ratio Metric
of O
x12 O
instead O
of O
x36 O
. O
We O
now O
explain O
in O
detail O
how O
the O
resolution Task
augmentation Task
is O
performed O
. O
We O
use O
6 O
scales O
of O
input O
which O
result O
in O
unpooled O
layer O
5 O
maps O
of O
varying O
resolution O
( O
see O
Table O
[ O
reference O
] O
for O
details O
) O
. O
These O
are O
then O
pooled O
and O
presented O
to O
the O
classifier Method
using O
the O
following O
procedure O
, O
illustrated O
in O
Fig O
. O
[ O
reference O
] O
: O
For O
a O
single O
image O
, O
at O
a O
given O
scale O
, O
we O
start O
with O
the O
unpooled Method
layer Method
5 Method
feature Method
maps Method
. O
Each O
of O
unpooled O
maps O
undergoes O
a O
3x3 O
max Method
pooling Method
operation Method
( O
non O
- O
overlapping O
regions O
) O
, O
repeated O
3x3 O
times O
for O
pixel O
offsets O
of O
. O
This O
produces O
a O
set O
of O
pooled O
feature O
maps O
, O
replicated O
( O
3x3 O
) O
times O
for O
different O
combinations O
. O
The O
classifier Method
( O
layers O
6 O
, O
7 O
, O
8 O
) O
has O
a O
fixed O
input O
size O
of O
5x5 O
and O
produces O
a O
- O
dimensional O
output O
vector O
for O
each O
location O
within O
the O
pooled O
maps O
. O
The O
classifier Method
is O
applied O
in O
sliding Method
- Method
window Method
fashion Method
to O
the O
pooled O
maps O
, O
yielding O
- O
dimensional O
output O
maps O
( O
for O
a O
given O
combination O
) O
. O
The O
output O
maps O
for O
different O
combinations O
are O
reshaped O
into O
a O
single O
3D O
output O
map O
( O
two O
spatial O
dimensions O
x O
classes O
) O
. O
These O
operations O
can O
be O
viewed O
as O
shifting O
the O
classifier O
’s O
viewing O
window O
by O
1 O
pixel O
through O
pooling Method
layers Method
without O
subsampling Method
and O
using O
skip Method
- Method
kernels Method
in O
the O
following O
layer O
( O
where O
values O
in O
the O
neighborhood O
are O
non O
- O
adjacent O
) O
. O
Or O
equivalently O
, O
as O
applying O
the O
final O
pooling Method
layer Method
and O
fully Method
- Method
connected Method
stack Method
at O
every O
possible O
offset O
, O
and O
assembling O
the O
results O
by O
interleaving O
the O
outputs O
. O
The O
procedure O
above O
is O
repeated O
for O
the O
horizontally O
flipped O
version O
of O
each O
image O
. O
We O
then O
produce O
the O
final O
classification Task
by O
( O
i O
) O
taking O
the O
spatial O
max O
for O
each O
class O
, O
at O
each O
scale O
and O
flip O
; O
( O
ii O
) O
averaging O
the O
resulting O
- O
dimensional O
vectors O
from O
different O
scales O
and O
flips O
and O
( O
iii O
) O
taking O
the O
top O
- O
1 O
or O
top O
- O
5 O
elements O
( O
depending O
on O
the O
evaluation Metric
criterion Metric
) O
from O
the O
mean O
class O
vector O
. O
At O
an O
intuitive O
level O
, O
the O
two O
halves O
of O
the O
network O
— O
i.e. O
feature Method
extraction Method
layers Method
( O
1 O
- O
5 O
) O
and O
classifier Method
layers Method
( O
6 O
- O
output O
) O
— O
are O
used O
in O
opposite O
ways O
. O
In O
the O
feature Task
extraction Task
portion Task
, O
the O
filters O
are O
convolved O
across O
the O
entire O
image O
in O
one O
pass O
. O
From O
a O
computational O
perspective O
, O
this O
is O
far O
more O
efficient O
than O
sliding O
a O
fixed Method
- Method
size Method
feature Method
extractor Method
over O
the O
image O
and O
then O
aggregating O
the O
results O
from O
different O
locations O
. O
However O
, O
these O
principles O
are O
reversed O
for O
the O
classifier Method
portion Method
of O
the O
network O
. O
Here O
, O
we O
want O
to O
hunt O
for O
a O
fixed Method
- Method
size Method
representation Method
in O
the O
layer O
5 O
feature O
maps O
across O
different O
positions O
and O
scales O
. O
Thus O
the O
classifier Method
has O
a O
fixed O
- O
size O
5x5 O
input O
and O
is O
exhaustively O
applied O
to O
the O
layer O
5 O
maps O
. O
The O
exhaustive Method
pooling Method
scheme Method
( O
with O
single O
pixel O
shifts O
) O
ensures O
that O
we O
can O
obtain O
fine O
alignment O
between O
the O
classifier Method
and O
the O
representation O
of O
the O
object O
in O
the O
feature O
map O
. O
subsection O
: O
Results O
In O
Table O
[ O
reference O
] O
, O
we O
experiment O
with O
different O
approaches O
, O
and O
compare O
them O
to O
the O
single Method
network Method
model Method
of O
Krizhevsky O
et O
al O
. O
[ O
] O
for O
reference O
. O
The O
approach O
described O
above O
, O
with O
6 O
scales O
, O
achieves O
a O
top Metric
- Metric
5 Metric
error Metric
rate Metric
of O
13.6 O
% O
. O
As O
might O
be O
expected O
, O
using O
fewer O
scales O
hurts O
performance O
: O
the O
single Method
- Method
scale Method
model Method
is O
worse O
with O
16.97 O
% O
top Metric
- Metric
5 Metric
error Metric
. O
The O
fine Method
stride Method
technique Method
illustrated O
in O
Fig O
. O
[ O
reference O
] O
brings O
a O
relatively O
small O
improvement O
in O
the O
single O
scale O
regime O
, O
but O
is O
also O
of O
importance O
for O
the O
multi Task
- Task
scale Task
gains Task
shown O
here O
. O
We O
report O
the O
test O
set O
results O
of O
the O
2013 O
competition O
in O
Fig O
. O
[ O
reference O
] O
where O
our O
model O
( O
OverFeat Method
) O
obtained O
14.2 O
% O
accuracy Metric
by O
voting O
of O
7 O
ConvNets Method
( O
each O
trained O
with O
different O
initializations O
) O
and O
ranked O
5th O
out O
of O
18 O
teams O
. O
The O
best O
accuracy Metric
using O
only O
ILSVRC13 Material
data Material
was O
11.7 O
% O
. O
Pre O
- O
training O
with O
extra O
data O
from O
the O
ImageNet Material
Fall11 Material
dataset Material
improved O
this O
number O
to O
11.2 O
% O
. O
In O
post O
- O
competition O
work O
, O
we O
improve O
the O
OverFeat Metric
results Metric
down O
to O
13.6 O
% O
error Metric
by O
using O
bigger O
models O
( O
more O
features O
and O
more O
layers O
) O
. O
Due O
to O
time O
constraints O
, O
these O
bigger O
models O
are O
not O
fully O
trained O
, O
more O
improvements O
are O
expected O
to O
appear O
in O
time O
. O
subsection O
: O
ConvNets Method
and O
Sliding Method
Window Method
Efficiency Method
In O
contrast O
to O
many O
sliding Method
- Method
window Method
approaches Method
that O
compute O
an O
entire O
pipeline O
for O
each O
window O
of O
the O
input O
one O
at O
a O
time O
, O
ConvNets Method
are O
inherently O
efficient O
when O
applied O
in O
a O
sliding O
fashion O
because O
they O
naturally O
share O
computations O
common O
to O
overlapping O
regions O
. O
When O
applying O
our O
network O
to O
larger O
images O
at O
test O
time O
, O
we O
simply O
apply O
each O
convolution Method
over O
the O
extent O
of O
the O
full O
image O
. O
This O
extends O
the O
output O
of O
each O
layer O
to O
cover O
the O
new O
image O
size O
, O
eventually O
producing O
a O
map O
of O
output O
class O
predictions O
, O
with O
one O
spatial O
location O
for O
each O
“ O
window O
” O
( O
field O
of O
view O
) O
of O
input O
. O
This O
is O
diagrammed O
in O
Fig O
. O
[ O
reference O
] O
. O
Convolutions Method
are O
applied O
bottom O
- O
up O
, O
so O
that O
the O
computations O
common O
to O
neighboring O
windows O
need O
only O
be O
done O
once O
. O
Note O
that O
the O
last O
layers O
of O
our O
architecture O
are O
fully Method
connected Method
linear Method
layers Method
. O
At O
test O
time O
, O
these O
layers O
are O
effectively O
replaced O
by O
convolution Method
operations Method
with O
kernels O
of O
1x1 O
spatial O
extent O
. O
The O
entire O
ConvNet Method
is O
then O
simply O
a O
sequence Method
of Method
convolutions Method
, O
max Method
- Method
pooling Method
and O
thresholding Method
operations Method
exclusively O
. O
section O
: O
Localization Task
Starting O
from O
our O
classification Method
- Method
trained Method
network Method
, O
we O
replace O
the O
classifier Method
layers Method
by O
a O
regression Method
network Method
and O
train O
it O
to O
predict O
object O
bounding O
boxes O
at O
each O
spatial O
location O
and O
scale O
. O
We O
then O
combine O
the O
regression Method
predictions Method
together O
, O
along O
with O
the O
classification O
results O
at O
each O
location O
, O
as O
we O
now O
describe O
. O
subsection O
: O
Generating Task
Predictions Task
To O
generate O
object Task
bounding Task
box Task
predictions Task
, O
we O
simultaneously O
run O
the O
classifier Method
and Method
regressor Method
networks Method
across O
all O
locations O
and O
scales O
. O
Since O
these O
share O
the O
same O
feature Method
extraction Method
layers Method
, O
only O
the O
final O
regression Method
layers Method
need O
to O
be O
recomputed O
after O
computing O
the O
classification Method
network Method
. O
The O
output O
of O
the O
final O
softmax Method
layer Method
for O
a O
class O
at O
each O
location O
provides O
a O
score O
of O
confidence O
that O
an O
object O
of O
class O
is O
present O
( O
though O
not O
necessarily O
fully O
contained O
) O
in O
the O
corresponding O
field O
of O
view O
. O
Thus O
we O
can O
assign O
a O
confidence O
to O
each O
bounding O
box O
. O
subsection O
: O
Regressor Method
Training Method
The O
regression Method
network Method
takes O
as O
input O
the O
pooled O
feature O
maps O
from O
layer O
5 O
. O
It O
has O
2 O
fully O
- O
connected O
hidden O
layers O
of O
size O
4096 O
and O
1024 O
channels O
, O
respectively O
. O
The O
final O
output O
layer O
has O
4 O
units O
which O
specify O
the O
coordinates O
for O
the O
bounding O
box O
edges O
. O
As O
with O
classification Task
, O
there O
are O
( O
3x3 O
) O
copies O
throughout O
, O
resulting O
from O
the O
shifts O
. O
The O
architecture O
is O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
We O
fix O
the O
feature Method
extraction Method
layers Method
( O
1 O
- O
5 O
) O
from O
the O
classification Method
network Method
and O
train O
the O
regression Method
network Method
using O
an O
loss O
between O
the O
predicted O
and O
true O
bounding O
box O
for O
each O
example O
. O
The O
final O
regressor Method
layer Method
is O
class O
- O
specific O
, O
having O
1000 O
different O
versions O
, O
one O
for O
each O
class O
. O
We O
train O
this O
network O
using O
the O
same O
set O
of O
scales O
as O
described O
in O
Section O
[ O
reference O
] O
. O
We O
compare O
the O
prediction O
of O
the O
regressor Method
net Method
at O
each O
spatial O
location O
with O
the O
ground O
- O
truth O
bounding O
box O
, O
shifted O
into O
the O
frame O
of O
reference O
of O
the O
regressor O
’s O
translation O
offset O
within O
the O
convolution O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
. O
However O
, O
we O
do O
not O
train O
the O
regressor Method
on O
bounding O
boxes O
with O
less O
than O
50 O
% O
overlap O
with O
the O
input O
field O
of O
view O
: O
since O
the O
object O
is O
mostly O
outside O
of O
these O
locations O
, O
it O
will O
be O
better O
handled O
by O
regression Method
windows Method
that O
do O
contain O
the O
object O
. O
Training O
the O
regressors Method
in O
a O
multi O
- O
scale O
manner O
is O
important O
for O
the O
across Task
- Task
scale Task
prediction Task
combination Task
. O
Training O
on O
a O
single O
scale O
will O
perform O
well O
on O
that O
scale O
and O
still O
perform O
reasonably O
on O
other O
scales O
. O
However O
training O
multi O
- O
scale O
will O
make O
predictions O
match O
correctly O
across O
scales O
and O
exponentially O
increase O
the O
confidence O
of O
the O
merged O
predictions O
. O
In O
turn O
, O
this O
allows O
us O
to O
perform O
well O
with O
a O
few O
scales O
only O
, O
rather O
than O
many O
scales O
as O
is O
typically O
the O
case O
in O
detection Task
. O
The O
typical O
ratio O
from O
one O
scale O
to O
another O
in O
pedestrian Task
detection Task
is O
about O
1.05 O
to O
1.1 O
, O
here O
however O
we O
use O
a O
large O
ratio O
of O
approximately O
1.4 O
( O
this O
number O
differs O
for O
each O
scale O
since O
dimensions O
are O
adjusted O
to O
fit O
exactly O
the O
stride O
of O
our O
network O
) O
which O
allows O
us O
to O
run O
our O
system O
faster O
. O
subsection O
: O
Combining Task
Predictions Task
We O
combine O
the O
individual O
predictions O
( O
see O
Fig O
. O
[ O
reference O
] O
) O
via O
a O
greedy Method
merge Method
strategy Method
applied O
to O
the O
regressor O
bounding O
boxes O
, O
using O
the O
following O
algorithm O
. O
Assign O
to O
the O
set O
of O
classes O
in O
the O
top O
for O
each O
scale O
, O
found O
by O
taking O
the O
maximum O
detection O
class O
outputs O
across O
spatial O
locations O
for O
that O
scale O
. O
Assign O
to O
the O
set O
of O
bounding O
boxes O
predicted O
by O
the O
regressor Method
network Method
for O
each O
class O
in O
, O
across O
all O
spatial O
locations O
at O
scale O
. O
Assign O
Repeat O
merging O
until O
done O
: O
If O
, O
stop O
. O
Otherwise O
, O
set O
In O
the O
above O
, O
we O
compute O
match_score Method
using O
the O
sum O
of O
the O
distance O
between O
centers O
of O
the O
two O
bounding O
boxes O
and O
the O
intersection O
area O
of O
the O
boxes O
. O
box_merge Method
compute O
the O
average O
of O
the O
bounding O
boxes O
’ O
coordinates O
. O
The O
final O
prediction Task
is O
given O
by O
taking O
the O
merged O
bounding O
boxes O
with O
maximum O
class O
scores O
. O
This O
is O
computed O
by O
cumulatively O
adding O
the O
detection O
class O
outputs O
associated O
with O
the O
input O
windows O
from O
which O
each O
bounding O
box O
was O
predicted O
. O
See O
Fig O
. O
[ O
reference O
] O
for O
an O
example O
of O
bounding O
boxes O
merged O
into O
a O
single O
high O
- O
confidence O
bounding O
box O
. O
In O
that O
example O
, O
some O
turtle O
and O
whale O
bounding O
boxes O
appear O
in O
the O
intermediate O
multi O
- O
scale O
steps O
, O
but O
disappear O
in O
the O
final O
detection Task
image Task
. O
Not O
only O
do O
these O
bounding O
boxes O
have O
low O
classification Metric
confidence Metric
( O
at O
most O
0.11 O
and O
0.12 O
respectively O
) O
, O
their O
collection O
is O
not O
as O
coherent O
as O
the O
bear O
bounding O
boxes O
to O
get O
a O
significant O
confidence O
boost O
. O
The O
bear O
boxes O
have O
a O
strong O
confidence O
( O
approximately O
0.5 O
on O
average O
per O
scale O
) O
and O
high O
matching Metric
scores Metric
. O
Hence O
after O
merging O
, O
many O
bear O
bounding O
boxes O
are O
fused O
into O
a O
single O
very O
high O
confidence O
box O
, O
while O
false O
positives O
disappear O
below O
the O
detection Metric
threshold Metric
due O
their O
lack O
of O
bounding O
box O
coherence O
and O
confidence O
. O
This O
analysis O
suggest O
that O
our O
approach O
is O
naturally O
more O
robust O
to O
false O
positives O
coming O
from O
the O
pure Method
- Method
classification Method
model Method
than O
traditional O
non Method
- Method
maximum Method
suppression Method
, O
by O
rewarding O
bounding O
box O
coherence O
. O
subsection O
: O
Experiments O
We O
apply O
our O
network O
to O
the O
Imagenet Material
2012 Material
validation Material
set Material
using O
the O
localization Metric
criterion Metric
specified O
for O
the O
competition O
. O
The O
results O
for O
this O
are O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
Fig O
. O
[ O
reference O
] O
shows O
the O
results O
of O
the O
2012 O
and O
2013 O
localization Task
competitions Task
( O
the O
train O
and O
test O
data O
are O
the O
same O
for O
both O
of O
these O
years O
) O
. O
Our O
method O
is O
the O
winner O
of O
the O
2013 O
competition O
with O
29.9 O
% O
error Metric
. O
Our O
multiscale Method
and Method
multi Method
- Method
view Method
approach Method
was O
critical O
to O
obtaining O
good O
performance O
, O
as O
can O
be O
seen O
in O
Fig O
. O
[ O
reference O
] O
: O
Using O
only O
a O
single O
centered O
crop O
, O
our O
regressor Method
network Method
achieves O
an O
error Metric
rate Metric
of O
40 O
% O
. O
By O
combining O
regressor Method
predictions Method
from O
all O
spatial O
locations O
at O
two O
scales O
, O
we O
achieve O
a O
vastly O
better O
error Metric
rate Metric
of O
31.5 O
% O
. O
Adding O
a O
third O
and O
fourth O
scale O
further O
improves O
performance O
to O
30.0 O
% O
error O
. O
Using O
a O
different O
top O
layer O
for O
each O
class O
in O
the O
regressor Method
network Method
for O
each O
class O
( O
Per O
- O
Class O
Regressor O
( O
PCR O
) O
in O
Fig O
. O
[ O
reference O
] O
) O
surprisingly O
did O
not O
outperform O
using O
only O
a O
single O
network O
shared O
among O
all O
classes O
( O
44.1 O
% O
vs. O
31.3 O
% O
) O
. O
This O
may O
be O
because O
there O
are O
relatively O
few O
examples O
per O
class O
annotated O
with O
bounding O
boxes O
in O
the O
training O
set O
, O
while O
the O
network O
has O
1000 O
times O
more O
top O
- O
layer O
parameters O
, O
resulting O
in O
insufficient O
training O
. O
It O
is O
possible O
this O
approach O
may O
be O
improved O
by O
sharing O
parameters O
only O
among O
similar O
classes O
( O
e.g. O
training O
one O
network O
for O
all O
classes O
of O
dogs O
, O
another O
for O
vehicles O
, O
etc O
. O
) O
. O
section O
: O
Detection Task
Detection Task
training Task
is O
similar O
to O
classification Task
training Task
but O
in O
a O
spatial O
manner O
. O
Multiple O
location O
of O
an O
image O
may O
be O
trained O
simultaneously O
. O
Since O
the O
model O
is O
convolutional O
, O
all O
weights O
are O
shared O
among O
all O
locations O
. O
The O
main O
difference O
with O
the O
localization Task
task Task
, O
is O
the O
necessity O
to O
predict O
a O
background O
class O
when O
no O
object O
is O
present O
. O
Traditionally O
, O
negative O
examples O
are O
initially O
taken O
at O
random O
for O
training O
. O
Then O
the O
most O
offending O
negative O
errors O
are O
added O
to O
the O
training O
set O
in O
bootstrapping O
passes O
. O
Independent O
bootstrapping O
passes O
render O
training O
complicated O
and O
risk O
potential O
mismatches O
between O
the O
negative O
examples O
collection O
and O
training O
times O
. O
Additionally O
, O
the O
size O
of O
bootstrapping O
passes O
needs O
to O
be O
tuned O
to O
make O
sure O
training O
does O
not O
overfit O
on O
a O
small O
set O
. O
To O
circumvent O
all O
these O
problems O
, O
we O
perform O
negative Task
training Task
on O
the O
fly O
, O
by O
selecting O
a O
few O
interesting O
negative O
examples O
per O
image O
such O
as O
random O
ones O
or O
most O
offending O
ones O
. O
This O
approach O
is O
more O
computationally O
expensive O
, O
but O
renders O
the O
procedure O
much O
simpler O
. O
And O
since O
the O
feature Method
extraction Method
is O
initially O
trained O
with O
the O
classification Task
task Task
, O
the O
detection Task
fine Task
- Task
tuning Task
is O
not O
as O
long O
anyway O
. O
In O
Fig O
. O
[ O
reference O
] O
, O
we O
report O
the O
results O
of O
the O
ILSVRC Task
2013 Task
competition Task
where O
our O
detection Method
system Method
ranked O
3rd O
with O
19.4 O
% O
mean Metric
average Metric
precision Metric
( O
mAP Metric
) O
. O
We O
later O
established O
a O
new O
detection Task
state O
of O
the O
art O
with O
24.3 O
% O
mAP Metric
. O
Note O
that O
there O
is O
a O
large O
gap O
between O
the O
top O
3 O
methods O
and O
other O
teams O
( O
the O
4th O
method O
yields O
11.5 O
% O
mAP Metric
) O
. O
Additionally O
, O
our O
approach O
is O
considerably O
different O
from O
the O
top O
2 O
other O
systems O
which O
use O
an O
initial O
segmentation Method
step Method
to O
reduce O
candidate O
windows O
from O
approximately O
200 O
, O
000 O
to O
2 O
, O
000 O
. O
This O
technique O
speeds O
up O
inference Task
and O
substantially O
reduces O
the O
number O
of O
potential O
false O
positives O
. O
suggest O
that O
detection Metric
accuracy Metric
drops O
when O
using O
dense O
sliding O
window O
as O
opposed O
to O
selective Method
search Method
which O
discards O
unlikely O
object O
locations O
hence O
reducing O
false O
positives O
. O
Combined O
with O
our O
method O
, O
we O
may O
observe O
similar O
improvements O
as O
seen O
here O
between O
traditional O
dense Method
methods Method
and O
segmentation Method
based Method
methods Method
. O
It O
should O
also O
be O
noted O
that O
we O
did O
not O
fine O
tune O
on O
the O
detection Metric
validation Metric
set Metric
as O
NEC O
and O
UvA O
did O
. O
The O
validation Metric
and Metric
test Metric
set Metric
distributions Metric
differ O
significantly O
enough O
from O
the O
training O
set O
that O
this O
alone O
improves O
results O
by O
approximately O
1 O
point O
. O
The O
improvement O
between O
the O
two O
OverFeat Metric
results Metric
in O
Fig O
. O
[ O
reference O
] O
are O
due O
to O
longer O
training Metric
times Metric
and O
the O
use O
of O
context O
, O
i.e. O
each O
scale O
also O
uses O
lower O
resolution O
scales O
as O
input O
. O
section O
: O
Discussion O
We O
have O
presented O
a O
multi Method
- Method
scale Method
, Method
sliding Method
window Method
approach Method
that O
can O
be O
used O
for O
classification Task
, Task
localization Task
and Task
detection Task
. O
We O
applied O
it O
to O
the O
ILSVRC Material
2013 Material
datasets Material
, O
and O
it O
currently O
ranks O
4 O
th O
in O
classification Task
, O
1 O
st O
in O
localization Task
and O
1 O
st O
in O
detection Task
. O
A O
second O
important O
contribution O
of O
our O
paper O
is O
explaining O
how O
ConvNets Method
can O
be O
effectively O
used O
for O
detection Task
and Task
localization Task
tasks Task
. O
These O
were O
never O
addressed O
in O
and O
thus O
we O
are O
the O
first O
to O
explain O
how O
this O
can O
be O
done O
in O
the O
context O
of O
ImageNet Material
2012 Material
. O
The O
scheme O
we O
propose O
involves O
substantial O
modifications O
to O
networks O
designed O
for O
classification Task
, O
but O
clearly O
demonstrate O
that O
ConvNets Method
are O
capable O
of O
these O
more O
challenging O
tasks O
. O
Our O
localization Method
approach Method
won O
the O
2013 O
ILSVRC Metric
competition Metric
and O
significantly O
outperformed O
all O
2012 O
and O
2013 O
approaches O
. O
The O
detection Method
model Method
was O
among O
the O
top O
performers O
during O
the O
competition O
, O
and O
ranks O
first O
in O
post O
- O
competition O
results O
. O
We O
have O
proposed O
an O
integrated Method
pipeline Method
that O
can O
perform O
different O
tasks O
while O
sharing O
a O
common O
feature O
extraction O
base O
, O
entirely O
learned O
directly O
from O
the O
pixels O
. O
Our O
approach O
might O
still O
be O
improved O
in O
several O
ways O
. O
( O
i O
) O
For O
localization Task
, O
we O
are O
not O
currently O
back O
- O
propping O
through O
the O
whole O
network O
; O
doing O
so O
is O
likely O
to O
improve O
performance O
. O
( O
ii O
) O
We O
are O
using O
loss Method
, O
rather O
than O
directly O
optimizing O
the O
intersection Metric
- Metric
over Metric
- Metric
union Metric
( O
IOU Metric
) O
criterion O
on O
which O
performance O
is O
measured O
. O
Swapping O
the O
loss O
to O
this O
should O
be O
possible O
since O
IOU Metric
is O
still O
differentiable O
, O
provided O
there O
is O
some O
overlap O
. O
( O
iii O
) O
Alternate O
parameterizations O
of O
the O
bounding O
box O
may O
help O
to O
decorrelate O
the O
outputs O
, O
which O
will O
aid O
network Method
training Method
. O
. O
/ O
bib O
/ O
bibli O
, O
.. O
/ O
bib O
/ O
refs O
, O
.. O
/ O
bib O
/ O
bib O
- O
bengio O
, O
.. O
/ O
bib O
/ O
bib O
- O
bibli O
, O
.. O
/ O
bib O
/ O
bib O
- O
convnets O
, O
.. O
/ O
bib O
/ O
bib O
- O
deep O
, O
.. O
/ O
bib O
/ O
bib O
- O
deeplearning O
, O
.. O
/ O
bib O
/ O
bib O
- O
lecun O
, O
.. O
/ O
bib O
/ O
bib O
- O
jarrett O
. O
section O
: O
Appendix O
: O
Additional O
Model O
Details O
