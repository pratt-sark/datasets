document	O
:	O
Compressing	Task
Word	Task
Embeddings	Task
via	O
Deep	Method
Compositional	Method
Code	Method
Learning	Method
Natural	Method
language	Method
processing	Method
(	O
NLP	Method
)	O
models	O
often	O
require	O
a	O
massive	O
number	O
of	O
parameters	O
for	O
word	O
embeddings	O
,	O
resulting	O
in	O
a	O
large	O
storage	O
or	O
memory	O
footprint	O
.	O
Deploying	O
neural	O
NLP	Method
models	O
to	O
mobile	Task
devices	Task
requires	O
compressing	O
the	O
word	O
embeddings	O
without	O
any	O
significant	O
sacrifices	O
in	O
performance	O
.	O
For	O
this	O
purpose	O
,	O
we	O
propose	O
to	O
construct	O
the	O
embeddings	O
with	O
few	O
basis	O
vectors	O
.	O
For	O
each	O
word	O
,	O
the	O
composition	O
of	O
basis	O
vectors	O
is	O
determined	O
by	O
a	O
hash	Method
code	Method
.	O
To	O
maximize	O
the	O
compression	Metric
rate	Metric
,	O
we	O
adopt	O
the	O
multi	Method
-	Method
codebook	Method
quantization	Method
approach	Method
instead	O
of	O
binary	Method
coding	Method
scheme	Method
.	O
Each	O
code	O
is	O
composed	O
of	O
multiple	O
discrete	O
numbers	O
,	O
such	O
as	O
,	O
where	O
the	O
value	O
of	O
each	O
component	O
is	O
limited	O
to	O
a	O
fixed	O
range	O
.	O
We	O
propose	O
to	O
directly	O
learn	O
the	O
discrete	O
codes	O
in	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
by	O
applying	O
the	O
Gumbel	Method
-	Method
softmax	Method
trick	Method
.	O
Experiments	O
show	O
the	O
compression	Metric
rate	Metric
achieves	O
in	O
a	O
sentiment	Task
analysis	Task
task	Task
and	O
in	O
machine	Task
translation	Task
tasks	Task
without	O
performance	O
loss	O
.	O
In	O
both	O
tasks	O
,	O
the	O
proposed	O
method	O
can	O
improve	O
the	O
model	O
performance	O
by	O
slightly	O
lowering	O
the	O
compression	Metric
rate	Metric
.	O
Compared	O
to	O
other	O
approaches	O
such	O
as	O
character	Task
-	Task
level	Task
segmentation	Task
,	O
the	O
proposed	O
method	O
is	O
language	O
-	O
independent	O
and	O
does	O
not	O
require	O
modifications	O
to	O
the	O
network	Method
architecture	Method
.	O
section	O
:	O
Introduction	O
Word	Method
embeddings	Method
play	O
an	O
important	O
role	O
in	O
neural	Method
-	Method
based	Method
natural	Method
language	Method
processing	Method
(	O
NLP	Method
)	O
models	O
.	O
Neural	Method
word	Method
embeddings	Method
encapsulate	O
the	O
linguistic	O
information	O
of	O
words	O
in	O
continuous	O
vectors	O
.	O
However	O
,	O
as	O
each	O
word	O
is	O
assigned	O
an	O
independent	O
embedding	O
vector	O
,	O
the	O
number	O
of	O
parameters	O
in	O
the	O
embedding	O
matrix	O
can	O
be	O
huge	O
.	O
For	O
example	O
,	O
when	O
each	O
embedding	O
has	O
500	O
dimensions	O
,	O
the	O
network	O
has	O
to	O
hold	O
100	O
M	O
embedding	O
parameters	O
to	O
represent	O
200	O
K	O
words	O
.	O
In	O
practice	O
,	O
for	O
a	O
simple	O
sentiment	Method
analysis	Method
model	Method
,	O
the	O
word	O
embedding	O
parameters	O
account	O
for	O
98.8	O
%	O
of	O
the	O
total	O
parameters	O
.	O
As	O
only	O
a	O
small	O
portion	O
of	O
the	O
word	O
embeddings	O
is	O
selected	O
in	O
the	O
forward	O
pass	O
,	O
the	O
giant	O
embedding	O
matrix	O
usually	O
does	O
not	O
cause	O
a	O
speed	O
issue	O
.	O
However	O
,	O
the	O
massive	O
number	O
of	O
parameters	O
in	O
the	O
neural	Method
network	Method
results	O
in	O
a	O
large	O
storage	O
or	O
memory	Metric
footprint	Metric
.	O
When	O
other	O
components	O
of	O
the	O
neural	Method
network	Method
are	O
also	O
large	O
,	O
the	O
model	O
may	O
fail	O
to	O
fit	O
into	O
GPU	O
memory	O
during	O
training	O
.	O
Moreover	O
,	O
as	O
the	O
demand	O
for	O
low	Task
-	Task
latency	Task
neural	Task
computation	Task
for	O
mobile	Task
platforms	Task
increases	O
,	O
some	O
neural	Method
-	Method
based	Method
models	Method
are	O
expected	O
to	O
run	O
on	O
mobile	O
devices	O
.	O
Thus	O
,	O
it	O
is	O
becoming	O
more	O
important	O
to	O
compress	O
the	O
size	O
of	O
NLP	Method
models	O
for	O
deployment	O
to	O
devices	O
with	O
limited	O
memory	O
or	O
storage	O
capacity	O
.	O
In	O
this	O
study	O
,	O
we	O
attempt	O
to	O
reduce	O
the	O
number	O
of	O
parameters	O
used	O
in	O
word	O
embeddings	O
without	O
hurting	O
the	O
model	O
performance	O
.	O
Neural	Method
networks	Method
are	O
known	O
for	O
the	O
significant	O
redundancy	O
in	O
the	O
connections	O
Denil2013PredictingPI	O
.	O
In	O
this	O
work	O
,	O
we	O
further	O
hypothesize	O
that	O
learning	O
independent	O
embeddings	O
causes	O
more	O
redundancy	O
in	O
the	O
embedding	O
vectors	O
,	O
as	O
the	O
inter	O
-	O
similarity	O
among	O
words	O
is	O
ignored	O
.	O
Some	O
words	O
are	O
very	O
similar	O
regarding	O
the	O
semantics	O
.	O
For	O
example	O
,	O
“	O
dog	O
”	O
and	O
“	O
dogs	O
”	O
have	O
almost	O
the	O
same	O
meaning	O
,	O
except	O
one	O
is	O
plural	O
.	O
To	O
efficiently	O
represent	O
these	O
two	O
words	O
,	O
it	O
is	O
desirable	O
to	O
share	O
information	O
between	O
the	O
two	O
embeddings	O
.	O
However	O
,	O
a	O
small	O
portion	O
in	O
both	O
vectors	O
still	O
has	O
to	O
be	O
trained	O
independently	O
to	O
capture	O
the	O
syntactic	O
difference	O
.	O
Following	O
the	O
intuition	O
of	O
creating	O
partially	O
shared	O
embeddings	O
,	O
instead	O
of	O
assigning	O
each	O
word	O
a	O
unique	O
ID	O
,	O
we	O
represent	O
each	O
word	O
with	O
a	O
code	O
.	O
Each	O
component	O
is	O
an	O
integer	O
number	O
in	O
.	O
Ideally	O
,	O
similar	O
words	O
should	O
have	O
similar	O
codes	O
.	O
For	O
example	O
,	O
we	O
may	O
desire	O
and	O
.	O
Once	O
we	O
have	O
obtained	O
such	O
compact	O
codes	O
for	O
all	O
words	O
in	O
the	O
vocabulary	O
,	O
we	O
use	O
embedding	O
vectors	O
to	O
represent	O
the	O
codes	O
rather	O
than	O
the	O
unique	O
words	O
.	O
More	O
specifically	O
,	O
we	O
create	O
codebooks	O
,	O
each	O
containing	O
codeword	O
vectors	O
.	O
The	O
embedding	O
of	O
a	O
word	O
is	O
computed	O
by	O
summing	O
up	O
the	O
codewords	O
corresponding	O
to	O
all	O
the	O
components	O
in	O
the	O
code	O
as	O
where	O
is	O
the	O
-	O
th	O
codeword	O
in	O
the	O
codebook	O
.	O
In	O
this	O
way	O
,	O
the	O
number	O
of	O
vectors	O
in	O
the	O
embedding	O
matrix	O
will	O
be	O
,	O
which	O
is	O
usually	O
much	O
smaller	O
than	O
the	O
vocabulary	O
size	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
gives	O
an	O
intuitive	O
comparison	O
between	O
the	O
compositional	Method
approach	Method
and	O
the	O
conventional	O
approach	O
(	O
assigning	O
unique	O
IDs	O
)	O
.	O
The	O
codes	O
of	O
all	O
the	O
words	O
can	O
be	O
stored	O
in	O
an	O
integer	O
matrix	O
,	O
denoted	O
by	O
.	O
Thus	O
,	O
the	O
storage	O
footprint	O
of	O
the	O
embedding	Method
layer	Method
now	O
depends	O
on	O
the	O
total	O
size	O
of	O
the	O
combined	O
codebook	O
and	O
the	O
code	O
matrix	O
.	O
Although	O
the	O
number	O
of	O
embedding	O
vectors	O
can	O
be	O
greatly	O
reduced	O
by	O
using	O
such	O
coding	Method
approach	Method
,	O
we	O
want	O
to	O
prevent	O
any	O
serious	O
degradation	O
in	O
performance	O
compared	O
to	O
the	O
models	O
using	O
normal	Method
embeddings	Method
.	O
In	O
other	O
words	O
,	O
given	O
a	O
set	O
of	O
baseline	O
word	O
embeddings	O
,	O
we	O
wish	O
to	O
find	O
a	O
set	O
of	O
codes	O
and	O
combined	O
codebook	O
that	O
can	O
produce	O
the	O
embeddings	O
with	O
the	O
same	O
effectiveness	O
as	O
.	O
A	O
safe	O
and	O
straight	O
-	O
forward	O
way	O
is	O
to	O
minimize	O
the	O
squared	O
distance	O
between	O
the	O
baseline	O
embeddings	O
and	O
the	O
composed	O
embeddings	O
as	O
where	O
is	O
the	O
vocabulary	O
size	O
.	O
The	O
baseline	O
embeddings	O
can	O
be	O
a	O
set	O
of	O
pre	Method
-	Method
trained	Method
vectors	Method
such	O
as	O
word2vec	Method
Mikolov2013DistributedRO	O
or	O
GloVe	Method
pennington2014GloVe	O
embeddings	O
.	O
In	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
the	O
baseline	O
embedding	O
matrix	O
is	O
approximated	O
by	O
codewords	O
selected	O
from	O
codebooks	O
.	O
The	O
selection	O
of	O
codewords	O
is	O
controlled	O
by	O
the	O
code	O
.	O
Such	O
problem	O
of	O
learning	Task
compact	Task
codes	Task
with	O
multiple	O
codebooks	O
is	O
formalized	O
and	O
discussed	O
in	O
the	O
research	O
field	O
of	O
compression	Task
-	Task
based	Task
source	Task
coding	Task
,	O
known	O
as	O
product	Task
quantization	Task
Jgou2011ProductQF	O
and	O
additive	Task
quantization	Task
Babenko2014AdditiveQF	O
,	O
Martinez2016RevisitingAQ	O
.	O
Previous	O
works	O
learn	O
compositional	O
codes	O
so	O
as	O
to	O
enable	O
an	O
efficient	O
similarity	Task
search	Task
of	Task
vectors	Task
.	O
In	O
this	O
work	O
,	O
we	O
utilize	O
such	O
codes	O
for	O
a	O
different	O
purpose	O
,	O
that	O
is	O
,	O
constructing	O
word	Task
embeddings	Task
with	O
drastically	O
fewer	O
parameters	O
.	O
Due	O
to	O
the	O
discreteness	O
in	O
the	O
hash	O
codes	O
,	O
it	O
is	O
usually	O
difficult	O
to	O
directly	O
optimize	O
the	O
objective	O
function	O
in	O
Eq	O
.	O
[	O
reference	O
]	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
simple	O
and	O
straight	O
-	O
forward	O
method	O
to	O
learn	O
the	O
codes	O
in	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
.	O
We	O
utilize	O
the	O
Gumbel	Method
-	Method
softmax	Method
trick	Method
Maddison2016TheCD	Method
,	O
Jang2016CategoricalRW	O
to	O
find	O
the	O
best	O
discrete	Method
codes	Method
that	O
minimize	O
the	O
loss	O
.	O
Besides	O
the	O
simplicity	O
,	O
this	O
approach	O
also	O
allows	O
one	O
to	O
use	O
any	O
arbitrary	O
differentiable	O
loss	O
function	O
,	O
such	O
as	O
cosine	O
similarity	O
.	O
The	O
contribution	O
of	O
this	O
work	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
We	O
propose	O
to	O
utilize	O
the	O
compositional	Method
coding	Method
approach	Method
for	O
constructing	O
the	O
word	Task
embeddings	Task
with	O
significantly	O
fewer	O
parameters	O
.	O
In	O
the	O
experiments	O
,	O
we	O
show	O
that	O
over	O
98	O
%	O
of	O
the	O
embedding	O
parameters	O
can	O
be	O
eliminated	O
in	O
sentiment	Task
analysis	Task
task	Task
without	O
affecting	O
performance	O
.	O
In	O
machine	Task
translation	Task
tasks	Task
,	O
the	O
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
reaches	O
.	O
We	O
propose	O
a	O
direct	Method
learning	Method
approach	Method
for	O
the	O
codes	O
in	O
an	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
,	O
with	O
a	O
Gumbel	Method
-	Method
softmax	Method
layer	Method
to	O
encourage	O
the	O
discreteness	O
.	O
The	O
neural	Method
network	Method
for	O
learning	Task
codes	Task
will	O
be	O
packaged	O
into	O
a	O
tool	O
.	O
With	O
the	O
learned	O
codes	O
and	O
basis	O
vectors	O
,	O
the	O
computation	Method
graph	Method
for	O
composing	Task
embeddings	Task
is	O
fairly	O
easy	O
to	O
implement	O
,	O
and	O
does	O
not	O
require	O
modifications	O
to	O
other	O
parts	O
in	O
the	O
neural	Method
network	Method
.	O
section	O
:	O
Related	O
Work	O
Existing	O
works	O
for	O
compressing	Method
neural	Method
networks	Method
include	O
low	Method
-	Method
precision	Method
computation	Method
vanhoucke2011improving	O
,	O
hwang2014fixed	O
,	O
courbariaux2014low	O
,	O
Anwar2015FixedPO	O
,	O
quantization	O
Chen2015CompressingNN	O
,	O
han2015deep	O
,	O
Zhou2017IncrementalNQ	O
,	O
network	Method
pruning	Method
LeCun1989OptimalBD	O
,	O
Hassibi1992SecondOD	O
,	O
Han2015LearningBW	O
,	O
Wen2016LearningSS	O
and	O
knowledge	Task
distillation	Task
Hinton2015DistillingTK	O
.	O
Network	Method
quantization	Method
such	O
as	O
HashedNet	Method
Chen2015CompressingNN	O
forces	O
the	O
weight	O
matrix	O
to	O
have	O
few	O
real	O
weights	O
,	O
with	O
a	O
hash	O
function	O
to	O
determine	O
the	O
weight	Method
assignment	Method
.	O
To	O
capture	O
the	O
non	O
-	O
uniform	O
nature	O
of	O
the	O
networks	O
,	O
DeepCompression	Method
han2015deep	O
groups	O
weight	O
values	O
into	O
clusters	O
based	O
on	O
pre	O
-	O
trained	O
weight	Method
matrices	Method
.	O
The	O
weight	O
assignment	O
for	O
each	O
value	O
is	O
stored	O
in	O
the	O
form	O
of	O
Huffman	Method
codes	Method
.	O
However	O
,	O
as	O
the	O
embedding	O
matrix	O
is	O
tremendously	O
big	O
,	O
the	O
number	O
of	O
hash	Method
codes	Method
a	O
model	O
need	O
to	O
maintain	O
is	O
still	O
large	O
even	O
with	O
Huffman	Method
coding	Method
.	O
Network	Method
pruning	Method
works	O
in	O
a	O
different	O
way	O
that	O
makes	O
a	O
network	O
sparse	O
.	O
Iterative	Method
pruning	Method
Han2015LearningBW	O
prunes	O
a	O
weight	O
value	O
if	O
its	O
absolute	O
value	O
is	O
smaller	O
than	O
a	O
threshold	O
.	O
The	O
remaining	O
network	O
weights	O
are	O
retrained	O
after	O
pruning	O
.	O
Some	O
recent	O
works	O
See2016CompressionON	O
,	O
Zhang2017TowardsCA	O
also	O
apply	O
iterative	Method
pruning	Method
to	O
prune	O
80	O
%	O
of	O
the	O
connections	O
for	O
neural	Method
machine	Method
translation	Method
models	Method
.	O
In	O
this	O
paper	O
,	O
we	O
compare	O
the	O
proposed	O
method	O
with	O
iterative	Method
pruning	Method
.	O
The	O
problem	O
of	O
learning	Task
compact	Task
codes	Task
considered	O
in	O
this	O
paper	O
is	O
closely	O
related	O
to	O
learning	Task
to	O
hash	O
Weiss2008SpectralH	O
,	O
Kulis2009LearningTH	O
,	O
Liu2012SupervisedHW	O
,	O
which	O
aims	O
to	O
learn	O
the	O
hash	O
codes	O
for	O
vectors	O
to	O
facilitate	O
the	O
approximate	Task
nearest	Task
neighbor	Task
search	Task
.	O
Initiated	O
by	O
product	Task
quantization	Task
Jgou2011ProductQF	O
,	O
subsequent	O
works	O
such	O
as	O
additive	Method
quantization	Method
Babenko2014AdditiveQF	O
explore	O
the	O
use	O
of	O
multiple	O
codebooks	O
for	O
source	Task
coding	Task
,	O
resulting	O
in	O
compositional	O
codes	O
.	O
We	O
also	O
adopt	O
the	O
coding	Method
scheme	Method
of	O
additive	Method
quantization	Method
for	O
its	O
storage	Metric
efficiency	Metric
.	O
Previous	O
works	O
mainly	O
focus	O
on	O
performing	O
efficient	O
similarity	Task
search	Task
of	Task
image	Task
descriptors	Task
.	O
In	O
this	O
work	O
,	O
we	O
put	O
more	O
focus	O
on	O
reducing	O
the	O
codebook	O
sizes	O
and	O
learning	O
efficient	Method
codes	Method
to	O
avoid	O
performance	O
loss	O
.	O
Joulin2016FastTextzipCT	O
utilizes	O
an	O
improved	O
version	O
of	O
product	Task
quantization	Task
to	O
compress	O
text	Method
classification	Method
models	Method
.	O
However	O
,	O
to	O
match	O
the	O
baseline	O
performance	O
,	O
much	O
longer	O
hash	O
codes	O
are	O
required	O
by	O
product	Task
quantization	Task
.	O
This	O
will	O
be	O
detailed	O
in	O
Section	O
[	O
reference	O
]	O
.	O
To	O
learn	O
the	O
codebooks	O
and	O
code	Method
assignment	Method
,	O
additive	Method
quantization	Method
alternatively	O
optimizes	O
the	O
codebooks	O
and	O
the	O
discrete	O
codes	O
.	O
The	O
learning	Task
of	Task
code	Task
assignment	Task
is	O
performed	O
by	O
Beam	Method
Search	Method
algorithm	Method
when	O
the	O
codebooks	O
are	O
fixed	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
straight	O
-	O
forward	O
method	O
to	O
directly	O
learn	O
the	O
code	O
assignment	O
and	O
codebooks	O
simutaneously	O
in	O
an	O
end	O
-	O
to	O
-	O
end	Method
neural	Method
network	Method
.	O
Some	O
recent	O
works	O
Xia2014SupervisedHF	O
,	O
Liu2016DeepSH	O
,	O
Yang2017SupervisedLO	O
in	O
learning	Task
to	O
hash	Task
also	O
utilize	O
neural	Method
networks	Method
to	O
produce	O
binary	O
codes	O
by	O
applying	O
binary	O
constrains	O
(	O
e.g.	O
,	O
sigmoid	O
function	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
encourage	O
the	O
discreteness	O
with	O
the	O
Gumbel	Method
-	Method
Softmax	Method
trick	Method
for	O
producing	O
compositional	Task
codes	Task
.	O
As	O
an	O
alternative	O
to	O
our	O
approach	O
,	O
one	O
can	O
also	O
reduce	O
the	O
number	O
of	O
unique	O
word	O
types	O
by	O
forcing	O
a	O
character	Method
-	Method
level	Method
segmentation	Method
.	O
Kim2016CharacterAwareNL	O
proposed	O
a	O
character	Method
-	Method
based	Method
neural	Method
language	Method
model	Method
,	O
which	O
applies	O
a	O
convolutional	Method
layer	Method
after	O
the	O
character	Method
embeddings	Method
.	O
Botha2017NaturalLP	O
propose	O
to	O
use	O
char	O
-	O
gram	O
as	O
input	O
features	O
,	O
which	O
are	O
further	O
hashed	O
to	O
save	O
space	O
.	O
Generally	O
,	O
using	O
character	O
-	O
level	O
inputs	O
requires	O
modifications	O
to	O
the	O
model	Method
architecture	Method
.	O
Moreover	O
,	O
some	O
Asian	O
languages	O
such	O
as	O
Japanese	O
and	O
Chinese	O
retain	O
a	O
large	O
vocabulary	O
at	O
the	O
character	O
level	O
,	O
which	O
makes	O
the	O
character	Method
-	Method
based	Method
approach	Method
difficult	O
to	O
be	O
applied	O
.	O
In	O
contrast	O
,	O
our	O
approach	O
does	O
not	O
suffer	O
from	O
these	O
limitations	O
.	O
section	O
:	O
Advantage	O
of	O
Compositional	Method
Codes	Method
In	O
this	O
section	O
,	O
we	O
formally	O
describe	O
the	O
compositional	Method
coding	Method
approach	Method
and	O
analyze	O
its	O
merits	O
for	O
compressing	Task
word	Task
embeddings	Task
.	O
The	O
coding	Method
approach	Method
follows	O
the	O
scheme	O
in	O
additive	Task
quantization	Task
Babenko2014AdditiveQF	O
.	O
We	O
represent	O
each	O
word	O
with	O
a	O
compact	Method
code	Method
that	O
is	O
composed	O
of	O
components	O
such	O
that	O
.	O
Each	O
component	O
is	O
constrained	O
to	O
have	O
a	O
value	O
in	O
,	O
which	O
also	O
indicates	O
that	O
bits	O
are	O
required	O
to	O
store	O
each	O
code	O
.	O
For	O
convenience	O
,	O
is	O
selected	O
to	O
be	O
a	O
number	O
of	O
a	O
multiple	O
of	O
,	O
so	O
that	O
the	O
codes	O
can	O
be	O
efficiently	O
stored	O
.	O
If	O
we	O
restrict	O
each	O
component	O
to	O
values	O
of	O
0	O
or	O
1	O
,	O
the	O
code	O
for	O
each	O
word	O
will	O
be	O
a	O
binary	O
code	O
.	O
In	O
this	O
case	O
,	O
the	O
code	Task
learning	Task
problem	Task
is	O
equivalent	O
to	O
a	O
matrix	Task
factorization	Task
problem	Task
with	O
binary	Method
components	Method
.	O
Forcing	O
the	O
compact	Method
codes	Method
to	O
be	O
binary	O
numbers	O
can	O
be	O
beneficial	O
,	O
as	O
the	O
learning	Task
problem	Task
is	O
usually	O
easier	O
to	O
solve	O
in	O
the	O
binary	O
case	O
,	O
and	O
some	O
existing	O
optimization	Method
algorithms	Method
in	O
learning	Task
to	O
hash	Task
can	O
be	O
reused	O
.	O
However	O
,	O
the	O
compositional	Method
coding	Method
approach	Method
produces	O
shorter	O
codes	O
and	O
is	O
thus	O
more	O
storage	O
efficient	O
.	O
As	O
the	O
number	O
of	O
basis	O
vectors	O
is	O
regardless	O
of	O
the	O
vocabulary	O
size	O
,	O
the	O
only	O
uncertain	O
factor	O
contributing	O
to	O
the	O
model	Metric
size	Metric
is	O
the	O
size	O
of	O
the	O
hash	Method
codes	Method
,	O
which	O
is	O
proportional	O
to	O
the	O
vocabulary	O
size	O
.	O
Therefore	O
,	O
maintaining	O
short	O
codes	O
is	O
cruicial	O
in	O
our	O
work	O
.	O
Suppose	O
we	O
wish	O
the	O
model	O
to	O
have	O
a	O
set	O
of	O
basis	O
vectors	O
.	O
Then	O
in	O
the	O
binary	O
case	O
,	O
each	O
code	O
will	O
have	O
bits	O
.	O
For	O
the	O
compositional	Method
coding	Method
approach	Method
,	O
if	O
we	O
can	O
find	O
a	O
decomposition	O
such	O
that	O
,	O
then	O
each	O
code	O
will	O
have	O
bits	O
.	O
For	O
example	O
,	O
a	O
binary	O
code	O
will	O
have	O
a	O
length	O
of	O
256	O
bits	O
to	O
support	O
512	O
basis	O
vectors	O
.	O
In	O
contrast	O
,	O
a	O
compositional	Method
coding	Method
scheme	Method
will	O
produce	O
codes	O
of	O
only	O
128	O
bits	O
.	O
A	O
comparison	O
of	O
different	O
coding	Method
approaches	Method
is	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
also	O
report	O
the	O
number	O
of	O
basis	O
vectors	O
required	O
to	O
compute	O
an	O
embedding	O
as	O
a	O
measure	O
of	O
computational	Metric
cost	Metric
.	O
For	O
the	O
conventional	O
approach	O
,	O
the	O
number	O
of	O
vectors	O
is	O
identical	O
to	O
the	O
vocabulary	O
size	O
and	O
the	O
computation	O
is	O
basically	O
a	O
single	O
indexing	Method
operation	Method
.	O
In	O
the	O
case	O
of	O
binary	Task
codes	Task
,	O
the	O
computation	O
for	O
constructing	O
an	O
embedding	Task
involves	O
a	O
summation	O
over	O
basis	O
vectors	O
.	O
For	O
the	O
compositional	Method
approach	Method
,	O
the	O
number	O
of	O
vectors	O
required	O
to	O
construct	O
an	O
embedding	O
vector	O
is	O
.	O
Both	O
the	O
binary	Method
and	Method
compositional	Method
approaches	Method
have	O
significantly	O
fewer	O
vectors	O
in	O
the	O
embedding	O
matrix	O
.	O
The	O
compositional	Method
coding	Method
approach	Method
provides	O
a	O
better	O
balance	O
with	O
shorter	O
codes	O
and	O
lower	O
computational	Metric
cost	Metric
.	O
section	O
:	O
Code	Method
Learning	Method
with	O
Gumbel	Method
-	Method
Softmax	Method
Let	O
be	O
the	O
original	O
embedding	O
matrix	O
,	O
where	O
each	O
embedding	O
vector	O
has	O
dimensions	O
.	O
By	O
using	O
the	O
reconstruction	Metric
loss	Metric
as	O
the	O
objective	Metric
function	Metric
in	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
we	O
are	O
actually	O
finding	O
an	O
approximate	Method
matrix	Method
factorization	Method
,	O
where	O
is	O
a	O
basis	O
matrix	O
for	O
the	O
-	Method
th	Method
component	Method
.	O
is	O
a	O
code	Method
matrix	Method
in	O
which	O
each	O
row	O
is	O
an	O
-	O
dimensional	O
one	O
-	O
hot	O
vector	O
.	O
If	O
we	O
let	O
be	O
the	O
one	O
-	O
hot	O
vector	O
corresponding	O
to	O
the	O
code	O
component	O
for	O
word	O
,	O
the	O
computation	O
of	O
the	O
word	Task
embeddings	Task
can	O
be	O
reformulated	O
as	O
Therefore	O
,	O
the	O
problem	O
of	O
learning	Task
discrete	Task
codes	Task
can	O
be	O
converted	O
to	O
a	O
problem	O
of	O
finding	O
a	O
set	O
of	O
optimal	O
one	O
-	O
hot	O
vectors	O
and	O
source	Method
dictionaries	Method
that	O
minimize	O
the	O
reconstruction	Metric
loss	Metric
.	O
The	O
Gumbel	Method
-	Method
softmax	Method
reparameterization	Method
trick	Method
Maddison2016TheCD	O
,	O
Jang2016CategoricalRW	O
is	O
useful	O
for	O
parameterizing	O
a	O
discrete	O
distribution	O
such	O
as	O
the	O
-	O
dimensional	O
one	O
-	O
hot	O
vectors	O
in	O
Eq	O
.	O
[	O
reference	O
]	O
.	O
By	O
applying	O
the	O
Gumbel	Method
-	Method
softmax	Method
trick	Method
,	O
the	O
-	O
th	O
elemement	O
in	O
is	O
computed	O
as	O
where	O
is	O
a	O
noise	O
term	O
that	O
is	O
sampled	O
from	O
the	O
Gumbel	Method
distribution	Method
,	O
whereas	O
is	O
the	O
temperature	O
of	O
the	O
softmax	Method
.	O
In	O
our	O
model	O
,	O
the	O
vector	O
is	O
computed	O
by	O
a	O
simple	O
neural	Method
network	Method
with	O
a	O
single	O
hidden	Method
layer	Method
as	O
In	O
our	O
experiments	O
,	O
the	O
hidden	O
layer	O
always	O
has	O
a	O
size	O
of	O
.	O
We	O
found	O
that	O
a	O
fixed	O
temperature	O
of	O
just	O
works	O
well	O
.	O
The	O
Gumbel	Method
-	Method
softmax	Method
trick	Method
is	O
applied	O
to	O
to	O
obtain	O
.	O
Then	O
,	O
the	O
model	O
reconstructs	O
the	O
embedding	O
with	O
Eq	O
.	O
[	O
reference	O
]	O
and	O
computes	O
the	O
reconstruction	O
loss	O
with	O
Eq	O
.	O
[	O
reference	O
]	O
.	O
The	O
model	O
architecture	O
of	O
the	O
end	Method
-	Method
to	Method
-	Method
end	Method
neural	Method
network	Method
is	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
,	O
which	O
is	O
effectively	O
an	O
auto	Method
-	Method
encoder	Method
with	O
a	O
Gumbel	Method
-	Method
softmax	Method
middle	Method
layer	Method
.	O
The	O
whole	O
neural	Method
network	Method
for	O
coding	Task
learning	Task
has	O
five	O
parameters	O
(	O
)	O
.	O
Once	O
the	O
coding	Method
learning	Method
model	Method
is	O
trained	O
,	O
the	O
code	O
for	O
each	O
word	O
can	O
be	O
easily	O
obtained	O
by	O
applying	O
to	O
the	O
one	O
-	O
hot	O
vectors	O
.	O
The	O
basis	O
vectors	O
(	O
codewords	O
)	O
for	O
composing	O
the	O
embeddings	O
can	O
be	O
found	O
as	O
the	O
row	O
vectors	O
in	O
the	O
weight	O
matrix	O
.	O
For	O
general	O
NLP	Method
tasks	O
,	O
one	O
can	O
learn	O
the	O
compositional	O
codes	O
from	O
publicly	O
available	O
word	O
vectors	O
such	O
as	O
GloVe	Method
vectors	O
.	O
However	O
,	O
for	O
some	O
tasks	O
such	O
as	O
machine	Task
translation	Task
,	O
the	O
word	O
embeddings	O
are	O
usually	O
jointly	O
learned	O
with	O
other	O
parts	O
of	O
the	O
neural	Method
network	Method
.	O
For	O
such	O
tasks	O
,	O
one	O
has	O
to	O
first	O
train	O
a	O
normal	Method
model	Method
to	O
obtain	O
the	O
baseline	O
embeddings	O
.	O
Then	O
,	O
based	O
on	O
the	O
trained	O
embedding	O
matrix	O
,	O
one	O
can	O
learn	O
a	O
set	O
of	O
task	O
-	O
specific	O
codes	O
.	O
As	O
the	O
reconstructed	O
embeddings	O
are	O
not	O
identical	O
to	O
the	O
original	O
embeddings	O
,	O
the	O
model	O
parameters	O
other	O
than	O
the	O
embedding	O
matrix	O
have	O
to	O
be	O
retrained	O
again	O
.	O
The	O
code	Method
learning	Method
model	Method
can	O
not	O
be	O
jointly	O
trained	O
with	O
the	O
machine	Method
translation	Method
model	Method
as	O
it	O
takes	O
far	O
more	O
iterations	O
for	O
the	O
coding	Method
layer	Method
to	O
converge	O
to	O
one	O
-	O
hot	O
vectors	O
.	O
section	O
:	O
Experiments	O
In	O
our	O
experiments	O
,	O
we	O
focus	O
on	O
evaluating	O
the	O
maximum	Metric
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
of	O
word	Method
embeddings	Method
on	O
two	O
typical	O
NLP	Method
tasks	O
:	O
sentiment	Task
analysis	Task
and	O
machine	Task
translation	Task
.	O
We	O
compare	O
the	O
model	O
performance	O
and	O
the	O
size	O
of	O
embedding	O
layer	O
with	O
the	O
baseline	Method
model	Method
and	O
the	O
iterative	Method
pruning	Method
method	Method
Han2015LearningBW	Method
.	O
Please	O
note	O
that	O
the	O
sizes	O
of	O
other	O
parts	O
in	O
the	O
neural	Method
networks	Method
are	O
not	O
included	O
in	O
our	O
results	O
.	O
For	O
dense	O
matrices	O
,	O
we	O
report	O
the	O
size	O
of	O
dumped	O
numpy	O
arrays	O
.	O
For	O
the	O
sparse	O
matrices	O
,	O
we	O
report	O
the	O
size	O
of	O
dumped	O
compressed	O
sparse	O
column	O
matrices	O
(	O
csc_matrix	Method
)	O
in	O
scipy	Method
.	O
All	O
float	O
numbers	O
take	O
32	O
bits	O
storage	O
.	O
We	O
enable	O
the	O
“	O
compressed	O
”	O
option	O
when	O
dumping	O
the	O
matrices	O
,	O
without	O
this	O
option	O
,	O
the	O
file	O
size	O
is	O
about	O
times	O
bigger	O
.	O
subsection	O
:	O
Code	Method
Learning	Method
To	O
learn	O
efficient	O
compact	O
codes	O
for	O
each	O
word	O
,	O
our	O
proposed	O
method	O
requires	O
a	O
set	O
of	O
baseline	Method
embedding	Method
vectors	Method
.	O
For	O
the	O
sentiment	Task
analysis	Task
task	Task
,	O
we	O
learn	O
the	O
codes	O
based	O
on	O
the	O
publicly	O
available	O
GloVe	Method
vectors	O
.	O
For	O
the	O
machine	Task
translation	Task
task	Task
,	O
we	O
first	O
train	O
a	O
normal	Method
neural	Method
machine	Method
translation	Method
(	O
NMT	Method
)	O
model	O
to	O
obtain	O
task	O
-	O
specific	O
word	O
embeddings	O
.	O
Then	O
we	O
learn	O
the	O
codes	O
using	O
the	O
pre	O
-	O
trained	O
embeddings	Method
.	O
We	O
train	O
the	O
end	Method
-	Method
to	Method
-	Method
end	Method
network	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
to	O
learn	O
the	O
codes	O
automatically	O
.	O
In	O
each	O
iteration	O
,	O
a	O
small	O
batch	O
of	O
the	O
embeddings	O
is	O
sampled	O
uniformly	O
from	O
the	O
baseline	Method
embedding	Method
matrix	Method
.	O
The	O
network	O
parameters	O
are	O
optimized	O
to	O
minimize	O
the	O
reconstruction	O
loss	O
of	O
the	O
sampled	O
embeddings	O
.	O
In	O
our	O
experiments	O
,	O
the	O
batch	O
size	O
is	O
set	O
to	O
128	O
.	O
We	O
use	O
Adam	Method
optimizer	Method
kingma2014adam	O
with	O
a	O
fixed	O
learning	Metric
rate	Metric
of	O
0.0001	O
.	O
The	O
training	O
is	O
run	O
for	O
200	O
K	O
iterations	O
.	O
Every	O
1	O
,	O
000	O
iterations	O
,	O
we	O
examine	O
the	O
loss	O
on	O
a	O
fixed	O
validation	O
set	O
and	O
save	O
the	O
parameters	O
if	O
the	O
loss	O
decreases	O
.	O
We	O
evenly	O
distribute	O
the	O
model	Method
training	Method
to	O
4	O
GPUs	O
using	O
the	O
nccl	Method
package	Method
,	O
so	O
that	O
one	O
round	O
of	O
code	Method
learning	Method
takes	O
around	O
15	O
minutes	O
to	O
complete	O
.	O
subsection	O
:	O
Sentiment	Task
Analysis	Task
Dataset	O
:	O
For	O
sentiment	Task
analysis	Task
,	O
we	O
use	O
a	O
standard	O
separation	O
of	O
IMDB	O
movie	O
review	O
dataset	O
Maas2011LearningWV	O
,	O
which	O
contains	O
25k	O
reviews	O
for	O
training	O
and	O
25	O
K	O
reviews	O
for	O
testing	O
purpose	O
.	O
We	O
lowercase	O
and	O
tokenize	O
all	O
texts	O
with	O
the	O
nltk	Method
package	Method
.	O
We	O
choose	O
the	O
300	O
-	O
dimensional	O
uncased	O
GloVe	Method
word	O
vectors	O
(	O
trained	O
on	O
42B	O
tokens	O
of	O
Common	O
Crawl	O
data	O
)	O
as	O
our	O
baseline	O
embeddings	O
.	O
The	O
vocabulary	O
for	O
the	O
model	Task
training	Task
contains	O
all	O
words	O
appears	O
both	O
in	O
the	O
IMDB	O
dataset	O
and	O
the	O
GloVe	Method
vocabulary	O
,	O
which	O
results	O
in	O
around	O
75	O
K	O
words	O
.	O
We	O
truncate	O
the	O
texts	O
of	O
reviews	O
to	O
assure	O
they	O
are	O
not	O
longer	O
than	O
400	O
words	O
.	O
Model	Method
architecture	Method
:	O
Both	O
the	O
baseline	Method
model	Method
and	O
the	O
compressed	Method
models	Method
have	O
the	O
same	O
computational	O
graph	O
except	O
the	O
embedding	Method
layer	Method
.	O
The	O
model	O
is	O
composed	O
of	O
a	O
single	O
LSTM	Method
layer	Method
with	O
150	O
hidden	O
units	O
and	O
a	O
softmax	Method
layer	Method
for	O
predicting	O
the	O
binary	O
label	O
.	O
For	O
the	O
baseline	Method
model	Method
,	O
the	O
embedding	Method
layer	Method
contains	O
a	O
large	O
embedding	O
matrix	O
initialized	O
by	O
GloVe	Method
embeddings	O
.	O
For	O
the	O
compressed	Method
models	Method
based	O
on	O
the	O
compositional	Method
coding	Method
,	O
the	O
embedding	Method
layer	Method
maintains	O
a	O
matrix	O
of	O
basis	O
vectors	O
.	O
Suppose	O
we	O
use	O
a	O
coding	Method
scheme	Method
,	O
the	O
basis	O
matrix	O
will	O
then	O
have	O
a	O
shape	O
of	O
,	O
which	O
is	O
initialized	O
by	O
the	O
concatenated	O
weight	O
matrices	O
in	O
the	O
code	Method
learning	Method
model	Method
.	O
The	O
embedding	O
parameters	O
for	O
both	O
models	O
remain	O
fixed	O
during	O
the	O
training	O
.	O
For	O
the	O
models	O
with	O
network	Method
pruning	Method
,	O
the	O
sparse	Method
embedding	Method
matrix	Method
is	O
finetuned	O
during	O
training	O
.	O
Training	O
details	O
:	O
The	O
models	O
are	O
trained	O
with	O
Adam	Method
optimizer	Method
for	O
15	O
epochs	O
with	O
a	O
fixed	O
learning	Metric
rate	Metric
of	O
.	O
At	O
the	O
end	O
of	O
each	O
epoch	O
,	O
we	O
evaluate	O
the	O
loss	Metric
on	O
a	O
small	O
validation	O
set	O
.	O
The	O
parameters	O
with	O
lowest	O
validation	Metric
loss	Metric
are	O
saved	O
.	O
Results	O
:	O
For	O
different	O
settings	O
of	O
the	O
number	O
of	O
components	O
and	O
the	O
number	O
of	O
codewords	O
,	O
we	O
train	O
the	O
code	Method
learning	Method
network	Method
.	O
The	O
average	Metric
reconstruction	Metric
loss	Metric
on	O
a	O
fixed	O
validation	O
set	O
is	O
summarized	O
in	O
the	O
left	O
of	O
Table	O
[	O
reference	O
]	O
.	O
For	O
reference	O
,	O
we	O
also	O
report	O
the	O
total	O
size	O
(	O
MB	O
)	O
of	O
the	O
embedding	O
layer	O
in	O
the	O
right	O
table	O
,	O
which	O
includes	O
the	O
sizes	O
of	O
the	O
basis	O
matrix	O
and	O
the	O
hash	O
table	O
.	O
We	O
can	O
see	O
that	O
increasing	O
either	O
or	O
can	O
effectively	O
decrease	O
the	O
reconstruction	Metric
loss	Metric
.	O
However	O
,	O
setting	O
to	O
a	O
large	O
number	O
will	O
result	O
in	O
longer	O
hash	O
codes	O
,	O
thus	O
significantly	O
increase	O
the	O
size	O
of	O
the	O
embedding	Method
layer	Method
.	O
Hence	O
,	O
it	O
is	O
important	O
to	O
choose	O
correct	O
numbers	O
for	O
and	O
to	O
balance	O
the	O
performance	O
and	O
model	Metric
size	Metric
.	O
To	O
see	O
how	O
the	O
reconstructed	Metric
loss	Metric
translates	O
to	O
the	O
classification	Metric
accuracy	Metric
,	O
we	O
train	O
the	O
sentiment	Method
analysis	Method
model	Method
for	O
different	O
settings	O
of	O
code	O
schemes	O
and	O
report	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
.	O
The	O
baseline	O
model	O
using	O
75k	O
GloVe	Method
embeddings	O
achieves	O
an	O
accuracy	Metric
of	O
87.18	O
with	O
an	O
embedding	Method
matrix	Method
using	O
78	O
MB	O
of	O
storage	O
.	O
In	O
this	O
task	O
,	O
forcing	O
a	O
high	O
compression	Metric
rate	Metric
with	O
iterative	Method
pruning	Method
degrades	O
the	O
classification	Metric
accuracy	Metric
.	O
We	O
also	O
show	O
the	O
results	O
using	O
normalized	Method
product	Method
quantization	Method
(	O
NPQ	Method
)	O
Joulin2016FastTextzipCT	O
.	O
We	O
quantize	O
the	O
filtered	O
GloVe	Method
embeddings	O
with	O
the	O
codes	O
provided	O
by	O
the	O
authors	O
,	O
and	O
train	O
the	O
models	O
based	O
on	O
the	O
quantized	O
embeddings	O
.	O
To	O
make	O
the	O
results	O
comparable	O
,	O
we	O
report	O
the	O
codebook	Metric
size	Metric
in	O
numpy	O
format	O
.	O
For	O
our	O
proposed	O
methods	O
,	O
the	O
maximum	Metric
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
is	O
achieved	O
by	O
a	O
coding	Method
scheme	Method
.	O
In	O
this	O
case	O
,	O
the	O
total	O
size	O
of	O
the	O
embedding	O
layer	O
is	O
1.23	O
MB	O
,	O
which	O
is	O
equivalent	O
to	O
a	O
compression	Metric
rate	Metric
of	O
98.4	O
%	O
.	O
We	O
also	O
found	O
the	O
classification	Metric
accuracy	Metric
can	O
be	O
substantially	O
improved	O
with	O
a	O
slightly	O
lower	O
compression	Metric
rate	Metric
.	O
The	O
improved	O
model	O
performance	O
may	O
be	O
a	O
byproduct	O
of	O
the	O
strong	Method
regularization	Method
.	O
subsection	O
:	O
Machine	Task
Translation	Task
Dataset	O
:	O
For	O
machine	Task
translation	Task
tasks	Task
,	O
we	O
experiment	O
on	O
IWSLT	Task
2014	Task
German	Task
-	Task
to	Task
-	Task
English	Task
translation	Task
task	Task
cettolo2014report	O
and	O
ASPEC	Task
English	Task
-	Task
to	Task
-	Task
Japanese	Task
translation	Task
task	Task
NAKAZAWA16.621	O
.	O
The	O
IWSLT14	O
training	O
data	O
contains	O
178	O
K	O
sentence	O
pairs	O
,	O
which	O
is	O
a	O
small	O
dataset	O
for	O
machine	Task
translation	Task
.	O
We	O
utilize	O
moses	Method
toolkit	Method
Koehn2007MosesOS	Method
to	O
tokenize	O
and	O
lowercase	O
both	O
sides	O
of	O
the	O
texts	O
.	O
Then	O
we	O
concatenate	O
all	O
five	O
TED	O
/	O
TEDx	O
development	O
and	O
test	O
corpus	O
to	O
form	O
a	O
test	O
set	O
containing	O
6750	O
sentence	O
pairs	O
.	O
We	O
apply	O
byte	Method
-	Method
pair	Method
encoding	Method
Sennrich2016NeuralMT	O
to	O
transform	O
the	O
texts	O
to	O
subword	O
level	O
so	O
that	O
the	O
vocabulary	O
has	O
a	O
size	O
of	O
20	O
K	O
for	O
each	O
language	O
.	O
For	O
evaluation	O
,	O
we	O
report	O
tokenized	Metric
BLEU	Metric
using	O
“	O
multi	O
-	O
bleu.perl	O
”	O
.	O
The	O
ASPEC	O
dataset	O
contains	O
300	O
M	O
bilingual	O
pairs	O
in	O
the	O
training	O
data	O
with	O
the	O
automatically	O
estimated	O
quality	O
scores	O
provided	O
for	O
each	O
pair	O
.	O
We	O
only	O
use	O
the	O
first	O
150	O
M	O
pairs	O
for	O
training	O
the	O
models	O
.	O
The	O
English	Material
texts	Material
are	O
tokenized	O
by	O
moses	O
toolkit	O
whereas	O
the	O
Japanese	O
texts	O
are	O
tokenized	O
by	O
kytea	O
kytea	O
.	O
The	O
vocabulary	O
size	O
for	O
each	O
language	O
is	O
reduced	O
to	O
40	O
K	O
using	O
byte	Method
-	Method
pair	Method
encoding	Method
.	O
The	O
evaluation	O
is	O
performed	O
using	O
a	O
standard	O
kytea	Method
-	Method
based	Method
post	Method
-	Method
processing	Method
script	Method
for	O
this	O
dataset	O
.	O
Model	Method
architecture	Method
:	O
In	O
our	O
preliminary	O
experiments	O
,	O
we	O
found	O
a	O
coding	Method
works	O
well	O
for	O
a	O
vanilla	O
NMT	Method
model	O
.	O
As	O
it	O
is	O
more	O
meaningful	O
to	O
test	O
on	O
a	O
high	O
-	O
performance	O
model	O
,	O
we	O
applied	O
several	O
techniques	O
to	O
improve	O
the	O
performance	O
.	O
The	O
model	O
has	O
a	O
standard	O
bi	Method
-	Method
directional	Method
encoder	Method
composed	O
of	O
two	O
LSTM	Method
layers	Method
similar	O
to	O
bahdanau2014neural	Method
.	O
The	O
decoder	Method
contains	O
two	O
LSTM	Method
layers	Method
.	O
Residual	Method
connection	Method
He2016DeepRL	O
with	O
a	O
scaling	O
factor	O
of	O
is	O
applied	O
to	O
the	O
two	O
decoder	O
states	O
to	O
compute	O
the	O
outputs	O
.	O
All	O
LSTMs	Method
and	Method
embeddings	Method
have	O
256	O
hidden	O
units	O
in	O
the	O
IWSLT14	Task
task	Task
and	O
1000	O
hidden	O
units	O
in	O
ASPEC	Task
task	Task
.	O
The	O
decoder	O
states	O
are	O
firstly	O
linearly	O
transformed	O
to	O
600	O
-	O
dimensional	O
vectors	O
before	O
computing	O
the	O
final	O
softmax	O
.	O
Dropout	Method
with	O
a	O
rate	O
of	O
0.2	O
is	O
applied	O
everywhere	O
except	O
the	O
recurrent	Method
computation	Method
.	O
We	O
apply	O
Key	Method
-	Method
Value	Method
Attention	Method
Miller2016KeyValueMN	O
to	O
the	O
first	O
decoder	O
,	O
where	O
the	O
query	O
is	O
the	O
sum	O
of	O
the	O
feedback	O
embedding	O
and	O
the	O
previous	O
decoder	O
state	O
and	O
the	O
keys	O
are	O
computed	O
by	O
linear	Method
transformation	Method
of	Method
encoder	Method
states	Method
.	O
Training	O
details	O
:	O
All	O
models	O
are	O
trained	O
by	O
Nesterov	Method
’s	Method
accelerated	Method
gradient	Method
nesterov1983method	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.25	O
.	O
We	O
evaluate	O
the	O
smoothed	Metric
BLEU	Metric
smoothed_bleu	O
on	O
a	O
validation	O
set	O
composed	O
of	O
50	O
batches	O
every	O
7	O
,	O
000	O
iterations	O
.	O
The	O
learning	Metric
rate	Metric
is	O
reduced	O
by	O
a	O
factor	O
of	O
10	O
if	O
no	O
improvement	O
is	O
observed	O
in	O
3	O
validation	O
runs	O
.	O
The	O
training	O
ends	O
after	O
the	O
learning	Metric
rate	Metric
is	O
reduced	O
three	O
times	O
.	O
Similar	O
to	O
the	O
code	Method
learning	Method
,	O
the	O
training	O
is	O
distributed	O
to	O
4	O
GPUs	O
,	O
each	O
GPU	Method
computes	O
a	O
mini	O
-	O
batch	O
of	O
16	O
samples	O
.	O
We	O
firstly	O
train	O
a	O
baseline	O
NMT	Method
model	O
to	O
obtain	O
the	O
task	O
-	O
specific	O
embeddings	O
for	O
all	O
in	O
-	O
vocabulary	O
words	O
in	O
both	O
languages	O
.	O
Then	O
based	O
on	O
these	O
baseline	O
embeddings	O
,	O
we	O
obtain	O
the	O
hash	O
codes	O
and	O
basis	O
vectors	O
by	O
training	O
the	O
code	Method
learning	Method
model	Method
.	O
Finally	O
,	O
the	O
NMT	Method
models	O
using	O
compositional	Method
coding	Method
are	O
retrained	O
by	O
plugging	O
in	O
the	O
reconstructed	O
embeddings	O
.	O
Note	O
that	O
the	O
embedding	Method
layer	Method
is	O
fixed	O
in	O
this	O
phase	O
,	O
other	O
parameters	O
are	O
retrained	O
from	O
random	O
initial	O
values	O
.	O
Results	O
:	O
The	O
experimental	O
results	O
are	O
summarized	O
in	O
Table	O
[	O
reference	O
]	O
.	O
All	O
translations	O
are	O
decoded	O
by	O
the	O
beam	Method
search	Method
with	O
a	O
beam	O
size	O
of	O
5	O
.	O
The	O
performance	O
of	O
iterative	Method
pruning	Method
varies	O
between	O
tasks	O
.	O
The	O
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
reaches	O
92	O
%	O
on	O
ASPEC	O
dataset	O
by	O
pruning	O
90	O
%	O
of	O
the	O
connections	O
.	O
However	O
,	O
with	O
the	O
same	O
pruning	Metric
ratio	Metric
,	O
a	O
modest	O
performance	O
loss	O
is	O
observed	O
in	O
IWSLT14	O
dataset	O
.	O
For	O
the	O
models	O
using	O
compositional	Method
coding	Method
,	O
the	O
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
is	O
94	O
%	O
for	O
the	O
IWSLT14	O
dataset	O
and	O
99	O
%	O
for	O
the	O
ASPEC	O
dataset	O
.	O
Similar	O
to	O
the	O
sentiment	Task
analysis	Task
task	Task
,	O
a	O
significant	O
performance	O
improvement	O
can	O
be	O
observed	O
by	O
slightly	O
lowering	O
the	O
compression	Metric
rate	Metric
.	O
Note	O
that	O
the	O
sizes	O
of	O
NMT	Method
models	O
are	O
still	O
quite	O
large	O
due	O
to	O
the	O
big	O
softmax	O
layer	O
and	O
the	O
recurrent	Method
layers	Method
,	O
which	O
are	O
not	O
reported	O
in	O
the	O
table	O
.	O
Please	O
refer	O
to	O
existing	O
works	O
such	O
as	O
Zhang2017TowardsCA	O
for	O
the	O
techniques	O
of	O
compressing	Task
layers	Task
other	O
than	O
word	O
embeddings	O
.	O
section	O
:	O
Qualitative	Task
Analysis	Task
subsection	O
:	O
Examples	O
of	O
Learned	Method
Codes	Method
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
some	O
examples	O
of	O
learned	O
codes	O
based	O
on	O
the	O
300	O
-	O
dimensional	O
uncased	O
GloVe	Method
embeddings	O
used	O
in	O
the	O
sentiment	Task
analysis	Task
task	Task
.	O
We	O
can	O
see	O
that	O
the	O
model	O
learned	O
to	O
assign	O
similar	O
codes	O
to	O
the	O
words	O
with	O
similar	O
meanings	O
.	O
Such	O
a	O
code	Method
-	Method
sharing	Method
mechanism	Method
can	O
significantly	O
reduce	O
the	O
redundancy	O
of	O
the	O
word	O
embeddings	O
,	O
thus	O
helping	O
to	O
achieve	O
a	O
high	O
compression	Metric
rate	Metric
.	O
subsection	O
:	O
Analysis	O
of	O
Code	Metric
Efficiency	Metric
Besides	O
the	O
performance	O
,	O
we	O
also	O
care	O
about	O
the	O
storage	Metric
efficiency	Metric
of	O
the	O
codes	O
.	O
In	O
the	O
ideal	O
situation	O
,	O
all	O
codewords	O
shall	O
be	O
fully	O
utilized	O
to	O
convey	O
a	O
fraction	O
of	O
meaning	O
.	O
However	O
,	O
as	O
the	O
codes	O
are	O
automatically	O
learned	O
,	O
it	O
is	O
possible	O
that	O
some	O
codewords	O
are	O
abandoned	O
during	O
the	O
training	O
.	O
In	O
extreme	O
cases	O
,	O
some	O
“	O
dead	O
”	O
codewords	O
can	O
be	O
used	O
by	O
none	O
of	O
the	O
words	O
.	O
To	O
analyze	O
the	O
code	Metric
efficiency	Metric
,	O
we	O
count	O
the	O
number	O
of	O
words	O
that	O
contain	O
a	O
specific	O
subcode	O
in	O
each	O
component	O
.	O
Figure	O
[	O
reference	O
]	O
gives	O
a	O
visualization	O
of	O
the	O
code	Metric
balance	Metric
for	O
three	O
coding	Method
schemes	Method
.	O
Each	O
column	O
shows	O
the	O
counts	O
of	O
the	O
subcodes	O
of	O
a	O
specific	O
component	O
.	O
In	O
our	O
experiments	O
,	O
when	O
using	O
a	O
coding	Method
scheme	Method
,	O
we	O
found	O
31	O
%	O
of	O
the	O
words	O
have	O
a	O
subcode	O
“	O
0	O
”	O
for	O
the	O
first	O
component	O
,	O
while	O
the	O
subcode	O
“	O
1	O
”	O
is	O
only	O
used	O
by	O
5	O
%	O
of	O
the	O
words	O
.	O
The	O
assignment	O
of	O
codes	O
is	O
more	O
balanced	O
for	O
larger	O
coding	Method
schemes	Method
.	O
In	O
any	O
coding	Method
scheme	Method
,	O
even	O
the	O
most	O
unpopular	O
codeword	O
is	O
used	O
by	O
about	O
1000	O
words	O
.	O
This	O
result	O
indicates	O
that	O
the	O
code	Method
learning	Method
model	Method
is	O
capable	O
of	O
assigning	O
codes	O
efficiently	O
without	O
wasting	O
a	O
codeword	O
.	O
section	O
:	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
method	O
for	O
reducing	O
the	O
number	O
of	O
parameters	O
required	O
in	O
word	Task
embeddings	Task
.	O
Instead	O
of	O
assigning	O
each	O
unique	O
word	O
an	O
embedding	O
vector	O
,	O
we	O
compose	O
the	O
embedding	O
vectors	O
using	O
a	O
small	O
set	O
of	O
basis	O
vectors	O
.	O
The	O
selection	O
of	O
basis	O
vectors	O
is	O
governed	O
by	O
the	O
hash	O
code	O
of	O
each	O
word	O
.	O
We	O
apply	O
the	O
compositional	Method
coding	Method
approach	Method
to	O
maximize	O
the	O
storage	Metric
efficiency	Metric
.	O
The	O
proposed	O
method	O
works	O
by	O
eliminating	O
the	O
redundancy	O
inherent	O
in	O
representing	O
similar	O
words	O
with	O
independent	O
embeddings	O
.	O
In	O
our	O
work	O
,	O
we	O
propose	O
a	O
simple	O
way	O
to	O
directly	O
learn	O
the	O
discrete	O
codes	O
in	O
a	O
neural	Method
network	Method
with	O
Gumbel	Method
-	Method
softmax	Method
trick	Method
.	O
The	O
results	O
show	O
that	O
the	O
size	O
of	O
the	O
embedding	Method
layer	Method
was	O
reduced	O
by	O
98	O
%	O
in	O
IMDB	Task
sentiment	Task
analysis	Task
task	Task
and	O
in	O
machine	Task
translation	Task
tasks	Task
without	O
affecting	O
the	O
performance	O
.	O
Our	O
approach	O
achieves	O
a	O
high	O
loss	Metric
-	Metric
free	Metric
compression	Metric
rate	Metric
by	O
considering	O
the	O
semantic	O
inter	O
-	O
similarity	O
among	O
different	O
words	O
.	O
In	O
qualitative	Task
analysis	Task
,	O
we	O
found	O
the	O
learned	O
codes	O
of	O
similar	O
words	O
are	O
very	O
close	O
in	O
Hamming	O
space	O
.	O
As	O
our	O
approach	O
maintains	O
a	O
dense	O
basis	O
matrix	O
,	O
it	O
has	O
the	O
potential	O
to	O
be	O
further	O
compressed	O
by	O
applying	O
pruning	Method
techniques	Method
to	O
the	O
dense	O
matrix	O
.	O
The	O
advantage	O
of	O
compositional	Method
coding	Method
approach	Method
will	O
be	O
more	O
significant	O
if	O
the	O
size	O
of	O
embedding	O
layer	O
is	O
dominated	O
by	O
the	O
hash	Method
codes	Method
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Appendix	O
:	O
Shared	O
Codes	O
In	O
both	O
tasks	O
,	O
when	O
we	O
use	O
a	O
small	O
code	Method
decomposition	Method
,	O
we	O
found	O
some	O
hash	O
codes	O
are	O
assigned	O
to	O
multiple	O
words	O
.	O
Table	O
[	O
reference	O
]	O
lists	O
some	O
samples	O
of	O
shared	O
codes	O
with	O
their	O
corresponding	O
words	O
from	O
the	O
sentiment	Task
analysis	Task
task	Task
.	O
This	O
phenomenon	O
does	O
not	O
cause	O
a	O
problem	O
in	O
either	O
task	O
,	O
as	O
the	O
words	O
only	O
have	O
shared	O
codes	O
when	O
they	O
have	O
almost	O
the	O
same	O
sentiments	O
or	O
target	O
translations	O
.	O
