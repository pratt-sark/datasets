document	O
:	O
A	O
Joint	Method
Many	Method
-	Method
Task	Method
Model	Method
:	O
Growing	O
a	O
Neural	Method
Network	Method
for	O
Multiple	Task
NLP	Task
Tasks	Task
Transfer	Task
and	O
multi	Task
-	Task
task	Task
learning	Task
have	O
traditionally	O
focused	O
on	O
either	O
a	O
single	O
source	O
-	O
target	O
pair	O
or	O
very	O
few	O
,	O
similar	O
tasks	O
.	O
Ideally	O
,	O
the	O
linguistic	O
levels	O
of	O
morphology	O
,	O
syntax	O
and	O
semantics	O
would	O
benefit	O
each	O
other	O
by	O
being	O
trained	O
in	O
a	O
single	O
model	O
.	O
We	O
introduce	O
a	O
joint	Method
many	Method
-	Method
task	Method
model	Method
together	O
with	O
a	O
strategy	O
for	O
successively	O
growing	O
its	O
depth	O
to	O
solve	O
increasingly	O
complex	O
tasks	O
.	O
Higher	O
layers	O
include	O
shortcut	O
connections	O
to	O
lower	O
-	O
level	O
task	O
predictions	O
to	O
reflect	O
linguistic	O
hierarchies	O
.	O
We	O
use	O
a	O
simple	O
regularization	O
term	O
to	O
allow	O
for	O
optimizing	O
all	O
model	O
weights	O
to	O
improve	O
one	O
task	O
’s	O
loss	O
without	O
exhibiting	O
catastrophic	O
interference	O
of	O
the	O
other	O
tasks	O
.	O
Our	O
single	O
end	Method
-	Method
to	Method
-	Method
end	Method
model	Method
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
or	O
competitive	O
results	O
on	O
five	O
different	O
tasks	O
from	O
tagging	Task
,	O
parsing	Task
,	O
relatedness	Task
,	O
and	O
entailment	Task
tasks	Task
.	O
section	O
:	O
Introduction	O
The	O
potential	O
for	O
leveraging	O
multiple	O
levels	O
of	O
representation	O
has	O
been	O
demonstrated	O
in	O
various	O
ways	O
in	O
the	O
field	O
of	O
Natural	Task
Language	Task
Processing	Task
(	O
NLP	Task
)	O
.	O
For	O
example	O
,	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	Method
)	O
tags	O
are	O
used	O
for	O
syntactic	Method
parsers	Method
.	O
The	O
parsers	Method
are	O
used	O
to	O
improve	O
higher	Task
-	Task
level	Task
tasks	Task
,	O
such	O
as	O
natural	Task
language	Task
inference	Task
chen2016snli	O
and	O
machine	Task
translation	Task
eriguchi2016	O
.	O
These	O
systems	O
are	O
often	O
pipelines	O
and	O
not	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O
Deep	Method
NLP	Method
models	Method
have	O
yet	O
shown	O
benefits	O
from	O
predicting	O
many	O
increasingly	O
complex	O
tasks	O
each	O
at	O
a	O
successively	O
deeper	O
layer	O
.	O
Existing	O
models	O
often	O
ignore	O
linguistic	O
hierarchies	O
by	O
predicting	O
different	O
tasks	O
either	O
entirely	O
separately	O
or	O
at	O
the	O
same	O
depth	O
collobert2011senna	O
.	O
We	O
introduce	O
a	O
Joint	Method
Many	Method
-	Method
Task	Method
(	O
JMT	Method
)	O
model	O
,	O
outlined	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
which	O
predicts	O
increasingly	O
complex	O
NLP	Task
tasks	Task
at	O
successively	O
deeper	O
layers	O
.	O
Unlike	O
traditional	O
pipeline	Method
systems	Method
,	O
our	O
single	O
JMT	Method
model	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
for	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
semantic	O
relatedness	Task
,	O
and	O
textual	Task
entailment	Task
,	O
by	O
considering	O
linguistic	O
hierarchies	O
.	O
We	O
propose	O
an	O
adaptive	Method
training	Method
and	Method
regularization	Method
strategy	Method
to	O
grow	O
this	O
model	O
in	O
its	O
depth	O
.	O
With	O
the	O
help	O
of	O
this	O
strategy	O
we	O
avoid	O
catastrophic	O
interference	O
between	O
the	O
tasks	O
.	O
Our	O
model	O
is	O
motivated	O
by	O
sogaard2016	O
who	O
showed	O
that	O
predicting	O
two	O
different	O
tasks	O
is	O
more	O
accurate	O
when	O
performed	O
in	O
different	O
layers	O
than	O
in	O
the	O
same	O
layer	O
collobert2011senna	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
single	O
model	O
achieves	O
competitive	O
results	O
for	O
all	O
of	O
the	O
five	O
different	O
tasks	O
,	O
demonstrating	O
that	O
using	O
linguistic	O
hierarchies	O
is	O
more	O
important	O
than	O
handling	O
different	O
tasks	O
in	O
the	O
same	O
layer	O
.	O
section	O
:	O
The	O
Joint	Method
Many	Method
-	Method
Task	Method
Model	Method
This	O
section	O
describes	O
the	O
inference	Method
procedure	Method
of	O
our	O
model	O
,	O
beginning	O
at	O
the	O
lowest	O
level	O
and	O
working	O
our	O
way	O
to	O
higher	O
layers	O
and	O
more	O
complex	O
tasks	O
;	O
our	O
model	O
handles	O
the	O
five	O
different	O
tasks	O
in	O
the	O
order	O
of	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
semantic	O
relatedness	Task
,	O
and	O
textual	Task
entailment	Task
,	O
by	O
considering	O
linguistic	O
hierarchies	O
.	O
The	O
POS	Method
tags	O
are	O
used	O
for	O
chunking	Task
,	O
and	O
the	O
chunking	Task
tags	O
are	O
used	O
for	O
dependency	Task
parsing	Task
Attardi2008	O
.	O
tai2015treelstm	O
have	O
shown	O
that	O
dependencies	O
improve	O
the	O
relatedness	Task
task	O
.	O
The	O
relatedness	Task
and	O
entailment	O
tasks	O
are	O
closely	O
related	O
to	O
each	O
other	O
.	O
If	O
the	O
semantic	O
relatedness	Task
between	O
two	O
sentences	O
is	O
very	O
low	O
,	O
they	O
are	O
unlikely	O
to	O
entail	O
each	O
other	O
.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
make	O
use	O
of	O
the	O
information	O
from	O
the	O
relatedness	Task
task	O
for	O
improving	O
the	O
entailment	Task
task	Task
.	O
subsection	O
:	O
Word	Method
Representations	Method
For	O
each	O
word	O
in	O
the	O
input	O
sentence	O
of	O
length	O
,	O
we	O
use	O
two	O
types	O
of	O
embeddings	Method
.	O
Word	Task
embeddings	Task
:	O
We	O
use	O
Skip	Method
-	Method
gram	Method
mikolov2013word2vec	O
to	O
train	O
word	Method
embeddings	Method
.	O
Character	Task
embeddings	Task
:	O
Character	O
-	O
gram	O
embeddings	O
are	O
trained	O
by	O
the	O
same	O
Skip	Method
-	Method
gram	Method
objective	O
.	O
We	O
construct	O
the	O
character	O
-	O
gram	O
vocabulary	O
in	O
the	O
training	O
data	O
and	O
assign	O
an	O
embedding	O
for	O
each	O
entry	O
.	O
The	O
final	O
character	Method
embedding	Method
is	O
the	O
average	O
of	O
the	O
unique	O
character	Method
-	Method
gram	Method
embeddings	Method
of	O
.	O
For	O
example	O
,	O
the	O
character	O
-	O
grams	O
(	O
)	O
of	O
the	O
word	O
“	O
Cat	O
”	O
are	O
{	O
C	O
,	O
a	O
,	O
t	O
,	O
#	O
B#C	O
,	O
Ca	O
,	O
at	O
,	O
t#E	O
#	O
,	O
#	O
B#Ca	O
,	O
Cat	O
,	O
at#E	O
#	O
}	O
,	O
where	O
“	O
#	O
B	O
#	O
”	O
and	O
“	O
#	O
E	O
#	O
”	O
represent	O
the	O
beginning	O
and	O
the	O
end	O
of	O
each	O
word	O
,	O
respectively	O
.	O
Using	O
the	O
character	Method
embeddings	Method
efficiently	O
provides	O
morphological	O
features	O
.	O
Each	O
word	O
is	O
subsequently	O
represented	O
as	O
,	O
the	O
concatenation	O
of	O
its	O
corresponding	O
word	O
and	O
character	O
embeddings	O
shared	O
across	O
the	O
tasks	O
.	O
subsection	O
:	O
Word	Task
-	Task
Level	Task
Task	Task
:	O
POS	Method
Tagging	O
The	O
first	O
layer	O
of	O
the	O
model	O
is	O
a	O
bi	Method
-	Method
directional	Method
LSTM	Method
graves2005bilstm	O
,	O
hochreiter1997lstm	O
whose	O
hidden	O
states	O
are	O
used	O
to	O
predict	O
POS	Method
tags	O
.	O
We	O
use	O
the	O
following	O
Long	Method
Short	Method
-	Method
Term	Method
Memory	Method
(	O
LSTM	Method
)	O
units	O
for	O
the	O
forward	O
direction	O
:	O
where	O
we	O
define	O
the	O
input	O
as	O
,	O
i.e.	O
the	O
concatenation	O
of	O
the	O
previous	O
hidden	O
state	O
and	O
the	O
word	Method
representation	Method
of	O
.	O
The	O
backward	O
pass	O
is	O
expanded	O
in	O
the	O
same	O
way	O
,	O
but	O
a	O
different	O
set	O
of	O
weights	O
are	O
used	O
.	O
For	O
predicting	O
the	O
POS	Method
tag	O
of	O
,	O
we	O
use	O
the	O
concatenation	O
of	O
the	O
forward	O
and	O
backward	O
states	O
in	O
a	O
one	O
-	O
layer	O
bi	O
-	O
LSTM	Method
layer	O
corresponding	O
to	O
the	O
-	O
th	O
word	O
:	O
.	O
Then	O
each	O
is	O
fed	O
into	O
a	O
standard	O
classifier	Method
with	O
a	O
single	O
layer	O
which	O
outputs	O
the	O
probability	O
vector	O
for	O
each	O
of	O
the	O
POS	Method
tags	O
.	O
subsection	O
:	O
Word	Task
-	Task
Level	Task
Task	Task
:	O
Chunking	Task
Chunking	Task
is	O
also	O
a	O
word	Task
-	Task
level	Task
classification	Task
task	Task
which	O
assigns	O
a	O
chunking	Task
tag	O
(	O
B	O
-	O
NP	O
,	O
I	O
-	O
VP	O
,	O
etc	O
.	O
)	O
for	O
each	O
word	O
.	O
The	O
tag	O
specifies	O
the	O
region	O
of	O
major	O
phrases	O
(	O
e.g.	O
,	O
noun	O
phrases	O
)	O
in	O
the	O
sentence	O
.	O
Chunking	Task
is	O
performed	O
in	O
the	O
second	O
bi	O
-	O
LSTM	Method
layer	O
on	O
top	O
of	O
the	O
POS	Method
layer	O
.	O
When	O
stacking	O
the	O
bi	O
-	O
LSTM	Method
layers	O
,	O
we	O
use	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
with	O
input	O
,	O
where	O
is	O
the	O
hidden	O
state	O
of	O
the	O
first	O
(	O
POS	Method
)	O
layer	O
.	O
We	O
define	O
the	O
weighted	Method
label	Method
embedding	Method
as	O
follows	O
:	O
where	O
is	O
the	O
number	O
of	O
the	O
POS	Method
tags	O
,	O
is	O
the	O
probability	O
value	O
that	O
the	O
-	O
th	O
POS	Method
tag	O
is	O
assigned	O
to	O
,	O
and	O
is	O
the	O
corresponding	O
label	Method
embedding	Method
.	O
The	O
probability	O
values	O
are	O
predicted	O
by	O
the	O
POS	Method
layer	O
,	O
and	O
thus	O
no	O
gold	Metric
POS	Method
tags	O
are	O
needed	O
.	O
This	O
output	O
embedding	O
is	O
similar	O
to	O
the	O
-	O
best	O
POS	Method
tag	O
feature	O
which	O
has	O
been	O
shown	O
to	O
be	O
effective	O
in	O
syntactic	Task
tasks	Task
andor2016	O
,	O
alberti2016	O
.	O
For	O
predicting	O
the	O
chunking	Task
tags	Task
,	O
we	O
employ	O
the	O
same	O
strategy	O
as	O
POS	Method
tagging	O
by	O
using	O
the	O
concatenated	O
bi	O
-	O
directional	O
hidden	O
states	O
in	O
the	O
chunking	Task
layer	Task
.	O
We	O
also	O
use	O
a	O
single	O
hidden	Method
layer	Method
before	O
the	O
softmax	Method
classifier	Method
.	O
subsection	O
:	O
Syntactic	Task
Task	Task
:	O
Dependency	Task
Parsing	Task
D	Task
ependency	Task
parsing	Task
identifies	O
syntactic	O
relations	O
(	O
such	O
as	O
an	O
adjective	O
modifying	O
a	O
noun	O
)	O
between	O
word	O
pairs	O
in	O
a	O
sentence	O
.	O
We	O
use	O
the	O
third	O
bi	O
-	O
LSTM	Method
layer	O
to	O
classify	O
relations	O
between	O
all	O
pairs	O
of	O
words	O
.	O
The	O
input	O
vector	O
for	O
the	O
LSTM	Method
includes	O
hidden	O
states	O
,	O
word	Method
representations	Method
,	O
and	O
the	O
label	O
embeddings	O
for	O
the	O
two	O
previous	O
tasks	O
:	O
,	O
where	O
we	O
computed	O
the	O
chunking	Task
vector	O
in	O
a	O
similar	O
fashion	O
as	O
the	O
POS	Method
vector	O
in	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
We	O
predict	O
the	O
parent	O
node	O
(	O
head	O
)	O
for	O
each	O
word	O
.	O
Then	O
a	O
dependency	O
label	O
is	O
predicted	O
for	O
each	O
child	O
-	O
parent	O
pair	O
.	O
This	O
approach	O
is	O
related	O
to	O
biaffine2017	O
and	O
zhang2017head	O
,	O
where	O
the	O
main	O
difference	O
is	O
that	O
our	O
model	O
works	O
on	O
a	O
multi	Method
-	Method
task	Method
framework	Method
.	O
To	O
predict	O
the	O
parent	O
node	O
of	O
,	O
we	O
define	O
a	O
matching	O
function	O
between	O
and	O
the	O
candidates	O
of	O
the	O
parent	O
node	O
as	O
,	O
where	O
is	O
a	O
parameter	O
matrix	O
.	O
For	O
the	O
root	O
,	O
we	O
define	O
as	O
a	O
parameterized	O
vector	O
.	O
To	O
compute	O
the	O
probability	O
that	O
(	O
or	O
the	O
root	O
node	O
)	O
is	O
the	O
parent	O
of	O
,	O
the	O
scores	O
are	O
normalized	O
:	O
The	O
dependency	O
labels	O
are	O
predicted	O
using	O
as	O
input	O
to	O
a	O
classifier	Method
with	O
a	O
single	Method
layer	Method
.	O
We	O
greedily	O
select	O
the	O
parent	O
node	O
and	O
the	O
dependency	O
label	O
for	O
each	O
word	O
.	O
When	O
the	O
parsing	O
result	O
is	O
not	O
a	O
well	O
-	O
formed	O
tree	O
,	O
we	O
apply	O
the	O
first	Method
-	Method
order	Method
Eisner	Method
’s	Method
algorithm	Method
eisner1996	O
to	O
obtain	O
a	O
well	O
-	O
formed	O
tree	O
from	O
it	O
.	O
subsection	O
:	O
Semantic	Task
Task	Task
:	O
Semantic	O
relatedness	Task
The	O
next	O
two	O
tasks	O
model	O
the	O
semantic	O
relationships	O
between	O
two	O
input	O
sentences	O
.	O
The	O
first	O
task	O
measures	O
the	O
semantic	O
relatedness	Task
between	O
two	O
sentences	O
.	O
The	O
output	O
is	O
a	O
real	O
-	O
valued	O
relatedness	Task
score	O
for	O
the	O
input	O
sentence	O
pair	O
.	O
The	O
second	O
task	O
is	O
textual	Task
entailment	Task
,	O
which	O
requires	O
one	O
to	O
determine	O
whether	O
a	O
premise	O
sentence	O
entails	O
a	O
hypothesis	O
sentence	O
.	O
There	O
are	O
typically	O
three	O
classes	O
:	O
entailment	O
,	O
contradiction	O
,	O
and	O
neutral	O
.	O
We	O
use	O
the	O
fourth	O
and	O
fifth	O
bi	O
-	O
LSTM	Method
layer	O
for	O
the	O
relatedness	Task
and	O
entailment	Task
task	Task
,	O
respectively	O
.	O
Now	O
it	O
is	O
required	O
to	O
obtain	O
the	O
sentence	Method
-	Method
level	Method
representation	Method
rather	O
than	O
the	O
word	Method
-	Method
level	Method
representation	Method
used	O
in	O
the	O
first	O
three	O
tasks	O
.	O
We	O
compute	O
the	O
sentence	Method
-	Method
level	Method
representation	Method
as	O
the	O
element	O
-	O
wise	O
maximum	O
values	O
across	O
all	O
of	O
the	O
word	Method
-	Method
level	Method
representations	Method
in	O
the	O
fourth	O
layer	O
:	O
This	O
max	Method
-	Method
pooling	Method
technique	Method
has	O
proven	O
effective	O
in	O
text	Task
classification	Task
tasks	Task
lai2015maxpooling	O
.	O
To	O
model	O
the	O
semantic	O
relatedness	Task
between	O
and	O
,	O
we	O
follow	O
tai2015treelstm	O
.	O
The	O
feature	O
vector	O
for	O
representing	O
the	O
semantic	O
relatedness	Task
is	O
computed	O
as	O
follows	O
:	O
where	O
is	O
the	O
absolute	O
values	O
of	O
the	O
element	Method
-	Method
wise	Method
subtraction	Method
,	O
and	O
is	O
the	O
element	Method
-	Method
wise	Method
multiplication	Method
.	O
Then	O
is	O
fed	O
into	O
a	O
classifier	Method
with	O
a	O
single	O
hidden	Method
layer	Method
goodfellow2013	O
to	O
output	O
a	O
relatedness	Metric
score	Metric
(	O
from	O
1	O
to	O
5	O
in	O
our	O
case	O
)	O
.	O
subsection	O
:	O
Semantic	Task
Task	Task
:	O
Textual	Task
entailment	Task
For	O
entailment	Task
classification	Task
,	O
we	O
also	O
use	O
the	O
max	Method
-	Method
pooling	Method
technique	Method
as	O
in	O
the	O
semantic	O
relatedness	Task
task	O
.	O
To	O
classify	O
the	O
premise	O
-	O
hypothesis	O
pair	O
into	O
one	O
of	O
the	O
three	O
classes	O
,	O
we	O
compute	O
the	O
feature	O
vector	O
as	O
in	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
except	O
that	O
we	O
do	O
not	O
use	O
the	O
absolute	O
values	O
of	O
the	O
element	O
-	O
wise	O
subtraction	O
,	O
because	O
we	O
need	O
to	O
identify	O
which	O
is	O
the	O
premise	O
(	O
or	O
hypothesis	O
)	O
.	O
Then	O
is	O
fed	O
into	O
a	O
classifier	Method
.	O
To	O
use	O
the	O
output	O
from	O
the	O
relatedness	Task
layer	O
directly	O
,	O
we	O
use	O
the	O
label	O
embeddings	O
for	O
the	O
relatedness	Task
task	O
.	O
More	O
concretely	O
,	O
we	O
compute	O
the	O
class	O
label	O
embeddings	O
for	O
the	O
semantic	O
relatedness	Task
task	O
similar	O
to	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
The	O
final	O
feature	O
vectors	O
that	O
are	O
concatenated	O
and	O
fed	O
into	O
the	O
entailment	Method
classifier	Method
are	O
the	O
weighted	O
relatedness	Task
label	O
embedding	O
and	O
the	O
feature	O
vector	O
.	O
We	O
use	O
three	O
hidden	O
layers	O
before	O
the	O
classifier	Method
.	O
section	O
:	O
Training	O
the	O
JMT	Method
Model	O
The	O
model	O
is	O
trained	O
jointly	O
over	O
all	O
datasets	O
.	O
During	O
each	O
epoch	O
,	O
the	O
optimization	Task
iterates	O
over	O
each	O
full	O
training	O
dataset	O
in	O
the	O
same	O
order	O
as	O
the	O
corresponding	O
tasks	O
described	O
in	O
the	O
modeling	O
section	O
.	O
subsection	O
:	O
Pre	O
-	O
Training	O
Word	Method
Representations	Method
We	O
pre	O
-	O
train	O
word	Method
embeddings	Method
using	O
the	O
Skip	Method
-	Method
gram	Method
model	O
with	O
negative	Method
sampling	Method
mikolov2013word2vec	O
.	O
We	O
also	O
pre	O
-	O
train	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
using	O
Skip	Method
-	Method
gram	Method
.	O
The	O
only	O
difference	O
is	O
that	O
each	O
input	O
word	Method
embedding	Method
is	O
replaced	O
with	O
its	O
corresponding	O
average	Method
character	Method
-	Method
gram	Method
embedding	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
These	O
embeddings	O
are	O
fine	O
-	O
tuned	O
during	O
the	O
model	Method
training	Method
.	O
We	O
denote	O
the	O
embedding	O
parameters	O
as	O
.	O
subsection	O
:	O
Training	O
the	O
POS	Method
Layer	O
Let	O
denote	O
the	O
set	O
of	O
model	O
parameters	O
associated	O
with	O
the	O
POS	Method
layer	O
,	O
where	O
is	O
the	O
set	O
of	O
the	O
weight	O
matrices	O
in	O
the	O
first	O
bi	O
-	O
LSTM	Method
and	O
the	O
classifier	Method
,	O
and	O
is	O
the	O
set	O
of	O
the	O
bias	O
vectors	O
.	O
The	O
objective	O
function	O
to	O
optimize	O
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
probability	O
value	O
that	O
the	O
correct	O
label	O
is	O
assigned	O
to	O
in	O
the	O
sentence	O
,	O
is	O
the	O
L2	O
-	O
norm	O
regularization	O
term	O
,	O
and	O
is	O
a	O
hyperparameter	O
.	O
We	O
call	O
the	O
second	O
regularization	Method
term	Method
a	O
successive	Method
regularization	Method
term	Method
.	O
The	O
successive	Method
regularization	Method
is	O
based	O
on	O
the	O
idea	O
that	O
we	O
do	O
not	O
want	O
the	O
model	O
to	O
forget	O
the	O
information	O
learned	O
for	O
the	O
other	O
tasks	O
.	O
In	O
the	O
case	O
of	O
POS	Method
tagging	O
,	O
the	O
regularization	Method
is	O
applied	O
to	O
,	O
and	O
is	O
the	O
embedding	O
parameter	O
after	O
training	O
the	O
final	O
task	O
in	O
the	O
top	O
-	O
most	O
layer	O
at	O
the	O
previous	O
training	O
epoch	O
.	O
is	O
a	O
hyperparameter	O
.	O
subsection	O
:	O
Training	O
the	O
Chunking	Task
Layer	O
The	O
objective	Metric
function	Metric
is	O
defined	O
as	O
follows	O
:	O
which	O
is	O
similar	O
to	O
that	O
of	O
POS	Method
tagging	O
,	O
and	O
is	O
,	O
where	O
and	O
are	O
the	O
weight	O
and	O
bias	O
parameters	O
including	O
those	O
in	O
,	O
and	O
is	O
the	O
set	O
of	O
the	O
POS	Method
label	O
embeddings	O
.	O
is	O
the	O
one	O
after	O
training	O
the	O
POS	Method
layer	O
at	O
the	O
current	O
training	O
epoch	O
.	O
subsection	O
:	O
Training	O
the	O
Dependency	Method
Layer	Method
The	O
objective	Metric
function	Metric
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
probability	O
value	O
assigned	O
to	O
the	O
correct	O
parent	O
node	O
for	O
,	O
and	O
is	O
the	O
probability	O
value	O
assigned	O
to	O
the	O
correct	O
dependency	O
label	O
for	O
the	O
child	O
-	O
parent	O
pair	O
.	O
is	O
defined	O
as	O
,	O
where	O
and	O
are	O
the	O
weight	O
and	O
bias	O
parameters	O
including	O
those	O
in	O
,	O
and	O
is	O
the	O
set	O
of	O
the	O
chunking	Task
label	O
embeddings	O
.	O
subsection	O
:	O
Training	O
the	O
Relatedness	Method
Layer	Method
Following	O
tai2015treelstm	O
,	O
the	O
objective	Metric
function	Metric
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
gold	Metric
distribution	O
over	O
the	O
defined	O
relatedness	Task
scores	O
,	O
is	O
the	O
predicted	O
distribution	O
given	O
the	O
the	O
sentence	O
representations	O
,	O
and	O
is	O
the	O
KL	O
-	O
divergence	O
between	O
the	O
two	O
distributions	O
.	O
is	O
defined	O
as	O
.	O
subsection	O
:	O
Training	O
the	O
Entailment	Method
Layer	Method
The	O
objective	Metric
function	Metric
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
probability	O
value	O
that	O
the	O
correct	O
label	O
is	O
assigned	O
to	O
the	O
premise	O
-	O
hypothesis	O
pair	O
.	O
is	O
defined	O
as	O
,	O
where	O
is	O
the	O
set	O
of	O
the	O
relatedness	Task
label	O
embeddings	O
.	O
section	O
:	O
Related	O
Work	O
Many	O
deep	Method
learning	Method
approaches	Method
have	O
proven	O
to	O
be	O
effective	O
in	O
a	O
variety	O
of	O
NLP	Task
tasks	Task
and	O
are	O
becoming	O
more	O
and	O
more	O
complex	O
.	O
They	O
are	O
typically	O
designed	O
to	O
handle	O
single	O
tasks	O
,	O
or	O
some	O
of	O
them	O
are	O
designed	O
as	O
general	Method
-	Method
purpose	Method
models	Method
kumar2016dmn	O
,	O
sutskever2014seq2seq	O
but	O
applied	O
to	O
different	O
tasks	O
independently	O
.	O
For	O
handling	O
multiple	Task
NLP	Task
tasks	Task
,	O
multi	Method
-	Method
task	Method
learning	Method
models	Method
with	O
deep	Method
neural	Method
networks	Method
have	O
been	O
proposed	O
collobert2011senna	O
,	O
luong2016mtl	O
,	O
and	O
more	O
recently	O
sogaard2016	O
have	O
suggested	O
that	O
using	O
different	O
layers	O
for	O
different	O
tasks	O
is	O
more	O
effective	O
than	O
using	O
the	O
same	O
layer	O
in	O
jointly	Task
learning	Task
closely	Task
-	Task
related	Task
tasks	Task
,	O
such	O
as	O
POS	Method
tagging	O
and	O
chunking	Task
.	O
However	O
,	O
the	O
number	O
of	O
tasks	O
was	O
limited	O
or	O
they	O
have	O
very	O
similar	O
task	O
settings	O
like	O
word	Method
-	Method
level	Method
tagging	Method
,	O
and	O
it	O
was	O
not	O
clear	O
how	O
lower	O
-	O
level	O
tasks	O
could	O
be	O
also	O
improved	O
by	O
combining	O
higher	O
-	O
level	O
tasks	O
.	O
More	O
related	O
to	O
our	O
work	O
,	O
godwin2016multi	O
also	O
followed	O
sogaard2016	O
to	O
jointly	O
learn	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
and	O
language	Method
modeling	Method
,	O
and	O
zhang2016stackprop	O
have	O
shown	O
that	O
it	O
is	O
effective	O
to	O
jointly	O
learn	O
POS	Method
tagging	O
and	O
dependency	Task
parsing	Task
by	O
sharing	O
internal	Method
representations	Method
.	O
In	O
the	O
field	O
of	O
relation	Task
extraction	Task
,	O
miwa2016rel	O
proposed	O
a	O
joint	Method
learning	Method
model	Method
for	O
entity	Task
detection	Task
and	O
relation	Task
extraction	Task
.	O
All	O
of	O
them	O
suggest	O
the	O
importance	O
of	O
multi	Task
-	Task
task	Task
learning	Task
,	O
and	O
we	O
investigate	O
the	O
potential	O
of	O
handling	O
different	O
types	O
of	O
NLP	Task
tasks	Task
rather	O
than	O
closely	O
-	O
related	O
ones	O
in	O
a	O
single	O
hierarchical	Method
deep	Method
model	Method
.	O
In	O
the	O
field	O
of	O
computer	Task
vision	Task
,	O
some	O
transfer	Method
and	Method
multi	Method
-	Method
task	Method
learning	Method
approaches	Method
have	O
also	O
been	O
proposed	O
li2016multi	O
,	O
misra2016multi	O
.	O
For	O
example	O
,	O
misra2016multi	O
proposed	O
a	O
multi	Method
-	Method
task	Method
learning	Method
model	Method
to	O
handle	O
different	O
tasks	O
.	O
However	O
,	O
they	O
assume	O
that	O
each	O
data	O
sample	O
has	O
annotations	O
for	O
the	O
different	O
tasks	O
,	O
and	O
do	O
not	O
explicitly	O
consider	O
task	O
hierarchies	O
.	O
Recently	O
,	O
rusu2016progressive	O
have	O
proposed	O
a	O
progressive	Method
neural	Method
network	Method
model	Method
to	O
handle	O
multiple	Task
reinforcement	Task
learning	Task
tasks	Task
,	O
such	O
as	O
Atari	Task
games	Task
.	O
Like	O
our	O
JMT	Method
model	O
,	O
their	O
model	O
is	O
also	O
successively	O
trained	O
according	O
to	O
different	O
tasks	O
using	O
different	O
layers	O
called	O
columns	O
in	O
their	O
paper	O
.	O
In	O
their	O
model	O
,	O
once	O
the	O
first	O
task	O
is	O
completed	O
,	O
the	O
model	O
parameters	O
for	O
the	O
first	O
task	O
are	O
fixed	O
,	O
and	O
then	O
the	O
second	O
task	O
is	O
handled	O
with	O
new	O
model	O
parameters	O
.	O
Therefore	O
,	O
accuracy	Metric
of	O
the	O
previously	O
trained	O
tasks	O
is	O
never	O
improved	O
.	O
In	O
NLP	Task
tasks	Task
,	O
multi	Task
-	Task
task	Task
learning	Task
has	O
the	O
potential	O
to	O
improve	O
not	O
only	O
higher	O
-	O
level	O
tasks	O
,	O
but	O
also	O
lower	O
-	O
level	O
tasks	O
.	O
Rather	O
than	O
fixing	O
the	O
pre	O
-	O
trained	O
model	O
parameters	O
,	O
our	O
successive	Method
regularization	Method
allows	O
our	O
model	O
to	O
continuously	O
train	O
the	O
lower	O
-	O
level	O
tasks	O
without	O
significant	O
accuracy	Metric
drops	Metric
.	O
section	O
:	O
Experimental	O
Settings	O
subsection	O
:	O
Datasets	O
POS	Method
tagging	O
:	O
To	O
train	O
the	O
POS	Method
tagging	O
layer	O
,	O
we	O
used	O
the	O
Wall	Material
Street	Material
Journal	Material
(	O
WSJ	Material
)	O
portion	O
of	O
Penn	Material
Treebank	Material
,	O
and	O
followed	O
the	O
standard	O
split	O
for	O
the	O
training	O
(	O
Section	O
0	O
-	O
18	O
)	O
,	O
development	O
(	O
Section	O
19	O
-	O
21	O
)	O
,	O
and	O
test	O
(	O
Section	O
22	O
-	O
24	O
)	O
sets	O
.	O
The	O
evaluation	Metric
metric	Metric
is	O
the	O
word	Metric
-	Metric
level	Metric
accuracy	Metric
.	O
Chunking	Task
:	O
For	O
chunking	Task
,	O
we	O
also	O
used	O
the	O
WSJ	Material
corpus	O
,	O
and	O
followed	O
the	O
standard	O
split	O
for	O
the	O
training	O
(	O
Section	O
15	O
-	O
18	O
)	O
and	O
test	O
(	O
Section	O
20	O
)	O
sets	O
as	O
in	O
the	O
CoNLL	Material
2000	Material
shared	Material
task	Material
.	O
We	O
used	O
Section	O
19	O
as	O
the	O
development	O
set	O
and	O
employed	O
the	O
IOBES	Method
tagging	Method
scheme	Method
.	O
The	O
evaluation	Metric
metric	Metric
is	O
the	O
F1	Metric
score	Metric
defined	O
in	O
the	O
shared	Task
task	Task
.	O
Dependency	Task
parsing	Task
:	O
We	O
also	O
used	O
the	O
WSJ	Material
corpus	O
for	O
dependency	Task
parsing	Task
,	O
and	O
followed	O
the	O
standard	O
split	O
for	O
the	O
training	O
(	O
Section	O
2	O
-	O
21	O
)	O
,	O
development	O
(	O
Section	O
22	O
)	O
,	O
and	O
test	O
(	O
Section	O
23	O
)	O
sets	O
.	O
We	O
obtained	O
Stanford	O
style	O
dependencies	O
using	O
the	O
version	O
3.3.0	O
of	O
the	O
Stanford	Method
converter	Method
.	O
The	O
evaluation	Metric
metrics	Metric
are	O
the	O
Unlabeled	Metric
Attachment	Metric
Score	Metric
(	O
UAS	Metric
)	O
and	O
the	O
Labeled	Metric
Attachment	Metric
Score	Metric
(	O
LAS	Metric
)	O
,	O
and	O
punctuations	O
are	O
excluded	O
for	O
the	O
evaluation	O
.	O
Semantic	O
relatedness	Task
:	O
For	O
the	O
semantic	O
relatedness	Task
task	O
,	O
we	O
used	O
the	O
SICK	Material
dataset	Material
marelli2014	O
,	O
and	O
followed	O
the	O
standard	O
split	O
for	O
the	O
training	O
,	O
development	O
,	O
and	O
test	O
sets	O
.	O
The	O
evaluation	Metric
metric	Metric
is	O
the	O
Mean	Metric
Squared	Metric
Error	Metric
(	O
MSE	Metric
)	O
between	O
the	O
gold	Metric
and	O
predicted	Metric
scores	Metric
.	O
Textual	Task
entailment	Task
:	O
For	O
textual	Task
entailment	Task
,	O
we	O
also	O
used	O
the	O
SICK	Material
dataset	Material
and	O
exactly	O
the	O
same	O
data	O
split	O
as	O
the	O
semantic	O
relatedness	Task
dataset	O
.	O
The	O
evaluation	Metric
metric	Metric
is	O
the	O
accuracy	Metric
.	O
subsection	O
:	O
Training	O
Details	O
We	O
set	O
the	O
dimensionality	O
of	O
the	O
embeddings	O
and	O
the	O
hidden	O
states	O
in	O
the	O
bi	Method
-	Method
LSTMs	Method
to	O
100	O
.	O
At	O
each	O
training	O
epoch	O
,	O
we	O
trained	O
our	O
model	O
in	O
the	O
order	O
of	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
semantic	O
relatedness	Task
,	O
and	O
textual	Task
entailment	Task
.	O
We	O
used	O
mini	Method
-	Method
batch	Method
stochastic	Method
gradient	Method
decent	Method
and	O
empirically	O
found	O
it	O
effective	O
to	O
use	O
a	O
gradient	Method
clipping	Method
method	Method
with	O
growing	O
clipping	O
values	O
for	O
the	O
different	O
tasks	O
;	O
concretely	O
,	O
we	O
employed	O
the	O
simple	O
function	O
:	O
,	O
where	O
is	O
the	O
number	O
of	O
bi	O
-	O
LSTM	Method
layers	O
involved	O
in	O
each	O
task	O
,	O
and	O
is	O
the	O
maximum	O
value	O
.	O
We	O
applied	O
our	O
successive	Method
regularization	Method
to	O
our	O
model	O
,	O
along	O
with	O
L2	Method
-	Method
norm	Method
regularization	Method
and	O
dropout	Method
dropout2014ver	O
.	O
More	O
details	O
are	O
summarized	O
in	O
the	O
supplemental	O
material	O
.	O
section	O
:	O
Results	O
and	O
Discussion	O
Table	O
[	O
reference	O
]	O
shows	O
our	O
results	O
on	O
the	O
test	O
sets	O
of	O
the	O
five	O
tasks	O
.	O
The	O
column	O
“	O
Single	O
”	O
shows	O
the	O
results	O
of	O
handling	O
each	O
task	O
separately	O
using	O
single	Method
-	Method
layer	Method
bi	Method
-	Method
LSTMs	Method
,	O
and	O
the	O
column	O
“	O
JMT	Method
”	O
shows	O
the	O
results	O
of	O
our	O
JMT	Method
model	O
.	O
The	O
single	O
task	O
settings	O
only	O
use	O
the	O
annotations	O
of	O
their	O
own	O
tasks	O
.	O
For	O
example	O
,	O
when	O
handling	O
dependency	Task
parsing	Task
as	O
a	O
single	O
task	O
,	O
the	O
POS	Method
and	O
chunking	Task
tags	O
are	O
not	O
used	O
.	O
We	O
can	O
see	O
that	O
all	O
results	O
of	O
the	O
five	O
tasks	O
are	O
improved	O
in	O
our	O
JMT	Method
model	O
,	O
which	O
shows	O
that	O
our	O
JMT	Method
model	O
can	O
handle	O
the	O
five	O
different	O
tasks	O
in	O
a	O
single	O
model	O
.	O
Our	O
JMT	Method
model	O
allows	O
us	O
to	O
access	O
arbitrary	O
information	O
learned	O
from	O
the	O
different	O
tasks	O
.	O
If	O
we	O
want	O
to	O
use	O
the	O
model	O
just	O
as	O
a	O
POS	Method
tagger	O
,	O
we	O
can	O
use	O
only	O
first	O
bi	O
-	O
LSTM	Method
layer	O
.	O
Table	O
[	O
reference	O
]	O
also	O
shows	O
the	O
results	O
of	O
five	O
subsets	O
of	O
the	O
different	O
tasks	O
.	O
For	O
example	O
,	O
in	O
the	O
case	O
of	O
“	O
JMT	Method
”	Method
,	O
only	O
the	O
first	O
three	O
layers	O
of	O
the	O
bi	Method
-	Method
LSTMs	Method
are	O
used	O
to	O
handle	O
the	O
three	O
tasks	O
.	O
In	O
the	O
case	O
of	O
“	O
JMT	Method
”	Method
,	O
only	O
the	O
top	O
two	O
layers	O
are	O
used	O
as	O
a	O
two	O
-	O
layer	O
bi	O
-	O
LSTM	Method
by	O
omitting	O
all	O
information	O
from	O
the	O
first	O
three	O
layers	O
.	O
The	O
results	O
of	O
the	O
closely	O
-	O
related	O
tasks	O
(	O
“	O
AB	O
”	O
,	O
“	O
ABC	O
”	O
,	O
and	O
“	O
DE	O
”	O
)	O
show	O
that	O
our	O
JMT	Method
model	O
improves	O
both	O
of	O
the	O
high	O
-	O
level	O
and	O
low	Metric
-	Metric
level	Metric
tasks	Metric
.	O
The	O
results	O
of	O
“	O
JMT	Method
”	O
and	O
“	O
JMT	Method
”	Method
show	O
that	O
the	O
parsing	Task
task	Task
can	O
be	O
improved	O
by	O
the	O
semantic	Task
tasks	Task
.	O
It	O
should	O
be	O
noted	O
that	O
in	O
our	O
analysis	O
on	O
the	O
greedy	Task
parsing	Task
results	O
of	O
the	O
“	O
JMT	Method
”	Method
setting	Method
,	O
we	O
have	O
found	O
that	O
more	O
than	O
95	O
%	O
are	O
well	O
-	O
formed	O
dependency	O
trees	O
on	O
the	O
development	O
set	O
.	O
In	O
the	O
1	O
,	O
700	O
sentences	O
of	O
the	O
development	O
data	O
,	O
11	O
results	O
have	O
multiple	O
root	O
notes	O
,	O
11	O
results	O
have	O
no	O
root	O
nodes	O
,	O
and	O
61	O
results	O
have	O
cycles	O
.	O
These	O
83	O
parsing	O
results	O
are	O
converted	O
into	O
well	O
-	O
formed	O
trees	O
by	O
Eisner	Method
’s	Method
algorithm	Method
,	O
and	O
the	O
accuracy	Metric
does	O
not	O
significantly	O
change	O
(	O
UAS	Metric
:	O
94.52	O
%	O
94.53	O
%	O
,	O
LAS	Metric
:	O
92.61	O
%	O
92.62	O
%	O
)	O
.	O
subsection	O
:	O
Comparison	O
with	O
Published	O
Results	O
paragraph	O
:	O
POS	Method
tagging	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
POS	Method
tagging	O
,	O
and	O
our	O
JMT	Method
model	O
achieves	O
the	O
score	O
close	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
The	O
best	O
result	O
to	O
date	O
has	O
been	O
achieved	O
by	O
ling2015charlstm	O
,	O
which	O
uses	O
character	Method
-	Method
based	Method
LSTMs	Method
.	O
Incorporating	O
the	O
character	Method
-	Method
based	Method
encoders	Method
into	O
our	O
JMT	Method
model	O
would	O
be	O
an	O
interesting	O
direction	O
,	O
but	O
we	O
have	O
shown	O
that	O
the	O
simple	O
pre	O
-	O
trained	O
character	Method
-	Method
gram	Method
embeddings	Method
lead	O
to	O
the	O
promising	O
result	O
.	O
paragraph	O
:	O
Chunking	Task
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
chunking	Task
,	O
and	O
our	O
JMT	Method
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O
sogaard2016	O
proposed	O
to	O
jointly	O
learn	O
POS	Method
tagging	O
and	O
chunking	Task
in	O
different	O
layers	O
,	O
but	O
they	O
only	O
showed	O
improvement	O
for	O
chunking	Task
.	O
By	O
contrast	O
,	O
our	O
results	O
show	O
that	O
the	O
low	Task
-	Task
level	Task
tasks	Task
are	O
also	O
improved	O
.	O
paragraph	O
:	O
Dependency	Method
parsing	Method
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
dependency	Task
parsing	Task
by	O
using	O
only	O
the	O
WSJ	Material
corpus	O
in	O
terms	O
of	O
the	O
dependency	O
annotations	O
.	O
It	O
is	O
notable	O
that	O
our	O
simple	O
greedy	Method
dependency	Method
parser	Method
outperforms	O
the	O
model	O
in	O
andor2016	O
which	O
is	O
based	O
on	O
beam	Method
search	Method
with	O
global	O
information	O
.	O
The	O
result	O
suggests	O
that	O
the	O
bi	Method
-	Method
LSTMs	Method
efficiently	O
capture	O
global	O
information	O
necessary	O
for	O
dependency	Task
parsing	Task
.	O
Moreover	O
,	O
our	O
single	O
task	O
result	O
already	O
achieves	O
high	O
accuracy	Metric
without	O
the	O
POS	Method
and	O
chunking	Task
information	O
.	O
The	O
best	O
result	O
to	O
date	O
has	O
been	O
achieved	O
by	O
the	O
model	O
propsoed	O
in	O
biaffine2017	O
,	O
which	O
uses	O
higher	Method
dimensional	Method
representations	Method
than	O
ours	O
and	O
proposes	O
a	O
more	O
sophisticated	O
attention	Method
mechanism	Method
called	O
biaffine	Method
attention	Method
.	O
It	O
should	O
be	O
promising	O
to	O
incorporate	O
their	O
attention	Method
mechanism	Method
into	O
our	O
parsing	Method
component	Method
.	O
paragraph	O
:	O
Semantic	O
relatedness	Task
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
semantic	O
relatedness	Task
task	O
,	O
and	O
our	O
JMT	Method
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O
The	O
result	O
of	O
“	O
JMT	Method
”	Method
is	O
already	O
better	O
than	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
Both	O
of	O
zhou2016coling	O
and	O
tai2015treelstm	O
explicitly	O
used	O
syntactic	O
trees	O
,	O
and	O
zhou2016coling	O
relied	O
on	O
attention	Method
mechanisms	Method
.	O
However	O
,	O
our	O
method	O
uses	O
the	O
simple	O
max	Method
-	Method
pooling	Method
strategy	Method
,	O
which	O
suggests	O
that	O
it	O
is	O
worth	O
investigating	O
such	O
simple	O
methods	O
before	O
developing	O
complex	O
methods	O
for	O
simple	O
tasks	O
.	O
Currently	O
,	O
our	O
JMT	Method
model	O
does	O
not	O
explicitly	O
use	O
the	O
learned	O
dependency	O
structures	O
,	O
and	O
thus	O
the	O
explicit	O
use	O
of	O
the	O
output	O
from	O
the	O
dependency	Method
layer	Method
should	O
be	O
an	O
interesting	O
direction	O
of	O
future	O
work	O
.	O
paragraph	O
:	O
Textual	O
entailment	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
textual	Task
entailment	Task
,	O
and	O
our	O
JMT	Method
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
.	O
The	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
in	O
yin2016abcnn	O
relied	O
on	O
attention	Method
mechanisms	Method
and	O
dataset	O
-	O
specific	O
data	Method
pre	Method
-	Method
processing	Method
and	O
features	O
.	O
Again	O
,	O
our	O
simple	O
max	Method
-	Method
pooling	Method
strategy	Method
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
boosted	O
by	O
the	O
joint	Method
training	Method
.	O
These	O
results	O
show	O
the	O
importance	O
of	O
jointly	O
handling	O
related	O
tasks	O
.	O
subsection	O
:	O
Analysis	O
on	O
the	O
Model	Method
Architectures	Method
We	O
investigate	O
the	O
effectiveness	O
of	O
our	O
model	O
in	O
detail	O
.	O
All	O
of	O
the	O
results	O
shown	O
in	O
this	O
section	O
are	O
the	O
development	O
set	O
results	O
.	O
paragraph	O
:	O
Shortcut	O
connections	O
Our	O
JMT	Method
model	O
feeds	O
the	O
word	Method
representations	Method
into	O
all	O
of	O
the	O
bi	O
-	O
LSTM	Method
layers	O
,	O
which	O
is	O
called	O
the	O
shortcut	O
connection	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
“	O
JMT	Method
”	Method
with	O
and	O
without	O
the	O
shortcut	O
connections	O
.	O
The	O
results	O
without	O
the	O
shortcut	O
connections	O
are	O
shown	O
in	O
the	O
column	O
of	O
“	O
w	O
/	O
o	O
SC	O
”	O
.	O
These	O
results	O
clearly	O
show	O
that	O
the	O
importance	O
of	O
the	O
shortcut	O
connections	O
,	O
and	O
in	O
particular	O
,	O
the	O
semantic	Task
tasks	Task
in	O
the	O
higher	O
layers	O
strongly	O
rely	O
on	O
the	O
shortcut	O
connections	O
.	O
That	O
is	O
,	O
simply	O
stacking	O
the	O
LSTM	Method
layers	O
is	O
not	O
sufficient	O
to	O
handle	O
a	O
variety	O
of	O
NLP	Task
tasks	Task
in	O
a	O
single	O
model	O
.	O
In	O
the	O
supplementary	O
material	O
,	O
it	O
is	O
qualitatively	O
shown	O
how	O
the	O
shortcut	O
connections	O
work	O
in	O
our	O
model	O
.	O
paragraph	O
:	O
Output	O
label	Method
embeddings	Method
Table	O
[	O
reference	O
]	O
also	O
shows	O
the	O
results	O
without	O
using	O
the	O
output	O
labels	O
of	O
the	O
POS	Method
,	O
chunking	Task
,	O
and	O
relatedness	Task
layers	O
,	O
in	O
the	O
column	O
of	O
“	O
w	O
/	O
o	O
LE	O
”	O
.	O
These	O
results	O
show	O
that	O
the	O
explicit	O
use	O
of	O
the	O
output	O
information	O
from	O
the	O
classifiers	Method
of	O
the	O
lower	Method
layers	Method
is	O
important	O
in	O
our	O
JMT	Method
model	O
.	O
The	O
results	O
in	O
the	O
column	O
of	O
“	O
w	O
/	O
o	O
SC	O
&	O
LE	O
”	O
are	O
the	O
ones	O
without	O
both	O
of	O
the	O
shortcut	O
connections	O
and	O
the	O
label	O
embeddings	O
.	O
paragraph	O
:	O
Different	O
layers	O
for	O
different	O
tasks	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
our	O
“	O
JMT	Method
”	Method
setting	Method
and	O
that	O
of	O
not	O
using	O
the	O
shortcut	O
connections	O
and	O
the	O
label	O
embeddings	O
(	O
“	O
w	O
/	O
o	O
SC	O
&	O
LE	O
”	O
)	O
as	O
in	O
Table	O
[	O
reference	O
]	O
.	O
In	O
addition	O
,	O
in	O
the	O
column	O
of	O
“	O
All	O
-	O
3	O
”	O
,	O
we	O
show	O
the	O
results	O
of	O
using	O
the	O
highest	O
(	O
i.e.	O
,	O
the	O
third	O
)	O
layer	O
for	O
all	O
of	O
the	O
three	O
tasks	O
without	O
any	O
shortcut	O
connections	O
and	O
label	O
embeddings	O
,	O
and	O
thus	O
the	O
two	O
settings	O
“	O
w	Method
/	Method
o	Method
SC	Method
&	Method
LE	Method
”	O
and	O
“	O
All	O
-	O
3	O
”	O
require	O
exactly	O
the	O
same	O
number	O
of	O
the	O
model	O
parameters	O
.	O
The	O
“	O
All	O
-	O
3	O
”	O
setting	O
is	O
similar	O
to	O
the	O
multi	Method
-	Method
task	Method
model	Method
of	O
collobert2011senna	O
in	O
that	O
task	O
-	O
specific	O
output	O
layers	O
are	O
used	O
but	O
most	O
of	O
the	O
model	O
parameters	O
are	O
shared	O
.	O
The	O
results	O
show	O
that	O
using	O
the	O
same	O
layers	O
for	O
the	O
three	O
different	O
tasks	O
hampers	O
the	O
effectiveness	O
of	O
our	O
JMT	Method
model	O
,	O
and	O
the	O
design	O
of	O
the	O
model	O
is	O
much	O
more	O
important	O
than	O
the	O
number	O
of	O
the	O
model	O
parameters	O
.	O
paragraph	O
:	O
Successive	Method
regularization	Method
In	O
Table	O
[	O
reference	O
]	O
,	O
the	O
column	O
of	O
“	O
w	O
/	O
o	O
SR	O
”	O
shows	O
the	O
results	O
of	O
omitting	O
the	O
successive	O
regularization	O
terms	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
We	O
can	O
see	O
that	O
the	O
accuracy	Metric
of	O
chunking	Task
is	O
improved	O
by	O
the	O
successive	Method
regularization	Method
,	O
while	O
other	O
results	O
are	O
not	O
affected	O
so	O
much	O
.	O
The	O
chunking	Task
dataset	O
used	O
here	O
is	O
relatively	O
small	O
compared	O
with	O
other	O
low	Task
-	Task
level	Task
tasks	Task
,	O
POS	Method
tagging	O
and	O
dependency	Task
parsing	Task
.	O
Thus	O
,	O
these	O
results	O
suggest	O
that	O
the	O
successive	Method
regularization	Method
is	O
effective	O
when	O
dataset	O
sizes	O
are	O
imbalanced	O
.	O
paragraph	O
:	O
Vertical	O
connections	O
We	O
investigated	O
our	O
JMT	Method
results	O
without	O
using	O
the	O
vertical	O
connections	O
in	O
the	O
five	Method
-	Method
layer	Method
bi	Method
-	Method
LSTMs	Method
.	O
More	O
concretely	O
,	O
when	O
constructing	O
the	O
input	O
vectors	O
,	O
we	O
do	O
not	O
use	O
the	O
bi	O
-	O
LSTM	Method
hidden	O
states	O
of	O
the	O
previous	O
layers	O
.	O
Table	O
[	O
reference	O
]	O
also	O
shows	O
the	O
JMT	Method
results	O
with	O
and	O
without	O
the	O
vertical	O
connections	O
.	O
As	O
shown	O
in	O
the	O
column	O
of	O
“	O
w	O
/	O
o	O
VC	O
”	O
,	O
we	O
observed	O
the	O
competitive	O
results	O
.	O
Therefore	O
,	O
in	O
the	O
target	O
tasks	O
used	O
in	O
our	O
model	O
,	O
sharing	O
the	O
word	O
representations	O
and	O
the	O
output	O
label	O
embeddings	O
is	O
more	O
effective	O
than	O
just	O
stacking	O
the	O
bi	O
-	O
LSTM	Method
layers	O
.	O
paragraph	O
:	O
Order	O
of	O
training	O
Our	O
JMT	Method
model	O
iterates	O
the	O
training	O
process	O
in	O
the	O
order	O
described	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Our	O
hypothesis	O
is	O
that	O
it	O
is	O
important	O
to	O
start	O
from	O
the	O
lower	O
-	O
level	O
tasks	O
and	O
gradually	O
move	O
to	O
the	O
higher	O
-	O
level	O
tasks	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
training	O
our	O
model	O
by	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
tasks	O
for	O
each	O
epoch	O
in	O
the	O
column	O
of	O
“	O
Random	O
”	O
.	O
We	O
see	O
that	O
the	O
scores	O
of	O
the	O
semantic	Task
tasks	Task
drop	O
by	O
the	O
random	Method
strategy	Method
.	O
In	O
our	O
preliminary	O
experiments	O
,	O
we	O
have	O
found	O
that	O
constructing	O
the	O
mini	O
-	O
batch	O
samples	O
from	O
different	O
tasks	O
also	O
hampers	O
the	O
effectiveness	O
of	O
our	O
model	O
,	O
which	O
also	O
supports	O
our	O
hypothesis	O
.	O
paragraph	O
:	O
Depth	O
The	O
single	O
task	O
settings	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
are	O
obtained	O
by	O
using	O
single	Method
layer	Method
bi	Method
-	Method
LSTMs	Method
,	O
but	O
in	O
our	O
JMT	Method
model	O
,	O
the	O
higher	O
-	O
level	O
tasks	O
use	O
successively	O
deeper	O
layers	O
.	O
To	O
investigate	O
the	O
gap	O
between	O
the	O
different	O
number	O
of	O
the	O
layers	O
for	O
each	O
task	O
,	O
we	O
also	O
show	O
the	O
results	O
of	O
using	O
multi	Method
-	Method
layer	Method
bi	Method
-	Method
LSTMs	Method
for	O
the	O
single	O
task	O
settings	O
,	O
in	O
the	O
column	O
of	O
“	O
Single	O
+	O
”	O
in	O
Table	O
[	O
reference	O
]	O
.	O
More	O
concretely	O
,	O
we	O
use	O
the	O
same	O
number	O
of	O
the	O
layers	O
with	O
our	O
JMT	Method
model	O
;	O
for	O
example	O
,	O
three	O
layers	O
are	O
used	O
for	O
dependency	Task
parsing	Task
,	O
and	O
five	O
layers	O
are	O
used	O
for	O
textual	Task
entailment	Task
.	O
As	O
shown	O
in	O
these	O
results	O
,	O
deeper	O
layers	O
do	O
not	O
always	O
lead	O
to	O
better	O
results	O
,	O
and	O
the	O
joint	Method
learning	Method
is	O
more	O
important	O
than	O
making	O
the	O
models	O
complex	O
only	O
for	O
single	O
tasks	O
.	O
paragraph	O
:	O
Character	Method
-	Method
gram	Method
embeddings	Method
Finally	O
,	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
for	O
the	O
three	O
single	O
tasks	O
with	O
and	O
without	O
the	O
pre	O
-	O
trained	O
character	Method
-	Method
gram	Method
embeddings	Method
.	O
The	O
column	O
of	O
“	O
W	O
&	O
C	O
”	O
corresponds	O
to	O
using	O
both	O
of	O
the	O
word	O
and	O
character	O
-	O
gram	O
embeddings	O
,	O
and	O
that	O
of	O
“	O
Only	O
W	O
”	O
corresponds	O
to	O
using	O
only	O
the	O
word	O
embeddings	O
.	O
These	O
results	O
clearly	O
show	O
that	O
jointly	O
using	O
the	O
pre	O
-	O
trained	O
word	O
and	O
character	O
-	O
gram	O
embeddings	O
is	O
helpful	O
in	O
improving	O
the	O
results	O
.	O
The	O
pre	Method
-	Method
training	Method
of	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
is	O
also	O
effective	O
;	O
for	O
example	O
,	O
without	O
the	O
pre	Method
-	Method
training	Method
,	O
the	O
POS	Method
accuracy	O
drops	O
from	O
97.52	O
%	O
to	O
97.38	O
%	O
and	O
the	O
chunking	Task
accuracy	O
drops	O
from	O
95.65	O
%	O
to	O
95.14	O
%	O
.	O
subsection	O
:	O
Discussion	O
paragraph	O
:	O
Training	O
strategies	O
In	O
our	O
JMT	Method
model	O
,	O
it	O
is	O
not	O
obvious	O
when	O
to	O
stop	O
the	O
training	O
while	O
trying	O
to	O
maximize	O
the	O
scores	O
of	O
all	O
the	O
five	O
tasks	O
.	O
We	O
focused	O
on	O
maximizing	O
the	O
accuracy	Metric
of	O
dependency	Task
parsing	Task
on	O
the	O
development	O
data	O
in	O
our	O
experiments	O
.	O
However	O
,	O
the	O
sizes	O
of	O
the	O
training	O
data	O
are	O
different	O
across	O
the	O
different	O
tasks	O
;	O
for	O
example	O
,	O
the	O
semantic	Task
tasks	Task
include	O
only	O
4	O
,	O
500	O
sentence	O
pairs	O
,	O
and	O
the	O
dependency	Task
parsing	Task
dataset	O
includes	O
39	O
,	O
832	O
sentences	O
with	O
word	O
-	O
level	O
annotations	O
.	O
Thus	O
,	O
in	O
general	O
,	O
dependency	Task
parsing	Task
requires	O
more	O
training	O
epochs	O
than	O
the	O
semantic	Task
tasks	Task
,	O
but	O
currently	O
,	O
our	O
model	O
trains	O
all	O
of	O
the	O
tasks	O
for	O
the	O
same	O
training	O
epochs	O
.	O
The	O
same	O
strategy	O
for	O
decreasing	O
the	O
learning	Metric
rate	Metric
is	O
also	O
shared	O
across	O
all	O
the	O
different	O
tasks	O
,	O
although	O
our	O
growing	Method
gradient	Method
clipping	Method
method	Method
described	O
in	O
Section	O
[	O
reference	O
]	O
helps	O
improve	O
the	O
results	O
.	O
Indeed	O
,	O
we	O
observed	O
that	O
better	O
scores	O
of	O
the	O
semantic	Task
tasks	Task
can	O
be	O
achieved	O
before	O
the	O
accuracy	Metric
of	O
dependency	Task
parsing	Task
reaches	O
the	O
best	O
score	O
.	O
Developing	O
a	O
method	O
for	O
achieving	O
the	O
best	O
scores	O
for	O
all	O
of	O
the	O
tasks	O
at	O
the	O
same	O
time	O
is	O
important	O
future	O
work	O
.	O
paragraph	O
:	O
More	O
tasks	O
Our	O
JMT	Method
model	O
has	O
the	O
potential	O
of	O
handling	O
more	O
tasks	O
than	O
the	O
five	O
tasks	O
used	O
in	O
our	O
experiments	O
;	O
examples	O
include	O
entity	Task
detection	Task
and	O
relation	Task
extraction	Task
as	O
in	O
miwa2016rel	O
as	O
well	O
as	O
language	Method
modeling	Method
godwin2016multi	O
.	O
It	O
is	O
also	O
a	O
promising	O
direction	O
to	O
train	O
each	O
task	O
for	O
multiple	O
domains	O
by	O
focusing	O
on	O
domain	Task
adaptation	Task
sogaard2016	O
.	O
In	O
particular	O
,	O
incorporating	O
language	Method
modeling	Method
tasks	O
provides	O
an	O
opportunity	O
to	O
use	O
large	O
text	O
data	O
.	O
Such	O
large	O
text	O
data	O
was	O
used	O
in	O
our	O
experiments	O
to	O
pre	O
-	O
train	O
the	O
word	Method
and	Method
character	Method
-	Method
gram	Method
embeddings	Method
.	O
However	O
,	O
it	O
would	O
be	O
preferable	O
to	O
efficiently	O
use	O
it	O
for	O
improving	O
the	O
entire	O
model	O
.	O
paragraph	O
:	O
Task	Task
-	Task
oriented	Task
learning	Task
of	Task
low	Task
-	Task
level	Task
tasks	Task
Each	O
task	O
in	O
our	O
JMT	Method
model	O
is	O
supervised	O
by	O
its	O
corresponding	O
dataset	O
.	O
However	O
,	O
it	O
would	O
be	O
possible	O
to	O
learn	O
low	O
-	O
level	O
tasks	O
by	O
optimizing	O
high	O
-	O
level	O
tasks	O
,	O
because	O
the	O
model	O
parameters	O
of	O
the	O
low	O
-	O
level	O
tasks	O
can	O
be	O
directly	O
modified	O
by	O
learning	O
the	O
high	O
-	O
level	O
tasks	O
.	O
One	O
example	O
has	O
already	O
been	O
presented	O
in	O
hashimoto2017lgp	O
,	O
where	O
our	O
JMT	Method
model	O
is	O
extended	O
to	O
learning	O
task	Task
-	Task
oriented	Task
latent	Task
graph	Task
structures	Task
of	O
sentences	O
by	O
training	O
our	O
dependency	Task
parsing	Task
component	O
according	O
to	O
a	O
neural	O
machine	Task
translation	Task
objective	O
.	O
section	O
:	O
Conclusion	O
We	O
presented	O
a	O
joint	Method
many	Method
-	Method
task	Method
model	Method
to	O
handle	O
multiple	O
NLP	Task
tasks	Task
with	O
growing	O
depth	O
in	O
a	O
single	O
end	O
-	O
to	O
-	O
end	Method
model	Method
.	O
Our	O
model	O
is	O
successively	O
trained	O
by	O
considering	O
linguistic	O
hierarchies	O
,	O
directly	O
feeding	O
word	Method
representations	Method
into	O
all	O
layers	O
,	O
explicitly	O
using	O
low	O
-	O
level	O
predictions	O
,	O
and	O
applying	O
successive	Method
regularization	Method
.	O
In	O
experiments	O
on	O
five	O
NLP	Task
tasks	Task
,	O
our	O
single	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
or	O
competitive	O
results	O
on	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
semantic	O
relatedness	Task
,	O
and	O
textual	Task
entailment	Task
.	O
subsubsection	O
:	O
Acknowledgments	O
We	O
thank	O
the	O
anonymous	O
reviewers	O
and	O
the	O
Salesforce	O
Research	O
team	O
members	O
for	O
their	O
fruitful	O
comments	O
and	O
discussions	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Supplemental	O
Material	O
appendix	O
:	O
Training	O
Details	O
paragraph	O
:	O
Pre	O
-	O
training	O
embeddings	O
We	O
used	O
the	O
word2vec	Method
toolkit	Method
to	O
pre	O
-	O
train	O
the	O
word	Method
embeddings	Method
.	O
We	O
created	O
our	O
training	O
corpus	O
by	O
selecting	O
lowercased	Material
English	Material
Wikipedia	Material
text	Material
and	O
obtained	O
100	O
-	O
dimensional	O
Skip	Method
-	Method
gram	Method
word	O
embeddings	O
trained	O
with	O
the	O
context	O
window	O
size	O
1	O
,	O
the	O
negative	Method
sampling	Method
method	O
(	O
15	O
negative	O
samples	O
)	O
,	O
and	O
the	O
sub	Method
-	Method
sampling	Method
method	Method
(	O
of	O
the	O
sub	Method
-	Method
sampling	Method
coefficient	Method
)	O
.	O
We	O
also	O
pre	O
-	O
trained	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
using	O
the	O
same	O
parameter	O
settings	O
with	O
the	O
case	O
-	O
sensitive	O
Wikipedia	O
text	O
.	O
We	O
trained	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
for	O
in	O
the	O
pre	Task
-	Task
training	Task
step	Task
.	O
paragraph	O
:	O
Embedding	Task
initialization	Task
We	O
used	O
the	O
pre	O
-	O
trained	O
word	O
embeddings	O
to	O
initialize	O
the	O
word	O
embeddings	O
,	O
and	O
the	O
word	O
vocabulary	O
was	O
built	O
based	O
on	O
the	O
training	O
data	O
of	O
the	O
five	O
tasks	O
.	O
All	O
words	O
in	O
the	O
training	O
data	O
were	O
included	O
in	O
the	O
word	O
vocabulary	O
,	O
and	O
we	O
employed	O
the	O
word	Method
-	Method
dropout	Method
method	Method
kiperwasser2016	O
to	O
train	O
the	O
word	Method
embedding	Method
for	O
the	O
unknown	O
words	O
.	O
We	O
also	O
built	O
the	O
character	O
-	O
gram	O
vocabulary	O
for	O
,	O
following	O
wieting2016	O
,	O
and	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
were	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
embeddings	O
.	O
All	O
of	O
the	O
label	O
embeddings	O
were	O
initialized	O
with	O
uniform	O
random	O
values	O
in	O
,	O
where	O
is	O
the	O
dimensionality	O
of	O
the	O
label	O
embeddings	O
and	O
is	O
the	O
number	O
of	O
labels	O
.	O
paragraph	O
:	O
Weight	Method
initialization	Method
The	O
dimensionality	O
of	O
the	O
hidden	O
layers	O
in	O
the	O
bi	Method
-	Method
LSTMs	Method
was	O
set	O
to	O
100	O
.	O
We	O
initialized	O
all	O
of	O
the	O
softmax	O
parameters	O
and	O
bias	O
vectors	O
,	O
except	O
for	O
the	O
forget	O
biases	O
in	O
the	O
LSTMs	Method
,	O
with	O
zeros	O
,	O
and	O
the	O
weight	O
matrix	O
and	O
the	O
root	O
node	O
vector	O
for	O
dependency	Task
parsing	Task
were	O
also	O
initialized	O
with	O
zeros	O
.	O
All	O
of	O
the	O
forget	O
biases	O
were	O
initialized	O
with	O
ones	O
.	O
The	O
other	O
weight	O
matrices	O
were	O
initialized	O
with	O
uniform	O
random	O
values	O
in	O
,	O
where	O
and	O
are	O
the	O
number	O
of	O
rows	O
and	O
columns	O
of	O
the	O
matrices	O
,	O
respectively	O
.	O
paragraph	O
:	O
Optimization	Task
At	O
each	O
epoch	O
,	O
we	O
trained	O
our	O
model	O
in	O
the	O
order	O
of	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
semantic	O
relatedness	Task
,	O
and	O
textual	Task
entailment	Task
.	O
We	O
used	O
mini	Method
-	Method
batch	Method
stochastic	Method
gradient	Method
decent	Method
to	O
train	O
our	O
model	O
.	O
The	O
mini	Metric
-	Metric
batch	Metric
size	Metric
was	O
set	O
to	O
25	O
for	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
and	O
the	O
SICK	Task
tasks	Task
,	O
and	O
15	O
for	O
dependency	Task
parsing	Task
.	O
We	O
used	O
a	O
gradient	Method
clipping	Method
strategy	Method
with	O
growing	O
clipping	O
values	O
for	O
the	O
different	O
tasks	O
;	O
concretely	O
,	O
we	O
employed	O
the	O
simple	O
function	O
:	O
,	O
where	O
is	O
the	O
number	O
of	O
bi	O
-	O
LSTM	Method
layers	O
involved	O
in	O
each	O
task	O
,	O
and	O
is	O
the	O
maximum	O
value	O
.	O
The	O
learning	Metric
rate	Metric
at	O
the	O
-	O
th	O
epoch	O
was	O
set	O
to	O
,	O
where	O
is	O
the	O
initial	O
learning	Metric
rate	Metric
,	O
and	O
is	O
the	O
hyperparameter	O
to	O
decrease	O
the	O
learning	Metric
rate	Metric
.	O
We	O
set	O
to	O
1.0	O
and	O
to	O
0.3	O
.	O
At	O
each	O
epoch	O
,	O
the	O
same	O
learning	O
rate	O
was	O
shared	O
across	O
all	O
of	O
the	O
tasks	O
.	O
paragraph	O
:	O
Regularization	O
We	O
set	O
the	O
regularization	O
coefficient	O
to	O
for	O
the	O
LSTM	Method
weight	O
matrices	O
,	O
for	O
the	O
weight	O
matrices	O
in	O
the	O
classifiers	Method
,	O
and	O
for	O
the	O
successive	O
regularization	O
term	O
excluding	O
the	O
classifier	O
parameters	O
of	O
the	O
lower	Task
-	Task
level	Task
tasks	Task
,	O
respectively	O
.	O
The	O
successive	O
regularization	O
coefficient	O
for	O
the	O
classifier	O
parameters	O
was	O
set	O
to	O
.	O
We	O
also	O
used	O
dropout	Method
dropout2014ver	O
.	O
The	O
dropout	Method
rate	O
was	O
set	O
to	O
0.2	O
for	O
the	O
vertical	O
connections	O
in	O
the	O
multi	Method
-	Method
layer	Method
bi	Method
-	Method
LSTMs	Method
pham2015dropout	O
,	O
the	O
word	Method
representations	Method
and	O
the	O
label	O
embeddings	O
of	O
the	O
entailment	Method
layer	Method
,	O
and	O
the	O
classifier	Method
of	O
the	O
POS	Method
tagging	O
,	O
chunking	Task
,	O
dependency	Task
parsing	Task
,	O
and	O
entailment	Task
.	O
A	O
different	O
dropout	Method
rate	O
of	O
0.4	O
was	O
used	O
for	O
the	O
word	Method
representations	Method
and	O
the	O
label	O
embeddings	O
of	O
the	O
POS	Method
,	O
chunking	Task
,	O
and	O
dependency	O
layers	O
,	O
and	O
the	O
classifier	Method
of	O
the	O
relatedness	Task
layer	O
.	O
appendix	O
:	O
Details	O
of	O
Character	Method
-	Method
Gram	Method
Embeddings	Method
Here	O
we	O
first	O
describe	O
the	O
pre	Method
-	Method
training	Method
process	Method
of	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
in	O
detail	O
and	O
then	O
show	O
further	O
analysis	O
on	O
the	O
results	O
in	O
Table	O
12	O
.	O
subsection	O
:	O
Pre	Task
-	Task
Training	Task
with	O
Skip	Task
-	Task
Gram	Task
Objective	Task
We	O
pre	O
-	O
train	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
using	O
the	O
objective	O
function	O
of	O
the	O
Skip	Method
-	Method
gram	Method
model	O
with	O
negative	Method
sampling	Method
mikolov2013word2vec	O
.	O
We	O
build	O
the	O
vocabulary	O
of	O
the	O
character	O
-	O
grams	O
based	O
on	O
the	O
training	O
corpus	O
,	O
the	O
case	Material
-	Material
sensitive	Material
English	Material
Wikipedia	Material
text	Material
.	O
This	O
is	O
because	O
such	O
case	O
-	O
sensitive	O
information	O
is	O
important	O
in	O
handling	O
some	O
types	O
of	O
words	O
like	O
named	O
entities	O
.	O
Assuming	O
that	O
a	O
word	O
has	O
its	O
corresponding	O
character	O
-	O
grams	O
,	O
where	O
any	O
overlaps	O
and	O
unknown	O
ones	O
are	O
removed	O
.	O
Then	O
the	O
word	O
is	O
represented	O
with	O
an	O
embedding	Method
computed	O
as	O
follows	O
:	O
where	O
is	O
the	O
parameterized	Method
embedding	Method
of	O
the	O
character	O
-	O
gram	O
,	O
and	O
the	O
computation	O
of	O
is	O
exactly	O
the	O
same	O
as	O
the	O
one	O
used	O
in	O
our	O
JMT	Method
model	O
explained	O
in	O
Section	O
2.1	O
.	O
The	O
remaining	O
part	O
of	O
the	O
pre	Task
-	Task
training	Task
process	Task
is	O
the	O
same	O
as	O
the	O
original	O
Skip	Method
-	Method
gram	Method
model	O
.	O
For	O
each	O
word	O
-	O
context	O
pair	O
in	O
the	O
training	O
corpus	O
,	O
negative	O
context	O
words	O
are	O
sampled	O
,	O
and	O
the	O
objective	Metric
function	Metric
is	O
defined	O
as	O
follows	O
:	O
where	O
is	O
the	O
logistic	Method
sigmoid	Method
function	Method
,	O
is	O
the	O
weight	O
vector	O
for	O
the	O
context	O
word	O
,	O
and	O
is	O
a	O
negative	O
sample	O
.	O
It	O
should	O
be	O
noted	O
that	O
the	O
weight	O
vectors	O
for	O
the	O
context	O
words	O
are	O
parameterized	O
for	O
the	O
words	O
without	O
any	O
character	O
information	O
.	O
subsection	O
:	O
Effectiveness	O
on	O
Unknown	O
Words	O
One	O
expectation	O
from	O
the	O
use	O
of	O
the	O
character	Method
-	Method
gram	Method
embeddings	Method
is	O
to	O
better	O
handle	O
unknown	O
words	O
.	O
We	O
verified	O
this	O
assumption	O
in	O
the	O
single	Task
task	Task
setting	Task
for	O
POS	Method
tagging	O
,	O
based	O
on	O
the	O
results	O
reported	O
in	O
Table	O
12	O
.	O
Table	O
[	O
reference	O
]	O
shows	O
that	O
the	O
joint	O
use	O
of	O
the	O
word	Method
and	Method
character	Method
-	Method
gram	Method
embeddings	Method
improves	O
the	O
score	O
by	O
about	O
19	O
%	O
in	O
terms	O
of	O
the	O
accuracy	Metric
for	O
unknown	O
words	O
.	O
We	O
also	O
show	O
the	O
results	O
of	O
the	O
single	Method
task	Method
setting	Method
for	O
dependency	Task
parsing	Task
in	O
Table	O
[	O
reference	O
]	O
.	O
Again	O
,	O
we	O
can	O
see	O
that	O
using	O
the	O
character	O
-	O
level	O
information	O
is	O
effective	O
,	O
and	O
in	O
particular	O
,	O
the	O
improvement	O
of	O
the	O
LAS	Metric
score	O
is	O
large	O
.	O
These	O
results	O
suggest	O
that	O
it	O
is	O
better	O
to	O
use	O
not	O
only	O
the	O
word	O
embeddings	O
,	O
but	O
also	O
the	O
character	O
-	O
gram	O
embeddings	O
by	O
default	O
.	O
Recently	O
,	O
the	O
joint	O
use	O
of	O
word	O
and	O
character	O
information	O
has	O
proven	O
to	O
be	O
effective	O
in	O
language	Method
modeling	Method
miyamoto2016char	O
,	O
but	O
just	O
using	O
the	O
simple	O
character	Method
-	Method
gram	Method
embeddings	Method
is	O
fast	O
and	O
also	O
effective	O
.	O
appendix	O
:	O
Analysis	O
on	O
Dependency	Task
Parsing	Task
Our	O
dependency	Method
parser	Method
is	O
based	O
on	O
the	O
idea	O
of	O
predicting	O
a	O
head	O
(	O
or	O
parent	O
)	O
for	O
each	O
word	O
,	O
and	O
thus	O
the	O
parsing	O
results	O
do	O
not	O
always	O
lead	O
to	O
correct	O
trees	O
.	O
To	O
inspect	O
this	O
aspect	O
,	O
we	O
checked	O
the	O
parsing	Task
results	O
on	O
the	O
development	O
set	O
(	O
1	O
,	O
700	O
sentences	O
)	O
,	O
using	O
the	O
“	O
JMT	Method
”	Method
setting	Method
.	O
In	O
the	O
dependency	O
annotations	O
used	O
in	O
this	O
work	O
,	O
each	O
sentence	O
has	O
only	O
one	O
root	O
node	O
,	O
and	O
we	O
have	O
found	O
11	O
sentences	O
with	O
multiple	O
root	O
nodes	O
and	O
11	O
sentences	O
with	O
no	O
root	O
nodes	O
in	O
our	O
parsing	Task
results	O
.	O
We	O
show	O
two	O
examples	O
below	O
:	O
Underneath	O
the	O
headline	O
“	O
Diversification	O
,	O
”	O
it	O
counsels	O
,	O
“	O
Based	O
on	O
the	O
events	O
of	O
the	O
past	O
week	O
,	O
all	O
investors	O
need	O
to	O
know	O
their	O
portfolios	O
are	O
balanced	O
to	O
help	O
protect	O
them	O
against	O
the	O
market	O
’s	O
volatility	O
.	O
”	O
Mr.	O
Eskandarian	O
,	O
who	O
resigned	O
his	O
Della	O
Femina	O
post	O
in	O
September	O
,	O
becomes	O
chairman	O
and	O
chief	O
executive	O
of	O
Arnold	O
.	O
In	O
the	O
example	O
(	O
a	O
)	O
,	O
the	O
two	O
boldfaced	O
words	O
“	O
counsels	O
”	O
and	O
“	O
need	O
”	O
are	O
predicted	O
as	O
child	O
nodes	O
of	O
the	O
root	O
node	O
,	O
and	O
the	O
underlined	O
word	O
“	O
counsels	O
”	O
is	O
the	O
correct	O
one	O
based	O
on	O
the	O
gold	Metric
annotations	O
.	O
This	O
example	O
sentence	O
(	O
a	O
)	O
consists	O
of	O
multiple	O
internal	O
sentences	O
,	O
and	O
our	O
parser	Method
misunderstood	O
that	O
both	O
of	O
the	O
two	O
verbs	O
are	O
the	O
heads	O
of	O
the	O
sentence	O
.	O
In	O
the	O
example	O
(	O
b	O
)	O
,	O
none	O
of	O
the	O
words	O
is	O
connected	O
to	O
the	O
root	O
node	O
,	O
and	O
the	O
correct	O
child	O
node	O
of	O
the	O
root	O
is	O
the	O
underlined	O
word	O
“	O
chairman	O
”	O
.	O
Without	O
the	O
internal	O
phrase	O
“	O
who	O
resigned	O
…	O
in	O
September	O
”	O
,	O
the	O
example	O
sentence	O
(	O
b	O
)	O
is	O
very	O
simple	O
,	O
but	O
we	O
have	O
found	O
that	O
such	O
a	O
simplified	O
sentence	O
is	O
still	O
not	O
parsed	O
correctly	O
.	O
In	O
many	O
cases	O
,	O
verbs	O
are	O
linked	O
to	O
the	O
root	O
nodes	O
,	O
but	O
sometimes	O
other	O
types	O
of	O
words	O
like	O
nouns	O
can	O
be	O
the	O
candidates	O
.	O
In	O
our	O
model	O
,	O
the	O
single	O
parameterized	O
vector	O
is	O
used	O
to	O
represent	O
the	O
root	O
node	O
for	O
each	O
sentence	O
.	O
Therefore	O
,	O
the	O
results	O
of	O
the	O
examples	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
suggest	O
that	O
it	O
would	O
be	O
needed	O
to	O
capture	O
various	O
types	O
of	O
root	O
nodes	O
,	O
and	O
using	O
sentence	Method
-	Method
dependent	Method
root	Method
representations	Method
would	O
lead	O
to	O
better	O
results	O
in	O
future	O
work	O
.	O
appendix	O
:	O
Analysis	O
on	O
Semantic	Task
Tasks	Task
We	O
inspected	O
the	O
development	O
set	O
results	O
on	O
the	O
semantic	Task
tasks	Task
using	O
the	O
“	O
JMT	Method
”	Method
setting	Method
.	O
In	O
our	O
model	O
,	O
the	O
highest	O
-	O
level	O
task	O
is	O
the	O
textual	O
entailment	Task
task	Task
.	O
We	O
show	O
an	O
example	O
premise	O
-	O
hypothesis	O
pair	O
which	O
is	O
misclassified	O
in	O
our	O
results	O
:	O
Premise	O
:	O
“	O
A	O
surfer	O
is	O
riding	O
a	O
big	O
wave	O
across	O
dark	O
green	O
water	O
”	O
,	O
and	O
Hypothesis	O
:	O
“	O
The	O
surfer	O
is	O
riding	O
a	O
small	O
wave	O
”	O
.	O
The	O
predicted	O
label	O
is	O
entailment	O
,	O
but	O
the	O
gold	Metric
label	O
is	O
contradiction	O
.	O
This	O
example	O
is	O
very	O
easy	O
by	O
focusing	O
on	O
the	O
difference	O
between	O
the	O
two	O
words	O
“	O
big	O
”	O
and	O
“	O
small	O
”	O
.	O
However	O
,	O
our	O
model	O
fails	O
to	O
correctly	O
classify	O
this	O
example	O
because	O
there	O
are	O
few	O
opportunities	O
to	O
learn	O
the	O
difference	O
.	O
Our	O
model	O
relies	O
on	O
the	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
based	O
on	O
word	Method
co	Method
-	Method
occurrence	Method
statistics	Method
mikolov2013word2vec	O
,	O
and	O
it	O
is	O
widely	O
known	O
that	O
such	O
co	Method
-	Method
occurrence	Method
-	Method
based	Method
embeddings	Method
can	O
rarely	O
discriminate	O
between	O
antonyms	O
and	O
synonyms	O
ono2015ant	O
.	O
Moreover	O
,	O
the	O
other	O
four	O
tasks	O
in	O
our	O
JMT	Method
model	O
do	O
not	O
explicitly	O
provide	O
the	O
opportunities	O
to	O
learn	O
such	O
semantic	O
aspects	O
.	O
Even	O
in	O
the	O
training	O
data	O
of	O
the	O
textual	O
entailment	Task
task	Task
,	O
we	O
can	O
find	O
only	O
one	O
example	O
to	O
learn	O
the	O
difference	O
between	O
the	O
two	O
words	O
,	O
which	O
is	O
not	O
enough	O
to	O
obtain	O
generalization	O
capacities	O
.	O
Therefore	O
,	O
it	O
is	O
worth	O
investigating	O
the	O
explicit	O
use	O
of	O
external	O
dictionaries	O
or	O
the	O
use	O
of	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
learned	O
with	O
such	O
dictionaries	O
ono2015ant	O
,	O
to	O
see	O
whether	O
our	O
JMT	Method
model	O
is	O
further	O
improved	O
not	O
only	O
for	O
the	O
semantic	Task
tasks	Task
,	O
but	O
also	O
for	O
the	O
low	Task
-	Task
level	Task
tasks	Task
.	O
appendix	O
:	O
How	O
Do	O
Shared	O
Embeddings	O
Change	O
In	O
our	O
JMT	Method
model	O
,	O
the	O
word	O
and	O
character	O
-	O
gram	O
embedding	O
matrices	O
are	O
shared	O
across	O
all	O
of	O
the	O
five	O
different	O
tasks	O
.	O
To	O
better	O
qualitatively	O
explain	O
the	O
importance	O
of	O
the	O
shortcut	O
connections	O
shown	O
in	O
Table	O
7	O
,	O
we	O
inspected	O
how	O
the	O
shared	O
embeddings	O
change	O
when	O
fed	O
into	O
the	O
different	O
bi	O
-	O
LSTM	Method
layers	O
.	O
More	O
concretely	O
,	O
we	O
checked	O
closest	O
neighbors	O
in	O
terms	O
of	O
the	O
cosine	Metric
similarity	Metric
for	O
the	O
word	O
representations	O
before	O
and	O
after	O
fed	O
into	O
the	O
forward	O
LSTM	Method
layers	O
.	O
In	O
particular	O
,	O
we	O
used	O
the	O
corresponding	O
part	O
of	O
in	O
Eq	O
.	O
(	O
1	O
)	O
to	O
perform	O
linear	Task
transformation	Task
of	Task
the	Task
input	Task
embeddings	Task
,	O
because	O
directly	O
affects	O
the	O
hidden	O
states	O
of	O
the	O
LSTMs	Method
.	O
Thus	O
,	O
this	O
is	O
a	O
context	Method
-	Method
independent	Method
analysis	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
examples	O
of	O
the	O
word	O
“	O
standing	O
”	O
.	O
The	O
row	O
of	O
“	O
Embedding	O
”	O
shows	O
the	O
cases	O
of	O
using	O
the	O
shared	O
embeddings	O
,	O
and	O
the	O
others	O
show	O
the	O
results	O
of	O
using	O
the	O
linear	Method
-	Method
transformed	Method
embeddings	Method
.	O
In	O
the	O
column	O
of	O
“	O
Only	O
word	O
”	O
,	O
the	O
results	O
of	O
using	O
only	O
the	O
word	O
embeddings	O
are	O
shown	O
.	O
The	O
closest	O
neighbors	O
in	O
the	O
case	O
of	O
“	O
Embedding	O
”	O
capture	O
the	O
semantic	O
similarity	O
,	O
but	O
after	O
fed	O
into	O
the	O
POS	Method
layer	O
,	O
the	O
semantic	Metric
similarity	Metric
is	O
almost	O
washed	O
out	O
.	O
This	O
is	O
not	O
surprising	O
because	O
it	O
is	O
sufficient	O
to	O
cluster	O
the	O
words	O
of	O
the	O
same	O
POS	Method
tags	O
:	O
here	O
,	O
NN	O
,	O
VBG	O
,	O
etc	O
.	O
In	O
the	O
chunking	Task
layer	Task
,	O
the	O
similarity	O
in	O
terms	O
of	O
verbs	O
is	O
captured	O
,	O
and	O
this	O
is	O
because	O
it	O
is	O
sufficient	O
to	O
identify	O
the	O
coarse	O
chunking	Task
tags	O
:	O
here	O
,	O
VP	O
.	O
In	O
the	O
dependency	O
layer	O
,	O
the	O
closest	O
neighbors	O
are	O
adverbs	O
,	O
gerunds	O
of	O
verbs	O
,	O
and	O
nouns	O
,	O
and	O
all	O
of	O
them	O
can	O
be	O
child	O
nodes	O
of	O
verbs	O
in	O
dependency	O
trees	O
.	O
However	O
,	O
this	O
information	O
is	O
not	O
sufficient	O
in	O
further	O
classifying	O
the	O
dependency	O
labels	O
.	O
Then	O
we	O
can	O
see	O
that	O
in	O
the	O
column	O
of	O
“	O
Word	O
and	O
char	O
”	O
,	O
jointly	O
using	O
the	O
character	O
-	O
gram	O
embeddings	O
adds	O
the	O
morphological	O
information	O
,	O
and	O
as	O
shown	O
in	O
Table	O
12	O
,	O
the	O
LAS	Metric
score	O
is	O
substantially	O
improved	O
.	O
In	O
the	O
case	O
of	O
semantic	Task
tasks	Task
,	O
the	O
projected	Method
embeddings	Method
capture	O
not	O
only	O
syntactic	O
,	O
but	O
also	O
semantic	O
similarities	O
.	O
These	O
results	O
show	O
that	O
different	O
tasks	O
need	O
different	O
aspects	O
of	O
the	O
word	O
similarities	O
,	O
and	O
our	O
JMT	Method
model	O
efficiently	O
transforms	O
the	O
shared	O
embeddings	O
for	O
the	O
different	O
tasks	O
by	O
the	O
simple	O
linear	Method
transformation	Method
.	O
Therefore	O
,	O
without	O
the	O
shortcut	O
connections	O
,	O
the	O
information	O
about	O
the	O
word	Method
representations	Method
are	O
fed	O
into	O
the	O
semantic	Task
tasks	Task
after	O
transformed	O
in	O
the	O
lower	O
layers	O
where	O
the	O
semantic	O
similarities	O
are	O
not	O
always	O
important	O
.	O
Indeed	O
,	O
the	O
results	O
of	O
the	O
semantic	Task
tasks	Task
are	O
very	O
poor	O
without	O
the	O
shortcut	O
connections	O
.	O
