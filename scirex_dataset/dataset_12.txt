InfoGAN Method
: O
Interpretable Method
Representation Method
Learning Method
by O
Information Method
Maximizing Method
Generative Method
Adversarial Method
Nets Method
section O
: O
Abstract O
This O
paper O
describes O
InfoGAN Method
, O
an O
information Method
- Method
theoretic Method
extension Method
to O
the O
Generative Method
Adversarial Method
Network Method
that O
is O
able O
to O
learn O
disentangled Method
representations Method
in O
a O
completely O
unsupervised Method
manner Method
. O
InfoGAN Method
is O
a O
generative Method
adversarial Method
network Method
that O
also O
maximizes O
the O
mutual O
information O
between O
a O
small O
subset O
of O
the O
latent O
variables O
and O
the O
observation O
. O
We O
derive O
a O
lower Metric
bound Metric
of O
the O
mutual Metric
information Metric
objective Metric
that O
can O
be O
optimized O
efficiently O
. O
Specifically O
, O
InfoGAN Method
successfully O
disentangles O
writing O
styles O
from O
digit O
shapes O
on O
the O
MNIST Material
dataset Material
, O
pose O
from O
lighting O
of O
3D O
rendered O
images O
, O
and O
background O
digits O
from O
the O
central O
digit O
on O
the O
SVHN O
dataset O
. O
It O
also O
discovers O
visual O
concepts O
that O
include O
hair O
styles O
, O
presence O
/ O
absence O
of O
eyeglasses O
, O
and O
emotions O
on O
the O
CelebA O
face O
dataset O
. O
Experiments O
show O
that O
InfoGAN Method
learns O
interpretable Method
representations Method
that O
are O
competitive O
with O
representations O
learned O
by O
existing O
supervised Method
methods Method
. O
section O
: O
Introduction O
Unsupervised Task
learning Task
can O
be O
described O
as O
the O
general O
problem O
of O
extracting Task
value Task
from O
unlabelled O
data O
which O
exists O
in O
vast O
quantities O
. O
A O
popular O
framework O
for O
unsupervised Task
learning Task
is O
that O
of O
representation Method
learning Method
[ O
reference O
][ O
reference O
] O
, O
whose O
goal O
is O
to O
use O
unlabelled O
data O
to O
learn O
a O
representation O
that O
exposes O
important O
semantic O
features O
as O
easily O
decodable O
factors O
. O
A O
method O
that O
can O
learn O
such O
representations O
is O
likely O
to O
exist O
[ O
reference O
] O
, O
and O
to O
be O
useful O
for O
many O
downstream Task
tasks Task
which O
include O
classification Task
, O
regression Task
, O
visualization Task
, O
and O
policy Task
learning Task
in O
reinforcement Task
learning Task
. O
While O
unsupervised Task
learning Task
is O
ill O
- O
posed O
because O
the O
relevant O
downstream Task
tasks Task
are O
unknown O
at O
training O
time O
, O
a O
disentangled Method
representation Method
, O
one O
which O
explicitly O
represents O
the O
salient O
attributes O
of O
a O
data O
instance O
, O
should O
be O
helpful O
for O
the O
relevant O
but O
unknown O
tasks O
. O
For O
example O
, O
for O
a O
dataset O
of O
faces O
, O
a O
useful O
disentangled Method
representation Method
may O
allocate O
a O
separate O
set O
of O
dimensions O
for O
each O
of O
the O
following O
attributes O
: O
facial O
expression O
, O
eye O
color O
, O
hairstyle O
, O
presence O
or O
absence O
of O
eyeglasses O
, O
and O
the O
identity O
of O
the O
corresponding O
person O
. O
A O
disentangled Method
representation Method
can O
be O
useful O
for O
natural Task
tasks Task
that O
require O
knowledge O
of O
the O
salient O
attributes O
of O
the O
data O
, O
which O
include O
tasks O
like O
face Task
recognition Task
and O
object Task
recognition Task
. O
It O
is O
not O
the O
case O
for O
unnatural Task
supervised Task
tasks Task
, O
where O
the O
goal O
could O
be O
, O
for O
example O
, O
to O
determine O
whether O
the O
number O
of O
red O
pixels O
in O
an O
image O
is O
even O
or O
odd O
. O
Thus O
, O
to O
be O
useful O
, O
an O
unsupervised Task
learning Task
algorithm O
must O
in O
effect O
correctly O
guess O
the O
likely O
set O
of O
downstream O
classification Task
tasks O
without O
being O
directly O
exposed O
to O
them O
. O
A O
significant O
fraction O
of O
unsupervised Task
learning Task
research Task
is O
driven O
by O
generative Method
modelling Method
. O
It O
is O
motivated O
by O
the O
belief O
that O
the O
ability O
to O
synthesize O
, O
or O
" O
create O
" O
the O
observed O
data O
entails O
some O
form O
of O
understanding O
, O
and O
it O
is O
hoped O
that O
a O
good O
generative Method
model Method
will O
automatically O
learn O
a O
disentangled Method
representation Method
, O
even O
though O
it O
is O
easy O
to O
construct O
perfect O
generative Method
models Method
with O
arbitrarily O
bad O
representations O
. O
The O
most O
prominent O
generative Method
models Method
are O
the O
variational Method
autoencoder Method
( Method
VAE Method
) O
[ O
reference O
] O
and O
the O
generative Method
adversarial Method
network Method
( Method
GAN Method
) O
[ O
reference O
] O
. O
In O
this O
paper O
, O
we O
present O
a O
simple O
modification O
to O
the O
generative Method
adversarial Method
network Method
objective Method
that O
encourages O
it O
to O
learn O
interpretable Task
and Task
meaningful Task
representations Task
. O
We O
do O
so O
by O
maximizing O
the O
mutual O
information O
between O
a O
fixed O
small O
subset O
of O
the O
GAN O
's O
noise O
variables O
and O
the O
observations O
, O
which O
turns O
out O
to O
be O
relatively O
straightforward O
. O
Despite O
its O
simplicity O
, O
we O
found O
our O
method O
to O
be O
surprisingly O
effective O
: O
it O
was O
able O
to O
discover O
highly O
semantic O
and O
meaningful O
hidden Method
representations Method
on O
a O
number O
of O
image O
datasets O
: O
digits O
( O
MNIST Material
) O
, O
faces O
( O
CelebA O
) O
, O
and O
house O
numbers O
( O
SVHN O
) O
. O
The O
quality O
of O
our O
unsupervised Method
disentangled Method
representation Method
matches O
previous O
works O
that O
made O
use O
of O
supervised O
label O
information O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
These O
results O
suggest O
that O
generative Method
modelling Method
augmented O
with O
a O
mutual O
information O
cost O
could O
be O
a O
fruitful O
approach O
for O
learning Task
disentangled Task
representations Task
. O
In O
the O
remainder O
of O
the O
paper O
, O
we O
begin O
with O
a O
review O
of O
the O
related O
work O
, O
noting O
the O
supervision O
that O
is O
required O
by O
previous O
methods O
that O
learn O
disentangled Method
representations Method
. O
Then O
we O
review O
GANs Method
, O
which O
is O
the O
basis O
of O
InfoGAN Method
. O
We O
describe O
how O
maximizing O
mutual O
information O
results O
in O
interpretable Method
representations Method
and O
derive O
a O
simple O
and O
efficient O
algorithm O
for O
doing O
so O
. O
Finally O
, O
in O
the O
experiments O
section O
, O
we O
first O
compare O
InfoGAN Method
with O
prior O
approaches O
on O
relatively O
clean O
datasets O
and O
then O
show O
that O
InfoGAN Method
can O
learn O
interpretable Method
representations Method
on O
complex O
datasets O
where O
no O
previous O
unsupervised Method
approach Method
is O
known O
to O
learn O
representations O
of O
comparable Metric
quality Metric
. O
section O
: O
Related O
Work O
There O
exists O
a O
large O
body O
of O
work O
on O
unsupervised Task
representation Task
learning Task
. O
Early O
methods O
were O
based O
on O
stacked Method
( Method
often Method
denoising Method
) Method
autoencoders Method
or O
restricted Method
Boltzmann Method
machines Method
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
A O
lot O
of O
promising O
recent O
work O
originates O
from O
the O
Skip Method
- Method
gram Method
model Method
[ O
reference O
] O
, O
which O
inspired O
the O
skip O
- O
thought O
vectors O
[ O
reference O
] O
and O
several O
techniques O
for O
unsupervised Task
feature Task
learning Task
of Task
images Task
[ O
reference O
] O
. O
Another O
intriguing O
line O
of O
work O
consists O
of O
the O
ladder Method
network Method
[ O
reference O
] O
, O
which O
has O
achieved O
spectacular O
results O
on O
a O
semi Method
- Method
supervised Method
variant Method
of O
the O
MNIST Material
dataset Material
. O
More O
recently O
, O
a O
model O
based O
on O
the O
VAE Method
has O
achieved O
even O
better O
semi Metric
- Metric
supervised Metric
results Metric
on O
MNIST Material
[ O
reference O
] O
. O
GANs Method
[ O
reference O
] O
have O
been O
used O
by O
Radford O
et O
al O
. O
[ O
reference O
] O
to O
learn O
an O
image Method
representation Method
that O
supports O
basic O
linear Method
algebra Method
on O
code O
space O
. O
Lake O
et O
al O
. O
[ O
reference O
] O
have O
been O
able O
to O
learn O
representations O
using O
probabilistic Method
inference Method
over O
Bayesian Method
programs Method
, O
which O
achieved O
convincing O
one O
- O
shot Task
learning Task
results O
on O
the O
OMNI Material
dataset Material
. O
In O
addition O
, O
prior O
research O
attempted O
to O
learn O
disentangled Method
representations Method
using O
supervised O
data O
. O
One O
class O
of O
such O
methods O
trains O
a O
subset O
of O
the O
representation O
to O
match O
the O
supplied O
label O
using O
supervised Method
learning Method
: O
bilinear Method
models Method
[ O
reference O
] O
separate O
style O
and O
content O
; O
multi Method
- Method
view Method
perceptron Method
[ O
reference O
] O
separate O
face O
identity O
and O
view O
point O
; O
and O
Yang O
et O
al O
. O
[ O
reference O
] O
developed O
a O
recurrent Method
variant Method
that O
generates O
a O
sequence O
of O
latent Method
factor Method
transformations Method
. O
Similarly O
, O
VAEs Method
[ O
reference O
] O
and O
Adversarial Method
Autoencoders Method
[ O
reference O
] O
were O
shown O
to O
learn O
representations O
in O
which O
class O
label O
is O
separated O
from O
other O
variations O
. O
Recently O
several O
weakly Method
supervised Method
methods Method
were O
developed O
to O
remove O
the O
need O
of O
explicitly O
labeling O
variations O
. O
disBM Method
[ O
reference O
] O
is O
a O
higher Method
- Method
order Method
Boltzmann Method
machine Method
which O
learns O
a O
disentangled Method
representation Method
by O
" O
clamping O
" O
a O
part O
of O
the O
hidden O
units O
for O
a O
pair O
of O
data O
points O
that O
are O
known O
to O
match O
in O
all O
but O
one O
factors O
of O
variation O
. O
DC Method
- Method
IGN Method
[ O
reference O
] O
extends O
this O
" O
clamping Method
" Method
idea Method
to O
VAE Method
and O
successfully O
learns O
graphics Method
codes Method
that O
can O
represent O
pose O
and O
light O
in O
3D O
rendered O
images O
. O
This O
line O
of O
work O
yields O
impressive O
results O
, O
but O
they O
rely O
on O
a O
supervised Method
grouping Method
of O
the O
data O
that O
is O
generally O
not O
available O
. O
Whitney O
et O
al O
. O
[ O
reference O
] O
proposed O
to O
alleviate O
the O
grouping O
requirement O
by O
learning O
from O
consecutive O
frames O
of O
images O
and O
use O
temporal O
continuity O
as O
supervisory O
signal O
. O
Unlike O
the O
cited O
prior O
works O
that O
strive O
to O
recover O
disentangled O
representations O
, O
InfoGAN Method
requires O
no O
supervision O
of O
any O
kind O
. O
To O
the O
best O
of O
our O
knowledge O
, O
the O
only O
other O
unsupervised Method
method Method
that O
learns O
disentangled Method
representations Method
is O
hossRBM Method
[ O
reference O
] O
, O
a O
higher O
- O
order O
extension O
of O
the O
spike Method
- Method
and Method
- Method
slab Method
restricted Method
Boltzmann Method
machine Method
that O
can O
disentangle O
emotion O
from O
identity O
on O
the O
Toronto O
Face O
Dataset O
[ O
reference O
] O
. O
However O
, O
hossRBM Method
can O
only O
disentangle O
discrete O
latent O
factors O
, O
and O
its O
computation Metric
cost Metric
grows O
exponentially O
in O
the O
number O
of O
factors O
. O
InfoGAN Method
can O
disentangle O
both O
discrete O
and O
continuous O
latent O
factors O
, O
scale O
to O
complicated O
datasets O
, O
and O
typically O
requires O
no O
more O
training O
time O
than O
regular Method
GAN Method
. O
section O
: O
Background O
: O
Generative Method
Adversarial Method
Networks Method
Goodfellow O
et O
al O
. O
[ O
reference O
] O
introduced O
the O
Generative Method
Adversarial Method
Networks Method
( Method
GAN Method
) Method
, O
a O
framework O
for O
training O
deep Method
generative Method
models Method
using O
a O
minimax Method
game Method
. O
The O
goal O
is O
to O
learn O
a O
generator Method
distribution Method
P Method
G Method
( O
x O
) O
that O
matches O
the O
real O
data O
distribution O
P O
data O
( O
x O
) O
. O
Instead O
of O
trying O
to O
explicitly O
assign O
probability O
to O
every O
x O
in O
the O
data O
distribution O
, O
GAN Method
learns O
a O
generator Method
network Method
G Method
that O
generates O
samples O
from O
the O
generator Method
distribution Method
P Method
G Method
by O
transforming O
a O
noise O
variable O
z O
∼ O
P O
noise O
( O
z O
) O
into O
a O
sample O
G O
( O
z O
) O
. O
This O
generator O
is O
trained O
by O
playing O
against O
an O
adversarial Method
discriminator Method
network Method
D Method
that O
aims O
to O
distinguish O
between O
samples O
from O
the O
true O
data O
distribution O
P O
data O
and O
the O
generator O
's O
distribution O
P O
G O
. O
So O
for O
a O
given O
generator O
, O
the O
optimal O
discriminator Method
is O
D O
( O
x O
) O
= O
P O
data O
( O
x O
)/( O
P O
data O
( O
x O
) O
+ O
P O
G O
( O
x O
) O
) O
. O
More O
formally O
, O
the O
minimax Task
game Task
is O
given O
by O
the O
following O
expression O
: O
section O
: O
Mutual O
Information O
for O
Inducing Task
Latent Task
Codes Task
The O
GAN Method
formulation Method
uses O
a O
simple O
factored O
continuous O
input O
noise O
vector O
z O
, O
while O
imposing O
no O
restrictions O
on O
the O
manner O
in O
which O
the O
generator O
may O
use O
this O
noise O
. O
As O
a O
result O
, O
it O
is O
possible O
that O
the O
noise O
will O
be O
used O
by O
the O
generator O
in O
a O
highly O
entangled O
way O
, O
causing O
the O
individual O
dimensions O
of O
z O
to O
not O
correspond O
to O
semantic O
features O
of O
the O
data O
. O
However O
, O
many O
domains O
naturally O
decompose O
into O
a O
set O
of O
semantically O
meaningful O
factors O
of O
variation O
. O
For O
instance O
, O
when O
generating O
images O
from O
the O
MNIST Material
dataset Material
, O
it O
would O
be O
ideal O
if O
the O
model O
automatically O
chose O
to O
allocate O
a O
discrete O
random O
variable O
to O
represent O
the O
numerical O
identity O
of O
the O
digit O
( O
0 O
- O
9 O
) O
, O
and O
chose O
to O
have O
two O
additional O
continuous O
variables O
that O
represent O
the O
digit O
's O
angle O
and O
thickness O
of O
the O
digit O
's O
stroke O
. O
It O
is O
the O
case O
that O
these O
attributes O
are O
both O
independent O
and O
salient O
, O
and O
it O
would O
be O
useful O
if O
we O
could O
recover O
these O
concepts O
without O
any O
supervision O
, O
by O
simply O
specifying O
that O
an O
MNIST Material
digit O
is O
generated O
by O
an O
independent O
1 O
- O
of O
- O
10 O
variable O
and O
two O
independent O
continuous O
variables O
. O
In O
this O
paper O
, O
rather O
than O
using O
a O
single O
unstructured O
noise O
vector O
, O
we O
propose O
to O
decompose O
the O
input O
noise O
vector O
into O
two O
parts O
: O
( O
i O
) O
z O
, O
which O
is O
treated O
as O
source O
of O
incompressible O
noise O
; O
( O
ii O
) O
c O
, O
which O
we O
will O
call O
the O
latent O
code O
and O
will O
target O
the O
salient O
structured O
semantic O
features O
of O
the O
data O
distribution O
. O
Mathematically O
, O
we O
denote O
the O
set O
of O
structured O
latent O
variables O
by O
c O
1 O
, O
c O
2 O
, O
. O
. O
. O
, O
c O
L O
. O
In O
its O
simplest O
form O
, O
we O
may O
assume O
a O
factored Method
distribution Method
, O
given O
by O
For O
ease O
of O
notation O
, O
we O
will O
use O
latent O
codes O
c O
to O
denote O
the O
concatenation O
of O
all O
latent O
variables O
c O
i O
. O
We O
now O
propose O
a O
method O
for O
discovering O
these O
latent O
factors O
in O
an O
unsupervised O
way O
: O
we O
provide O
the O
generator Method
network Method
with O
both O
the O
incompressible O
noise O
z O
and O
the O
latent O
code O
c O
, O
so O
the O
form O
of O
the O
generator O
becomes O
G O
( O
z O
, O
c O
) O
. O
However O
, O
in O
standard O
GAN Method
, O
the O
generator Method
is O
free O
to O
ignore O
the O
additional O
latent O
code O
c O
by O
finding O
a O
solution O
satisfying O
P O
G O
( O
x|c O
) O
= O
P O
G O
( O
x O
) O
. O
To O
cope O
with O
the O
problem O
of O
trivial O
codes O
, O
we O
propose O
an O
information Method
- Method
theoretic Method
regularization Method
: O
there O
should O
be O
high O
mutual O
information O
between O
latent O
codes O
c O
and O
generator Method
distribution Method
G Method
( Method
z Method
, O
c O
) O
. O
Thus O
I O
( O
c O
; O
G O
( O
z O
, O
c O
) O
) O
should O
be O
high O
. O
In O
information Method
theory Method
, O
mutual O
information O
between O
X O
and O
Y O
, O
I O
( O
X O
; O
Y O
) O
, O
measures O
the O
" O
amount O
of O
information O
" O
learned O
from O
knowledge O
of O
random O
variable O
Y O
about O
the O
other O
random O
variable O
X. O
The O
mutual Metric
information Metric
can O
be O
expressed O
as O
the O
difference O
of O
two O
entropy O
terms O
: O
This O
definition O
has O
an O
intuitive O
interpretation O
: O
I O
( O
X O
; O
Y O
) O
is O
the O
reduction O
of O
uncertainty O
in O
X O
when O
Y O
is O
observed O
. O
If O
X O
and O
Y O
are O
independent O
, O
then O
I O
( O
X O
; O
Y O
) O
= O
0 O
, O
because O
knowing O
one O
variable O
reveals O
nothing O
about O
the O
other O
; O
by O
contrast O
, O
if O
X O
and O
Y O
are O
related O
by O
a O
deterministic O
, O
invertible O
function O
, O
then O
maximal O
mutual O
information O
is O
attained O
. O
This O
interpretation O
makes O
it O
easy O
to O
formulate O
a O
cost O
: O
given O
any O
x O
∼ O
P O
G O
( O
x O
) O
, O
we O
want O
P O
G O
( O
c|x O
) O
to O
have O
a O
small O
entropy O
. O
In O
other O
words O
, O
the O
information O
in O
the O
latent O
code O
c O
should O
not O
be O
lost O
in O
the O
generation Task
process Task
. O
Similar O
mutual Metric
information Metric
inspired Metric
objectives Metric
have O
been O
considered O
before O
in O
the O
context O
of O
clustering Task
[ O
reference O
][ O
reference O
][ O
reference O
] O
. O
Therefore O
, O
we O
propose O
to O
solve O
the O
following O
information Method
- Method
regularized Method
minimax Method
game Method
: O
section O
: O
Variational Task
Mutual Task
Information Task
Maximization Task
In O
practice O
, O
the O
mutual O
information O
term O
I O
( O
c O
; O
G O
( O
z O
, O
c O
) O
) O
is O
hard O
to O
maximize O
directly O
as O
it O
requires O
access O
to O
the O
posterior O
P O
( O
c|x O
) O
. O
Fortunately O
we O
can O
obtain O
a O
lower O
bound O
of O
it O
by O
defining O
an O
auxiliary Method
distribution Method
Q Method
( Method
c|x Method
) O
to O
approximate O
P O
( O
c|x O
) O
: O
This O
technique O
of O
lower Task
bounding Task
mutual Task
information Task
is O
known O
as O
Variational Task
Information Task
Maximization Task
[ O
reference O
] O
. O
We O
note O
in O
addition O
that O
the O
entropy O
of O
latent Method
codes Method
H Method
( Method
c Method
) Method
can O
be O
optimized O
over O
as O
well O
since O
for O
common O
distributions O
it O
has O
a O
simple O
analytical O
form O
. O
However O
, O
in O
this O
paper O
we O
opt O
for O
simplicity O
by O
fixing O
the O
latent O
code O
distribution O
and O
we O
will O
treat O
H O
( O
c O
) O
as O
a O
constant O
. O
So O
far O
we O
have O
bypassed O
the O
problem O
of O
having O
to O
compute O
the O
posterior O
P O
( O
c|x O
) O
explicitly O
via O
this O
lower O
bound O
but O
we O
still O
need O
to O
be O
able O
to O
sample O
from O
the O
posterior O
in O
the O
inner O
expectation O
. O
Next O
we O
state O
a O
simple O
lemma O
, O
with O
its O
proof O
deferred O
to O
Appendix O
, O
that O
removes O
the O
need O
to O
sample O
from O
the O
posterior O
. O
Lemma O
5.1 O
For O
random O
variables O
X O
, O
Y O
and O
function O
f O
( O
x O
, O
y O
) O
under O
suitable O
regularity O
conditions O
: O
By O
using O
Lemma O
A.1 O
, O
we O
can O
define O
a O
variational Metric
lower Metric
bound Metric
, O
L O
I O
( O
G O
, O
Q O
) O
, O
of O
the O
mutual Metric
information Metric
, O
We O
note O
that O
L O
I O
( O
G O
, O
Q O
) O
is O
easy O
to O
approximate O
with O
Monte Method
Carlo Method
simulation Method
. O
In O
particular O
, O
L O
I O
can O
be O
maximized O
w.r.t O
. O
Q O
directly O
and O
w.r.t O
. O
G O
via O
the O
reparametrization Method
trick Method
. O
Hence O
L O
I O
( O
G O
, O
Q O
) O
can O
be O
added O
to O
GAN Method
's Method
objectives Method
with O
no O
change O
to O
GAN Method
's Method
training Method
procedure Method
and O
we O
call O
the O
resulting O
algorithm O
Information Method
Maximizing Method
Generative Method
Adversarial Method
Networks Method
( O
InfoGAN Method
) O
. O
Eq O
[ O
reference O
] O
shows O
that O
the O
lower O
bound O
becomes O
tight O
as O
the O
auxiliary O
distribution O
Q O
approaches O
the O
true O
posterior O
distribution O
: O
In O
addition O
, O
we O
know O
that O
when O
the O
variational Metric
lower Metric
bound Metric
attains O
its O
maximum O
L O
I O
( O
G O
, O
Q O
) O
= O
H O
( O
c O
) O
for O
discrete Method
latent Method
codes Method
, O
the O
bound O
becomes O
tight O
and O
the O
maximal Metric
mutual Metric
information Metric
is O
achieved O
. O
In O
Appendix O
, O
we O
note O
how O
InfoGAN Method
can O
be O
connected O
to O
the O
Wake Method
- Method
Sleep Method
algorithm Method
[ O
reference O
] O
to O
provide O
an O
alternative O
interpretation O
. O
Hence O
, O
InfoGAN Method
is O
defined O
as O
the O
following O
minimax Method
game Method
with O
a O
variational Method
regularization Method
of Method
mutual Method
information Method
and O
a O
hyperparameter O
λ O
: O
6 O
Implementation O
In O
practice O
, O
we O
parametrize O
the O
auxiliary Method
distribution Method
Q Method
as O
a O
neural Method
network Method
. O
In O
most O
experiments O
, O
Q O
and O
D O
share O
all O
convolutional Method
layers Method
and O
there O
is O
one O
final O
fully Method
connected Method
layer Method
to O
output O
parameters O
for O
the O
conditional O
distribution O
Q O
( O
c|x O
) O
, O
which O
means O
InfoGAN Method
only O
adds O
a O
negligible Metric
computation Metric
cost Metric
to O
GAN Method
. O
We O
have O
also O
observed O
that O
L O
I O
( O
G O
, O
Q O
) O
always O
converges O
faster O
than O
normal Method
GAN Method
objectives Method
and O
hence O
InfoGAN Method
essentially O
comes O
for O
free O
with O
GAN Method
. O
For O
categorical O
latent O
code O
c O
i O
, O
we O
use O
the O
natural O
choice O
of O
softmax O
nonlinearity O
to O
represent O
Q O
( O
c O
i O
|x O
) O
. O
For O
continuous O
latent O
code O
c O
j O
, O
there O
are O
more O
options O
depending O
on O
what O
is O
the O
true O
posterior O
P O
( O
c O
j O
|x O
) O
. O
In O
our O
experiments O
, O
we O
have O
found O
that O
simply O
treating O
Q O
( O
c O
j O
|x O
) O
as O
a O
factored Method
Gaussian Method
is O
sufficient O
. O
Even O
though O
InfoGAN Method
introduces O
an O
extra O
hyperparameter O
λ O
, O
it O
's O
easy O
to O
tune O
and O
simply O
setting O
to O
1 O
is O
sufficient O
for O
discrete O
latent O
codes O
. O
When O
the O
latent O
code O
contains O
continuous O
variables O
, O
a O
smaller O
λ O
is O
typically O
used O
to O
ensure O
that O
λL O
I O
( O
G O
, O
Q O
) O
, O
which O
now O
involves O
differential O
entropy O
, O
is O
on O
the O
same O
scale O
as O
GAN O
objectives O
. O
Since O
GAN Method
is O
known O
to O
be O
difficult O
to O
train O
, O
we O
design O
our O
experiments O
based O
on O
existing O
techniques O
introduced O
by O
DC Method
- Method
GAN Method
[ O
reference O
] O
, O
which O
are O
enough O
to O
stabilize O
InfoGAN Method
training O
and O
we O
did O
not O
have O
to O
introduce O
new O
trick O
. O
Detailed O
experimental O
setup O
is O
described O
in O
Appendix O
. O
section O
: O
Experiments O
The O
first O
goal O
of O
our O
experiments O
is O
to O
investigate O
if O
mutual Metric
information Metric
can O
be O
maximized O
efficiently O
. O
The O
second O
goal O
is O
to O
evaluate O
if O
InfoGAN Method
can O
learn O
disentangled O
and O
interpretable Method
representations Method
by O
making O
use O
of O
the O
generator O
to O
vary O
only O
one O
latent O
factor O
at O
a O
time O
in O
order O
to O
assess O
if O
varying O
such O
factor O
results O
in O
only O
one O
type O
of O
semantic O
variation O
in O
generated O
images O
. O
DC Method
- Method
IGN Method
[ O
reference O
] O
also O
uses O
this O
method O
to O
evaluate O
their O
learned O
representations O
on O
3D O
image O
datasets O
, O
on O
which O
we O
also O
apply O
InfoGAN Method
to O
establish O
direct O
comparison O
. O
To O
evaluate O
whether O
the O
mutual O
information O
between O
latent O
codes O
c O
and O
generated O
images O
G O
( O
z O
, O
c O
) O
can O
be O
maximized O
efficiently O
with O
proposed O
method O
, O
we O
train O
InfoGAN Method
on O
MNIST Material
dataset Material
with O
a O
uniform O
categorical O
distribution O
on O
latent O
codes O
c O
∼ O
Cat O
( O
K O
= O
10 O
, O
p O
= O
0.1 O
) O
. O
In O
Fig O
1 O
, O
the O
lower O
bound O
L O
I O
( O
G O
, O
Q O
) O
is O
quickly O
maximized O
to O
H O
( O
c O
) O
≈ O
2.30 O
, O
which O
means O
the O
bound O
( O
4 O
) O
is O
tight O
and O
maximal O
mutual O
information O
is O
achieved O
. O
section O
: O
Mutual Task
Information Task
Maximization Task
As O
a O
baseline O
, O
we O
also O
train O
a O
regular Method
GAN Method
with O
an O
auxiliary O
distribution O
Q O
when O
the O
generator Method
is O
not O
explicitly O
encouraged O
to O
maximize O
the O
mutual O
information O
with O
the O
latent O
codes O
. O
Since O
we O
use O
expressive Method
neural Method
network Method
to O
parametrize O
Q O
, O
we O
can O
assume O
that O
Q O
reasonably O
approximates O
the O
true O
posterior O
P O
( O
c|x O
) O
and O
hence O
there O
is O
little O
mutual O
information O
between O
latent O
codes O
and O
generated O
images O
in O
regular Method
GAN Method
. O
We O
note O
that O
with O
a O
different O
neural Method
network Method
architecture Method
, O
there O
might O
be O
a O
higher O
mutual O
information O
between O
latent O
codes O
and O
generated O
images O
even O
though O
we O
have O
not O
observed O
such O
case O
in O
our O
experiments O
. O
This O
comparison O
is O
meant O
to O
demonstrate O
that O
in O
a O
regular Method
GAN Method
, O
there O
is O
no O
guarantee O
that O
the O
generator Method
will O
make O
use O
of O
the O
latent O
codes O
. O
section O
: O
Disentangled Method
Representation Method
To O
disentangle O
digit O
shape O
from O
styles O
on O
MNIST Material
, O
we O
choose O
to O
model O
the O
latent O
codes O
with O
one O
categorical O
code O
, O
c O
1 O
∼ O
Cat O
( O
K O
= O
10 O
, O
p O
= O
0.1 O
) O
, O
which O
can O
model O
discontinuous O
variation O
in O
data O
, O
and O
two O
continuous Method
codes Method
that O
can O
capture O
variations O
that O
are O
continuous O
in O
nature O
: O
c O
2 O
, O
c O
3 O
∼ O
Unif O
( O
−1 O
, O
1 O
) O
. O
In O
Figure O
2 O
, O
we O
show O
that O
the O
discrete Method
code Method
c Method
1 Method
captures O
drastic O
change O
in O
shape O
. O
Changing O
categorical O
code O
c O
1 O
switches O
between O
digits O
most O
of O
the O
time O
. O
In O
fact O
even O
if O
we O
just O
train O
InfoGAN Method
without O
any O
label O
, O
c O
1 O
can O
be O
used O
as O
a O
classifier Method
that O
achieves O
5 O
% O
error Metric
rate Metric
in O
classifying O
MNIST Material
digits O
by O
matching O
each O
category O
in O
c O
1 O
to O
a O
digit O
type O
. O
In O
the O
second O
row O
of O
Figure O
2a O
, O
we O
can O
observe O
a O
digit O
7 O
is O
classified O
as O
a O
9 O
. O
Continuous Method
codes Method
c O
2 O
, O
c O
3 O
capture O
continuous O
variations O
in O
style O
: O
c O
2 O
models O
rotation O
of O
digits O
and O
c O
3 O
controls O
the O
width O
. O
What O
is O
remarkable O
is O
that O
in O
both O
cases O
, O
the O
generator O
does O
not O
simply O
stretch O
or O
rotate O
the O
digits O
but O
instead O
adjust O
other O
details O
like O
thickness O
or O
stroke O
style O
to O
make O
sure O
the O
resulting O
images O
are O
natural O
looking O
. O
As O
a O
test O
to O
check O
whether O
the O
latent Method
representation Method
learned O
by O
InfoGAN Method
is O
generalizable O
, O
we O
manipulated O
the O
latent O
codes O
in O
an O
exaggerated O
way O
: O
instead O
of O
plotting O
latent O
codes O
from O
−1 O
to O
1 O
, O
we O
plot O
it O
from O
−2 O
to O
2 O
covering O
a O
wide O
region O
that O
the O
network O
was O
never O
trained O
on O
and O
we O
still O
get O
meaningful O
generalization Task
. O
Next O
we O
evaluate O
InfoGAN Method
on O
two O
datasets O
of O
3D O
images O
: O
faces O
[ O
reference O
] O
and O
chairs O
[ O
reference O
] O
, O
on O
which O
DC Method
- Method
IGN Method
was O
shown O
to O
learn O
highly O
interpretable O
graphics O
codes O
. O
On O
the O
faces O
dataset O
, O
DC Method
- Method
IGN Method
learns O
to O
represent O
latent O
factors O
as O
azimuth O
( O
pose O
) O
, O
elevation O
, O
and O
lighting O
as O
continuous O
latent O
variables O
by O
using O
supervision O
. O
Using O
the O
same O
dataset O
, O
we O
demonstrate O
that O
InfoGAN Method
learns O
a O
disentangled Method
representation Method
that O
recover O
azimuth O
( O
pose O
) O
, O
elevation O
, O
and O
lighting O
on O
the O
same O
dataset O
. O
In O
this O
experiment O
, O
we O
choose O
to O
model O
the O
latent O
codes O
with O
five O
continuous O
codes O
, O
c O
i O
∼ O
Unif O
( O
−1 O
, O
1 O
) O
with O
1 O
≤ O
i O
≤ O
5 O
. O
Since O
DC Method
- Method
IGN Method
requires O
supervision O
, O
it O
was O
previously O
not O
possible O
to O
learn O
a O
latent O
code O
for O
a O
variation O
that O
's O
unlabeled O
and O
hence O
salient O
latent O
factors O
of O
variation O
can O
not O
be O
discovered O
automatically O
from O
data O
. O
By O
contrast O
, O
InfoGAN Method
is O
able O
to O
discover O
such O
variation O
on O
its O
own O
: O
for O
instance O
, O
in O
Figure O
3d O
latent O
code O
that O
smoothly O
changes O
a O
face O
from O
wide O
to O
narrow O
is O
learned O
even O
though O
this O
variation O
was O
neither O
explicitly O
generated O
or O
labeled O
in O
prior O
work O
. O
On O
the O
chairs O
dataset O
, O
DC Method
- Method
IGN Method
can O
learn O
a O
continuous Method
code Method
that O
representes O
rotation O
. O
InfoGAN Method
again O
is O
able O
to O
learn O
the O
same O
concept O
as O
a O
continuous O
code O
( O
Figure O
4a O
) O
and O
we O
show O
in O
addition O
that O
InfoGAN Method
is O
also O
able O
to O
continuously O
interpolate O
between O
similar O
chair O
types O
of O
different O
widths O
using O
a O
single O
continuous Method
code Method
( O
Figure O
4b O
) O
. O
In O
this O
experiment O
, O
we O
choose O
to O
model O
the O
latent O
factors O
with O
four O
categorical O
codes O
, O
c O
1 O
, O
c O
2 O
, O
c O
3 O
, O
c O
4 O
∼ O
Cat O
( O
K O
= O
20 O
, O
p O
= O
0.05 O
) O
and O
one O
continuous O
code O
c O
5 O
∼ O
Unif O
( O
−1 O
, O
1 O
) O
. O
Next O
we O
evaluate O
InfoGAN Method
on O
the O
Street O
View O
House O
Number O
( O
SVHN O
) O
dataset O
, O
which O
is O
significantly O
more O
challenging O
to O
learn O
an O
interpretable Method
representation Method
because O
it O
is O
noisy O
, O
containing O
images O
of O
variable O
- O
resolution O
and O
distracting O
digits O
, O
and O
it O
does O
not O
have O
multiple O
variations O
of O
the O
same O
object O
. O
In O
this O
experiment O
, O
we O
make O
use O
of O
four O
10−dimensional O
categorical O
variables O
and O
two O
uniform O
continuous O
variables O
as O
latent O
codes O
. O
We O
show O
two O
of O
the O
learned O
latent O
factors O
in O
Figure O
5 O
. O
Finally O
we O
show O
in O
Figure O
6 O
that O
InfoGAN Method
is O
able O
to O
learn O
many O
visual O
concepts O
on O
another O
challenging O
dataset O
: O
CelebA O
[ O
reference O
] O
, O
which O
includes O
200 O
, O
000 O
celebrity O
images O
with O
large O
pose O
variations O
and O
background O
clutter O
. O
In O
this O
dataset O
, O
we O
model O
the O
latent O
variation O
as O
10 O
uniform O
categorical O
variables O
, O
each O
of O
dimension O
10 O
. O
Surprisingly O
, O
even O
in O
this O
complicated O
dataset O
, O
InfoGAN Method
can O
recover O
azimuth O
as O
in O
3D O
images O
even O
though O
in O
this O
dataset O
no O
single O
face O
appears O
in O
multiple O
pose O
positions O
. O
Moreover O
InfoGAN Method
can O
disentangle O
other O
highly O
semantic O
variations O
like O
presence O
or O
absence O
of O
glasses O
, O
hairstyles O
and O
emotion O
, O
demonstrating O
a O
level O
of O
visual Task
understanding Task
is O
acquired O
without O
any O
supervision O
. O
, O
we O
show O
that O
the O
continuous Method
code Method
can O
alternatively O
learn O
to O
capture O
the O
widths O
of O
different O
chair O
types O
, O
and O
smoothly O
interpolate O
between O
them O
. O
For O
each O
factor O
, O
we O
present O
the O
representation O
that O
most O
resembles O
prior O
supervised O
results O
[ O
reference O
] O
out O
of O
5 O
random O
runs O
to O
provide O
direct O
comparison O
. O
section O
: O
Conclusion O
This O
paper O
introduces O
a O
representation Method
learning Method
algorithm Method
called O
Information Method
Maximizing Method
Generative Method
Adversarial Method
Networks Method
( O
InfoGAN Method
) O
. O
In O
contrast O
to O
previous O
approaches O
, O
which O
require O
supervision O
, O
InfoGAN Method
is O
completely O
unsupervised O
and O
learns O
interpretable Method
and Method
disentangled Method
representations Method
on O
challenging O
datasets O
. O
In O
addition O
, O
InfoGAN Method
adds O
only O
negligible Metric
computation Metric
cost Metric
on O
top O
of O
GAN Method
and O
is O
easy O
to O
train O
. O
The O
core O
idea O
of O
using O
mutual O
information O
to O
induce Task
representation Task
can O
be O
applied O
to O
other O
methods O
like O
VAE Method
[ O
reference O
] O
, O
which O
is O
a O
promising O
area O
of O
future O
work O
. O
Other O
possible O
extensions O
to O
this O
work O
include O
: O
learning O
hierarchical Method
latent Method
representations Method
, O
improving O
semi Method
- Method
supervised Method
learning Method
with O
better O
codes O
[ O
reference O
] O
, O
and O
using O
InfoGAN Method
as O
a O
high Method
- Method
dimensional Method
data Method
discovery Method
tool Method
. O
A O
Proof O
of O
Lemma O
5.1 O
Lemma O
A.1 O
For O
random O
variables O
X O
, O
Y O
and O
function O
f O
( O
x O
, O
y O
) O
under O
suitable O
regularity O
conditions O
: O
B O
Interpretation O
as O
" O
Sleep Method
- Method
Sleep Method
" Method
Algorithm Method
We O
note O
that O
InfoGAN Method
can O
be O
viewed O
as O
a O
Helmholtz Method
machine Method
[ O
reference O
] O
: O
P O
G O
( O
x|c O
) O
is O
the O
generative Method
distribution Method
and O
Q Method
( Method
c|x Method
) O
is O
the O
recognition Method
distribution Method
. O
Wake Method
- Method
Sleep Method
algorithm Method
[ O
reference O
] O
was O
proposed O
to O
train O
Helmholtz Method
machines Method
by O
performing O
" O
wake O
" O
phase O
and O
" O
sleep O
" O
phase O
updates O
. O
The O
" O
wake Method
" Method
phase Method
update Method
proceeds O
by O
optimizing O
the O
variational O
lower O
bound O
of O
log O
P O
G O
( O
x O
) O
w.r.t O
. O
generator O
: O
max O
The O
" O
sleep O
" O
phase O
updates O
the O
auxiliary Method
distribution Method
Q Method
by O
" O
dreaming O
" O
up O
samples O
from O
current O
generator Method
distribution Method
rather O
than O
drawing O
from O
real Method
data Method
distribution Method
: O
Hence O
we O
can O
see O
that O
when O
we O
optimize O
the O
surrogate O
loss O
L O
I O
w.r.t O
. O
Q O
, O
the O
update Method
step Method
is O
exactly O
the O
" O
sleep Method
" Method
phase Method
update Method
in O
Wake Method
- Method
Sleep Method
algorithm Method
. O
InfoGAN Method
differs O
from O
Wake O
- O
Sleep O
when O
we O
optimize O
L O
I O
w.r.t O
. O
G O
, O
encouraging O
the O
generator Method
network Method
G Method
to O
make O
use O
of O
latent O
codes O
c O
for O
the O
whole O
prior O
distribution O
on O
latent O
codes O
P O
( O
c O
) O
. O
Since O
InfoGAN Method
also O
updates O
generator O
in O
" O
sleep O
" O
phase O
, O
our O
method O
can O
be O
interpreted O
as O
" O
Sleep Method
- Method
Sleep Method
" Method
algorithm Method
. O
This O
interpretation O
highlights O
InfoGAN Method
's O
difference O
from O
previous O
generative Method
modeling Method
techniques Method
: O
the O
generator Method
is O
explicitly O
encouraged O
to O
convey O
information O
in O
latent O
codes O
and O
suggests O
that O
the O
same O
principle O
can O
be O
applied O
to O
other O
generative Method
models Method
. O
section O
: O
C O
Experiment O
Setup O
For O
all O
experiments O
, O
we O
use O
Adam Method
[ O
reference O
] O
for O
online Task
optimization Task
and O
apply O
batch Method
normalization Method
[ O
reference O
] O
after O
most O
layers O
, O
the O
details O
of O
which O
are O
specified O
for O
each O
experiment O
. O
We O
use O
an O
up Method
- Method
convolutional Method
architecture Method
for O
the O
generator Method
networks Method
[ O
reference O
] O
. O
We O
use O
leaky Method
rectified Method
linear Method
units Method
( O
lRELU Method
) O
[ O
reference O
] O
with O
leaky O
rate O
0.1 O
as O
the O
nonlinearity O
applied O
to O
hidden O
layers O
of O
the O
discrminator Method
networks Method
, O
and O
normal Method
rectified Method
linear Method
units Method
( O
RELU Method
) O
for O
the O
generator Method
networks Method
. O
Unless O
noted O
otherwise O
, O
learning Metric
rate Metric
is O
2e O
- O
4 O
for O
D O
and O
1e O
- O
3 O
for O
G O
; O
λ O
is O
set O
to O
1 O
. O
For O
discrete O
latent O
codes O
, O
we O
apply O
a O
softmax O
nonlinearity O
over O
the O
corresponding O
units O
in O
the O
recognition Method
network Method
output O
. O
For O
continuous O
latent O
codes O
, O
we O
parameterize O
the O
approximate O
posterior O
through O
a O
diagonal Method
Gaussian Method
distribution Method
, O
and O
the O
recognition Method
network Method
outputs O
its O
mean O
and O
standard O
deviation O
, O
where O
the O
standard O
deviation O
is O
parameterized O
through O
an O
exponential Method
transformation Method
of O
the O
network O
output O
to O
ensure O
positivity O
. O
The O
details O
for O
each O
set O
of O
experiments O
are O
presented O
below O
. O
section O
: O
C.1 O
MNIST Material
The O
network Method
architectures Method
are O
shown O
in O
Table O
1 O
. O
The O
discriminator Method
D Method
and O
the O
recognition Method
network Method
Q Method
shares O
most O
of O
the O
network O
. O
For O
this O
task O
, O
we O
use O
1 O
ten O
- O
dimensional O
categorical O
code O
, O
2 O
continuous O
latent O
codes O
and O
62 O
noise O
variables O
, O
resulting O
in O
a O
concatenated O
dimension O
of O
74 O
. O
section O
: O
C.2 O
SVHN O
The O
network Method
architectures Method
are O
shown O
in O
Table O
2 O
. O
The O
discriminator Method
D Method
and O
the O
recognition Method
network Method
Q Method
shares O
most O
of O
the O
network O
. O
For O
this O
task O
, O
we O
use O
4 O
ten O
- O
dimensional O
categorical O
code O
, O
4 O
continuous O
latent O
codes O
and O
124 O
noise O
variables O
, O
resulting O
in O
a O
concatenated O
dimension O
of O
168 O
. O
section O
: O
C.3 O
CelebA O
The O
network Method
architectures Method
are O
shown O
in O
Table O
3 O
. O
The O
discriminator Method
D Method
and O
the O
recognition Method
network Method
Q Method
shares O
most O
of O
the O
network O
. O
For O
this O
task O
, O
we O
use O
10 O
ten O
- O
dimensional O
categorical O
code O
and O
128 O
noise O
variables O
, O
resulting O
in O
a O
concatenated O
dimension O
of O
228 O
. O
section O
: O
C.4 O
Faces O
The O
network Method
architectures Method
are O
shown O
in O
Table O
4 O
. O
The O
discriminator Method
D Method
and O
the O
recognition Method
network Method
Q Method
shares O
the O
same O
network O
, O
and O
only O
have O
separate O
output O
units O
at O
the O
last O
layer O
. O
For O
this O
task O
, O
we O
use O
5 O
continuous O
latent O
codes O
and O
128 O
noise O
variables O
, O
so O
the O
input O
to O
the O
generator O
has O
dimension O
133 O
. O
We O
used O
separate O
configurations O
for O
each O
learned O
variation O
, O
shown O
in O
Table O
5 O
. O
section O
: O
C.5 O
Chairs O
The O
network Method
architectures Method
are O
shown O
in O
Table O
6 O
. O
The O
discriminator Method
D Method
and O
the O
recognition Method
network Method
Q Method
shares O
the O
same O
network O
, O
and O
only O
have O
separate O
output O
units O
at O
the O
last O
layer O
. O
For O
this O
task O
, O
we O
use O
1 O
continuous O
latent O
code O
, O
3 O
discrete Method
latent Method
codes Method
( O
each O
with O
dimension O
20 O
) O
, O
and O
128 O
noise O
variables O
, O
so O
the O
input O
to O
the O
generator O
has O
dimension O
189 O
. O
section O
: O
section O
: O
We O
used O
separate O
configurations O
for O
each O
learned O
variation O
, O
shown O
in O
Table O
7 O
. O
For O
this O
task O
, O
we O
found O
it O
necessary O
to O
use O
different O
regularization O
coefficients O
for O
the O
continuous O
and O
discrete O
latent O
codes O
. O
section O
: O
