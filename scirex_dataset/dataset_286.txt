TYING O
WORD O
VECTORS O
AND O
WORD O
CLASSIFIERS Method
: O
A O
LOSS Method
FRAMEWORK Method
FOR O
LANGUAGE Task
MODELING Task
section O
: O
ABSTRACT O
Recurrent Method
neural Method
networks Method
have O
been O
very O
successful O
at O
predicting Task
sequences Task
of Task
words Task
in O
tasks Task
such O
as O
language Task
modeling Task
. O
However O
, O
all O
such O
models O
are O
based O
on O
the O
conventional O
classification Method
framework Method
, O
where O
the O
model O
is O
trained O
against O
one O
- O
hot O
targets O
, O
and O
each O
word O
is O
represented O
both O
as O
an O
input O
and O
as O
an O
output O
in O
isolation O
. O
This O
causes O
inefficiencies O
in O
learning Task
both O
in O
terms O
of O
utilizing O
all O
of O
the O
information O
and O
in O
terms O
of O
the O
number O
of O
parameters O
needed O
to O
train O
. O
We O
introduce O
a O
novel O
theoretical Method
framework Method
that O
facilitates O
better O
learning Task
in O
language Task
modeling Task
, O
and O
show O
that O
our O
framework O
leads O
to O
tying O
together O
the O
input O
embedding O
and O
the O
output O
projection O
matrices O
, O
greatly O
reducing O
the O
number O
of O
trainable O
variables O
. O
Our O
framework O
leads O
to O
state O
of O
the O
art O
performance O
on O
the O
Penn Material
Treebank Material
with O
a O
variety O
of O
network Method
models Method
. O
section O
: O
INTRODUCTION O
Neural Method
network Method
models Method
have O
recently O
made O
tremendous O
progress O
in O
a O
variety O
of O
NLP Task
applications Task
such O
as O
speech Task
recognition Task
[ O
reference O
] O
, O
sentiment Task
analysis Task
[ O
reference O
] O
) O
, O
text Task
summarization Task
[ O
reference O
] O
, O
and O
machine Task
translation Task
[ O
reference O
] O
. O
Despite O
the O
overwhelming O
success O
achieved O
by O
recurrent Method
neural Method
networks Method
in O
modeling O
long Task
range Task
dependencies Task
between Task
words Task
, O
current O
recurrent Method
neural Method
network Method
language Method
models Method
( O
RNNLM Method
) Method
are O
based O
on O
the O
conventional O
classification Method
framework Method
, O
which O
has O
two O
major O
drawbacks O
: O
First O
, O
there O
is O
no O
assumed O
metric O
on O
the O
output O
classes O
, O
whereas O
there O
is O
evidence O
suggesting O
that O
learning Task
is O
improved O
when O
one O
can O
define O
a O
natural O
metric O
on O
the O
output O
space O
[ O
reference O
] O
. O
In O
language Task
modeling Task
, O
there O
is O
a O
well O
established O
metric O
space O
for O
the O
outputs O
( O
words O
in O
the O
language O
) O
based O
on O
word Method
embeddings Method
, O
with O
meaningful O
distances O
between O
words O
[ O
reference O
][ O
reference O
] O
. O
Second O
, O
in O
the O
classical O
framework O
, O
inputs O
and O
outputs O
are O
considered O
as O
isolated O
entities O
with O
no O
semantic O
link O
between O
them O
. O
This O
is O
clearly O
not O
the O
case O
for O
language Task
modeling Task
, O
where O
inputs O
and O
outputs O
in O
fact O
live O
in O
identical O
spaces O
. O
Therefore O
, O
even O
for O
models O
with O
moderately O
sized O
vocabularies O
, O
the O
classical Method
framework Method
could O
be O
a O
vast O
source O
of O
inefficiency O
in O
terms O
of O
the O
number O
of O
variables O
in O
the O
model O
, O
and O
in O
terms O
of O
utilizing O
the O
information O
gathered O
by O
different O
parts O
of O
the O
model O
( O
e.g. O
inputs O
and O
outputs O
) O
. O
In O
this O
work O
, O
we O
introduce O
a O
novel O
loss Method
framework Method
for O
language Task
modeling Task
to O
remedy O
the O
above O
two O
problems O
. O
Our O
framework O
is O
comprised O
of O
two O
closely O
linked O
improvements O
. O
First O
, O
we O
augment O
the O
classical O
cross Method
- Method
entropy Method
loss Method
with O
an O
additional O
term O
which O
minimizes O
the O
KL O
- O
divergence O
between O
the O
model O
's O
prediction O
and O
an O
estimated O
target O
distribution O
based O
on O
the O
word O
embeddings O
space O
. O
This O
estimated O
distribution O
uses O
knowledge O
of O
word O
vector O
similarity O
. O
We O
then O
theoretically O
analyze O
this O
loss O
, O
and O
this O
leads O
to O
a O
second O
and O
synergistic O
improvement O
: O
tying O
together O
two O
large O
matrices O
by O
reusing O
the O
input O
word O
embedding O
matrix O
as O
the O
output O
classification O
matrix O
. O
We O
empirically O
validate O
our O
theory O
in O
a O
practical O
setting O
, O
with O
much O
milder O
assumptions O
than O
those O
in O
theory O
. O
We O
also O
find O
empirically O
that O
for O
large Task
networks Task
, O
most O
of O
the O
improvement O
could O
be O
achieved O
by O
only O
reusing O
the O
word O
embeddings O
. O
We O
test O
our O
framework O
by O
performing O
extensive O
experiments O
on O
the O
Penn Material
Treebank Material
corpus Material
, O
a O
dataset O
widely O
used O
for O
benchmarking O
language Method
models Method
[ O
reference O
][ O
reference O
] O
. O
We O
demonstrate O
that O
models O
trained O
using O
our O
proposed O
framework O
significantly O
outperform O
models O
trained O
using O
the O
conventional O
framework O
. O
We O
also O
perform O
experiments O
on O
the O
newly O
introduced O
Wikitext Material
- Material
2 Material
dataset O
[ O
reference O
] O
, O
and O
verify O
that O
the O
empirical O
performance O
of O
our O
proposed O
framework O
is O
consistent O
across O
different O
datasets O
. O
section O
: O
BACKGROUND O
: O
RECURRENT Method
NEURAL Method
NETWORK Method
LANGUAGE Method
MODEL Method
In O
any O
variant O
of O
recurrent Method
neural Method
network Method
language Method
model Method
( O
RNNLM Method
) Method
, O
the O
goal O
is O
to O
predict O
the O
next O
word O
indexed O
by O
t O
in O
a O
sequence O
of O
one O
- O
hot O
word O
tokens O
( O
y O
* O
1 O
, O
. O
. O
. O
y O
* O
N O
) O
as O
follows O
: O
The O
matrix O
L O
∈ O
R O
dx×|V O
| O
is O
the O
word O
embedding O
matrix O
, O
where O
d O
x O
is O
the O
word O
embedding O
dimension O
and O
|V O
| O
is O
the O
size O
of O
the O
vocabulary O
. O
The O
function O
f O
( O
. O
, O
. O
) O
represents O
the O
recurrent Method
neural Method
network Method
which O
takes O
in O
the O
current O
input O
and O
the O
previous O
hidden O
state O
and O
produces O
the O
next O
hidden O
state O
. O
W O
∈ O
R O
|V O
|×d O
h O
and O
b O
∈ O
R O
|V O
| O
are O
the O
the O
output O
projection O
matrix O
and O
the O
bias O
, O
respectively O
, O
and O
d O
h O
is O
the O
size O
of O
the O
RNN O
hidden O
state O
. O
The O
|V O
| O
dimensional Method
y Method
t Method
models O
the O
discrete O
probability O
distribution O
for O
the O
next O
word O
. O
Note O
that O
the O
above O
formulation O
does O
not O
make O
any O
assumptions O
about O
the O
specifics O
of O
the O
recurrent Method
neural Method
units Method
, O
and O
f O
could O
be O
replaced O
with O
a O
standard O
recurrent Method
unit Method
, O
a O
gated Method
recurrent Method
unit Method
( O
GRU Method
) O
[ O
reference O
] O
, O
a O
long Method
- Method
short Method
term Method
memory Method
( O
LSTM Method
) O
unit O
[ O
reference O
] O
, O
etc O
. O
For O
our O
experiments O
, O
we O
use O
LSTM Method
units O
with O
two O
layers O
. O
Given O
y O
t O
for O
the O
t O
th O
example O
, O
a O
loss O
is O
calculated O
for O
that O
example O
. O
The O
loss O
used O
in O
the O
RNNLMs Method
is O
almost O
exclusively O
the O
cross Metric
- Metric
entropy Metric
between O
y O
t O
and O
the O
observed O
one O
- O
hot O
word O
token O
, O
y O
* O
t O
: O
We O
shall O
refer O
to O
y O
t O
as O
the O
model Method
prediction Method
distribution Method
for O
the O
t O
th O
example O
, O
and O
y O
* O
t O
as O
the O
empirical O
target O
distribution O
( O
both O
are O
in O
fact O
conditional O
distributions O
given O
the O
history O
) O
. O
Since O
crossentropy O
and O
Kullback O
- O
Leibler O
divergence O
are O
equivalent O
when O
the O
target O
distribution O
is O
one O
- O
hot O
, O
we O
can O
rewrite O
the O
loss O
for O
the O
t O
th O
example O
as O
Therefore O
, O
we O
can O
think O
of O
the O
optimization Task
of O
the O
conventional Task
loss Task
in O
an O
RNNLM Method
as O
trying O
to O
minimize O
the O
distance O
1 O
between O
the O
model Method
prediction Method
distribution Method
( O
y O
) O
and O
the O
empirical O
target O
distribution O
( O
y O
* O
) O
, O
which O
, O
with O
many O
training O
examples O
, O
will O
get O
close O
to O
minimizing O
distance O
to O
the O
actual O
target O
distribution O
. O
In O
the O
framework O
which O
we O
will O
introduce O
, O
we O
utilize O
Kullback O
- O
Leibler O
divergence O
as O
opposed O
to O
cross O
- O
entropy O
due O
to O
its O
intuitive O
interpretation O
as O
a O
distance O
between O
distributions O
, O
although O
the O
two O
are O
not O
equivalent O
in O
our O
framework O
. O
In O
above O
, O
α O
is O
a O
hyperparameter O
to O
be O
adjusted O
, O
andŷ O
t O
is O
almost O
identical O
to O
the O
regular Method
model Method
prediction Method
distribution Method
y O
t O
with O
the O
exception O
that O
the O
logits O
are O
divided O
by O
a O
temperature O
parameter O
τ O
. O
We O
defineỹ O
t O
as O
some O
probability Method
distribution Method
that O
estimates O
the O
true O
data O
distribution O
( O
conditioned O
on O
the O
word O
history O
) O
which O
satisfies O
Eỹ O
t O
= O
Ey O
* O
t O
. O
The O
goal O
of O
this O
framework O
is O
to O
minimize O
the O
1 O
We O
note O
, O
however O
, O
that O
Kullback O
- O
Leibler O
divergence O
is O
not O
a O
valid O
distance Metric
metric Metric
. O
distribution O
distance O
between O
the O
prediction Method
distribution Method
and O
a O
more O
accurate O
estimate O
of O
the O
true O
data O
distribution O
. O
To O
understand O
the O
effect O
of O
optimizing Task
in O
this O
setting O
, O
let O
's O
focus O
on O
an O
ideal O
case O
in O
which O
we O
are O
given O
the O
true O
data O
distribution O
so O
thatỹ O
t O
= O
Ey O
* O
t O
, O
and O
we O
only O
use O
the O
augmented Method
loss Method
, O
J O
aug O
. O
We O
will O
carry O
out O
our O
investigation O
through O
stochastic Method
gradient Method
descent Method
, O
which O
is O
the O
technique O
dominantly O
used O
for O
training O
neural Method
networks Method
. O
The O
gradient O
of O
J O
aug O
t O
with O
respect O
to O
the O
logits O
W O
h O
t O
is O
Let O
's O
denote O
by O
e O
j O
∈ O
R O
|V O
| O
the O
vector O
whose O
j O
th O
entry O
is O
1 O
, O
and O
others O
are O
zero O
. O
We O
can O
then O
rewrite O
( O
3.4 O
) O
as O
( O
3.5 O
) O
Implication O
of O
( O
3.5 O
) O
is O
the O
following O
: O
Every O
time O
the O
optimizer O
sees O
one O
training O
example O
, O
it O
takes O
a O
step O
not O
only O
on O
account O
of O
the O
label O
seen O
, O
but O
it O
proceeds O
taking O
into O
account O
all O
the O
class O
labels O
for O
which O
the O
conditional O
probability O
is O
not O
zero O
, O
and O
the O
relative O
step O
size O
for O
each O
step O
is O
given O
by O
the O
conditional O
probability O
for O
that O
label O
, O
ỹ O
t O
, O
i O
. O
Furthermore O
, O
this O
is O
a O
much O
less O
noisy O
update O
since O
the O
target O
distribution O
is O
exact O
and O
deterministic O
. O
Therefore O
, O
unless O
all O
the O
examples O
exclusively O
belong O
to O
a O
specific O
class O
with O
probability O
1 O
, O
the O
optimization Method
will O
act O
much O
differently O
and O
train O
with O
greatly O
improved O
supervision O
. O
The O
idea O
proposed O
in O
the O
recent O
work O
by O
[ O
reference O
] O
might O
be O
considered O
as O
an O
application O
of O
this O
framework O
, O
where O
they O
try O
to O
obtain O
a O
good O
set O
ofỹ O
's O
by O
training O
very O
large O
models O
and O
using O
the O
model Method
prediction Method
distributions Method
of O
those O
. O
Although O
finding O
a O
goodỹ O
in O
general O
is O
rather O
nontrivial O
, O
in O
the O
context O
of O
language Task
modeling Task
we O
can O
hope O
to O
achieve O
this O
by O
exploiting O
the O
inherent O
metric O
space O
of O
classes O
encoded O
into O
the O
model O
, O
namely O
the O
space Method
of Method
word Method
embeddings Method
. O
Specifically O
, O
we O
propose O
the O
following O
forỹ O
: O
In O
words O
, O
we O
first O
find O
the O
target O
word O
vector O
which O
corresponds O
to O
the O
target O
word O
token O
( O
resulting O
in O
u O
t O
) O
, O
and O
then O
take O
the O
inner O
product O
of O
the O
target O
word O
vector O
with O
all O
the O
other O
word O
vectors O
to O
get O
an O
unnormalized Method
probability Method
distribution Method
. O
We O
adjust O
this O
with O
the O
same O
temperature O
parameter O
τ O
used O
for O
obtainingŷ O
t O
and O
apply O
softmax Method
. O
The O
target Method
distribution Method
estimate Method
, O
ỹ O
, O
therefore O
measures O
the O
similarity O
between O
the O
word O
vectors O
and O
assigns O
similar O
probability O
masses O
to O
words O
that O
the O
language Method
model Method
deems O
close O
. O
Note O
that O
the O
estimation O
ofỹ O
with O
this O
procedure O
is O
iterative O
, O
and O
the O
estimates O
ofỹ O
in O
the O
initial O
phase O
of O
the O
training Task
are O
not O
necessarily O
informative O
. O
However O
, O
as O
training O
procedes O
, O
we O
expectỹ O
to O
capture O
the O
word O
statistics O
better O
and O
yield O
a O
consistently O
more O
accurate O
estimate O
of O
the O
true O
data O
distribution O
. O
section O
: O
THEORETICALLY Task
DRIVEN Task
REUSE Task
OF Task
WORD Task
EMBEDDINGS Task
We O
now O
theoretically O
motivate O
and O
introduce O
a O
second O
modification O
to O
improve O
learning Task
in O
the O
language Method
model Method
. O
We O
do O
this O
by O
analyzing O
the O
proposed O
augmented Method
loss Method
in O
a O
particular O
setting O
, O
and O
observe O
an O
implicit O
core O
mechanism O
of O
this O
loss O
. O
We O
then O
make O
our O
proposition O
by O
making O
this O
mechanism O
explicit O
. O
We O
start O
by O
introducing O
our O
setting O
for O
the O
analysis O
. O
We O
restrict O
our O
attention O
to O
the O
case O
where O
the O
input O
embedding O
dimension O
is O
equal O
to O
the O
dimension O
of O
the O
RNN O
hidden O
state O
, O
i.e. O
We O
also O
set O
b O
= O
0 O
in O
( O
2.3 O
) O
so O
that O
y O
t O
= O
W O
h O
t O
. O
We O
only O
use O
the O
augmented Method
loss Method
, O
i.e. O
J O
tot O
= O
J O
aug O
, O
and O
we O
assume O
that O
we O
can O
achieve O
zero O
training Metric
loss Metric
. O
Finally O
, O
we O
set O
the O
temperature O
parameter O
τ O
to O
be O
large O
. O
We O
first O
show O
that O
when O
the O
temperature O
parameter O
, O
τ O
, O
is O
high O
enough O
, O
J O
aug O
t O
acts O
to O
match O
the O
logits O
of O
the O
prediction O
distribution O
to O
the O
logits O
of O
the O
the O
more O
informative O
labels O
, O
ỹ O
. O
We O
proceed O
in O
the O
same O
way O
as O
was O
done O
in O
[ O
reference O
] O
to O
make O
an O
identical O
argument O
. O
Particularly O
, O
we O
consider O
the O
derivative O
of O
J O
aug O
t O
with O
respect O
to O
the O
entries O
of O
the O
logits O
produced O
by O
the O
neural Method
network Method
. O
Let O
's O
denote O
by O
l O
i O
the O
i O
th O
column O
of O
L. O
Using O
the O
first Method
order Method
approximation Method
of Method
exponential Method
function Method
around O
zero O
( O
exp O
( O
x O
) O
≈ O
1 O
+ O
x O
) O
, O
we O
can O
approximateỹ O
t O
( O
same O
holds O
forŷ O
t O
) O
at O
high O
temperatures O
as O
follows O
: O
ỹ O
We O
can O
further O
simplify O
( O
4.1 O
) O
if O
we O
assume O
that O
u O
t O
, O
l O
j O
= O
0 O
on O
average O
: O
By O
replacingỹ O
t O
andŷ O
t O
in O
( O
3.4 O
) O
with O
their O
simplified O
forms O
according O
to O
( O
4.2 O
) O
, O
we O
get O
which O
is O
the O
desired O
result O
that O
augmented Method
loss Method
tries O
to O
match O
the O
logits O
of O
the O
model O
to O
the O
logits O
of O
y O
's O
. O
Since O
the O
training O
loss O
is O
zero O
by O
assumption O
, O
we O
necessarily O
have O
for O
each O
training O
example O
, O
i.e. O
, O
gradient O
contributed O
by O
each O
example O
is O
zero O
. O
Provided O
that O
W O
and O
L O
are O
full O
rank O
matrices O
and O
there O
are O
more O
linearly O
independent O
examples O
of O
h O
t O
's O
than O
the O
embedding O
dimension O
d O
, O
we O
get O
that O
the O
space O
spanned O
by O
the O
columns O
of O
L Method
T Method
is O
equivalent O
to O
that O
spanned O
by O
the O
columns O
of O
W O
. O
Let O
's O
now O
introduce O
a O
square Method
matrix Method
T O
and O
W O
span O
the O
same O
column O
space O
) O
. O
In O
this O
case O
, O
we O
can O
rewrite O
In O
other O
words O
, O
by O
reusing O
the O
embedding O
matrix O
in O
the O
output Method
projection Method
layer Method
( O
with O
a O
transpose O
) O
and O
letting O
the O
neural Method
network Method
do O
the O
necessary O
linear Method
mapping Method
h O
→ O
Ah O
, O
we O
get O
the O
same O
result O
as O
we O
would O
have O
in O
the O
first O
place O
. O
Although O
the O
above O
scenario O
could O
be O
difficult O
to O
exactly O
replicate O
in O
practice O
, O
it O
uncovers O
a O
mechanism O
through O
which O
our O
proposed O
loss Method
augmentation Method
acts O
, O
which O
is O
trying O
to O
constrain O
the O
output O
( O
unnormalized O
) O
probability O
space O
to O
a O
small O
subspace O
governed O
by O
the O
embedding O
matrix O
. O
This O
suggests O
that O
we O
can O
make O
this O
mechanism O
explicit O
and O
constrain O
W O
= O
L Method
T Method
during O
training O
while O
setting O
the O
output O
bias O
, O
b O
, O
to O
zero O
. O
Doing O
so O
would O
not O
only O
eliminate O
a O
big O
matrix O
which O
dominates O
the O
network Metric
size Metric
for O
models O
with O
even O
moderately O
sized O
vocabularies O
, O
but O
it O
would O
also O
be O
optimal O
in O
our O
setting O
of O
loss Task
augmentation Task
as O
it O
would O
eliminate O
much O
work O
to O
be O
done O
by O
the O
augmented Method
loss Method
. O
section O
: O
RELATED O
WORK O
Since O
their O
introduction O
in O
[ O
reference O
] O
, O
many O
improvements O
have O
been O
proposed O
for O
RNNLMs Method
, O
including O
different O
dropout Method
methods Method
[ O
reference O
][ O
reference O
] O
, O
novel O
recurrent Method
units Method
[ O
reference O
] O
, O
and O
use O
of O
pointer Method
networks Method
to O
complement O
the O
recurrent Method
neural Method
network Method
[ O
reference O
] O
. O
However O
, O
none O
of O
the O
improvements O
dealt O
with O
the O
loss O
structure O
, O
and O
to O
the O
best O
of O
our O
knowledge O
, O
our O
work O
is O
the O
first O
to O
offer O
a O
new O
loss Method
framework Method
. O
Our O
technique O
is O
closely O
related O
to O
the O
one O
in O
[ O
reference O
] O
, O
where O
they O
also O
try O
to O
estimate O
a O
more O
informed O
data O
distribution O
and O
augment O
the O
conventional O
loss Method
with O
KL O
divergence O
between O
model Method
prediction Method
distribution Method
and O
the O
estimated O
data O
distribution O
. O
However O
, O
they O
estimate O
their O
data Method
distribution Method
by O
training O
large Method
networks Method
on O
the O
data O
and O
then O
use O
it O
to O
improve O
learning Task
in O
smaller Task
networks Task
. O
This O
is O
fundamentally O
different O
from O
our O
approach O
, O
where O
we O
improve O
learning Task
by O
transferring O
knowledge O
between O
different O
parts O
of O
the O
same O
network O
, O
in O
a O
self O
contained O
manner O
. O
The O
work O
we O
present O
in O
this O
paper O
is O
based O
on O
a O
report O
which O
was O
made O
public O
in O
[ O
reference O
] O
. O
We O
have O
recently O
come O
across O
a O
concurrent O
preprint O
[ O
reference O
] O
where O
the O
authors O
reuse O
the O
word O
embedding O
matrix O
in O
the O
output Method
projection Method
to O
improve O
language Task
modeling Task
. O
However O
, O
their O
work O
is O
purely O
empirical O
, O
and O
they O
do O
not O
provide O
any O
theoretical O
justification O
for O
their O
approach O
. O
Finally O
, O
we O
would O
like O
to O
note O
that O
the O
idea O
of O
using O
the O
same O
representation O
for O
input O
and O
output O
words O
has O
been O
explored O
in O
the O
past O
, O
and O
there O
exists O
language Method
models Method
which O
could O
be O
interpreted O
as O
simple O
neural Method
networks Method
with O
shared O
input O
and O
output O
embeddings O
[ O
reference O
][ O
reference O
] O
. O
However O
, O
shared O
input O
and O
output O
representations O
were O
implicitly O
built O
into O
these O
models O
, O
rather O
than O
proposed O
as O
a O
supplement O
to O
a O
baseline O
. O
Consequently O
, O
possibility O
of O
improvement O
was O
not O
particularly O
pursued O
by O
sharing O
input O
and O
output O
representations O
. O
section O
: O
EXPERIMENTS O
In O
our O
experiments O
, O
we O
use O
the O
Penn Material
Treebank Material
corpus Material
( O
PTB Material
) O
[ O
reference O
] O
, O
and O
the O
Wikitext Material
- Material
2 Material
dataset O
[ O
reference O
] O
. O
PTB Material
has O
been O
a O
standard O
dataset O
used O
for O
benchmarking O
language Method
models Method
. O
It O
consists O
of O
923k O
training O
, O
73k O
validation O
, O
and O
82k O
test O
words O
. O
The O
version O
of O
this O
dataset O
which O
we O
use O
is O
the O
one O
processed O
in O
[ O
reference O
] O
, O
with O
the O
most O
frequent O
10k O
words O
selected O
to O
be O
in O
the O
vocabulary O
and O
rest O
replaced O
with O
a O
an O
< O
unk O
> O
token O
2 O
. O
Wikitext Material
- Material
2 Material
is O
a O
dataset O
released O
recently O
as O
an O
alternative O
to O
PTB Material
3 Material
. O
It O
contains O
2 O
, O
088k O
training O
, O
217k O
validation O
, O
and O
245k O
test O
tokens O
, O
and O
has O
a O
vocabulary O
of O
33 O
, O
278 O
words O
; O
therefore O
, O
in O
comparison O
to O
PTB Material
, O
it O
is O
roughly O
2 O
times O
larger O
in O
dataset O
size O
, O
and O
3 O
times O
larger O
in O
vocabulary O
. O
section O
: O
MODEL O
AND O
TRAINING O
HIGHLIGHTS O
We O
closely O
follow O
the O
LSTM Method
based O
language O
model O
proposed O
in O
[ O
reference O
] O
for O
constructing O
our O
baseline O
model O
. O
Specifically O
, O
we O
use O
a O
2 O
- O
layer O
LSTM Method
with O
the O
same O
number O
of O
hidden O
units O
in O
each O
layer O
, O
and O
we O
use O
3 O
different O
network O
sizes O
: O
small O
( O
200 O
units O
) O
, O
medium O
( O
650 O
units O
) O
, O
and O
large O
( O
1500 O
units O
) O
. O
We O
train O
our O
models O
using O
stochastic Method
gradient Method
descent Method
, O
and O
we O
use O
a O
variant O
of O
the O
dropout Method
method Method
proposed O
in O
Gal O
( O
2015 O
) O
. O
We O
defer O
further O
details O
regarding O
training O
the O
models O
to O
section O
A O
of O
the O
appendix O
. O
We O
refer O
to O
our O
baseline O
network O
as O
variational Method
dropout Method
LSTM Method
, O
or O
VD Method
- Method
LSTM Method
in O
short O
. O
section O
: O
EMPIRICAL O
VALIDATION O
FOR O
THE O
THEORY O
OF O
REUSING Task
WORD Task
EMBEDDINGS Task
In O
Section O
4 O
, O
we O
showed O
that O
the O
particular O
loss Method
augmentation Method
scheme Method
we O
choose O
constrains O
the O
output O
projection O
matrix O
to O
be O
close O
to O
the O
input O
embedding O
matrix O
, O
without O
explicitly O
doing O
so O
by O
reusing O
the O
input O
embedding O
matrix O
. O
As O
a O
first O
experiment O
, O
we O
set O
out O
to O
validate O
this O
theoretical O
result O
. O
To O
do O
this O
, O
we O
try O
to O
simulate O
the O
setting O
in O
Section O
4 O
by O
doing O
the O
following O
: O
We O
select O
a O
randomly O
chosen O
20 O
, O
000 O
contiguous O
word O
sequence O
in O
the O
PTB Material
training O
set O
, O
and O
train O
a O
2 O
- O
layer O
LSTM Method
language O
model O
with O
300 O
units O
in O
each O
layer O
with O
loss Method
augmentation Method
by O
minimizing O
the O
following O
loss Metric
: O
Here O
, O
β O
is O
the O
proportion O
of O
the O
augmented Method
loss Method
used O
in O
the O
total O
loss O
, O
and O
J O
aug O
is O
scaled O
by O
τ O
2 O
|V O
| O
to O
approximately O
match O
the O
magnitudes O
of O
the O
derivatives O
of O
J O
and O
J O
aug O
( O
see O
( O
4.3 O
) O
) O
. O
Since O
we O
aim O
to O
achieve O
the O
minimum O
training Metric
loss Metric
possible O
, O
and O
the O
goal O
is O
to O
show O
a O
particular O
result O
rather O
than O
to O
achieve O
good O
generalization Task
, O
we O
do O
not O
use O
any O
kind O
of O
regularization Method
in O
the O
neural Method
network Method
( O
e.g. O
weight O
decay O
, O
dropout Method
) O
. O
For O
this O
set O
of O
experiments O
, O
we O
also O
constrain O
each O
row O
of O
the O
input O
embedding O
matrix O
to O
have O
a O
norm O
of O
1 O
because O
training Task
becomes O
difficult O
without O
this O
constraint O
when O
only O
augmented Method
loss Method
is O
used O
. O
After O
training Task
, O
we O
compute O
a O
metric O
that O
measures O
distance Metric
between O
the O
subspace O
spanned O
by O
the O
rows O
of O
the O
input O
embedding O
matrix O
, O
L O
, O
and O
that O
spanned O
by O
the O
columns O
of O
the O
output O
projection O
matrix O
, O
W O
. O
For O
this O
, O
we O
use O
a O
common O
metric O
based O
on O
the O
relative O
residual O
norm O
from O
projection O
of O
one O
matrix O
onto O
another O
[ O
reference O
] O
. O
The O
computed O
distance O
between O
the O
subspaces O
is O
1 O
when O
they O
are O
orthogonal O
, O
and O
0 O
when O
they O
are O
the O
same O
. O
Interested O
reader O
may O
refer O
to O
section O
B O
in O
the O
appendix O
for O
the O
details O
of O
this O
metric O
. O
Figure O
1 O
shows O
the O
results O
from O
two O
tests O
. O
In O
one O
( O
panel O
a O
) O
, O
we O
test O
the O
effect O
of O
using O
the O
augmented Method
loss Method
by O
sweeping O
β O
in O
( O
6.1 O
) O
from O
0 O
to O
1 O
at O
a O
reasonably O
high O
temperature O
( O
τ O
= O
10 O
) O
. O
With O
no O
loss Method
augmentation Method
( O
β O
= O
0 O
) O
, O
the O
distance O
is O
almost O
1 O
, O
and O
as O
more O
and O
more O
augmented Method
loss Method
is O
used O
the O
distance O
decreases O
rapidly O
, O
and O
eventually O
reaches O
around O
0.06 O
when O
only O
augmented Method
loss Method
is O
used O
. O
In O
the O
second O
test O
( O
panel O
b O
) O
, O
we O
set O
β O
= O
1 O
, O
and O
try O
to O
see O
the O
effect O
of O
the O
temperature O
on O
the O
subspace O
distance O
( O
remember O
the O
theory O
predicts O
low O
distance O
when O
τ O
→ O
∞ O
) O
. O
Notably O
, O
the O
augmented Method
loss Method
causes O
W O
to O
approach O
L Method
T Method
sufficiently O
even O
at O
temperatures O
as O
low O
as O
2 O
, O
although O
higher O
temperatures O
still O
lead O
to O
smaller O
subspace O
distances O
. O
These O
results O
confirm O
the O
mechanism O
through O
which O
our O
proposed O
loss Method
pushes O
W O
to O
learn O
the O
same O
column O
space O
as O
L Method
T Method
, O
and O
it O
suggests O
that O
reusing O
the O
input O
embedding O
matrix O
by O
explicitly O
constraining O
W O
= O
L Method
T Method
is O
not O
simply O
a O
kind O
of O
regularization Method
, O
but O
is O
in O
fact O
an O
optimal O
choice O
in O
our O
framework O
. O
What O
can O
be O
achieved O
separately O
with O
each O
of O
the O
two O
proposed O
improvements O
as O
well O
as O
with O
the O
two O
of O
them O
combined O
is O
a O
question O
of O
empirical O
nature O
, O
which O
we O
investigate O
in O
the O
next O
section O
. O
section O
: O
RESULTS O
ON O
PTB Material
AND O
WIKITEXT Material
- Material
2 Material
DATASETS O
In O
order O
to O
investigate O
the O
extent O
to O
which O
each O
of O
our O
proposed O
improvements O
helps O
with O
learning Task
, O
we O
train O
4 O
different O
models O
for O
each O
network O
size O
: O
( O
1 O
) O
2 Method
- Method
Layer Method
LSTM Method
with O
variational Method
dropout Method
( O
VD Method
- Method
LSTM Method
) O
( O
2 O
) O
2 Method
- Method
Layer Method
LSTM Method
with O
variational Method
dropout Method
and O
augmented Method
loss Method
( O
VD O
- O
LSTM Method
+ O
AL Method
) O
( O
3 O
) O
2 Method
- Method
Layer Method
LSTM Method
with O
variational Method
dropout Method
and O
reused Method
embeddings Method
( O
VD O
- O
LSTM Method
+ O
RE Method
) O
( O
4 O
) O
2 Method
- Method
Layer Method
LSTM Method
with O
variational Method
dropout Method
and O
both O
RE Method
and O
AL Method
( O
VD O
- O
LSTM Method
+ O
REAL O
) O
. O
Figure O
2 O
shows O
the O
validation Metric
perplexities Metric
of O
the O
four O
models O
during O
training O
on O
the O
PTB Material
corpus O
for O
small O
( O
panel O
a O
) O
and O
large O
( O
panel O
b O
) O
networks O
. O
All O
of O
AL Method
, O
RE Method
, O
and O
REAL Method
networks Method
significantly O
outperform O
the O
baseline O
in O
both O
cases O
. O
Table O
1 O
compares O
the O
final O
validation Metric
and Metric
test Metric
perplexities Metric
of O
the O
four O
models O
on O
both O
PTB Material
and O
Wikitext Material
- Material
2 Material
for O
each O
network O
size O
. O
In O
both O
datasets O
, O
both O
AL Method
and O
RE Method
improve O
upon O
the O
baseline O
individually O
, O
and O
using O
RE Method
and O
AL Method
together O
leads O
to O
the O
best O
performance O
. O
Based O
on O
performance O
comparisons O
, O
we O
make O
the O
following O
notes O
on O
the O
two O
proposed O
improvements O
: O
• O
AL Method
provides O
better O
performance O
gains O
for O
smaller O
networks O
. O
This O
is O
not O
surprising O
given O
the O
fact O
that O
small Method
models Method
are O
rather O
inflexible O
, O
and O
one O
would O
expect O
to O
see O
improved O
learning Task
by O
training O
against O
a O
more O
informative O
data O
distribution O
( O
contributed O
by O
the O
augmented Method
loss Method
) O
( O
see O
[ O
reference O
] O
) O
. O
For O
the O
smaller O
PTB Material
dataset O
, O
performance O
with O
AL Method
surpasses O
that O
with O
RE Method
. O
In O
comparison O
, O
for O
the O
larger O
Wikitext Material
- Material
2 Material
dataset O
, O
improvement O
by O
AL Method
is O
more O
limited O
. O
This O
is O
expected O
given O
larger O
training O
sets O
better O
represent O
the O
true O
data O
distribution O
, O
mitigating O
the O
supervision Task
problem Task
. O
In O
fact O
, O
we O
set O
out O
to O
validate O
this O
reasoning O
in O
a O
direct O
manner O
, O
and O
additionally O
train O
the O
small Method
networks Method
separately O
on O
the O
first O
and O
second O
halves O
of O
the O
Wikitext Material
- Material
2 Material
training O
set O
. O
This O
results O
in O
two O
distinct O
datasets O
which O
are O
each O
about O
the O
same O
size O
as O
PTB Material
( O
1044 O
K O
vs O
929 O
K O
) O
. O
As O
can O
be O
seen O
in O
Table O
2 O
has O
significantly O
improved O
competitive O
performance O
against O
RE Method
and O
REAL O
despite O
the O
fact O
that O
embedding Metric
size Metric
is O
3 O
times O
larger O
compared O
to O
PTB Material
. O
These O
results O
support O
our O
argument O
that O
the O
proposed O
augmented Method
loss Method
term O
acts O
to O
improve O
the O
amount O
of O
information O
gathered O
from O
the O
dataset O
. O
• O
RE Method
significantly O
outperforms O
AL Method
for O
larger O
networks O
. O
This O
indicates O
that O
, O
for O
large O
models O
, O
the O
more O
effective O
mechanism O
of O
our O
proposed O
framework O
is O
the O
one O
which O
enforces O
proximity O
between O
the O
output O
projection O
space O
and O
the O
input O
embedding O
space O
. O
From O
a O
model Metric
complexity Metric
perspective Metric
, O
the O
nontrivial O
gains O
offered O
by O
RE Method
for O
all O
network O
sizes O
and O
for O
both O
datasets O
could O
be O
largely O
attributed O
to O
its O
explicit O
function O
to O
reduce O
the O
model O
size O
while O
preserving O
the O
representational O
power O
according O
to O
our O
framework O
. O
We O
list O
in O
Table O
3 O
the O
comparison O
of O
models O
with O
and O
without O
our O
proposed O
modifications O
on O
the O
Penn Material
Treebank Material
Corpus O
. O
The O
best O
LSTM Method
model O
( O
VD O
- O
LSTM Method
+ O
REAL O
) O
outperforms O
all O
previous O
work O
which O
uses O
conventional O
framework O
, O
including O
large Method
ensembles Method
. O
The O
recently O
proposed O
recurrent Method
highway Method
networks Method
[ O
reference O
] O
when O
trained O
with O
reused Method
embeddings Method
( O
VD O
- O
RHN O
+ O
RE Method
) O
achieves O
the O
best O
overall O
performance O
, O
improving O
on O
VD Method
- Method
RHN Method
by O
a O
perplexity Metric
of O
2.5 O
. O
Table O
2 O
: O
Performance O
of O
the O
four O
different O
small Method
models Method
trained O
on O
the O
equally O
sized O
two O
partitions O
of O
Wikitext Material
- Material
2 Material
training O
set O
. O
These O
results O
are O
consistent O
with O
those O
on O
PTB Material
( O
see O
Table O
1 O
) O
, O
which O
has O
a O
similar O
training O
set O
size O
with O
each O
of O
these O
partitions O
, O
although O
its O
word Metric
embedding Metric
dimension Metric
is O
three O
times O
smaller O
. O
section O
: O
QUALITATIVE O
RESULTS O
One O
important O
feature O
of O
our O
framework O
that O
leads O
to O
better O
word Task
predictions Task
is O
the O
explicit O
mechanism O
to O
assign O
probabilities O
to O
words O
not O
merely O
according O
to O
the O
observed O
output O
statistics O
, O
but O
also O
considering O
the O
metric O
similarity O
between O
words O
. O
We O
observe O
direct O
consequences O
of O
this O
mechanism O
qualitatively O
in O
the O
Penn Material
Treebank Material
in O
different O
ways O
: O
First O
, O
we O
notice O
that O
the O
probability O
of O
generating O
the O
< O
unk O
> O
token O
with O
our O
proposed O
network O
( O
VD O
- O
LSTM Method
+ O
REAL O
) O
is O
significantly O
lower O
compared O
to O
the O
baseline Method
network Method
( O
VD Method
- Method
LSTM Method
) O
across O
many O
words O
. O
This O
could O
be O
explained O
by O
noting O
the O
fact O
that O
the O
< O
unk O
> O
token O
is O
an O
aggregated O
token O
rather O
than O
a O
specific O
word O
, O
and O
it O
is O
often O
not O
expected O
to O
be O
close O
to O
specific O
words O
in O
the O
word O
embedding O
space O
. O
We O
observe O
the O
same O
behavior O
with O
very O
frequent O
words O
such O
as O
" O
a O
" O
, O
" O
an O
" O
, O
and O
" O
the O
" O
, O
owing O
to O
the O
same O
fact O
that O
they O
are O
not O
correlated O
with O
particular O
words O
. O
Second O
, O
we O
not O
only O
observe O
better O
probability O
assignments O
for O
the O
target O
words O
, O
but O
we O
also O
observe O
relatively O
higher O
probability O
weights O
associated O
with O
the O
words O
close O
to O
the O
targets O
. O
Sometimes O
this O
happens O
in O
the O
form O
of O
predicting O
words O
semantically O
close O
together O
which O
are O
plausible O
even O
when O
the O
target O
word O
is O
not O
successfully O
captured O
by O
the O
model O
. O
We O
provide O
a O
few O
examples O
from O
the O
PTB Material
test O
set O
which O
compare O
the O
prediction Task
performance O
of O
1500 O
unit O
VD O
- O
LSTM Method
and O
1500 O
unit O
VD O
- O
LSTM Method
+ O
REAL O
in O
table O
4 O
. O
We O
would O
like O
to O
note O
that O
prediction Task
performance O
of O
VD O
- O
LSTM Method
+ O
RE Method
is O
similar O
to O
VD O
- O
LSTM Method
+ O
REAL O
for O
the O
large Task
network Task
. O
In O
this O
work O
, O
we O
introduced O
a O
novel O
loss Method
framework Method
for O
language Task
modeling Task
. O
Particularly O
, O
we O
showed O
that O
the O
metric O
encoded O
into O
the O
space O
of O
word O
embeddings O
could O
be O
used O
to O
generate O
a O
more O
informed O
data O
distribution O
than O
the O
one O
- O
hot O
targets O
, O
and O
that O
additionally O
training O
against O
this O
distribution O
improves O
learning Task
. O
We O
also O
showed O
theoretically O
that O
this O
approach O
lends O
itself O
to O
a O
second O
improvement O
, O
which O
is O
simply O
reusing O
the O
input O
embedding O
matrix O
in O
the O
output Method
projection Method
layer Method
. O
This O
has O
an O
additional O
benefit O
of O
reducing O
the O
number O
of O
trainable O
variables O
in O
the O
model O
. O
We O
empirically O
validated O
the O
theoretical O
link O
, O
and O
verified O
that O
both O
proposed O
changes O
do O
in O
fact O
belong O
to O
the O
same O
framework O
. O
In O
our O
experiments O
on O
the O
Penn Material
Treebank Material
corpus Material
and O
Wikitext Material
- Material
2 Material
, O
we O
showed O
that O
our O
framework O
outperforms O
the O
conventional O
one O
, O
and O
that O
even O
the O
simple O
modification O
of O
reusing O
the O
word O
embedding O
in O
the O
output Method
projection Method
layer Method
is O
sufficient O
for O
large Task
networks Task
. O
The O
improvements O
achieved O
by O
our O
framework O
are O
not O
unique O
to O
vanilla O
language Task
modeling Task
, O
and O
are O
readily O
applicable O
to O
other O
tasks O
which O
utilize O
language Method
models Method
such O
as O
neural Task
machine Task
translation Task
, O
speech Task
recognition Task
, O
and O
text Task
summarization Task
. O
This O
could O
lead O
to O
significant O
improvements O
in O
such O
models O
especially O
with O
large O
vocabularies O
, O
with O
the O
additional O
benefit O
of O
greatly O
reducing O
the O
number O
of O
parameters O
to O
be O
trained O
. O
section O
: O
APPENDIX O
A O
MODEL O
AND O
TRAINING O
DETAILS O
We O
begin O
training O
with O
a O
learning Metric
rate Metric
of O
1 O
and O
start O
decaying O
it O
with O
a O
constant O
rate O
after O
a O
certain O
epoch O
. O
This O
is O
5 O
, O
10 O
, O
and O
1 O
for O
the O
small Task
, Task
medium Task
, Task
and Task
large Task
networks Task
respectively O
. O
The O
decay Metric
rate Metric
is O
0.9 O
for O
the O
small Task
and Task
medium Task
networks Task
, O
and O
0.97 O
for O
the O
large O
network O
. O
For O
both O
PTB Material
and O
Wikitext Material
- Material
2 Material
datasets Material
, O
we O
unroll O
the O
network O
for O
35 O
steps O
for O
backpropagation Method
. O
We O
use O
gradient Method
clipping Method
[ O
reference O
] O
; O
i.e. O
we O
rescale O
the O
gradients O
using O
the O
global O
norm O
if O
it O
exceeds O
a O
certain O
value O
. O
For O
both O
datasets O
, O
this O
is O
5 O
for O
the O
small Task
and Task
the Task
medium Task
network Task
, O
and O
6 O
for O
the O
large O
network O
. O
We O
use O
the O
dropout Method
method Method
introduced O
in O
Gal O
( O
2015 O
) O
; O
particularly O
, O
we O
use O
the O
same O
dropout O
mask O
for O
each O
example O
through O
the O
unrolled Method
network Method
. O
Differently O
from O
what O
was O
proposed O
in O
Gal O
( O
2015 O
) O
, O
we O
tie O
the O
dropout O
weights O
for O
hidden O
states O
further O
, O
and O
we O
use O
the O
same O
mask O
when O
they O
are O
propagated O
as O
states O
in O
the O
current O
layer O
and O
when O
they O
are O
used O
as O
inputs O
for O
the O
next O
layer O
. O
We O
do O
n't O
use O
dropout O
in O
the O
input Method
embedding Method
layer Method
, O
and O
we O
use O
the O
same O
dropout O
probability O
for O
inputs O
and O
hidden O
states O
. O
For O
PTB Material
, O
dropout O
probabilities O
are O
0.7 O
, O
0.5 O
and O
0.35 O
for O
small O
, O
medium O
and O
large O
networks O
respectively O
. O
For O
Wikitext Material
- Material
2 Material
, O
probabilities O
are O
0.8 O
for O
the O
small O
and O
0.6 O
for O
the O
medium O
networks O
. O
When O
training O
the O
networks O
with O
the O
augmented Method
loss Method
( O
AL Method
) O
, O
we O
use O
a O
temperature O
τ O
= O
20 O
. O
We O
have O
empirically O
observed O
that O
setting O
α O
, O
the O
weight O
of O
the O
augmented Method
loss Method
, O
according O
to O
α O
= O
γτ O
for O
all O
the O
networks O
works O
satisfactorily O
. O
We O
set O
γ O
to O
values O
between O
0.5 O
and O
0.8 O
for O
the O
PTB Material
dataset O
, O
and O
between O
1.0 O
and O
1.5 O
for O
the O
Wikitext Material
- Material
2 Material
dataset O
. O
We O
would O
like O
to O
note O
that O
we O
have O
not O
observed O
sudden O
deteriorations O
in O
the O
performance O
with O
respect O
to O
moderate O
variations O
in O
either O
τ O
or O
α O
. O
section O
: O
B O
METRIC O
FOR O
CALCULATING Task
SUBSPACE Task
DISTANCES Task
In O
this O
section O
, O
we O
detail O
the O
metric O
used O
for O
computing O
the O
subspace O
distance O
between O
two O
matrices O
. O
The O
computed O
metric O
is O
closely O
related O
with O
the O
principle O
angles O
between O
subspaces O
, O
first O
defined O
in O
Jordan O
( O
1875 O
) O
. O
Our O
aim O
is O
to O
compute O
a O
metric O
distance O
between O
two O
given O
matrices O
, O
X O
and O
Y O
. O
We O
do O
this O
in O
three O
steps O
: O
( O
1 O
) O
Obtain O
two O
matrices O
with O
orthonormal O
columns O
, O
U O
and O
V O
, O
such O
that O
span O
( O
U O
) O
= O
span O
( O
X O
) O
and O
span O
( O
V O
) O
= O
span O
( O
Y O
) O
. O
U O
and O
V O
could O
be O
obtained O
with O
a O
QR Method
decomposition Method
. O
( O
2 O
) O
Calculate O
the O
projection O
of O
either O
one O
of O
U O
and O
V O
onto O
the O
other O
; O
e.g. O
do O
S O
= O
U O
U O
T O
V O
, O
where O
S O
is O
the O
projection O
of O
V O
onto O
U O
. O
Then O
calculate O
the O
residual O
matrix O
as O
R O
= O
V O
− O
S. O
We O
note O
that O
d O
as O
calculated O
above O
is O
a O
valid O
metric O
up O
to O
the O
equivalence O
set O
of O
matrices O
which O
span O
the O
same O
column O
space O
, O
although O
we O
are O
not O
going O
to O
show O
it O
. O
Instead O
, O
we O
will O
mention O
some O
metric O
properties O
of O
d O
, O
and O
relate O
it O
to O
the O
principal O
angles O
between O
the O
subspaces O
. O
We O
first O
work O
out O
an O
expression O
for O
d O
: O
where O
ρ O
i O
is O
the O
i O
th O
singular O
value O
of O
U O
T O
V O
, O
commonly O
referred O
to O
as O
the O
i O
th O
principle O
angle O
between O
the O
subspaces O
of O
X O
and O
Y O
, O
θ O
i O
. O
In O
above O
, O
we O
used O
the O
cyclic O
permutation O
property O
of O
the O
trace O
in O
the O
third O
and O
the O
fourth O
lines O
. O
section O
: O
Since O
d O
2 O
is O
1 O
C O
Trace O
( O
R O
T O
R O
) O
, O
it O
is O
always O
nonnegative O
, O
and O
it O
is O
only O
zero O
when O
the O
residual O
is O
zero O
, O
which O
is O
the O
case O
when O
span O
( O
X O
) O
= O
span O
( O
Y O
) O
. O
Further O
, O
it O
is O
symmetric O
between O
U O
and O
V O
due O
to O
the O
form O
of O
( O
B.1 O
) O
( O
singular O
values O
of O
V O
T O
U O
and O
V O
T O
U O
are O
the O
same O
) O
. O
Also O
, O
, O
namely O
the O
average O
of O
the O
sines O
of O
the O
principle O
angles O
, O
which O
is O
a O
quantity O
between O
0 O
and O
1 O
. O
section O
: O
