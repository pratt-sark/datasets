document	O
:	O
Towards	O
a	O
Neural	Method
Statistician	Method
An	O
efficient	O
learner	Method
is	O
one	O
who	O
reuses	O
what	O
they	O
already	O
know	O
to	O
tackle	O
a	O
new	O
problem	O
.	O
For	O
a	O
machine	Method
learner	Method
,	O
this	O
means	O
understanding	O
the	O
similarities	O
amongst	O
datasets	O
.	O
In	O
order	O
to	O
do	O
this	O
,	O
one	O
must	O
take	O
seriously	O
the	O
idea	O
of	O
working	O
with	O
datasets	O
,	O
rather	O
than	O
datapoints	O
,	O
as	O
the	O
key	O
objects	O
to	O
model	O
.	O
Towards	O
this	O
goal	O
,	O
we	O
demonstrate	O
an	O
extension	O
of	O
a	O
variational	Method
autoencoder	Method
that	O
can	O
learn	O
a	O
method	O
for	O
computing	Task
representations	Task
,	O
or	O
statistics	Task
,	Task
of	Task
datasets	Task
in	O
an	O
unsupervised	Method
fashion	Method
.	O
The	O
network	O
is	O
trained	O
to	O
produce	O
statistics	O
that	O
encapsulate	O
a	O
generative	Method
model	Method
for	O
each	O
dataset	O
.	O
Hence	O
the	O
network	O
enables	O
efficient	O
learning	Task
from	O
new	O
datasets	O
for	O
both	O
unsupervised	Task
and	Task
supervised	Task
tasks	Task
.	O
We	O
show	O
that	O
we	O
are	O
able	O
to	O
learn	O
statistics	O
that	O
can	O
be	O
used	O
for	O
:	O
clustering	Task
datasets	Task
,	O
transferring	O
generative	Method
models	Method
to	O
new	O
datasets	O
,	O
selecting	O
representative	O
samples	O
of	O
datasets	O
and	O
classifying	O
previously	O
unseen	O
classes	O
.	O
We	O
refer	O
to	O
our	O
model	O
as	O
a	O
neural	Method
statistician	Method
,	O
and	O
by	O
this	O
we	O
mean	O
a	O
neural	Method
network	Method
that	O
can	O
learn	O
to	O
compute	O
summary	O
statistics	O
of	O
datasets	O
without	O
supervision	O
.	O
section	O
:	O
Introduction	O
The	O
machine	Task
learning	Task
community	Task
is	O
well	O
-	O
practised	O
at	O
learning	Task
representations	Task
of	Task
data	Task
-	Task
points	Task
and	Task
sequences	Task
.	O
A	O
middle	O
-	O
ground	O
between	O
these	O
two	O
is	O
representing	O
,	O
or	O
summarizing	Task
,	Task
datasets	Task
-	Task
unordered	Task
collections	Task
of	Task
vectors	Task
,	O
such	O
as	O
photos	O
of	O
a	O
particular	O
person	O
,	O
recordings	O
of	O
a	O
given	O
speaker	O
or	O
a	O
document	O
as	O
a	O
bag	O
-	O
of	O
-	O
words	O
.	O
Where	O
these	O
sets	O
take	O
the	O
form	O
of	O
i.i.d	O
samples	O
from	O
some	O
distribution	O
,	O
such	O
summaries	O
are	O
called	O
statistics	Method
.	O
We	O
explore	O
the	O
idea	O
of	O
using	O
neural	Method
networks	Method
to	O
learn	O
statistics	O
and	O
we	O
refer	O
to	O
our	O
approach	O
as	O
a	O
neural	Method
statistician	Method
.	O
The	O
key	O
result	O
of	O
our	O
approach	O
is	O
a	O
statistic	Method
network	Method
that	O
takes	O
as	O
input	O
a	O
set	O
of	O
vectors	O
and	O
outputs	O
a	O
vector	O
of	O
summary	O
statistics	O
specifying	O
a	O
generative	Method
model	Method
of	O
that	O
set	O
-	O
a	O
mean	O
and	O
variance	O
specifying	O
a	O
Gaussian	Method
distribution	Method
in	O
a	O
latent	O
space	O
we	O
term	O
the	O
context	O
.	O
The	O
advantages	O
of	O
our	O
approach	O
are	O
that	O
it	O
is	O
:	O
Unsupervised	Method
:	O
It	O
provides	O
principled	O
and	O
unsupervised	O
way	O
to	O
learn	O
summary	O
statistics	O
as	O
the	O
output	O
of	O
a	O
variational	Method
encoder	Method
of	O
a	O
generative	Method
model	Method
.	O
Data	O
efficient	O
:	O
If	O
one	O
has	O
a	O
large	O
number	O
of	O
small	O
but	O
related	O
datasets	O
,	O
modelling	O
the	O
datasets	O
jointly	O
enables	O
us	O
to	O
gain	O
statistical	O
strength	O
.	O
Parameter	O
Efficient	O
:	O
By	O
using	O
summary	O
statistics	O
instead	O
of	O
say	O
categorical	O
labellings	O
of	O
each	O
dataset	O
,	O
we	O
decouple	O
the	O
number	O
of	O
parameters	O
of	O
the	O
model	O
from	O
the	O
number	O
of	O
datasets	O
.	O
Capable	O
of	O
few	Method
-	Method
shot	Method
learning	Method
:	O
If	O
the	O
datasets	O
correspond	O
to	O
examples	O
from	O
different	O
classes	O
,	O
class	O
embeddings	O
(	O
summary	O
statistics	O
associated	O
with	O
examples	O
from	O
a	O
class	O
)	O
,	O
allow	O
us	O
to	O
handle	O
new	O
classes	O
at	O
test	O
time	O
.	O
section	O
:	O
Problem	O
Statement	O
We	O
are	O
given	O
datasets	O
for	O
.	O
Each	O
dataset	O
consists	O
of	O
a	O
number	O
of	O
i.i.d	O
samples	O
from	O
an	O
associated	O
distribution	O
over	O
.	O
The	O
task	O
can	O
be	O
split	O
into	O
learning	Method
and	Method
inference	Method
components	Method
.	O
The	O
learning	Method
component	Method
is	O
to	O
produce	O
a	O
generative	Method
model	Method
for	O
each	O
dataset	O
.	O
We	O
assume	O
there	O
is	O
a	O
common	O
underlying	O
generative	Method
process	Method
such	O
that	O
for	O
drawn	O
from	O
.	O
We	O
refer	O
to	O
as	O
the	O
context	O
.	O
The	O
inference	Method
component	Method
is	O
to	O
give	O
an	O
approximate	O
posterior	O
over	O
the	O
context	O
for	O
a	O
given	O
dataset	O
produced	O
by	O
a	O
statistic	Method
network	Method
.	O
section	O
:	O
Neural	Method
Statistician	Method
In	O
order	O
to	O
exploit	O
the	O
assumption	O
of	O
a	O
hierarchical	Method
generative	Method
process	Method
over	O
datasets	O
we	O
will	O
use	O
a	O
‘	O
parameter	Method
-	Method
transfer	Method
approach	Method
’	O
[	O
see	O
][]	O
transfer_survey	O
to	O
extend	O
the	O
variational	Method
autoencoder	Method
model	Method
of	O
variational_autoencoder	Method
.	O
figure	O
Left	O
:	O
basic	O
hierarchical	Method
model	Method
,	O
where	O
the	O
plate	O
encodes	O
the	O
fact	O
that	O
the	O
context	O
variable	O
is	O
shared	O
across	O
each	O
item	O
in	O
a	O
given	O
dataset	O
.	O
Center	O
:	O
full	Method
neural	Method
statistician	Method
model	Method
with	O
three	O
latent	Method
layers	Method
.	O
Each	O
collection	O
of	O
incoming	O
edges	O
to	O
a	O
node	O
is	O
implemented	O
as	O
a	O
neural	Method
network	Method
,	O
the	O
input	O
of	O
which	O
is	O
the	O
concatenation	O
of	O
the	O
edges	O
’	O
sources	O
,	O
the	O
output	O
of	O
which	O
is	O
a	O
parameterization	O
of	O
a	O
distribution	O
over	O
the	O
random	O
variable	O
represented	O
by	O
that	O
node	O
.	O
Right	O
:	O
The	O
statistic	Method
network	Method
,	O
which	O
combines	O
the	O
data	O
via	O
an	O
exchangeable	Method
statistic	Method
layer	Method
.	O
subsection	O
:	O
Variational	Method
Autoencoder	Method
The	O
variational	Method
autoencoder	Method
is	O
a	O
latent	Method
variable	Method
model	Method
(	O
often	O
called	O
the	O
decoder	Method
)	O
with	O
parameters	O
.	O
For	O
each	O
observed	O
,	O
a	O
corresponding	O
latent	O
variable	O
is	O
drawn	O
from	O
so	O
that	O
The	O
generative	O
parameters	O
are	O
learned	O
by	O
introducing	O
a	O
recognition	Method
network	Method
(	O
also	O
called	O
an	O
encoder	Method
)	O
with	O
parameters	O
.	O
The	O
recognition	Method
network	Method
gives	O
an	O
approximate	O
posterior	O
over	O
the	O
latent	O
variables	O
that	O
can	O
then	O
be	O
used	O
to	O
give	O
the	O
standard	O
variational	Method
lower	Method
bound	Method
saul_variational	Method
on	O
the	O
single	O
-	O
datum	O
log	O
-	O
likelihood	O
.	O
I.e.	O
,	O
where	O
Likewise	O
the	O
full	O
-	O
data	O
log	O
likelihood	O
is	O
lower	O
bounded	O
by	O
the	O
sum	O
of	O
the	O
terms	O
over	O
the	O
whole	O
dataset	O
.	O
We	O
can	O
then	O
optimize	O
this	O
lower	O
bound	O
with	O
respect	O
to	O
and	O
using	O
the	O
reparameterization	O
trick	O
introduced	O
by	O
variational_autoencoder	Method
and	O
reparam_paper	O
to	O
get	O
a	O
Monte	Method
-	Method
Carlo	Method
estimate	Method
of	Method
the	Method
gradient	Method
.	O
subsection	O
:	O
Basic	O
Model	O
We	O
extend	O
the	O
variational	Method
autoencoder	Method
to	O
the	O
model	O
depicted	O
on	O
the	O
left	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
This	O
includes	O
a	O
latent	O
variable	O
,	O
the	O
context	O
,	O
that	O
varies	O
between	O
different	O
datasets	O
but	O
is	O
constant	O
,	O
a	O
priori	O
,	O
for	O
items	O
within	O
the	O
same	O
dataset	O
.	O
Now	O
,	O
the	O
likelihood	O
of	O
the	O
parameters	O
for	O
one	O
single	O
particular	O
dataset	O
is	O
given	O
by	O
The	O
prior	O
is	O
chosen	O
to	O
be	O
a	O
spherical	Method
Gaussian	Method
with	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O
The	O
conditional	O
is	O
Gaussian	O
with	O
diagonal	O
covariance	O
,	O
where	O
all	O
the	O
mean	O
and	O
variance	O
parameters	O
depend	O
on	O
through	O
a	O
neural	Method
network	Method
.	O
Similarly	O
the	O
observation	Method
model	Method
will	O
be	O
a	O
simple	O
likelihood	Method
function	Method
appropriate	O
to	O
the	O
data	O
modality	O
with	O
dependence	O
on	O
parameterized	O
by	O
a	O
neural	Method
network	Method
.	O
For	O
example	O
,	O
with	O
real	O
valued	O
data	O
,	O
a	O
diagonal	Method
Gaussian	Method
likelihood	Method
could	O
be	O
used	O
where	O
the	O
mean	O
and	O
log	O
variance	O
of	O
are	O
created	O
from	O
via	O
a	O
neural	Method
network	Method
.	O
We	O
use	O
approximate	Method
inference	Method
networks	Method
,	O
,	O
with	O
parameters	O
collected	O
into	O
,	O
to	O
once	O
again	O
enable	O
the	O
calculation	O
and	O
optimization	Task
of	O
a	O
variational	Metric
lower	Metric
bound	Metric
on	O
the	O
log	O
-	O
likelihood	O
.	O
The	O
single	Metric
dataset	Metric
log	Metric
likelihood	Metric
lower	Metric
bound	Metric
is	O
given	O
by	O
As	O
with	O
the	O
generative	Method
distributions	Method
,	O
the	O
likelihood	O
forms	O
for	O
and	O
are	O
diagonal	Method
Gaussian	Method
distributions	Method
,	O
where	O
all	O
the	O
mean	O
and	O
log	O
variance	O
parameters	O
in	O
each	O
distribution	O
are	O
produced	O
by	O
a	O
neural	Method
network	Method
taking	O
the	O
conditioning	O
variables	O
as	O
inputs	O
.	O
Note	O
that	O
accepts	O
as	O
input	O
a	O
dataset	O
and	O
we	O
refer	O
to	O
this	O
as	O
the	O
statistic	Method
network	Method
.	O
We	O
describe	O
this	O
in	O
Subsection	O
[	O
reference	O
]	O
.	O
The	O
full	Method
-	Method
data	Method
variational	Method
bound	Method
is	O
given	O
by	O
summing	O
the	O
variational	Method
bound	Method
for	O
each	O
dataset	O
in	O
our	O
collection	O
of	O
datasets	O
.	O
It	O
is	O
by	O
learning	O
the	O
difference	O
of	O
the	O
within	O
-	O
dataset	O
and	O
between	O
-	O
dataset	O
distributions	O
that	O
we	O
are	O
able	O
to	O
discover	O
an	O
appropriate	O
statistic	Method
network	Method
.	O
subsection	O
:	O
Full	O
Model	O
The	O
basic	O
model	O
works	O
well	O
for	O
modelling	O
simple	O
datasets	O
,	O
but	O
struggles	O
when	O
the	O
datasets	O
have	O
complex	O
internal	O
structure	O
.	O
To	O
increase	O
the	O
sophistication	O
of	O
the	O
model	O
we	O
use	O
multiple	O
stochastic	Method
layers	Method
and	O
introduce	O
skip	Method
-	Method
connections	Method
for	O
both	O
the	O
inference	Method
and	Method
generative	Method
networks	Method
.	O
The	O
generative	Method
model	Method
is	O
shown	O
graphically	O
in	O
Figure	O
[	O
reference	O
]	O
in	O
the	O
center	O
.	O
The	O
probability	O
of	O
a	O
dataset	O
is	O
then	O
given	O
by	O
where	O
the	O
are	O
again	O
Gaussian	Method
distributions	Method
where	O
the	O
mean	O
and	O
log	O
variance	O
are	O
given	O
as	O
the	O
output	O
of	O
neural	Method
networks	Method
.	O
The	O
generative	Method
process	Method
for	O
the	O
full	Method
model	Method
is	O
described	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
The	O
full	O
approximate	O
posterior	O
factorizes	O
analogously	O
as	O
For	O
convenience	O
we	O
give	O
the	O
variational	Metric
lower	Metric
bound	Metric
as	O
sum	O
of	O
a	O
three	O
parts	O
,	O
a	O
reconstruction	Method
term	Method
,	O
a	O
context	O
divergence	O
and	O
a	O
latent	O
divergence	O
:	O
The	O
skip	O
-	O
connections	O
and	O
allow	O
the	O
context	O
to	O
specify	O
a	O
more	O
precise	O
distribution	O
for	O
each	O
latent	O
variable	O
by	O
explaining	O
-	O
away	O
more	O
generic	O
aspects	O
of	O
the	O
dataset	O
at	O
each	O
stochastic	Method
layer	Method
.	O
This	O
architecture	O
was	O
inspired	O
by	O
recent	O
work	O
on	O
probabilistic	Method
ladder	Method
networks	Method
in	O
prob_ladder	Method
.	O
Complementing	O
these	O
are	O
the	O
skip	O
-	O
connections	O
from	O
each	O
latent	O
variable	O
to	O
the	O
observation	O
,	O
the	O
intuition	O
here	O
is	O
that	O
each	O
stochastic	Method
layer	Method
can	O
focus	O
on	O
representing	O
a	O
certain	O
level	O
of	O
abstraction	O
,	O
since	O
its	O
information	O
does	O
not	O
need	O
to	O
be	O
copied	O
into	O
the	O
next	O
layer	O
,	O
a	O
similar	O
approach	O
was	O
used	O
in	O
auxiliary_ladder	O
.	O
Once	O
again	O
,	O
note	O
that	O
we	O
are	O
maximizing	O
the	O
lower	O
bound	O
to	O
the	O
log	O
likelihood	O
over	O
many	O
datasets	O
:	O
we	O
want	O
to	O
maximize	O
the	O
expectation	O
of	O
over	O
all	O
datasets	O
.	O
We	O
do	O
this	O
optimization	Task
using	O
stochastic	Method
gradient	Method
descent	Method
.	O
In	O
contrast	O
to	O
a	O
variational	Method
autoencoder	Method
where	O
a	O
minibatch	Method
would	O
consist	O
of	O
a	O
subsample	O
of	O
datapoints	O
from	O
the	O
dataset	O
,	O
we	O
use	O
minibatches	Method
consisting	O
of	O
a	O
subsample	O
of	O
datasets	O
-	O
tensors	O
of	O
shape	O
(	O
batch	O
size	O
,	O
sample	O
size	O
,	O
number	O
of	O
features	O
)	O
.	O
subsection	O
:	O
Statistic	Method
Network	Method
In	O
addition	O
to	O
the	O
standard	O
inference	Method
networks	Method
we	O
require	O
a	O
statistic	Method
network	Method
to	O
give	O
an	O
approximate	O
posterior	O
over	O
the	O
context	O
given	O
a	O
dataset	O
.	O
This	O
inference	Method
network	Method
must	O
capture	O
the	O
exchangeability	O
of	O
the	O
data	O
in	O
.	O
We	O
use	O
a	O
feedforward	Method
neural	Method
network	Method
consisting	O
of	O
three	O
main	O
elements	O
:	O
An	O
instance	Method
encoder	Method
that	O
takes	O
each	O
individual	O
datapoint	O
to	O
a	O
vector	O
.	O
An	O
exchangeable	Method
instance	Method
pooling	Method
layer	Method
that	O
collapses	O
the	O
matrix	O
to	O
a	O
single	O
pre	O
-	O
statistic	O
vector	O
.	O
Examples	O
include	O
elementwise	O
means	O
,	O
sums	O
,	O
products	O
,	O
geometric	Method
means	Method
and	O
maximum	Method
.	O
We	O
use	O
the	O
sample	O
mean	O
for	O
all	O
experiments	O
.	O
A	O
final	O
post	Method
-	Method
pooling	Method
network	Method
that	O
takes	O
to	O
a	O
parameterization	O
of	O
a	O
diagonal	Method
Gaussian	Method
.	O
The	O
graphical	Method
model	Method
for	O
this	O
is	O
given	O
at	O
the	O
right	O
of	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
note	O
that	O
the	O
humble	O
sample	O
mean	O
already	O
gives	O
the	O
statistic	Method
network	Method
a	O
great	O
deal	O
of	O
representational	O
power	O
due	O
to	O
the	O
fact	O
that	O
the	O
instance	Method
encoder	Method
can	O
learn	O
a	O
representation	O
where	O
averaging	Task
makes	O
sense	O
.	O
For	O
example	O
since	O
the	O
instance	Method
encoder	Method
can	O
approximate	O
a	O
polynomial	O
on	O
a	O
compact	O
domain	O
,	O
and	O
so	O
can	O
the	O
post	Method
-	Method
pooling	Method
network	Method
,	O
a	O
statistic	Method
network	Method
can	O
approximate	O
any	O
moment	O
of	O
a	O
distribution	O
.	O
section	O
:	O
Related	O
Work	O
Due	O
to	O
the	O
general	O
nature	O
of	O
the	O
problem	O
considered	O
,	O
our	O
work	O
touches	O
on	O
many	O
different	O
topics	O
which	O
we	O
now	O
attempt	O
to	O
summarize	O
.	O
paragraph	O
:	O
Topic	Method
models	Method
and	O
graphical	Method
models	Method
The	O
form	O
of	O
the	O
graphical	Method
model	Method
in	O
Figure	O
[	O
reference	O
]	O
on	O
the	O
left	O
is	O
equivalent	O
to	O
that	O
of	O
a	O
standard	O
topic	Method
model	Method
.	O
In	O
contrast	O
to	O
traditional	O
topic	Method
models	Method
we	O
do	O
not	O
use	O
discrete	O
latent	O
variables	O
,	O
or	O
restrict	O
to	O
discrete	O
data	O
.	O
Work	O
such	O
as	O
that	O
by	O
has	O
extended	O
topic	Method
models	Method
in	O
various	O
directions	O
,	O
but	O
importantly	O
we	O
use	O
flexible	O
conditional	O
distributions	O
and	O
dependency	O
structures	O
parameterized	O
by	O
deep	Method
neural	Method
networks	Method
.	O
Recent	O
work	O
has	O
explored	O
neural	Method
networks	Method
for	O
document	Task
models	Task
[	O
see	O
e.g.	O
][]	O
neural_variational_text	O
but	O
has	O
been	O
limited	O
to	O
modelling	O
datapoints	O
with	O
little	O
internal	O
structure	O
.	O
Along	O
related	O
lines	O
are	O
‘	O
structured	Method
variational	Method
autoencoders	Method
’	O
[	O
see	O
][]	O
structured_vae	O
,	O
where	O
they	O
treat	O
the	O
general	O
problem	O
of	O
integrating	O
graphical	Method
models	Method
with	O
variational	Method
autoencoders	Method
.	O
paragraph	O
:	O
Transfer	Method
learning	Method
There	O
is	O
a	O
considerable	O
literature	O
on	O
transfer	Task
learning	Task
,	O
for	O
a	O
survey	O
see	O
transfer_survey	Task
.	O
There	O
they	O
discuss	O
‘	O
parameter	Method
-	Method
transfer	Method
’	Method
approaches	Method
whereby	O
parameters	O
or	O
priors	O
are	O
shared	O
across	O
datasets	O
,	O
and	O
our	O
work	O
fits	O
into	O
that	O
paradigm	O
.	O
For	O
examples	O
see	O
informative_vector_machine	O
where	O
share	O
they	O
priors	O
between	O
Gaussian	Method
processes	Method
,	O
and	O
multitask_svm	O
where	O
they	O
take	O
an	O
SVM	Method
-	Method
like	Method
approach	Method
to	O
share	O
kernels	O
.	O
paragraph	O
:	O
One	Method
-	Method
shot	Method
Learning	Method
Learning	Task
quickly	Task
from	O
small	O
amounts	O
of	O
data	O
is	O
a	O
topic	O
of	O
great	O
interest	O
.	O
omniglot	Material
use	O
Bayesian	Method
program	Method
induction	Method
for	O
one	Task
-	Task
shot	Task
generation	Task
and	Task
classification	Task
,	O
and	O
train	O
a	O
Siamese	Method
(	Method
)	Method
convolutional	Method
network	Method
for	O
one	Task
-	Task
shot	Task
image	Task
classification	Task
.	O
We	O
note	O
the	O
relation	O
to	O
the	O
recent	O
work	O
one_shot_generative	O
in	O
which	O
the	O
authors	O
use	O
a	O
conditional	Method
recurrent	Method
variational	Method
autoencoder	Method
capable	O
of	O
one	Method
-	Method
shot	Method
generalization	Method
by	O
taking	O
as	O
extra	O
input	O
a	O
conditioning	O
data	O
point	O
.	O
The	O
important	O
differences	O
here	O
are	O
that	O
we	O
jointly	O
model	O
datasets	O
and	O
datapoints	O
and	O
consider	O
datasets	O
of	O
any	O
size	O
.	O
Recent	O
approaches	O
to	O
one	Task
-	Task
shot	Task
classification	Task
are	O
matching	Task
networks	Task
matching	Task
(	O
which	O
was	O
concurrent	O
with	O
the	O
initial	O
preprint	O
of	O
this	O
work	O
)	O
,	O
and	O
related	O
previous	O
work	O
mann	O
.	O
The	O
former	O
can	O
be	O
considered	O
a	O
kind	O
of	O
differentiable	Method
nearest	Method
neighbour	Method
classifier	Method
,	O
and	O
the	O
latter	O
augments	O
their	O
network	O
with	O
memory	O
to	O
store	O
information	O
about	O
the	O
classification	Task
problem	Task
.	O
Both	O
are	O
trained	O
end	O
-	O
to	O
-	O
end	O
for	O
the	O
classification	Task
problem	Task
,	O
whereas	O
the	O
present	O
work	O
is	O
a	O
general	O
approach	O
to	O
learning	O
representations	Task
of	Task
datasets	Task
.	O
Probably	O
the	O
closest	O
previous	O
work	O
is	O
by	O
where	O
the	O
authors	O
learn	O
a	O
topic	Method
model	Method
over	O
the	O
activations	O
of	O
a	O
DBM	Method
for	O
one	Task
-	Task
shot	Task
learning	Task
.	O
Compared	O
with	O
their	O
work	O
we	O
use	O
modern	O
architectures	O
and	O
easier	O
to	O
train	O
VAEs	Method
,	O
in	O
particular	O
we	O
have	O
fast	O
and	O
amortized	O
feedforward	Method
inference	Method
for	O
test	O
(	O
and	O
training	O
)	O
datasets	O
,	O
avoiding	O
the	O
need	O
for	O
MCMC	Method
.	O
paragraph	O
:	O
Multiple	Method
-	Method
Instance	Method
Learning	Method
There	O
is	O
previous	O
work	O
on	O
classifying	Task
sets	Task
in	O
multiple	Task
-	Task
instance	Task
learning	Task
,	O
for	O
a	O
useful	O
survey	O
see	O
classification_with_sets	Task
.	O
Typical	O
approaches	O
involve	O
adapting	O
kernel	Method
based	Method
methods	Method
such	O
as	O
support	Method
measure	Method
machines	Method
support_measure_machines	Method
,	O
support	Method
distribution	Method
machines	Method
support_distribution_machines	O
and	O
multiple	Method
-	Method
instance	Method
-	Method
kernels	Method
multiple_instance_kernels	O
.	O
We	O
do	O
not	O
consider	O
applications	O
to	O
multiple	Task
-	Task
instance	Task
learning	Task
type	Task
problems	Task
here	O
,	O
but	O
it	O
may	O
be	O
fruitful	O
to	O
do	O
so	O
in	O
the	O
future	O
.	O
paragraph	O
:	O
Set2Seq	O
In	O
very	O
related	O
work	O
,	O
set2seq	O
explore	O
architectures	O
for	O
mapping	Task
sets	Task
to	Task
sequences	Task
.	O
There	O
they	O
use	O
an	O
LSTM	Method
to	O
repeatedly	O
compute	O
weighted	Method
-	Method
averages	Method
of	Method
the	Method
datapoints	Method
and	O
use	O
this	O
to	O
tackle	O
problems	O
such	O
as	O
sorting	O
a	O
list	O
of	O
numbers	O
.	O
The	O
main	O
difference	O
between	O
their	O
work	O
and	O
ours	O
is	O
that	O
they	O
primarily	O
consider	O
supervised	Task
problems	Task
,	O
whereas	O
we	O
present	O
a	O
general	O
unsupervised	Method
method	Method
for	O
learning	O
representations	O
of	O
sets	O
of	O
i.i.d	O
instances	O
.	O
In	O
future	O
work	O
we	O
may	O
also	O
explore	O
recurrently	O
computing	O
statistics	O
.	O
paragraph	O
:	O
ABC	O
There	O
has	O
also	O
been	O
work	O
on	O
learning	O
summary	Task
statistics	Task
for	O
Approximate	Task
Bayesian	Task
Computation	Task
by	O
either	O
learning	O
to	O
predict	O
the	O
parameters	O
generating	O
a	O
sample	O
as	O
a	O
supervised	Task
problem	Task
,	O
or	O
by	O
using	O
kernel	Method
embeddings	Method
as	O
infinite	Method
dimensional	Method
summary	Method
statistics	Method
.	O
See	O
the	O
work	O
by	O
kernel_bayes	Method
for	O
an	O
example	O
of	O
kernel	Method
-	Method
based	Method
approaches	Method
.	O
More	O
recently	O
deep_summary_statistics	Method
used	O
deep	Method
neural	Method
networks	Method
to	O
predict	O
the	O
parameters	O
generating	O
the	O
data	O
.	O
The	O
crucial	O
differences	O
are	O
that	O
their	O
problem	O
is	O
supervised	O
,	O
they	O
do	O
not	O
leverage	O
any	O
exchangeability	O
properties	O
the	O
data	O
may	O
have	O
,	O
nor	O
can	O
it	O
deal	O
with	O
varying	O
sample	O
sizes	O
.	O
section	O
:	O
Experimental	O
Results	O
Given	O
an	O
input	O
set	O
we	O
can	O
use	O
the	O
statistic	Method
network	Method
to	O
calculate	O
an	O
approximate	O
posterior	O
over	O
contexts	O
.	O
Under	O
the	O
generative	Method
model	Method
,	O
each	O
context	O
specifies	O
a	O
conditional	Method
model	Method
.	O
To	O
get	O
samples	O
from	O
the	O
model	O
corresponding	O
to	O
the	O
most	O
likely	O
posterior	O
value	O
of	O
,	O
we	O
set	O
to	O
the	O
mean	O
of	O
the	O
approximate	O
posterior	O
and	O
then	O
sample	O
directly	O
from	O
the	O
conditional	O
distributions	O
.	O
This	O
is	O
described	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
We	O
use	O
this	O
process	O
in	O
our	O
experiments	O
to	O
show	O
samples	O
.	O
In	O
all	O
experiments	O
,	O
we	O
use	O
the	O
Adam	Method
optimization	Method
algorithm	Method
adam	Method
to	O
optimize	O
the	O
parameters	O
of	O
the	O
generative	Method
models	Method
and	O
variational	Method
approximations	Method
.	O
Batch	Method
normalization	Method
batchnorm	Method
is	O
implemented	O
for	O
convolutional	Method
layers	Method
and	O
we	O
always	O
use	O
a	O
batch	O
size	O
of	O
.	O
We	O
primarily	O
use	O
the	O
Theano	Method
theano	Method
framework	Method
with	O
the	O
Lasagne	O
lasagne	O
library	O
,	O
but	O
the	O
final	O
experiments	O
with	O
face	O
data	O
were	O
done	O
using	O
Tensorflow	Method
tensorflow	Method
.	O
In	O
all	O
cases	O
experiments	O
were	O
terminated	O
after	O
a	O
given	O
number	O
of	O
epochs	O
when	O
training	O
appeared	O
to	O
have	O
sufficiently	O
converged	O
(	O
epochs	O
for	O
omniglot	Material
,	O
youtube	Material
and	O
spatial	O
MNIST	Material
examples	O
,	O
and	O
epochs	O
for	O
the	O
synthetic	O
experiment	O
)	O
.	O
subsection	O
:	O
Simple	O
1	Method
-	Method
D	Method
Distributions	Method
In	O
our	O
first	O
experiment	O
we	O
wanted	O
to	O
know	O
if	O
the	O
neural	Method
statistician	Method
will	O
learn	O
to	O
cluster	O
synthetic	Material
-	Material
D	Material
datasets	Material
by	O
distribution	O
family	O
.	O
We	O
generated	O
a	O
collection	O
of	O
synthetic	Material
-	Material
D	Material
datasets	Material
each	O
containing	O
samples	O
.	O
Datasets	O
consist	O
of	O
samples	O
from	O
either	O
an	O
Exponential	Method
,	Method
Gaussian	Method
,	Method
Uniform	Method
or	Method
Laplacian	Method
distribution	Method
with	O
equal	O
probability	O
.	O
Means	O
and	O
variances	O
are	O
sampled	O
from	O
and	O
respectively	O
.	O
The	O
training	O
data	O
contains	O
sets	O
.	O
The	O
architecture	O
for	O
this	O
experiment	O
contains	O
a	O
single	O
stochastic	Method
layer	Method
with	O
units	O
for	O
and	O
units	O
for	O
,	O
.	O
The	O
model	O
and	O
variational	Method
approximation	Method
are	O
each	O
a	O
diagonal	Method
Gaussian	Method
distribution	Method
with	O
all	O
mean	O
and	O
log	O
variance	O
parameters	O
given	O
by	O
a	O
network	O
composed	O
of	O
three	O
dense	Method
layers	Method
with	O
ReLU	O
activations	O
and	O
units	O
.	O
The	O
statistic	Method
network	Method
determining	O
the	O
mean	O
and	O
log	O
variance	O
parameters	O
of	O
posterior	O
over	O
context	O
variables	O
is	O
composed	O
of	O
three	O
dense	Method
layers	Method
before	O
and	O
after	O
pooling	Method
,	O
each	O
with	O
units	O
with	O
Rectified	Method
Linear	Method
Unit	Method
(	O
ReLU	Method
)	Method
activations	Method
.	O
Figure	O
[	O
reference	O
]	O
shows	O
3	Metric
-	Metric
D	Metric
scatter	Metric
plots	Metric
of	O
the	O
summary	Metric
statistics	Metric
learned	O
.	O
Notice	O
that	O
the	O
different	O
families	O
of	O
distribution	O
cluster	O
.	O
It	O
is	O
interesting	O
to	O
observe	O
that	O
the	O
Exponential	O
cluster	O
is	O
differently	O
orientated	O
to	O
the	O
others	O
,	O
perhaps	O
reflecting	O
the	O
fact	O
that	O
it	O
is	O
the	O
only	O
non	O
-	O
symmetric	O
distribution	O
.	O
We	O
also	O
see	O
that	O
between	O
the	O
Gaussian	O
and	O
Laplacian	O
clusters	O
there	O
is	O
an	O
area	O
of	O
ambiguity	O
which	O
is	O
as	O
one	O
might	O
expect	O
.	O
We	O
also	O
see	O
that	O
within	O
each	O
cluster	O
the	O
mean	O
and	O
variance	O
are	O
mapped	O
to	O
orthogonal	O
directions	O
.	O
subsection	O
:	O
Spatial	Material
MNIST	Material
Building	O
on	O
the	O
previous	O
experiments	O
we	O
investigate	O
2	O
-	O
D	O
datasets	O
that	O
have	O
complex	O
structure	O
,	O
but	O
the	O
datapoints	O
contain	O
little	O
information	O
by	O
themselves	O
,	O
making	O
it	O
a	O
good	O
test	O
of	O
the	O
statistic	Method
network	Method
.	O
We	O
created	O
a	O
dataset	O
called	O
spatial	Material
MNIST	Material
.	O
In	O
spatial	Material
MNIST	Material
each	O
image	O
from	O
MNIST	Material
mnist	O
is	O
turned	O
into	O
a	O
dataset	O
by	O
interpreting	O
the	O
normalized	O
pixel	O
intensities	O
as	O
a	O
probability	O
density	O
and	O
sampling	O
coordinate	O
values	O
.	O
An	O
example	O
is	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
This	O
creates	O
two	O
-	O
dimensional	O
spatial	O
datasets	O
.	O
We	O
used	O
a	O
sample	O
size	O
of	O
.	O
Note	O
that	O
since	O
the	O
pixel	O
coordinates	O
are	O
discrete	O
,	O
it	O
is	O
necessary	O
to	O
dequantize	O
them	O
by	O
adding	O
uniform	O
noise	O
to	O
the	O
coordinates	O
if	O
one	O
models	O
them	O
as	O
real	O
numbers	O
,	O
else	O
you	O
can	O
get	O
arbitrarily	O
high	O
densities	O
(	O
see	O
note_evaluation_generative	O
for	O
a	O
discussion	O
of	O
this	O
point	O
)	O
.	O
The	O
generative	Method
architecture	Method
for	O
this	O
experiment	O
contains	O
stochastic	Method
layers	Method
,	O
each	O
with	O
units	O
,	O
and	O
a	O
single	O
layer	O
with	O
units	O
.	O
The	O
means	O
and	O
log	O
variances	O
of	O
the	O
Gaussian	O
likelihood	O
for	O
,	O
and	O
each	O
subnetwork	O
for	O
in	O
both	O
the	O
encoder	Method
and	Method
decoder	Method
contained	O
dense	O
layers	O
with	O
ReLU	O
units	O
each	O
.	O
The	O
statistic	Method
network	Method
also	O
contained	O
3	O
dense	Method
layers	Method
pre	Method
-	Method
pooling	Method
and	O
3	O
dense	Method
layers	Method
post	Method
pooling	Method
with	O
256	O
ReLU	Method
units	Method
.	O
In	O
addition	O
to	O
being	O
able	O
to	O
sample	O
from	O
the	O
model	O
conditioned	O
on	O
a	O
set	O
of	O
inputs	O
,	O
we	O
can	O
also	O
summarize	O
a	O
dataset	O
by	O
choosing	O
a	O
subset	O
to	O
minimise	O
the	O
KL	O
divergence	O
of	O
from	O
.	O
We	O
do	O
this	O
greedily	O
by	O
iteratively	O
discarding	O
points	O
from	O
the	O
full	O
sample	O
.	O
Pseudocode	O
for	O
this	O
process	O
is	O
given	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
We	O
see	O
that	O
the	O
model	O
is	O
capable	O
of	O
handling	O
complex	O
arrangements	O
of	O
datapoints	O
.	O
We	O
also	O
see	O
that	O
it	O
can	O
select	O
sensible	O
subsets	O
of	O
a	O
dataset	O
as	O
a	O
summary	O
.	O
subsection	O
:	O
Omniglot	O
Next	O
we	O
work	O
with	O
the	O
OMNIGLOT	Material
data	Material
omniglot	Material
.	O
This	O
contains	O
1628	O
classes	O
of	O
handwritten	O
characters	O
but	O
with	O
just	O
20	O
examples	O
per	O
class	O
.	O
This	O
makes	O
it	O
an	O
excellent	O
test	O
-	O
bed	O
for	O
transfer	Task
/	Task
few	Task
-	Task
shot	Task
learning	Task
.	O
We	O
constructed	O
datasets	O
by	O
splitting	O
each	O
class	O
into	O
datasets	O
of	O
size	O
5	O
.	O
We	O
train	O
on	O
datasets	O
drawn	O
from	O
1200	O
classes	O
and	O
reserve	O
the	O
remaining	O
classes	O
to	O
test	O
few	Method
-	Method
shot	Method
sampling	Method
and	O
classification	Task
.	O
We	O
created	O
new	O
classes	O
by	O
rotating	O
and	O
reflecting	O
characters	O
.	O
We	O
resized	O
the	O
images	O
to	O
.	O
We	O
sampled	O
a	O
binarization	Method
of	O
each	O
image	O
for	O
each	O
epoch	O
.	O
We	O
also	O
randomly	O
applied	O
the	O
dilation	Method
operator	Method
from	O
computer	Method
vision	Method
as	O
further	O
data	Task
augmentation	Task
since	O
we	O
observed	O
that	O
the	O
stroke	O
widths	O
are	O
quite	O
uniform	O
in	O
the	O
OMNIGLOT	Material
data	Material
,	O
whereas	O
there	O
is	O
substantial	O
variation	O
in	O
MNIST	Material
,	O
this	O
augmentation	O
improved	O
the	O
visual	Metric
quality	Metric
of	O
the	O
few	O
-	O
shot	O
MNIST	Material
samples	O
considerably	O
and	O
increased	O
the	O
few	Metric
-	Metric
shot	Metric
classification	Metric
accuracy	Metric
by	O
about	O
percent	O
.	O
Finally	O
we	O
used	O
‘	O
sample	Method
dropout	Method
’	O
whereby	O
a	O
random	O
subset	O
of	O
each	O
dataset	O
was	O
removed	O
from	O
the	O
pooling	O
in	O
the	O
statistic	Method
network	Method
,	O
and	O
then	O
included	O
the	O
number	O
of	O
samples	O
remaining	O
as	O
an	O
extra	O
feature	O
.	O
This	O
was	O
beneficial	O
since	O
it	O
reduced	O
overfitting	O
and	O
also	O
allowed	O
the	O
statistic	Method
network	Method
to	O
learn	O
to	O
adjust	O
the	O
approximate	O
posterior	O
over	O
based	O
on	O
the	O
number	O
of	O
samples	O
.	O
We	O
used	O
a	O
single	O
stochastic	Method
layer	Method
with	O
16	O
units	O
for	O
,	O
and	O
units	O
for	O
.	O
We	O
used	O
a	O
shared	Method
convolutional	Method
encoder	Method
between	O
the	O
inference	Method
and	Method
statistic	Method
networks	Method
and	O
a	O
deconvolutional	Method
decoder	Method
network	Method
.	O
Full	O
details	O
of	O
the	O
networks	O
are	O
given	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
The	O
decoder	Method
used	O
a	O
Bernoulli	O
likelihood	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
show	O
two	O
examples	O
of	O
few	Method
-	Method
shot	Method
learning	Method
by	O
conditioning	O
on	O
samples	O
of	O
unseen	O
characters	O
from	O
OMNIGLOT	Material
,	O
and	O
conditioning	O
on	O
samples	O
of	O
digits	O
from	O
MNIST	Material
.	O
The	O
samples	O
are	O
mostly	O
of	O
a	O
high	O
-	O
quality	O
,	O
and	O
this	O
shows	O
that	O
the	O
neural	Method
statistician	Method
can	O
generalize	O
even	O
to	O
new	O
datasets	O
.	O
As	O
a	O
further	O
test	O
we	O
considered	O
few	Task
-	Task
shot	Task
classification	Task
of	O
both	O
unseen	Material
OMNIGLOT	Material
characters	Material
and	O
MNIST	Material
digits	O
.	O
Given	O
a	O
sets	O
of	O
labelled	O
examples	O
of	O
each	O
class	O
(	O
for	O
MNIST	Material
say	O
)	O
,	O
we	O
computed	O
the	O
approximate	O
posteriors	O
using	O
the	O
statistic	Method
network	Method
.	O
Then	O
for	O
each	O
test	O
image	O
we	O
also	O
computed	O
the	O
posterior	O
and	O
classified	O
it	O
according	O
to	O
the	O
training	O
dataset	O
minimizing	O
the	O
KL	O
divergence	O
from	O
the	O
test	O
context	O
to	O
the	O
training	O
context	O
.	O
This	O
process	O
is	O
described	O
in	O
Algorithm	O
[	O
reference	O
]	O
.	O
We	O
tried	O
this	O
with	O
either	O
1	O
or	O
5	O
labelled	O
examples	O
per	O
class	O
and	O
either	O
or	O
classes	O
.	O
For	O
each	O
trial	O
we	O
randomly	O
select	O
classes	O
,	O
randomly	O
select	O
training	O
examples	O
for	O
each	O
class	O
,	O
and	O
test	O
on	O
the	O
remaining	O
examples	O
.	O
This	O
process	O
is	O
repeated	O
100	O
times	O
and	O
the	O
results	O
averaged	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
[	O
reference	O
]	O
.	O
We	O
compare	O
to	O
a	O
number	O
of	O
results	O
reported	O
in	O
matching	Task
including	O
mann	O
and	O
siamese_one_shot	Method
.	O
Overall	O
we	O
see	O
that	O
the	O
neural	Method
statistician	Method
model	Method
can	O
be	O
used	O
as	O
a	O
strong	O
classifier	Method
,	O
particularly	O
for	O
the	O
-	Task
way	Task
tasks	Task
,	O
but	O
performs	O
worse	O
than	O
matching	Method
networks	Method
for	O
the	O
-	Task
way	Task
tasks	Task
.	O
One	O
important	O
advantage	O
that	O
matching	Method
networks	Method
have	O
is	O
that	O
,	O
whilst	O
each	O
class	O
is	O
processed	O
independently	O
in	O
our	O
model	O
,	O
the	O
representation	O
in	O
matching	Method
networks	Method
is	O
conditioned	O
on	O
all	O
of	O
the	O
classes	O
in	O
the	O
few	Task
-	Task
shot	Task
problem	Task
.	O
This	O
means	O
that	O
it	O
can	O
exaggerate	O
differences	O
between	O
similar	O
classes	O
,	O
which	O
are	O
more	O
likely	O
to	O
appear	O
in	O
a	O
20	Task
-	Task
way	Task
problem	Task
than	O
a	O
5	Task
-	Task
way	Task
problem	Task
.	O
subsection	O
:	O
Youtube	Material
Faces	Material
Finally	O
,	O
we	O
provide	O
a	O
proof	O
of	O
concept	O
for	O
generating	O
faces	O
of	O
a	O
particular	O
person	O
.	O
We	O
use	O
the	O
Youtube	Material
Faces	Material
Database	Material
from	O
youtube_faces	Material
.	O
It	O
contains	O
videos	O
of	O
different	O
people	O
.	O
We	O
use	O
the	O
aligned	O
and	O
cropped	O
to	O
face	O
version	O
,	O
resized	O
to	O
.	O
The	O
validation	O
and	O
test	O
sets	O
contain	O
unique	O
people	O
each	O
,	O
and	O
there	O
is	O
no	O
overlap	O
of	O
persons	O
between	O
data	O
splits	O
.	O
The	O
sets	O
were	O
created	O
by	O
sampling	O
frames	O
randomly	O
without	O
replacement	O
from	O
each	O
video	O
,	O
we	O
use	O
a	O
set	O
size	O
of	O
frames	O
.	O
We	O
resample	O
the	O
sets	O
for	O
the	O
training	O
data	O
each	O
epoch	O
.	O
Our	O
architecture	O
for	O
this	O
problem	O
is	O
based	O
on	O
one	O
presented	O
in	O
.	O
We	O
used	O
a	O
single	O
stochastic	Method
layer	Method
with	O
dimensional	O
latent	O
and	O
dimensional	O
variable	O
.	O
The	O
statistic	Method
network	Method
and	O
the	O
inference	Method
network	Method
share	O
a	O
common	O
convolutional	Method
encoder	Method
,	O
and	O
the	O
deocder	Method
uses	O
deconvolutional	Method
layers	Method
.	O
For	O
full	O
details	O
see	O
Appendix	O
[	O
reference	O
]	O
.	O
The	O
likelihood	O
function	O
is	O
a	O
Gaussian	Method
,	O
but	O
where	O
the	O
variance	O
parameters	O
are	O
shared	O
across	O
all	O
datapoints	O
,	O
this	O
was	O
found	O
to	O
make	O
training	O
faster	O
and	O
more	O
stable	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
Whilst	O
there	O
is	O
room	O
for	O
improvement	O
,	O
we	O
see	O
that	O
it	O
is	O
possible	O
to	O
specify	O
a	O
complex	O
distribution	O
on	O
-	O
the	O
-	O
fly	O
with	O
a	O
set	O
of	O
photos	O
of	O
a	O
previously	O
unseen	O
person	O
.	O
The	O
samples	O
conditioned	O
on	O
an	O
input	O
set	O
have	O
a	O
reasonable	O
likeness	O
of	O
the	O
input	O
faces	O
.	O
We	O
also	O
show	O
the	O
ability	O
of	O
the	O
model	O
to	O
generate	O
new	O
datasets	O
and	O
see	O
that	O
the	O
samples	O
have	O
a	O
consistent	O
identity	O
and	O
varied	O
poses	O
.	O
section	O
:	O
Conclusion	O
We	O
have	O
demonstrated	O
a	O
highly	O
flexible	O
model	O
on	O
a	O
variety	O
of	O
tasks	O
.	O
Going	O
forward	O
our	O
approach	O
will	O
naturally	O
benefit	O
from	O
advances	O
in	O
generative	Method
models	Method
as	O
we	O
can	O
simply	O
upgrade	O
our	O
base	O
generative	Method
model	Method
,	O
and	O
so	O
future	O
work	O
will	O
pursue	O
this	O
.	O
Compared	O
with	O
some	O
other	O
approaches	O
in	O
the	O
literature	O
for	O
few	Task
-	Task
shot	Task
learning	Task
,	O
our	O
requirement	O
for	O
supervision	Task
is	O
weaker	O
:	O
we	O
only	O
ask	O
at	O
training	O
time	O
that	O
we	O
are	O
given	O
datasets	O
,	O
but	O
we	O
do	O
not	O
need	O
labels	O
for	O
the	O
datasets	O
,	O
nor	O
even	O
information	O
on	O
whether	O
two	O
datasets	O
represent	O
the	O
same	O
or	O
different	O
classes	O
.	O
It	O
would	O
be	O
interesting	O
then	O
to	O
explore	O
application	O
areas	O
where	O
only	O
this	O
weaker	O
form	O
of	O
supervision	Method
is	O
available	O
.	O
There	O
are	O
two	O
important	O
limitations	O
to	O
this	O
work	O
,	O
firstly	O
that	O
the	O
method	O
is	O
dataset	O
hungry	O
:	O
it	O
will	O
likely	O
not	O
learn	O
useful	O
representations	O
of	O
datasets	O
given	O
only	O
a	O
small	O
number	O
of	O
them	O
.	O
Secondly	O
at	O
test	O
time	O
the	O
few	Metric
-	Metric
shot	Metric
fit	Metric
of	O
the	O
generative	Method
model	Method
will	O
not	O
be	O
greatly	O
improved	O
by	O
using	O
larger	O
datasets	O
unless	O
the	O
model	O
was	O
also	O
trained	O
on	O
similarly	O
large	O
datasets	O
.	O
The	O
latter	O
limitation	O
seems	O
like	O
a	O
promising	O
future	O
research	O
direction	O
-	O
bridging	O
the	O
gap	O
between	O
fast	Method
adaptation	Method
and	O
slow	Task
training	Task
.	O
subsubsection	O
:	O
Acknowledgments	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
the	O
EPSRC	O
Centre	O
for	O
Doctoral	Task
Training	Task
in	O
Data	Task
Science	Task
,	O
funded	O
by	O
the	O
UK	O
Engineering	O
and	O
Physical	O
Sciences	O
Research	O
Council	O
(	O
grant	O
EP	O
/	O
L016427	O
/	O
1	O
)	O
and	O
the	O
University	O
of	O
Edinburgh	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Appendix	O
A	O
:	O
Pseudocode	O
[	O
H	O
]	O
Sampling	O
a	O
dataset	O
of	O
size	O
k	O
sample	O
to	O
sample	O
to	O
sample	O
sample	O
[	O
H	O
]	O
Sampling	O
a	O
dataset	O
of	O
size	O
k	O
conditioned	O
on	O
a	O
dataset	O
of	O
size	O
m	O
Calculate	O
approximate	O
posterior	O
over	O
using	O
statistic	Method
network	Method
.	O
Set	O
to	O
be	O
the	O
mean	O
of	O
the	O
approximate	O
posterior	O
.	O
to	O
sample	O
to	O
sample	O
sample	O
[	O
H	O
]	O
Selecting	O
a	O
representative	O
sample	O
of	O
size	O
k	O
Calculate	O
approximate	O
posterior	O
over	O
using	O
statistic	Method
network	Method
.	O
to	O
[	O
H	O
]	O
K	O
-	O
way	O
few	Task
-	Task
shot	Task
classification	Task
approximate	Task
posterior	Task
over	O
given	O
query	O
point	O
to	O
appendix	O
:	O
Appendix	O
B	O
:	O
Further	O
Experimental	O
Details	O
subsection	O
:	O
Omniglot	O
subsection	O
:	O
Youtube	Material
faces	Material
