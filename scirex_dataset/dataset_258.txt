document O
: O
Learning O
a O
Hierarchical Method
Latent Method
- Method
Variable Method
Model Method
of Method
3D Method
Shapes Method
We O
propose O
the O
Variational Method
Shape Method
Learner Method
( O
VSL Method
) O
, O
a O
generative Method
model Method
that O
learns O
the O
underlying O
structure O
of O
voxelized O
3D O
shapes O
in O
an O
unsupervised Method
fashion Method
. O
Through O
the O
use O
of O
skip O
- O
connections O
, O
our O
model O
can O
successfully O
learn O
and O
infer O
a O
latent Task
, Task
hierarchical Task
representation Task
of Task
objects Task
. O
Furthermore O
, O
realistic O
3D O
objects O
can O
be O
easily O
generated O
by O
sampling O
the O
VSL Method
’s O
latent O
probabilistic O
manifold O
. O
We O
show O
that O
our O
generative Method
model Method
can O
be O
trained O
end O
- O
to O
- O
end O
from O
2D O
images O
to O
perform O
single Task
image Task
3D Task
model Task
retrieval Task
. O
Experiments O
show O
, O
both O
quantitatively O
and O
qualitatively O
, O
the O
improved O
generalization Metric
of O
our O
proposed O
model O
over O
a O
range O
of O
tasks O
, O
performing O
better O
or O
comparable O
to O
various O
state O
- O
of O
- O
the O
- O
art O
alternatives O
. O
[ O
table O
] O
skip=5pt O
section O
: O
Introduction O
Over O
the O
past O
several O
years O
, O
impressive O
strides O
have O
been O
made O
in O
the O
generative Task
modelling Task
of Task
3D Task
objects Task
. O
Much O
of O
this O
progress O
can O
be O
attributed O
to O
recent O
advances O
in O
artificial Method
neural Method
network Method
research Method
. O
Instead O
of O
the O
usual O
approach O
to O
representing Task
3D Task
shapes Task
with O
voxel O
occupancy O
vectors O
, O
promising O
recent O
work O
has O
taken O
to O
learning O
simple O
latent Method
representations Method
of O
such O
objects O
. O
Neural Method
architectures Method
that O
have O
been O
developed O
with O
this O
goal O
in O
mind O
include O
those O
based O
on O
deep Method
belief Method
networks Method
, O
deep Method
autoencoders Method
, O
and O
3D Method
convolutional Method
networks Method
. O
The O
positive O
progress O
made O
so O
far O
with O
neural Method
networks Method
has O
also O
led O
to O
the O
creation O
of O
several O
large Task
- Task
scale Task
3D Task
CAD Task
model Task
benchmarks Task
, O
notably O
ModelNet Material
and O
ShapeNet Material
. O
However O
, O
despite O
the O
progress O
made O
so O
far O
, O
one O
key O
weakness O
shared O
among O
all O
previous O
state O
- O
of O
- O
the O
- O
art O
approaches O
is O
that O
all O
of O
them O
have O
focused O
on O
learning O
a O
single O
( O
“ O
flat O
” O
) O
vector Method
representation Method
of Method
3D Method
shapes Method
. O
These O
include O
recent O
and O
powerful O
models O
such O
as O
the O
autoencoder Method
- Method
like Method
T Method
- Method
L Method
Network Method
and O
the O
probabilistic Method
3D Method
Generative Method
Adversarial Method
Network Method
( O
3D Method
- Method
GAN Method
) O
, O
which O
shared O
its O
latent Method
vector Method
representation Method
over O
multiple O
tasks O
. O
Other O
models O
, O
such O
as O
those O
of O
, O
further O
required O
additional O
supervision O
using O
information O
about O
camera O
viewpoints O
, O
shape O
keypoints O
, O
and O
segmentations O
. O
To O
describe O
the O
input O
with O
only O
a O
single O
layer O
of O
latent O
variables O
might O
be O
too O
restrictive O
an O
assumption O
, O
hindering O
the O
expressiveness O
of O
the O
underlying O
generative Method
model Method
learned O
. O
Having O
a O
multilevel O
latent O
structure O
, O
on O
the O
other O
hand O
, O
would O
allow O
for O
lower O
- O
level O
latent O
variables O
to O
focus O
on O
modelling O
features O
such O
as O
edges O
and O
the O
upper O
levels O
to O
learn O
to O
command O
those O
lower O
- O
level O
variables O
as O
to O
where O
to O
place O
those O
edges O
in O
order O
to O
form O
curves O
and O
shapes O
. O
This O
composition O
of O
latent O
( O
local O
) O
sub O
- O
structures O
would O
allow O
us O
to O
exploit O
the O
fact O
that O
most O
3D O
shapes O
usually O
have O
similar O
structure O
. O
Note O
that O
this O
course O
- O
to O
- O
fine Task
feature Task
extraction Task
process Task
is O
the O
very O
essence O
of O
abstraction Task
, O
yielding O
representations O
that O
can O
be O
easily O
constructed O
in O
terms O
of O
less O
abstract O
ones O
. O
Higher O
- O
level O
variables O
, O
or O
disentangled O
features O
, O
would O
be O
modelling O
complex O
interactions O
of O
low O
- O
level O
patterns O
. O
Thus O
, O
to O
encourage O
the O
learning Task
of Task
hierarchical Task
features Task
, O
we O
explicitly O
incorporate O
this O
as O
a O
prior O
in O
our O
model O
through O
explicit O
architectural O
constraints O
. O
In O
this O
paper O
, O
motivated O
by O
the O
argument O
for O
a O
hierarchical Method
representation Method
developed O
above O
and O
the O
promise O
shown O
in O
work O
such O
as O
that O
of O
, O
we O
show O
how O
to O
encourage O
a O
latent Method
- Method
variable Method
generative Method
model Method
to O
learn O
a O
hierarchy O
of O
latent O
variables O
through O
the O
use O
of O
synaptic Method
skip Method
- Method
connections Method
. O
These O
skip O
- O
connections O
encourage O
each O
layer O
of O
latent O
variables O
to O
model O
exactly O
one O
level O
of O
abstraction O
of O
the O
data O
. O
To O
efficiently O
learn O
such O
a O
latent O
structure O
, O
we O
further O
exploit O
recent O
advances O
in O
approximate Method
inference Method
to O
develop O
a O
variational Method
learning Method
procedure Method
. O
Empirically O
, O
we O
show O
that O
the O
learned O
generative Method
model Method
, O
which O
we O
call O
the O
Variational Method
Shape Method
Learner Method
, O
acquires O
rich O
representations O
of O
3D O
shapes O
that O
yield O
significantly O
improved O
performance O
across O
a O
multitude O
of O
3D Task
shape Task
tasks Task
. O
The O
main O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
We O
propose O
a O
novel O
latent Method
- Method
variable Method
model Method
, O
which O
we O
call O
the O
Variational Method
Shape Method
Learner Method
, O
which O
is O
capable O
of O
learning O
expressive Task
feature Task
representations Task
of Task
3D Task
shapes Task
. O
We O
observe O
impressive O
performance O
in O
shape Task
generation Task
and O
shape Task
arithmetic Task
in O
a O
large O
dataset O
. O
For O
both O
general Task
3D Task
model Task
building Task
and O
single Task
image Task
reconstruction Task
, O
we O
show O
that O
our O
model O
is O
fully O
unsupervised O
, O
requiring O
no O
extra O
human O
- O
generated O
information O
about O
segmentation Task
, O
keypoints O
, O
or O
pose O
information O
. O
We O
show O
that O
our O
model O
outperforms O
current O
state O
- O
of O
- O
the O
- O
art O
approaches O
in O
unsupervised Task
( Task
object Task
) Task
model Task
classification Task
while O
requiring O
significantly O
fewer O
learned O
feature Method
extractors Method
( O
a O
vector O
with O
less O
than O
100 O
dimensions O
compared O
to O
the O
3D Method
- Method
GAN Method
’s O
2.5 O
million O
dimensional O
vector O
) O
. O
In O
real Task
- Task
world Task
image Task
reconstruction Task
, O
our O
extensive O
set O
of O
experiments O
show O
that O
the O
proposed O
Variational Method
Shape Method
Learner Method
surpasses O
state O
- O
of O
- O
the O
- O
art O
in O
8 O
of O
10 O
classes O
. O
Half O
of O
these O
the O
VSL Method
surpasses O
by O
a O
large O
margin O
. O
section O
: O
Related O
Work O
3D Task
object Task
recognition Task
is O
a O
well O
- O
studied O
problem O
in O
the O
computer Task
vision Task
literature Task
. O
Early O
efforts O
often O
combined O
simple O
image Method
classification Method
methods Method
with O
hand Method
- Method
crafted Method
shape Method
descriptors Method
, O
requiring O
intensive O
effort O
on O
the O
side O
of O
the O
human O
data O
annotator O
. O
However O
, O
ever O
since O
the O
ImageNet Material
contest Material
of O
2012 O
, O
deep Method
convolutional Method
networks Method
( O
ConvNets Method
) O
have O
swept O
the O
vision Task
industry Task
, O
becoming O
nearly O
ubiquitous O
in O
countless O
applications O
. O
Research O
in O
learning Task
probabilistic Task
generative Task
models Task
has O
also O
benefited O
from O
the O
advances O
made O
by O
artificial Method
neural Method
networks Method
. O
Generative Method
Adversarial Method
Networks Method
( O
GANs Method
) O
, O
proposed O
in O
and O
Variational Method
auto Method
- Method
encoders Method
( O
VAEs Method
) O
, O
proposed O
in O
, O
are O
some O
of O
the O
most O
popular O
and O
important O
frameworks O
that O
have O
emerged O
from O
improvements O
in O
generative Method
modelling Method
. O
Successful O
adaptation O
of O
these O
frameworks O
range O
from O
a O
focus O
in O
natural Task
language Task
and Task
speech Task
processing Task
to O
realistic Task
image Task
synthesis Task
, O
yielding O
promising O
, O
positive O
results O
. O
Nevertheless O
, O
very O
little O
work O
, O
outside O
of O
, O
has O
focused O
on O
modeling O
3D Task
objects Task
, O
where O
generative Method
architectures Method
could O
be O
used O
to O
learn O
probabilistic O
embeddings O
. O
The O
model O
proposed O
in O
this O
paper O
will O
offer O
another O
step O
towards O
constructing O
powerful O
generative Method
models Method
of Method
3D Method
structures Method
. O
One O
study O
, O
amidst O
the O
rise O
of O
neural Method
network Method
- Method
based Method
approaches Method
to O
3D Task
object Task
recognition Task
, O
that O
is O
most O
relevant O
to O
this O
paper O
is O
that O
of O
, O
which O
presented O
promising O
results O
and O
a O
benchmark O
for O
3D Task
model Task
recognition Task
: O
ModelNet Material
. O
Following O
this O
key O
study O
, O
researchers O
have O
tried O
applying O
3D Method
ConvNets Method
, O
autoencoders Method
, O
and O
a O
variety O
of O
probabilistic Method
neural Method
generative Method
models Method
to O
the O
problem O
of O
3D Task
model Task
recognition Task
, O
with O
each O
study O
progressively O
advancing O
state O
- O
of O
- O
the O
- O
art O
. O
With O
respect O
to O
3D Task
object Task
generation Task
from O
2D O
images O
, O
commonly O
used O
methods O
can O
be O
roughly O
grouped O
into O
two O
categories O
: O
3D Method
voxel Method
prediction Method
and O
mesh Method
- Method
based Method
methods Method
. O
The O
3D Method
- Method
R2N2 Method
model Method
represents O
a O
more O
recent O
approach O
to O
the O
task O
, O
which O
involves O
training O
a O
recurrent Method
neural Method
network Method
to O
predict O
3D Task
voxels Task
from O
one O
or O
more O
2D O
images O
. O
also O
takes O
a O
recurrent Method
network Method
- Method
based Method
approach Method
, O
but O
receives O
a O
depth O
image O
as O
input O
rather O
than O
normal O
2D O
images O
. O
The O
learnable Method
stereo Method
system Method
processes O
one O
or O
more O
camera O
views O
and O
camera O
pose O
information O
to O
produce O
compelling O
3D O
object O
samples O
. O
Many O
of O
the O
above O
methods O
require O
multiple O
images O
and O
/ O
or O
additional O
human O
- O
provided O
information O
. O
Some O
approaches O
have O
attempted O
to O
minimize O
human O
involvement O
by O
developing O
weakly Method
- Method
supervised Method
schemes Method
, O
making O
use O
of O
image O
silhouettes O
to O
conduct O
3D Task
object Task
reconstruction Task
. O
Of O
the O
few O
unsupervised Method
neural Method
- Method
based Method
approaches Method
that O
exist O
, O
the O
T Method
- Method
L Method
network Method
is O
one O
of O
the O
most O
important O
, O
combining O
a O
convolutional Method
autoencoder Method
with O
an O
image Method
regressor Method
to O
encode O
a O
unified Method
vector Method
representation Method
of O
a O
given O
2D O
image O
. O
However O
, O
one O
fundamental O
issue O
with O
the O
T Method
- Method
L Method
Network Method
is O
its O
three O
- O
phase Method
training Method
procedure Method
, O
since O
jointly O
training O
the O
system Method
components Method
proves O
to O
be O
too O
difficult O
. O
The O
3D Method
- Method
GAN Method
offers O
a O
way O
to O
train O
3D Method
object Method
models Method
in O
an O
adversarial Method
learning Method
scheme Method
. O
However O
, O
GANs Method
are O
notoriously O
difficult O
to O
train O
, O
often O
due O
to O
ill O
- O
designed O
loss O
functions O
and O
the O
higher O
chance O
of O
zero O
gradients O
. O
In O
contrast O
to O
prior O
work O
, O
our O
approach O
, O
which O
is O
derived O
from O
a O
variational Method
Bayesian Method
perspective Method
view Method
of Method
learning Method
, O
naturally O
allows O
for O
joint Task
training Task
of O
all O
model O
parameters O
. O
Furthermore O
, O
our O
approach O
makes O
use O
of O
a O
well O
- O
formulated Method
loss Method
function Method
that O
circumvents O
the O
instability O
involved O
with O
adversarial Method
learning Method
while O
still O
being O
able O
to O
produce O
higher O
- O
quality O
samples O
. O
section O
: O
The O
Variational Method
Shape Method
Learner Method
In O
this O
section O
, O
we O
introduce O
our O
proposed O
model O
, O
the O
Variational Method
Shape Method
Learner Method
( O
VSL Method
) O
, O
which O
builds O
on O
the O
ideas O
of O
the O
Neural Method
Statistician Method
and O
the O
volumetric Method
convolutional Method
network Method
, O
the O
parameters O
of O
which O
the O
VSL Method
learns O
under O
a O
variational Method
inference Method
scheme Method
. O
subsection O
: O
The O
Design O
Philosophy O
It O
is O
well O
known O
that O
generative Method
models Method
, O
learned O
through O
variational Method
inference Method
, O
are O
excellent O
at O
reconstructing O
complex O
data O
but O
tend O
to O
produce O
blurry O
samples O
. O
This O
happens O
because O
there O
is O
uncertainty O
in O
the O
model O
’s O
predictions O
when O
we O
reconstruct O
the O
data O
from O
a O
latent O
space O
. O
As O
described O
above O
, O
previous O
approaches O
to O
3D Task
object Task
modelling Task
have O
focused O
on O
learning O
a O
single O
latent Method
representation Method
of Method
the Method
data Method
. O
However O
, O
this O
simple O
latent O
structure O
might O
be O
hindering O
the O
model O
’s O
ability O
to O
extract O
richer O
structure O
from O
the O
input O
distribution O
and O
thus O
lead O
to O
blurrier O
reconstructions O
. O
To O
improve O
the O
quality O
of O
the O
samples O
of O
generated O
objects O
, O
we O
introduce O
a O
more O
complex O
internal O
variable O
structure O
, O
with O
the O
specific O
goal O
of O
encouraging O
the O
learning O
of O
a O
hierarchical Method
arrangement Method
of Method
latent Method
feature Method
detectors Method
. O
The O
motivation O
for O
a O
latent O
hierarchy O
comes O
from O
the O
observation O
that O
objects O
under O
the O
same O
category O
usually O
have O
similar O
geometric O
structure O
. O
As O
can O
be O
seen O
in O
Figure O
[ O
reference O
] O
, O
we O
start O
from O
a O
global O
latent O
variable O
layer O
( O
horizontally O
depicted O
) O
that O
is O
hardwired O
to O
a O
set O
of O
local O
latent O
variables O
layers O
( O
vertically O
depicted O
) O
, O
each O
tasked O
with O
representing O
one O
level O
of O
feature Method
abstraction Method
. O
The O
skip O
- O
connections O
tie O
together O
the O
latent O
codes O
, O
and O
in O
a O
top O
- O
down O
directed O
fashion O
, O
local O
codes O
closer O
to O
the O
input O
will O
tend O
to O
represent O
lower O
- O
level O
features O
while O
local O
codes O
farther O
away O
from O
the O
input O
will O
tend O
towards O
representing O
higher O
- O
level O
features O
. O
The O
global O
latent O
vector O
can O
be O
thought O
of O
as O
a O
large O
pool O
of O
command O
units O
that O
ensures O
that O
each O
local O
code O
extracts O
information O
relative O
to O
its O
position O
in O
the O
hierarchy O
, O
forming O
an O
overall O
coherent O
structure O
. O
This O
explicit O
global O
- O
local O
form O
, O
and O
the O
way O
it O
constrains O
how O
information O
flows O
across O
it O
, O
lends O
itself O
to O
a O
straightforward O
parametrization O
of O
the O
generative Method
model Method
and O
furthermore O
ensures O
robustness Metric
, O
dramatically O
cutting O
down O
on O
over Task
- Task
fitting Task
. O
To O
make O
things O
easier O
for O
training O
via O
stochastic Method
back Method
- Method
propagation Method
, O
the O
local Method
codes Method
will O
be O
concatenated O
to O
a O
flattened O
structure O
when O
fed O
into O
the O
task Method
- Method
specific Method
models Method
, O
e.g. O
, O
a O
shape Method
classifier Method
or O
a O
voxel Method
reconstruction Method
module Method
. O
Ultimately O
, O
more O
realistic O
samples O
should O
be O
generated O
by O
an O
architecture O
supporting O
this O
kind O
of O
latent Method
- Method
variable Method
design Method
, O
since O
the O
local Method
variable Method
layers Method
will O
robustly O
encode O
hierarchical O
semantic O
cues O
in O
an O
unsupervised Method
fashion Method
. O
subsection O
: O
Model O
Objective O
: O
Variational Task
+ Task
Latent Task
Loss Task
The O
variational Method
auto Method
- Method
encoder Method
( O
VAE Method
) O
has O
recently O
been O
introduced O
as O
a O
powerful O
generative Method
model Method
for O
unsupervised Task
learning Task
. O
The O
generative Method
model Method
for O
a O
single O
data O
point O
with O
a O
latent O
variable O
can O
be O
parameterized O
by O
a O
neural Method
network Method
with O
parameters O
. O
The O
parameters O
are O
inferred O
by O
maximizing O
the O
variational O
lower O
bound O
, O
The O
inference Method
model Method
can O
also O
be O
parameterized O
by O
a O
deep Method
neural Method
network Method
. O
The O
inference O
and O
generative O
parameters O
are O
then O
jointly O
trained O
by O
optimizing O
Equation O
[ O
reference O
] O
using O
back Method
- Method
propagation Method
and O
stochastic Method
gradient Method
ascent Method
. O
To O
deal O
with O
the O
stochasticity O
of O
the O
latent O
variables O
, O
which O
, O
in O
VAE Method
models O
, O
are O
typically O
assumed O
to O
be O
Gaussian O
distributed O
, O
we O
use O
the O
re Method
- Method
parameterization Method
trick Method
in O
order O
to O
back O
- O
propagate O
through O
the O
operation O
of O
sampling O
the O
Gaussian O
variables O
. O
We O
refer O
the O
reader O
to O
for O
a O
much O
more O
detailed O
explanation O
. O
To O
learn O
the O
parameters O
of O
the O
VSL Method
latent O
- O
variable O
model O
, O
we O
will O
take O
a O
variational Method
inference Method
approach O
, O
where O
the O
goal O
is O
to O
learn O
a O
generative Method
model Method
, O
with O
generative O
parameters O
, O
using O
a O
recognition Method
model Method
, O
with O
variational O
parameters O
. O
The O
VSL Method
’s O
learning O
objective O
contains O
a O
standard O
reconstruction O
loss O
term O
as O
well O
as O
a O
regularization O
penalty O
over O
the O
latent O
variables O
. O
Furthermore O
, O
the O
loss O
contains O
an O
additional O
term O
for O
the O
latent O
variables O
, O
which O
is O
particularly O
relevant O
and O
useful O
for O
the O
3D Task
model Task
retrieval Task
task Task
of O
Section O
[ O
reference O
] O
. O
This O
extra O
term O
is O
a O
simple O
penalty O
imposed O
on O
the O
difference O
between O
the O
learned O
features O
of O
the O
image O
regressor O
and O
true O
latent O
features O
where O
denotes O
concatenation O
. O
We O
assume O
a O
fixed O
, O
spherical O
unit O
Gaussian O
prior O
, O
. O
The O
conditional O
distribution O
over O
each O
local O
latent O
code O
( O
) O
is O
defined O
as O
follows O
: O
where O
the O
first O
local O
code O
is O
simply O
: O
Note O
that O
and O
are O
also O
spherical Method
Gaussians Method
and O
contains O
the O
generative O
parameters O
. O
The O
( O
occupancy O
) O
probability O
for O
one O
voxel O
can O
then O
be O
calculated O
by O
, O
Let O
the O
reconstructed O
voxel O
be O
directly O
parameterized O
by O
occupancy O
probability O
. O
The O
loss O
for O
the O
input O
voxel O
of O
the O
VSL Method
is O
then O
calculated O
by O
the O
following O
equation O
: O
where O
each O
term O
in O
the O
equation O
above O
is O
defined O
as O
follows O
: O
Note O
that O
and O
, O
which O
weigh O
the O
contributions O
of O
the O
each O
term O
towards O
the O
overall O
cost Metric
, O
are O
tunable O
hyper O
- O
parameters O
. O
subsection O
: O
Encoder Method
: O
3D Method
- Method
ConvNet Method
+ Method
Skip Method
- Method
Connections Method
The O
global O
latent O
code O
is O
directly O
learned O
from O
the O
input O
voxel O
through O
three O
convolutional Method
layers Method
with O
kernel O
sizes O
, O
strides O
and O
channels O
. O
Each O
local O
latent O
code O
is O
conditioned O
on O
the O
global O
latent O
code O
, O
the O
input O
voxel O
, O
and O
the O
previous O
latent O
code O
( O
except O
for O
, O
which O
does O
not O
have O
a O
previous O
latent O
code O
) O
using O
two O
fully Method
- Method
connected Method
layers Method
with O
neurons O
each O
. O
The O
skip O
- O
connections O
between O
local O
codes O
help O
ease O
the O
process O
of O
learning O
hierarchical O
features O
( O
i.e. O
, O
improved O
gradient O
transmission O
) O
and O
force O
each O
local Method
code Method
to O
learn O
one O
level O
of O
abstraction O
. O
The O
approximate O
posterior O
for O
a O
single O
voxel O
is O
given O
by O
: O
where O
, O
the O
variational O
parameters O
, O
is O
parameterized O
by O
neural Method
networks Method
. O
represents O
the O
number O
of O
local O
latent O
codes O
. O
subsection O
: O
Decoder Method
: O
3D Task
- Task
DeConvNet Task
After O
we O
learn O
the O
global O
and O
local O
latent O
codes O
, O
we O
then O
concatenate O
them O
into O
a O
single O
vector O
as O
shown O
in O
Figure O
[ O
reference O
] O
in O
blue O
dashed O
lines O
. O
A O
3D Method
deconvolutional Method
neural Method
network Method
with O
dimensions O
symmetrical O
to O
the O
encoder O
of O
Section O
[ O
reference O
] O
is O
used O
to O
decode O
the O
learned O
latent O
features O
into O
a O
voxel O
. O
An O
element Method
- Method
wise Method
logistic Method
sigmoid Method
is O
applied O
to O
the O
output O
layer O
in O
order O
to O
convert O
the O
learned O
features O
to O
occupancy O
probabilities O
for O
each O
voxel O
cell O
. O
subsection O
: O
Image Task
Regressor Task
: O
2D Method
- Method
ConvNet Method
We O
use O
a O
standard O
2D Method
convolutional Method
network Method
to O
encode O
input O
RGB Material
images Material
into O
a O
feature O
space O
with O
the O
same O
dimension O
as O
the O
concatenation O
of O
global O
and O
local O
latent O
codes O
. O
The O
network O
contains O
four O
fully Method
- Method
convolutional Method
layers Method
with O
kernel O
sizes O
, O
strides O
, O
and O
channels O
. O
The O
last O
convolutional Method
layer Method
is O
flattened O
and O
fed O
into O
two O
fully Method
- Method
connected Method
layers Method
with O
200 O
and O
100 O
neurons O
each O
. O
Unlike O
the O
encoder Method
described O
in O
Section O
[ O
reference O
] O
, O
we O
apply O
dropout Method
before O
the O
last O
fully Method
- Method
connected Method
layer Method
. O
section O
: O
Experiments O
To O
evaluate O
the O
quality O
of O
our O
proposed O
generative Method
model Method
for O
3D Task
shapes Task
, O
we O
conduct O
several O
extensive O
experiments O
. O
In O
Section O
[ O
reference O
] O
, O
we O
investigate O
our O
model O
’s O
ability O
to O
generalize O
and O
synthesize O
through O
a O
shape Method
interpolation Method
experiment Method
and O
an O
nearest Method
neighbours Method
analysis Method
of O
random O
generated O
samples O
from O
the O
VSL Method
. O
Following O
this O
, O
in O
Section O
[ O
reference O
] O
, O
we O
evaluate O
our O
model O
on O
the O
task O
of O
unsupervised Task
shape Task
classification Task
by O
directly O
using O
the O
learned O
latent O
features O
on O
both O
the O
ModelNet10 Material
and O
ModelNet40 Material
datasets O
. O
We O
compare O
these O
results O
to O
previous O
supervised Method
and Method
unsupervised Method
state Method
- Method
of Method
- Method
the Method
- Method
art Method
methods Method
. O
Next O
, O
we O
test O
our O
model O
’s O
ability O
to O
reconstruct O
real O
- O
world O
image O
in O
Section O
[ O
reference O
] O
, O
comparing O
our O
results O
to O
3D O
- O
R2N2 Method
and O
NRSfM Method
. O
Finally O
, O
we O
demonstrate O
the O
richness O
of O
the O
VSL Method
’s O
learned O
semantic Method
embeddings Method
through O
vector Method
arithmetic Method
, O
using O
the O
latent O
features O
trained O
on O
ModelNet40 Material
for O
Section O
[ O
reference O
] O
. O
subsection O
: O
Datasets O
ModelNet Material
There O
are O
two O
variants O
of O
the O
ModelNet Material
dataset Material
, O
ModelNet10 Material
and O
ModelNet Material
40 Material
, O
introduced O
in O
, O
with O
10 O
and O
40 O
target O
classes O
respectively O
. O
ModelNet10 Material
has O
3D O
shapes O
which O
are O
pre O
- O
aligned O
with O
the O
same O
pose O
across O
all O
categories O
. O
In O
contrast O
, O
ModelNet40 Material
( O
which O
includes O
the O
shapes O
found O
in O
ModelNet10 Material
) O
features O
a O
variety O
of O
poses O
. O
We O
voxelize O
both O
ModelNet10 Material
and O
ModelNet40 Material
with O
resolution O
. O
To O
test O
our O
model O
’s O
ability O
to O
handle O
3D O
shapes O
of O
great O
variety O
and O
complexity O
, O
we O
use O
ModelNet40 Material
for O
most O
of O
the O
experiments O
, O
especially O
for O
those O
in O
Section O
[ O
reference O
] O
and O
[ O
reference O
] O
. O
Both O
ModelNet10 Material
and O
ModelNet40 Material
are O
used O
to O
conduct O
the O
shape Task
classification Task
experiments Task
. O
PASCAL Material
3D Material
The O
PASCAL Material
3D Material
dataset O
is O
composed O
of O
the O
images O
from O
the O
PASCAL Material
VOC Material
2012 Material
dataset Material
, O
augmented O
with O
3D O
annotations O
using O
PASCAL Material
3D Material
+ O
. O
We O
voxelize O
the O
3D Method
CAD Method
models Method
using O
resolution Method
and O
use O
the O
same O
training O
and O
testing O
splits O
of O
, O
which O
was O
also O
used O
in O
to O
conduct O
real O
- O
world O
image Task
reconstruction Task
( O
of O
which O
the O
experiment O
in O
Section O
[ O
reference O
] O
is O
based O
off O
of O
) O
. O
We O
use O
the O
bounding O
box O
information O
as O
provided O
in O
the O
dataset O
. O
Note O
that O
the O
only O
pre O
- O
processing O
we O
applied O
was O
image Task
cropping Task
and O
padding O
with O
0 O
- O
intensity O
pixels O
to O
create O
final O
samples O
of O
resolution O
( O
which O
was O
required O
for O
our O
model O
) O
. O
subsection O
: O
Training O
Protocol O
Training O
was O
the O
same O
across O
all O
experiments O
, O
with O
only O
minor O
details O
that O
were O
task O
- O
dependent O
. O
The O
architecture O
of O
the O
VSL Method
experimented O
with O
in O
this O
paper O
consisted O
of O
5 O
local O
latent O
codes O
, O
each O
made O
up O
of O
10 O
variables O
for O
ModelNet40 Material
and O
5 O
for O
ModelNet10 Material
. O
For O
ModelNet40 Material
, O
the O
global O
latent O
code O
was O
set O
to O
a O
dimensionality O
of O
20 O
variables O
, O
while O
for O
ModelNet10 Material
, O
it O
was O
set O
to O
10 O
variables O
. O
The O
hyper O
- O
parameter O
was O
set O
to O
across O
training O
on O
both O
ModelNet10 Material
and O
ModelNet40 Material
. O
We O
optimise O
parameters O
by O
maximizing O
the O
loss O
function O
defined O
in O
Equation O
[ O
reference O
] O
using O
the O
Adam Method
adaptive Method
learning Method
rate Method
, O
with O
step O
size O
set O
to O
. O
For O
the O
experiments O
of O
Sections O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
, O
over O
2500 O
epochs O
, O
parameter O
updates O
were O
calculated O
using O
mini O
- O
batches O
of O
200 O
samples O
on O
ModelNet40 Material
and O
100 O
samples O
on O
ModelNet10 Material
. O
For O
the O
experiment O
in O
Section O
[ O
reference O
] O
, O
we O
use O
5 O
local O
latent O
codes O
( O
each O
with O
dimensionality O
of O
5 O
) O
and O
a O
global O
latent O
code O
of O
20 O
variables O
for O
the O
jointly Method
trained Method
model Method
. O
For O
the O
separately O
trained O
model O
, O
we O
use O
3 O
local O
latent O
codes O
, O
each O
with O
dimensionality O
of O
2 O
, O
and O
a O
global O
latent O
code O
of O
dimensionality O
5 O
. O
Mini O
- O
batches O
of O
40 O
samples O
were O
use O
to O
compute O
gradients O
for O
the O
joint Method
model Method
while O
5 O
samples O
were O
used O
for O
the O
separately O
trained O
model O
. O
For O
both O
model O
variants O
, O
dropout Method
was O
to O
control O
for O
over Task
- Task
fitting Task
, O
with O
, O
and O
early O
stopping O
was O
employed O
( O
resulting O
in O
only O
150 O
epochs O
) O
. O
For O
Section O
[ O
reference O
] O
, O
which O
involved O
image Task
reconstruction Task
and O
thus O
required O
the O
loss O
term O
, O
instead O
of O
searching O
for O
an O
optimal O
value O
of O
the O
hyper O
- O
parameter O
through O
cross Method
- Method
validation Method
, O
we O
employed O
a O
“ O
warming O
- O
up O
” O
schedule O
, O
similar O
to O
that O
of O
. O
“ O
Warming Task
- Task
up Task
” O
involves O
gradually O
increasing O
( O
on O
a O
log O
- O
scale O
as O
depicted O
in O
Figure O
[ O
reference O
] O
) O
, O
which O
controls O
the O
relative O
weighting O
of O
in O
Equation O
[ O
reference O
] O
. O
The O
schedule O
is O
defined O
as O
follows O
, O
[ O
b O
] O
0.23 O
[ O
b O
] O
0.23 O
Figure O
[ O
reference O
] O
depicts O
, O
empirically O
, O
the O
benefits O
of O
employing O
a O
warming Method
- Method
up Method
schedule Method
over O
using O
a O
fixed O
, O
externally O
set O
coefficient O
for O
the O
term O
in O
our O
image Task
reconstruction Task
experiment Task
. O
We O
remark O
that O
using O
a O
warming Method
- Method
up Method
schedule Method
plays O
an O
essential O
role O
in O
acquiring O
good O
performance O
on O
the O
image Task
reconstruction Task
task Task
. O
subsection O
: O
Shape Task
Generation Task
and O
Learning Task
[ O
b O
] O
Intra Method
- Method
Class Method
Interpolation Method
( O
airplane Method
) O
[ O
b O
] O
Inter O
- O
Class Method
Interpolation Method
( O
chair O
→ O
bed O
) O
To O
examine O
our O
model O
’s O
ability O
to O
generate O
high Task
- Task
resolution Task
3D Task
shapes Task
with O
realistic O
details O
, O
we O
design O
a O
task O
that O
involves O
shape Task
generation Task
and O
shape Task
interpolation Task
. O
We O
add O
Gaussian O
noise O
to O
the O
learned O
latent Method
codes Method
on O
test O
data O
taken O
from O
ModelNet40 Material
and O
then O
use O
our O
model O
to O
generate O
“ O
unseen O
” O
samples O
that O
are O
similar O
to O
the O
input O
voxel O
. O
In O
effect O
, O
we O
generate O
objects O
from O
our O
VSL Method
model O
directly O
from O
vectors O
, O
without O
a O
reference O
object O
/ O
image O
. O
The O
results O
of O
our O
shape Task
interpolation Task
experiment O
, O
from O
both O
within O
- O
class O
and O
across O
- O
class O
perspectives O
, O
is O
presented O
in O
Figure O
[ O
reference O
] O
. O
It O
can O
be O
observed O
that O
the O
proposed O
VSL Method
shows O
the O
ability O
to O
smoothly O
transition O
between O
two O
objects O
. O
Our O
results O
on O
shape Task
generation Task
are O
shown O
in O
Figure O
[ O
reference O
] O
. O
Notably O
, O
in O
our O
visualizations O
, O
darker O
colours O
correspond O
to O
smaller O
occupancy O
probability O
while O
lighter O
corresponds O
to O
higher O
occupancy O
probability O
. O
We O
further O
compare O
to O
previous O
state O
- O
of O
- O
the O
- O
art O
results O
in O
shape Task
generation Task
, O
which O
are O
depicted O
in O
Figure O
[ O
reference O
] O
. O
During O
training Task
, O
we O
observed O
that O
our O
model O
was O
robust O
to O
different O
choices O
of O
the O
number O
and O
dimensionality O
of O
its O
local O
/ O
global O
latent O
codes O
. O
We O
provide O
the O
table O
below O
as O
an O
ablative O
analysis O
showing O
how O
test Metric
reconstruction Metric
error Metric
is O
affected O
by O
various O
settings O
of O
the O
latent O
variables O
. O
From O
the O
results O
, O
we O
can O
observe O
a O
clear O
trend O
that O
the O
network O
with O
higher O
dimensionality O
and O
greater O
number O
of O
latent O
variables O
tends O
to O
generate O
better O
results O
. O
However O
, O
increasing O
the O
number O
of O
network O
parameters O
to O
attain O
better O
accuracy Metric
also O
brings O
about O
slower Task
training Task
, O
an O
important O
trade O
- O
off O
that O
one O
will O
need O
to O
consider O
in O
various O
application O
scenarios O
. O
subsection O
: O
Shape Task
Classification Task
One O
way O
to O
test O
model O
expressiveness O
is O
to O
conduct O
shape Task
classification Task
directly O
using O
the O
learned O
embeddings Method
. O
We O
evaluate O
the O
features O
learned O
on O
the O
ModelNet Material
dataset Material
by O
concatenating O
both O
the O
global O
latent O
variable O
with O
the O
local O
latent O
layers O
, O
creating O
a O
single O
feature O
vector O
. O
We O
train O
a O
Support Method
Vector Method
Machine Method
with O
an O
RBF Method
kernel Method
for O
classification Task
using O
these O
“ O
pre O
- O
trained O
” O
embeddings O
. O
Table O
[ O
reference O
] O
shows O
the O
performance O
of O
previous O
state O
- O
of O
- O
the O
- O
art O
supervised Method
and Method
unsupervised Method
methods Method
in O
shape Task
classification Task
on O
both O
variants O
of O
the O
ModelNet Material
dataset Material
. O
Notably O
, O
the O
best O
unsupervised O
state O
- O
of O
- O
the O
- O
art O
results O
reported O
so O
far O
were O
from O
the O
3D Method
- Method
GAN Method
of Method
, O
which O
used O
features O
from O
3 O
layers O
of O
convolutional Method
networks Method
with O
total O
dimensions O
. O
This O
is O
a O
far O
larger O
feature O
space O
than O
that O
required O
by O
our O
model O
, O
which O
is O
simply O
( O
for O
10 Task
- Task
way Task
classification Task
) O
and O
( O
for O
40 Task
- Task
way Task
classification Task
) O
and O
reaches O
the O
exact O
same O
level O
of O
performance O
. O
The O
VSL Method
performs O
comparably O
to O
supervised O
state O
- O
of O
- O
the O
- O
art O
, O
outperforming O
models O
such O
as O
3D Method
ShapeNet Method
, O
DeepPano Method
, O
and O
the O
geometry Method
image Method
- Method
base Method
approach Method
, O
by O
a O
large O
margin O
, O
and O
comes O
close O
to O
models O
such O
as O
VoxNet Method
. O
In O
order O
to O
visualise O
the O
learned O
feature O
embeddings O
, O
we O
employ O
t Method
- Method
SNE Method
to O
map O
our O
high O
dimensional O
feature O
to O
a O
2D O
plane O
. O
The O
visualization O
is O
shown O
in O
Figure O
[ O
reference O
] O
. O
[ O
b O
] O
0.2 O
ModelNet10 Material
[ O
b O
] O
0.2 O
ModelNet40 Material
subsection O
: O
Single Task
Image Task
3D Task
Model Task
Retrieval Task
Real Task
- Task
world Task
, Task
single Task
image Task
3D Task
model Task
retrieval Task
is O
another O
application O
of O
the O
proposed O
VSL Method
model O
. O
This O
is O
a O
challenging O
problem O
, O
forcing O
a O
model O
to O
deal O
with O
real O
- O
world O
2D O
images O
under O
a O
variety O
of O
lighting O
conditions O
and O
resolutions O
. O
Furthermore O
, O
there O
are O
many O
instances O
of O
model O
occlusion O
as O
well O
as O
different O
colour O
gradings O
. O
To O
test O
our O
model O
on O
this O
application O
, O
we O
use O
the O
PASCAL Material
3D Material
dataset O
and O
utilize O
the O
same O
exact O
training O
and O
testing O
splits O
from O
. O
We O
compare O
our O
results O
with O
those O
reported O
for O
recent O
approaches O
, O
including O
the O
NRSfM Method
and Method
3D Method
- Method
R2N2 Method
models Method
. O
Note O
that O
these O
also O
used O
the O
exact O
same O
experimental O
configurations O
we O
did O
. O
For O
this O
task O
, O
we O
train O
our O
model O
in O
two O
different O
ways O
: O
1 O
) O
jointly O
on O
all O
categories O
, O
and O
2 O
) O
separately O
on O
each O
category O
. O
In O
Figure O
[ O
reference O
] O
, O
we O
observe O
better O
reconstructions O
from O
the O
( O
separately O
- O
trained O
) O
VSL Method
when O
compared O
to O
previous O
work O
. O
Unlike O
the O
NRSfM Method
, O
the O
VSL Method
does O
not O
require O
any O
segmentation O
, O
pose O
information O
, O
or O
keypoints O
. O
In O
addition O
, O
the O
VSL Method
is O
trained O
from O
scratch O
while O
the O
3D Method
- Method
R2N2 Method
is O
pre O
- O
trained O
using O
the O
ShapeNet Material
dataset Material
. O
However O
, O
the O
jointly O
- O
trained O
VSL Method
did O
not O
outperform O
the O
3D Method
- Method
R2N2 Method
, O
which O
is O
also O
jointly O
- O
trained O
. O
The O
performance O
gap O
is O
due O
to O
the O
fact O
that O
the O
3D Method
- Method
R2N2 Method
is O
specifically O
designed O
for O
image Task
reconstruction Task
and O
employs O
a O
residual Method
network Method
to O
help O
the O
model O
learn O
richer O
semantic O
features O
. O
Quantitatively O
, O
we O
compare O
our O
VSL Method
to O
the O
NRSfM Method
and O
two O
versions O
of O
3D Method
- Method
R2N2 Method
from Method
, O
one O
with O
an O
LSTM Method
structure Method
and O
another O
with O
a O
deep Method
residual Method
network Method
. O
Results O
( O
Intersection Metric
- Metric
of Metric
- Metric
Union Metric
) O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
Observe O
that O
our O
jointly Method
trained Method
model Method
performs O
comparably O
to O
the O
3D Method
- Method
R2N2 Method
LSTM Method
variant Method
while O
the O
separately O
trained O
version O
surpasses O
the O
3D Method
- Method
R2N2 Method
ResNet Method
structure Method
in O
8 O
out O
of O
10 O
categories O
, O
half O
of O
them O
by O
a O
wide O
margin O
. O
subsection O
: O
Shape Method
Arithmetic Method
Another O
way O
to O
explore O
the O
learned O
embeddings O
is O
to O
perform O
various O
vector Method
operations Method
on O
the O
latent O
space O
, O
much O
what O
was O
done O
in O
. O
We O
present O
some O
results O
of O
our O
shape Method
arithmetic Method
experiment O
in O
Figure O
[ O
reference O
] O
. O
Different O
from O
previous O
results O
, O
all O
of O
our O
objects O
are O
sampled O
from O
the O
model Method
embeddings Method
which O
were O
trained O
using O
the O
whole O
dataset O
with O
40 O
classes O
. O
Furthermore O
, O
unlike O
the O
blurrier O
generations O
of O
, O
the O
VSL Method
seems O
to O
generate O
very O
interesting O
combinations O
of O
the O
input O
embeddings O
without O
the O
need O
for O
any O
matching O
to O
actual O
3D O
shapes O
from O
the O
original O
dataset O
. O
The O
resultant O
objects O
appear O
to O
clearly O
embody O
the O
intuitive O
meaning O
of O
the O
vector O
operators O
. O
section O
: O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
the O
Variational Method
Shape Method
Learner Method
, O
a O
hierarchical Method
latent Method
- Method
variable Method
model Method
for O
3D Task
shape Task
modelling Task
, O
learnable O
through O
variational Method
inference Method
. O
In O
particular O
, O
we O
have O
demonstrated O
3D Task
shape Task
generation Task
results O
on O
a O
popular O
benchmark O
, O
the O
ModelNet Material
dataset Material
. O
We O
also O
used O
the O
learned O
embeddings O
of O
our O
model O
to O
obtain O
state O
- O
of O
- O
the O
- O
art O
in O
unsupervised Task
shape Task
classification Task
and O
furthermore O
showed O
that O
we O
could O
generate O
unseen O
shapes O
using O
shape O
arithmetic O
. O
Future O
work O
will O
entail O
a O
more O
thorough O
investigation O
of O
the O
embeddings O
learned O
by O
our O
hierarchical Method
latent Method
- Method
variable Method
model Method
as O
well O
as O
integration O
of O
better O
prior O
distributions O
into O
the O
framework O
. O
bibliography O
: O
References O
