document	O
:	O
GANs	Method
Trained	O
by	O
a	O
Two	O
Time	Method
-	Method
Scale	Method
Update	Method
Rule	Method
Converge	O
to	O
a	O
Local	Method
Nash	Method
Equilibrium	Method
Generative	Method
Adversarial	Method
Networks	Method
(	O
GANs	Method
)	O
excel	O
at	O
creating	O
realistic	Task
images	Task
with	O
complex	Method
models	Method
for	O
which	O
maximum	Task
likelihood	Task
is	O
infeasible	O
.	O
However	O
,	O
the	O
convergence	O
of	O
GAN	Method
training	Method
has	O
still	O
not	O
been	O
proved	O
.	O
We	O
propose	O
a	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
for	O
training	O
GANs	Method
with	O
stochastic	Method
gradient	Method
descent	Method
on	O
arbitrary	O
GAN	O
loss	O
functions	O
.	O
TTUR	Method
has	O
an	O
individual	O
learning	Metric
rate	Metric
for	O
both	O
the	O
discriminator	Method
and	O
the	O
generator	Method
.	O
Using	O
the	O
theory	O
of	O
stochastic	Method
approximation	Method
,	O
we	O
prove	O
that	O
the	O
TTUR	Method
converges	O
under	O
mild	O
assumptions	O
to	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
.	O
The	O
convergence	O
carries	O
over	O
to	O
the	O
popular	O
Adam	Task
optimization	Task
,	O
for	O
which	O
we	O
prove	O
that	O
it	O
follows	O
the	O
dynamics	O
of	O
a	O
heavy	Method
ball	Method
with	Method
friction	Method
and	O
thus	O
prefers	O
flat	O
minima	O
in	O
the	O
objective	O
landscape	O
.	O
For	O
the	O
evaluation	O
of	O
the	O
performance	O
of	O
GANs	Method
at	O
image	Task
generation	Task
,	O
we	O
introduce	O
the	O
‘	O
Fréchet	Metric
Inception	Metric
Distance	Metric
’	O
’	O
(	O
FID	Metric
)	O
which	O
captures	O
the	O
similarity	O
of	O
generated	O
images	O
to	O
real	O
ones	O
better	O
than	O
the	O
Inception	Metric
Score	Metric
.	O
In	O
experiments	O
,	O
TTUR	Method
improves	O
learning	Task
for	O
DCGANs	Method
and	O
Improved	O
Wasserstein	Method
GANs	Method
(	O
WGAN	Method
-	Method
GP	Method
)	O
outperforming	O
conventional	O
GAN	Method
training	Method
on	O
CelebA	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
SVHN	Material
,	O
LSUN	Material
Bedrooms	Material
,	O
and	O
the	O
One	Material
Billion	Material
Word	Material
Benchmark	Material
.	O
section	O
:	O
Introduction	O
Generative	Method
adversarial	Method
networks	Method
(	O
GANs	Method
)	O
have	O
achieved	O
outstanding	O
results	O
in	O
generating	O
realistic	O
images	O
and	O
producing	Task
text	Task
.	O
GANs	Method
can	O
learn	O
complex	O
generative	Method
models	Method
for	O
which	O
maximum	Method
likelihood	Method
or	O
a	O
variational	Method
approximations	Method
are	O
infeasible	O
.	O
Instead	O
of	O
the	O
likelihood	O
,	O
a	O
discriminator	Method
network	Method
serves	O
as	O
objective	O
for	O
the	O
generative	Method
model	Method
,	O
that	O
is	O
,	O
the	O
generator	Method
.	O
GAN	Method
learning	Method
is	O
a	O
game	O
between	O
the	O
generator	Method
,	O
which	O
constructs	O
synthetic	O
data	O
from	O
random	O
variables	O
,	O
and	O
the	O
discriminator	Method
,	O
which	O
separates	O
synthetic	O
data	O
from	O
real	O
world	O
data	O
.	O
The	O
generator	O
’s	O
goal	O
is	O
to	O
construct	O
data	O
in	O
such	O
a	O
way	O
that	O
the	O
discriminator	Method
can	O
not	O
tell	O
them	O
apart	O
from	O
real	O
world	O
data	O
.	O
Thus	O
,	O
the	O
discriminator	Method
tries	O
to	O
minimize	O
the	O
synthetic	Metric
-	Metric
real	Metric
discrimination	Metric
error	Metric
while	O
the	O
generator	Method
tries	O
to	O
maximize	O
this	O
error	O
.	O
Since	O
training	Method
GANs	Method
is	O
a	O
game	O
and	O
its	O
solution	O
is	O
a	O
Nash	O
equilibrium	O
,	O
gradient	Method
descent	Method
may	O
fail	O
to	O
converge	O
.	O
Only	O
local	O
Nash	O
equilibria	O
are	O
found	O
,	O
because	O
gradient	Method
descent	Method
is	O
a	O
local	Method
optimization	Method
method	Method
.	O
If	O
there	O
exists	O
a	O
local	O
neighborhood	O
around	O
a	O
point	O
in	O
parameter	O
space	O
where	O
neither	O
the	O
generator	O
nor	O
the	O
discriminator	Method
can	O
unilaterally	O
decrease	O
their	O
respective	O
losses	O
,	O
then	O
we	O
call	O
this	O
point	O
a	O
local	O
Nash	O
equilibrium	O
.	O
To	O
characterize	O
the	O
convergence	Metric
properties	Metric
of	O
training	Method
general	Method
GANs	Method
is	O
still	O
an	O
open	O
challenge	O
.	O
For	O
special	O
GAN	Method
variants	Method
,	O
convergence	O
can	O
be	O
proved	O
under	O
certain	O
assumptions	O
.	O
A	O
prerequisit	O
for	O
many	O
convergence	Task
proofs	Task
is	O
local	O
stability	O
which	O
was	O
shown	O
for	O
GANs	Method
by	O
Nagarajan	O
and	O
Kolter	O
for	O
a	O
min	Task
-	Task
max	Task
GAN	Task
setting	Task
.	O
However	O
,	O
Nagarajan	O
and	O
Kolter	O
require	O
for	O
their	O
proof	O
either	O
rather	O
strong	O
and	O
unrealistic	O
assumptions	O
or	O
a	O
restriction	O
to	O
a	O
linear	Method
discriminator	O
.	O
Recent	O
convergence	Method
proofs	Method
for	O
GANs	Method
hold	O
for	O
expectations	O
over	O
training	O
samples	O
or	O
for	O
the	O
number	O
of	O
examples	O
going	O
to	O
infinity	O
,	O
thus	O
do	O
not	O
consider	O
mini	Method
-	Method
batch	Method
learning	Method
which	O
leads	O
to	O
a	O
stochastic	O
gradient	O
.	O
Recently	O
actor	Method
-	Method
critic	Method
learning	Method
has	O
been	O
analyzed	O
using	O
stochastic	Method
approximation	Method
.	O
Prasad	O
et	O
al	O
.	O
showed	O
that	O
a	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
ensures	O
that	O
training	O
reaches	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
if	O
the	O
critic	Method
learns	O
faster	O
than	O
the	O
actor	O
.	O
Convergence	Task
was	O
proved	O
via	O
an	O
ordinary	Method
differential	Method
equation	Method
(	Method
ODE	Method
)	O
,	O
whose	O
stable	O
limit	O
points	O
coincide	O
with	O
stationary	O
local	O
Nash	O
equilibria	O
.	O
We	O
follow	O
the	O
same	O
approach	O
.	O
We	O
prove	O
that	O
GANs	Method
converge	O
to	O
a	O
local	O
Nash	O
equilibrium	O
when	O
trained	O
by	O
a	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
,	O
i.e.	O
,	O
when	O
discriminator	Method
and	Method
generator	Method
have	O
separate	O
learning	Metric
rates	Metric
.	O
This	O
also	O
leads	O
to	O
better	O
results	O
in	O
experiments	O
.	O
The	O
main	O
premise	O
is	O
that	O
the	O
discriminator	Method
converges	O
to	O
a	O
local	O
minimum	Metric
when	O
the	O
generator	Method
is	O
fixed	O
.	O
If	O
the	O
generator	O
changes	O
slowly	O
enough	O
,	O
then	O
the	O
discriminator	Method
still	O
converges	O
,	O
since	O
the	O
generator	O
perturbations	O
are	O
small	O
.	O
Besides	O
ensuring	O
convergence	O
,	O
the	O
performance	O
may	O
also	O
improve	O
since	O
the	O
discriminator	Method
must	O
first	O
learn	O
new	O
patterns	O
before	O
they	O
are	O
transferred	O
to	O
the	O
generator	Method
.	O
In	O
contrast	O
,	O
a	O
generator	Method
which	O
is	O
overly	O
fast	O
,	O
drives	O
the	O
discriminator	O
steadily	O
into	O
new	O
regions	O
without	O
capturing	O
its	O
gathered	O
information	O
.	O
In	O
recent	O
GAN	Method
implementations	Method
,	O
the	O
discriminator	Method
often	O
learned	O
faster	O
than	O
the	O
generator	Method
.	O
A	O
new	O
objective	O
slowed	O
down	O
the	O
generator	O
to	O
prevent	O
it	O
from	O
overtraining	O
on	O
the	O
current	O
discriminator	Method
.	O
The	O
Wasserstein	Method
GAN	Method
algorithm	O
uses	O
more	O
update	Method
steps	Method
for	O
the	O
discriminator	Method
than	O
for	O
the	O
generator	Method
.	O
We	O
compare	O
TTUR	Method
and	O
standard	O
GAN	Method
training	Method
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
at	O
the	O
left	O
panel	O
a	O
stochastic	O
gradient	O
example	O
on	O
CelebA	Material
for	O
original	Task
GAN	Task
training	Task
(	O
orig	Task
)	O
,	O
which	O
often	O
leads	O
to	O
oscillations	O
,	O
and	O
the	O
TTUR	Method
.	O
On	O
the	O
right	O
panel	O
an	O
example	O
of	O
a	O
4	Task
node	Task
network	Task
flow	Task
problem	Task
of	O
Zhang	O
et	O
al	O
.	O
is	O
shown	O
.	O
The	O
distance	O
between	O
the	O
actual	O
parameter	O
and	O
its	O
optimum	O
for	O
an	O
one	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
is	O
shown	O
across	O
iterates	O
.	O
When	O
the	O
upper	O
bounds	O
on	O
the	O
errors	O
are	O
small	O
,	O
the	O
iterates	O
return	O
to	O
a	O
neighborhood	O
of	O
the	O
optimal	O
solution	O
,	O
while	O
for	O
large	O
errors	O
the	O
iterates	O
may	O
diverge	O
(	O
see	O
also	O
Appendix	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Our	O
novel	O
contributions	O
in	O
this	O
paper	O
are	O
:	O
The	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
for	O
GANs	Method
,	O
We	O
proof	O
that	O
GANs	Method
trained	O
with	O
TTUR	Method
converge	O
to	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
,	O
The	O
description	O
of	O
Adam	Method
as	O
heavy	Method
ball	Method
with	Method
friction	Method
and	O
the	O
resulting	O
second	Method
order	Method
differential	Method
equation	Method
,	O
The	O
convergence	Method
of	Method
GANs	Method
trained	O
with	O
TTUR	Method
and	O
Adam	Method
to	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
,	O
We	O
introduce	O
the	O
‘	O
‘	O
Fréchet	Metric
Inception	Metric
Distance	Metric
’	O
’	O
(	O
FID	Metric
)	O
to	O
evaluate	O
GANs	Method
,	O
which	O
is	O
more	O
consistent	O
than	O
the	O
Inception	Metric
Score	Metric
.	O
section	O
:	O
Two	O
Time	Method
-	Method
Scale	Method
Update	Method
Rule	Method
for	O
GANs	Method
We	O
consider	O
a	O
discriminator	Method
with	Method
parameter	Method
vector	Method
and	O
a	O
generator	Method
with	O
parameter	O
vector	O
.	O
Learning	Method
is	O
based	O
on	O
a	O
stochastic	Method
gradient	Method
of	Method
the	Method
discriminator	Method
’s	Method
loss	Method
function	Method
and	O
a	O
stochastic	Method
gradient	Method
of	Method
the	Method
generator	Method
’s	Method
loss	Method
function	Method
.	O
The	O
loss	O
functions	O
and	O
can	O
be	O
the	O
original	O
as	O
introduced	O
in	O
Goodfellow	O
et	O
al	O
.	O
,	O
its	O
improved	O
versions	O
,	O
or	O
recently	O
proposed	O
losses	O
for	O
GANs	Method
like	O
the	O
Wasserstein	Method
GAN	Method
.	O
Our	O
setting	O
is	O
not	O
restricted	O
to	O
min	Method
-	Method
max	Method
GANs	Method
,	O
but	O
is	O
valid	O
for	O
all	O
other	O
,	O
more	O
general	O
GANs	Method
for	O
which	O
the	O
discriminator	O
’s	O
loss	O
function	O
is	O
not	O
necessarily	O
related	O
to	O
the	O
generator	O
’s	O
loss	O
function	O
.	O
The	O
gradients	Method
and	O
are	O
stochastic	O
,	O
since	O
they	O
use	O
mini	O
-	O
batches	O
of	O
real	O
world	O
samples	O
and	O
synthetic	O
samples	O
which	O
are	O
randomly	O
chosen	O
.	O
If	O
the	O
true	O
gradients	O
are	O
and	O
,	O
then	O
we	O
can	O
define	O
and	O
with	O
random	O
variables	O
and	O
.	O
Thus	O
,	O
the	O
gradients	O
and	O
are	O
stochastic	Method
approximations	Method
to	O
the	O
true	O
gradients	O
.	O
Consequently	O
,	O
we	O
analyze	O
convergence	O
of	O
GANs	Method
by	O
two	O
time	Method
-	Method
scale	Method
stochastic	Method
approximations	Method
algorithms	Method
.	O
For	O
a	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
,	O
we	O
use	O
the	O
learning	O
rates	O
and	O
for	O
the	O
discriminator	Method
and	O
the	O
generator	Method
update	Method
,	O
respectively	O
:	O
For	O
more	O
details	O
on	O
the	O
following	O
convergence	O
proof	O
and	O
its	O
assumptions	O
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
.	O
To	O
prove	O
convergence	O
of	O
GANs	Method
learned	O
by	O
TTUR	Method
,	O
we	O
make	O
the	O
following	O
assumptions	O
(	O
The	O
actual	O
assumption	O
is	O
ended	O
by	O
,	O
the	O
following	O
text	O
are	O
just	O
comments	O
and	O
explanations	O
)	O
:	O
The	O
gradients	O
and	O
are	O
Lipschitz	O
.	O
Consequently	O
,	O
networks	O
with	O
Lipschitz	O
smooth	O
activation	O
functions	O
like	O
ELUs	Method
(	Method
)	O
fulfill	O
the	O
assumption	O
but	O
not	O
ReLU	Method
networks	Method
.	O
,	O
,	O
,	O
,	O
The	O
stochastic	O
gradient	O
errors	O
and	O
are	O
martingale	Method
difference	Method
sequences	Method
w.r.t	O
.	O
the	O
increasing	O
-	O
field	O
with	O
and	O
,	O
where	O
and	O
are	O
positive	O
deterministic	O
constants	O
.	O
The	O
original	O
Assumption	O
(	O
A3	O
)	O
from	O
Borkar	O
1997	O
follows	O
from	O
Lemma	O
2	O
in	O
(	O
see	O
also	O
)	O
.	O
The	O
assumption	O
is	O
fulfilled	O
in	O
the	O
Robbins	Task
-	Task
Monro	Task
setting	Task
,	O
where	O
mini	O
-	O
batches	O
are	O
randomly	O
sampled	O
and	O
the	O
gradients	O
are	O
bounded	O
.	O
For	O
each	O
,	O
the	O
ODE	Method
has	O
a	O
local	O
asymptotically	O
stable	O
attractor	O
within	O
a	O
domain	O
of	O
attraction	O
such	O
that	O
is	O
Lipschitz	O
.	O
The	O
ODE	Method
has	O
a	O
local	O
asymptotically	O
stable	O
attractor	O
within	O
a	O
domain	O
of	O
attraction	O
.	O
The	O
discriminator	Method
must	O
converge	O
to	O
a	O
minimum	Metric
for	O
fixed	O
generator	O
parameters	O
and	O
the	O
generator	Method
,	O
in	O
turn	O
,	O
must	O
converge	O
to	O
a	O
minimum	Metric
for	O
this	O
fixed	O
discriminator	O
minimum	Metric
.	O
Borkar	O
1997	O
required	O
unique	O
global	O
asymptotically	O
stable	O
equilibria	O
.	O
The	O
assumption	O
of	O
global	O
attractors	O
was	O
relaxed	O
to	O
local	O
attractors	O
via	O
Assumption	O
(	O
A6	O
)	O
and	O
Theorem	O
2.7	O
in	O
Karmakar	O
&	O
Bhatnagar	O
.	O
See	O
for	O
more	O
details	O
Assumption	O
(	O
A6	O
)	O
in	O
the	O
Appendix	O
Section	O
[	O
reference	O
]	O
.	O
Here	O
,	O
the	O
GAN	O
objectives	O
may	O
serve	O
as	O
Lyapunov	O
functions	O
.	O
These	O
assumptions	O
of	O
locally	O
stable	O
ODEs	O
can	O
be	O
ensured	O
by	O
an	O
additional	O
weight	O
decay	O
term	O
in	O
the	O
loss	O
function	O
which	O
increases	O
the	O
eigenvalues	O
of	O
the	O
Hessian	O
.	O
Therefore	O
,	O
problems	O
with	O
a	O
region	Method
-	Method
wise	Method
constant	Method
discriminator	Method
that	O
has	O
zero	O
second	O
order	O
derivatives	O
are	O
avoided	O
.	O
For	O
further	O
discussion	O
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
(	O
C3	O
)	O
.	O
and	O
.	O
Typically	O
ensured	O
by	O
the	O
objective	O
or	O
a	O
weight	Method
decay	Method
term	Method
.	O
The	O
next	O
theorem	O
has	O
been	O
proved	O
in	O
the	O
seminal	O
paper	O
of	O
Borkar	O
1997	O
.	O
theorem	O
:	O
(	O
Borkar	O
)	O
.	O
If	O
the	O
assumptions	O
are	O
satisfied	O
,	O
then	O
the	O
updates	O
Eq	O
.	O
(	O
)	O
converge	O
to	O
(	O
θ*	O
,	O
⁢λ	O
(	O
θ	O
*	O
)	O
)	O
a.s	O
.	O
The	O
solution	O
is	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
,	O
since	O
as	O
well	O
as	O
are	O
local	O
asymptotically	O
stable	O
attractors	O
with	O
and	O
.	O
An	O
alternative	O
approach	O
to	O
the	O
proof	O
of	O
convergence	Task
using	O
the	O
Poisson	Method
equation	Method
for	O
ensuring	O
a	O
solution	O
to	O
the	O
fast	Method
update	Method
rule	Method
can	O
be	O
found	O
in	O
the	O
Appendix	O
Section	O
[	O
reference	O
]	O
.	O
This	O
approach	O
assumes	O
a	O
linear	Method
update	O
function	O
in	O
the	O
fast	Method
update	Method
rule	Method
which	O
,	O
however	O
,	O
can	O
be	O
a	O
linear	Method
approximation	O
to	O
a	O
nonlinear	O
gradient	O
.	O
For	O
the	O
rate	O
of	O
convergence	Metric
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
,	O
where	O
Section	O
[	O
reference	O
]	O
focuses	O
on	O
linear	Method
and	O
Section	O
[	O
reference	O
]	O
on	O
non	O
-	O
linear	Method
updates	O
.	O
For	O
equal	O
time	O
-	O
scales	O
it	O
can	O
only	O
be	O
proven	O
that	O
the	O
updates	O
revisit	O
an	O
environment	O
of	O
the	O
solution	O
infinitely	O
often	O
,	O
which	O
,	O
however	O
,	O
can	O
be	O
very	O
large	O
.	O
For	O
more	O
details	O
on	O
the	O
analysis	O
of	O
equal	O
time	O
-	O
scales	O
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
.	O
The	O
main	O
idea	O
of	O
the	O
proof	O
of	O
Borkar	O
is	O
to	O
use	O
perturbed	Method
ODEs	Method
according	O
to	O
Hirsch	O
1989	O
(	O
see	O
also	O
Appendix	O
Section	O
C	O
of	O
Bhatnagar	O
,	O
Prasad	O
,	O
&	O
Prashanth	O
2013	O
)	O
.	O
The	O
proof	O
relies	O
on	O
the	O
fact	O
that	O
there	O
eventually	O
is	O
a	O
time	O
point	O
when	O
the	O
perturbation	O
of	O
the	O
slow	Method
update	Method
rule	Method
is	O
small	O
enough	O
(	O
given	O
by	O
)	O
to	O
allow	O
the	O
fast	Method
update	Method
rule	Method
to	O
converge	O
.	O
For	O
experiments	O
with	O
TTUR	Method
,	O
we	O
aim	O
at	O
finding	O
learning	Metric
rates	Metric
such	O
that	O
the	O
slow	O
update	O
is	O
small	O
enough	O
to	O
allow	O
the	O
fast	O
to	O
converge	O
.	O
Typically	O
,	O
the	O
slow	O
update	O
is	O
the	O
generator	Method
and	O
the	O
fast	Method
update	Method
the	O
discriminator	Method
.	O
We	O
have	O
to	O
adjust	O
the	O
two	O
learning	O
rates	O
such	O
that	O
the	O
generator	Method
does	O
not	O
affect	O
discriminator	Method
learning	Method
in	O
a	O
undesired	O
way	O
and	O
perturb	O
it	O
too	O
much	O
.	O
However	O
,	O
even	O
a	O
larger	O
learning	Metric
rate	Metric
for	O
the	O
generator	Method
than	O
for	O
the	O
discriminator	Method
may	O
ensure	O
that	O
the	O
discriminator	Method
has	O
low	O
perturbations	O
.	O
Learning	Metric
rates	Metric
can	O
not	O
be	O
translated	O
directly	O
into	O
perturbation	O
since	O
the	O
perturbation	O
of	O
the	O
discriminator	Method
by	O
the	O
generator	O
is	O
different	O
from	O
the	O
perturbation	O
of	O
the	O
generator	O
by	O
the	O
discriminator	Method
.	O
section	O
:	O
Adam	Method
Follows	O
an	O
HBF	Method
ODE	O
and	O
Ensures	O
TTUR	Method
Convergence	O
[	O
3	O
,	O
r	O
,	O
,	O
[	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
]	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
,	O
where	O
the	O
ball	O
with	O
mass	O
overshoots	O
the	O
local	O
minimum	Metric
and	O
settles	O
at	O
the	O
flat	O
minimum	Metric
.	O
]	O
In	O
our	O
experiments	O
,	O
we	O
aim	O
at	O
using	O
Adam	Method
stochastic	Method
approximation	Method
to	O
avoid	O
mode	Task
collapsing	Task
.	O
GANs	Method
suffer	O
from	O
‘	O
‘	O
mode	Method
collapsing	Method
’	O
’	O
where	O
large	O
masses	O
of	O
probability	O
are	O
mapped	O
onto	O
a	O
few	O
modes	O
that	O
cover	O
only	O
small	O
regions	O
.	O
While	O
these	O
regions	O
represent	O
meaningful	O
samples	O
,	O
the	O
variety	O
of	O
the	O
real	O
world	O
data	O
is	O
lost	O
and	O
only	O
few	O
prototype	O
samples	O
are	O
generated	O
.	O
Different	O
methods	O
have	O
been	O
proposed	O
to	O
avoid	O
mode	Task
collapsing	Task
.	O
We	O
obviate	O
mode	Method
collapsing	Method
by	O
using	O
Adam	Method
stochastic	Method
approximation	Method
.	O
Adam	Method
can	O
be	O
described	O
as	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
(	O
HBF	Method
)	O
(	O
see	O
below	O
)	O
,	O
since	O
it	O
averages	O
over	O
past	O
gradients	O
.	O
This	O
averaging	O
corresponds	O
to	O
a	O
velocity	O
that	O
makes	O
the	O
generator	O
resistant	O
to	O
getting	O
pushed	O
into	O
small	O
regions	O
.	O
Adam	Method
as	O
an	O
HBF	Method
method	O
typically	O
overshoots	O
small	O
local	O
minima	O
that	O
correspond	O
to	O
mode	O
collapse	O
and	O
can	O
find	O
flat	O
minima	O
which	O
generalize	O
well	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
depicts	O
the	O
dynamics	O
of	O
HBF	Method
,	O
where	O
the	O
ball	O
settles	O
at	O
a	O
flat	O
minimum	Metric
.	O
Next	O
,	O
we	O
analyze	O
whether	O
GANs	Method
trained	O
with	O
TTUR	Method
converge	O
when	O
using	O
Adam	Method
.	O
For	O
more	O
details	O
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
.	O
We	O
recapitulate	O
the	O
Adam	Method
update	Method
rule	Method
at	O
step	O
,	O
with	O
learning	Metric
rate	Metric
,	O
exponential	Method
averaging	Method
factors	Method
for	O
the	O
first	O
and	O
for	O
the	O
second	O
moment	O
of	O
the	O
gradient	O
:	O
where	O
following	O
operations	O
are	O
meant	O
componentwise	O
:	O
the	O
product	O
,	O
the	O
square	O
root	O
,	O
and	O
the	O
division	O
in	O
the	O
last	O
line	O
.	O
Instead	O
of	O
learning	Metric
rate	Metric
,	O
we	O
introduce	O
the	O
damping	O
coefficient	O
with	O
for	O
.	O
Adam	Method
has	O
parameters	O
for	O
averaging	O
the	O
gradient	O
and	O
parametrized	O
by	O
a	O
positive	O
for	O
averaging	O
the	O
squared	O
gradient	O
.	O
These	O
parameters	O
can	O
be	O
considered	O
as	O
defining	O
a	O
memory	O
for	O
Adam	Task
.	O
To	O
characterize	O
and	O
in	O
the	O
following	O
,	O
we	O
define	O
the	O
exponential	O
memory	O
and	O
the	O
polynomial	O
memory	O
for	O
some	O
positive	O
constant	O
.	O
The	O
next	O
theorem	O
describes	O
Adam	O
by	O
a	O
differential	Method
equation	Method
,	O
which	O
in	O
turn	O
allows	O
to	O
apply	O
the	O
idea	O
of	O
perturbed	Method
ODEs	Method
to	O
TTUR	Method
.	O
Consequently	O
,	O
learning	Method
GANs	Method
with	O
TTUR	Method
and	O
Adam	Method
converges	Method
.	O
theorem	O
:	O
.	O
If	O
Adam	Method
is	O
used	O
with	O
=	O
β1	O
-	O
1⁢a	O
(+	O
n1	O
)	O
r	O
(	O
n	O
)	O
,	O
=	O
β2	O
-	O
1⁢αa	O
(+	O
n1	O
)	O
r	O
(	O
n	O
)	O
and	O
with	O
∇f	O
as	O
the	O
full	O
gradient	O
of	O
the	O
lower	O
bounded	O
,	O
continuously	O
differentiable	O
objective	O
f	O
,	O
then	O
for	O
stationary	O
second	O
moments	O
of	O
the	O
gradient	O
,	O
Adam	Method
follows	O
the	O
differential	Method
equation	Method
for	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
(	O
HBF	Method
)	O
:	O
Adam	Method
converges	Method
for	O
gradients	O
∇f	O
that	O
are	O
L	O
-	O
Lipschitz	O
.	O
proof	O
:	O
Proof	O
.	O
Gadat	O
et	O
al	O
.	O
derived	O
a	O
discrete	Method
and	Method
stochastic	Method
version	Method
of	Method
Polyak	Method
’s	Method
Heavy	Method
Ball	Method
method	Method
,	O
the	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
(	O
HBF	Method
)	O
:	O
These	O
update	Method
rules	Method
are	O
the	O
first	Method
moment	Method
update	Method
rules	Method
of	O
Adam	Method
.	O
The	O
HBF	Method
can	O
be	O
formulated	O
as	O
the	O
differential	Method
equation	Method
Eq	Method
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
Gadat	O
et	O
al	O
.	O
showed	O
that	O
the	O
update	Method
rules	Method
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
converge	O
for	O
loss	O
functions	O
with	O
at	O
most	O
quadratic	O
grow	O
and	O
stated	O
that	O
convergence	O
can	O
be	O
proofed	O
for	O
that	O
are	O
-	O
Lipschitz	O
.	O
Convergence	Task
has	O
been	O
proved	O
for	O
continuously	O
differentiable	O
that	O
is	O
quasiconvex	O
(	O
Theorem	O
3	O
in	O
Goudou	O
&	O
Munier	O
)	O
.	O
Convergence	Task
has	O
been	O
proved	O
for	O
that	O
is	O
-	O
Lipschitz	O
and	O
bounded	O
from	O
below	O
(	O
Theorem	O
3.1	O
in	O
Attouch	O
et	O
al	O
.	O
)	O
.	O
Adam	Method
normalizes	O
the	O
average	O
by	O
the	O
second	O
moments	O
of	O
of	O
the	O
gradient	O
:	O
.	O
is	O
componentwise	O
divided	O
by	O
the	O
square	O
root	O
of	O
the	O
components	O
of	O
.	O
We	O
assume	O
that	O
the	O
second	O
moments	O
of	O
are	O
stationary	O
,	O
i.e.	O
,	O
.	O
In	O
this	O
case	O
the	O
normalization	Task
can	O
be	O
considered	O
as	O
additional	O
noise	O
since	O
the	O
normalization	O
factor	O
randomly	O
deviates	O
from	O
its	O
mean	O
.	O
In	O
the	O
HBF	Method
interpretation	O
the	O
normalization	O
by	O
corresponds	O
to	O
introducing	O
gravitation	O
.	O
We	O
obtain	O
For	O
a	O
stationary	O
second	O
moment	O
and	O
,	O
we	O
have	O
.	O
We	O
use	O
a	O
componentwise	O
linear	Method
approximation	O
to	O
Adam	Method
’s	Method
second	Method
moment	Method
normalization	Method
,	O
where	O
all	O
operations	O
are	O
meant	O
componentwise	O
.	O
If	O
we	O
set	O
,	O
then	O
and	O
,	O
since	O
.	O
For	O
a	O
stationary	O
second	O
moment	O
,	O
the	O
random	O
variable	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
with	O
a	O
bounded	O
second	O
moment	O
.	O
Therefore	O
can	O
be	O
subsumed	O
into	O
in	O
update	Method
rules	Method
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
The	O
factor	O
can	O
be	O
componentwise	O
incorporated	O
into	O
the	O
gradient	O
which	O
corresponds	O
to	O
rescaling	O
the	O
parameters	O
without	O
changing	O
the	O
minimum	Metric
.	O
∎	O
According	O
to	O
Attouch	O
et	O
al	O
.	O
the	O
energy	O
,	O
that	O
is	O
,	O
a	O
Lyapunov	O
function	O
,	O
is	O
and	O
.	O
Since	O
Adam	Method
can	O
be	O
expressed	O
as	O
differential	Method
equation	Method
and	O
has	O
a	O
Lyapunov	O
function	O
,	O
the	O
idea	O
of	O
perturbed	Method
ODEs	Method
carries	O
over	O
to	O
Adam	Method
.	O
Therefore	O
the	O
convergence	O
of	O
Adam	Method
with	O
TTUR	Method
can	O
be	O
proved	O
via	O
two	O
time	Method
-	Method
scale	Method
stochastic	Method
approximation	Method
analysis	Method
like	O
in	O
Borkar	O
for	O
stationary	O
second	O
moments	O
of	O
the	O
gradient	O
.	O
In	O
the	O
Appendix	O
we	O
further	O
discuss	O
the	O
convergence	O
of	O
two	O
time	Method
-	Method
scale	Method
stochastic	Method
approximation	Method
algorithms	Method
with	O
additive	O
noise	O
,	O
linear	Method
update	O
functions	O
depending	O
on	O
Markov	O
chains	O
,	O
nonlinear	O
update	O
functions	O
,	O
and	O
updates	O
depending	O
on	O
controlled	Method
Markov	Method
processes	Method
.	O
Futhermore	O
,	O
the	O
Appendix	O
presents	O
work	O
on	O
the	O
rate	Metric
of	Metric
convergence	Metric
for	O
both	O
linear	Method
and	O
nonlinear	Method
update	Method
rules	Method
using	O
similar	O
techniques	O
as	O
the	O
local	Method
stability	Method
analysis	Method
of	O
Nagarajan	O
and	O
Kolter	O
.	O
Finally	O
,	O
we	O
elaborate	O
more	O
on	O
equal	O
time	O
-	O
scale	O
updates	O
,	O
which	O
are	O
investigated	O
for	O
saddle	Task
point	Task
problems	Task
and	O
actor	Method
-	Method
critic	Method
learning	Method
.	O
section	O
:	O
Experiments	O
paragraph	O
:	O
Performance	Metric
Measure	Metric
.	O
Before	O
presenting	O
the	O
experiments	O
,	O
we	O
introduce	O
a	O
quality	Metric
measure	Metric
for	O
models	O
learned	O
by	O
GANs	Method
.	O
The	O
objective	O
of	O
generative	Method
learning	Method
is	O
that	O
the	O
model	O
produces	O
data	O
which	O
matches	O
the	O
observed	O
data	O
.	O
Therefore	O
,	O
each	O
distance	O
between	O
the	O
probability	O
of	O
observing	O
real	O
world	O
data	O
and	O
the	O
probability	O
of	O
generating	O
model	O
data	O
can	O
serve	O
as	O
performance	Metric
measure	Metric
for	O
generative	Method
models	Method
.	O
However	O
,	O
defining	O
appropriate	O
performance	Metric
measures	Metric
for	O
generative	Method
models	Method
is	O
difficult	O
.	O
The	O
best	O
known	O
measure	O
is	O
the	O
likelihood	O
,	O
which	O
can	O
be	O
estimated	O
by	O
annealed	Method
importance	Method
sampling	Method
.	O
However	O
,	O
the	O
likelihood	O
heavily	O
depends	O
on	O
the	O
noise	O
assumptions	O
for	O
the	O
real	O
data	O
and	O
can	O
be	O
dominated	O
by	O
single	O
samples	O
.	O
Other	O
approaches	O
like	O
density	Method
estimates	Method
have	O
drawbacks	O
,	O
too	O
.	O
A	O
well	O
-	O
performing	O
approach	O
to	O
measure	O
the	O
performance	O
of	O
GANs	Method
is	O
the	O
‘	O
‘	O
Inception	Metric
Score	Metric
’	O
’	O
which	O
correlates	O
with	O
human	O
judgment	O
.	O
Generated	O
samples	O
are	O
fed	O
into	O
an	O
inception	Method
model	Method
that	O
was	O
trained	O
on	O
ImageNet	Material
.	O
Images	Material
with	O
meaningful	O
objects	O
are	O
supposed	O
to	O
have	O
low	O
label	O
(	O
output	O
)	O
entropy	Metric
,	O
that	O
is	O
,	O
they	O
belong	O
to	O
few	O
object	O
classes	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
entropy	O
across	O
images	O
should	O
be	O
high	O
,	O
that	O
is	O
,	O
the	O
variance	O
over	O
the	O
images	O
should	O
be	O
large	O
.	O
Drawback	O
of	O
the	O
Inception	Metric
Score	Metric
is	O
that	O
the	O
statistics	O
of	O
real	O
world	O
samples	O
are	O
not	O
used	O
and	O
compared	O
to	O
the	O
statistics	O
of	O
synthetic	O
samples	O
.	O
Next	O
,	O
we	O
improve	O
the	O
Inception	Metric
Score	Metric
.	O
The	O
equality	O
holds	O
except	O
for	O
a	O
non	O
-	O
measurable	O
set	O
if	O
and	O
only	O
if	O
for	O
a	O
basis	O
spanning	O
the	O
function	O
space	O
in	O
which	O
and	O
live	O
.	O
These	O
equalities	O
of	O
expectations	O
are	O
used	O
to	O
describe	O
distributions	O
by	O
moments	O
or	O
cumulants	Method
,	O
where	O
are	O
polynomials	O
of	O
the	O
data	O
.	O
We	O
generalize	O
these	O
polynomials	O
by	O
replacing	O
by	O
the	O
coding	Method
layer	Method
of	O
an	O
inception	Method
model	Method
in	O
order	O
to	O
obtain	O
vision	O
-	O
relevant	O
features	O
.	O
For	O
practical	O
reasons	O
we	O
only	O
consider	O
the	O
first	O
two	O
polynomials	O
,	O
that	O
is	O
,	O
the	O
first	O
two	O
moments	O
:	O
mean	O
and	O
covariance	O
.	O
The	O
Gaussian	Method
is	O
the	O
maximum	Method
entropy	Method
distribution	Method
for	O
given	O
mean	O
and	O
covariance	O
,	O
therefore	O
we	O
assume	O
the	O
coding	Method
units	Method
to	O
follow	O
a	O
multidimensional	O
Gaussian	O
.	O
The	O
difference	O
of	O
two	O
Gaussians	O
(	O
synthetic	O
and	O
real	O
-	O
world	O
images	O
)	O
is	O
measured	O
by	O
the	O
Fréchet	O
distance	O
also	O
known	O
as	O
Wasserstein	Method
-	Method
2	Method
distance	Method
.	O
We	O
call	O
the	O
Fréchet	O
distance	O
between	O
the	O
Gaussian	O
with	O
mean	O
obtained	O
from	O
and	O
the	O
Gaussian	O
with	O
mean	O
obtained	O
from	O
the	O
‘	O
‘	O
Fréchet	Metric
Inception	Metric
Distance	Metric
’	O
’	O
(	O
FID	Metric
)	O
,	O
which	O
is	O
given	O
by	O
:	O
Next	O
we	O
show	O
that	O
the	O
FID	Metric
is	O
consistent	O
with	O
increasing	O
disturbances	O
and	O
human	O
judgment	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
evaluates	O
the	O
FID	Metric
for	O
Gaussian	O
noise	O
,	O
Gaussian	O
blur	O
,	O
implanted	O
black	O
rectangles	O
,	O
swirled	O
images	O
,	O
salt	O
and	O
pepper	O
noise	O
,	O
and	O
CelebA	Material
dataset	O
contaminated	O
by	O
ImageNet	Material
images	Material
.	O
The	O
FID	Metric
captures	O
the	O
disturbance	O
level	O
very	O
well	O
.	O
In	O
the	O
experiments	O
we	O
used	O
the	O
FID	Metric
to	O
evaluate	O
the	O
performance	O
of	O
GANs	Method
.	O
For	O
more	O
details	O
and	O
a	O
comparison	O
between	O
FID	Metric
and	O
Inception	Metric
Score	Metric
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
,	O
where	O
we	O
show	O
that	O
FID	Metric
is	O
more	O
consistent	O
with	O
the	O
noise	O
level	O
than	O
the	O
Inception	Metric
Score	Metric
.	O
paragraph	O
:	O
Model	Task
Selection	Task
and	O
Evaluation	Task
.	O
We	O
compare	O
the	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
for	O
GANs	Method
with	O
the	O
original	O
GAN	Method
training	Method
to	O
see	O
whether	O
TTUR	Method
improves	O
the	O
convergence	Metric
speed	Metric
and	O
performance	O
of	O
GANs	Method
.	O
We	O
have	O
selected	O
Adam	Method
stochastic	Method
optimization	Method
to	O
reduce	O
the	O
risk	O
of	O
mode	Task
collapsing	Task
.	O
The	O
advantage	O
of	O
Adam	Method
has	O
been	O
confirmed	O
by	O
MNIST	Task
experiments	Task
,	O
where	O
Adam	Method
indeed	O
considerably	O
reduced	O
the	O
cases	O
for	O
which	O
we	O
observed	O
mode	O
collapsing	O
.	O
Although	O
TTUR	Method
ensures	O
that	O
the	O
discriminator	Method
converges	O
during	O
learning	Task
,	O
practicable	O
learning	Metric
rates	Metric
must	O
be	O
found	O
for	O
each	O
experiment	O
.	O
We	O
face	O
a	O
trade	O
-	O
off	O
since	O
the	O
learning	Metric
rates	Metric
should	O
be	O
small	O
enough	O
(	O
e.g.	O
for	O
the	O
generator	Method
)	O
to	O
ensure	O
convergence	O
but	O
at	O
the	O
same	O
time	O
should	O
be	O
large	O
enough	O
to	O
allow	O
fast	Method
learning	Method
.	O
For	O
each	O
of	O
the	O
experiments	O
,	O
the	O
learning	Metric
rates	Metric
have	O
been	O
optimized	O
to	O
be	O
large	O
while	O
still	O
ensuring	O
stable	O
training	O
which	O
is	O
indicated	O
by	O
a	O
decreasing	O
FID	Metric
or	O
Jensen	Metric
-	Metric
Shannon	Metric
-	Metric
divergence	Metric
(	O
JSD	Metric
)	O
.	O
We	O
further	O
fixed	O
the	O
time	O
point	O
for	O
stopping	O
training	O
to	O
the	O
update	O
step	O
when	O
the	O
FID	Metric
or	O
Jensen	O
-	O
Shannon	O
-	O
divergence	O
of	O
the	O
best	O
models	O
was	O
no	O
longer	O
decreasing	O
.	O
For	O
some	O
models	O
,	O
we	O
observed	O
that	O
the	O
FID	Metric
diverges	O
or	O
starts	O
to	O
increase	O
at	O
a	O
certain	O
time	O
point	O
.	O
An	O
example	O
of	O
this	O
behaviour	O
is	O
shown	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
The	O
performance	O
of	O
generative	Method
models	Method
is	O
evaluated	O
via	O
the	O
Fréchet	Metric
Inception	Metric
Distance	Metric
(	O
FID	Metric
)	O
introduced	O
above	O
.	O
For	O
the	O
One	Task
Billion	Task
Word	Task
experiment	Task
,	O
the	O
normalized	O
JSD	Metric
served	O
as	O
performance	Metric
measure	Metric
.	O
For	O
computing	O
the	O
FID	Metric
,	O
we	O
propagated	O
all	O
images	O
from	O
the	O
training	O
dataset	O
through	O
the	O
pretrained	Method
Inception	Method
-	Method
v3	Method
model	Method
following	O
the	O
computation	O
of	O
the	O
Inception	Metric
Score	Metric
,	O
however	O
,	O
we	O
use	O
the	O
last	O
pooling	Method
layer	Method
as	O
coding	Method
layer	Method
.	O
For	O
this	O
coding	Method
layer	Method
,	O
we	O
calculated	O
the	O
mean	O
and	O
the	O
covariance	O
matrix	O
.	O
Thus	O
,	O
we	O
approximate	O
the	O
first	O
and	O
second	O
central	O
moment	O
of	O
the	O
function	O
given	O
by	O
the	O
Inception	Method
coding	Method
layer	Method
under	O
the	O
real	O
world	O
distribution	O
.	O
To	O
approximate	O
these	O
moments	O
for	O
the	O
model	Method
distribution	Method
,	O
we	O
generate	O
50	O
,	O
000	O
images	O
,	O
propagate	O
them	O
through	O
the	O
Inception	Method
-	Method
v3	Method
model	Method
,	O
and	O
then	O
compute	O
the	O
mean	O
and	O
the	O
covariance	O
matrix	O
.	O
For	O
computational	Metric
efficiency	Metric
,	O
we	O
evaluate	O
the	O
FID	Metric
every	O
1	O
,	O
000	O
DCGAN	Method
mini	O
-	O
batch	O
updates	O
,	O
every	O
5	O
,	O
000	O
WGAN	Method
-	Method
GP	Method
outer	O
iterations	O
for	O
the	O
image	Task
experiments	Task
,	O
and	O
every	O
100	O
outer	O
iterations	O
for	O
the	O
WGAN	Method
-	Method
GP	Method
language	O
model	O
.	O
For	O
the	O
one	O
time	Task
-	Task
scale	Task
updates	Task
a	O
WGAN	Method
-	Method
GP	Method
outer	O
iteration	O
for	O
the	O
image	Method
model	Method
consists	O
of	O
five	O
discriminator	Method
mini	Method
-	Method
batches	Method
and	O
ten	O
discriminator	Method
mini	Method
-	O
batches	O
for	O
the	O
language	Method
model	Method
,	O
where	O
we	O
follow	O
the	O
original	O
implementation	O
.	O
For	O
TTUR	Method
however	O
,	O
the	O
discriminator	Method
is	O
updated	O
only	O
once	O
per	O
iteration	O
.	O
We	O
repeat	O
the	O
training	O
for	O
each	O
single	O
time	O
-	O
scale	O
(	O
orig	O
)	O
and	O
TTUR	Method
learning	O
rate	O
eight	O
times	O
for	O
the	O
image	O
datasets	O
and	O
ten	O
times	O
for	O
the	O
language	O
benchmark	O
.	O
Additionally	O
to	O
the	O
mean	O
FID	Metric
training	O
progress	O
we	O
show	O
the	O
minimum	Metric
and	O
maximum	Metric
FID	Metric
over	O
all	O
runs	O
at	O
each	O
evaluation	O
time	O
-	O
step	O
.	O
For	O
more	O
details	O
,	O
implementations	O
and	O
further	O
results	O
see	O
Appendix	O
Section	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
paragraph	O
:	O
Simple	O
Toy	Material
Data	Material
.	O
We	O
first	O
want	O
to	O
demonstrate	O
the	O
difference	O
between	O
a	O
single	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
and	O
TTUR	Method
on	O
a	O
simple	O
toy	Task
min	Task
/	Task
max	Task
problem	Task
where	O
a	O
saddle	O
point	O
should	O
be	O
found	O
.	O
The	O
objective	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
left	O
)	O
has	O
a	O
saddle	O
point	O
at	O
and	O
fulfills	O
assumption	O
A4	O
.	O
The	O
norm	O
measures	O
the	O
distance	O
of	O
the	O
parameter	O
vector	O
to	O
the	O
saddle	O
point	O
.	O
We	O
update	O
by	O
gradient	Method
descent	Method
in	O
and	O
gradient	Method
ascent	Method
in	O
using	O
additive	O
Gaussian	O
noise	O
in	O
order	O
to	O
simulate	O
a	O
stochastic	Method
update	Method
.	O
The	O
updates	O
should	O
converge	O
to	O
the	O
saddle	O
point	O
with	O
objective	O
value	O
and	O
the	O
norm	O
.	O
In	O
Fig	O
.	O
[	O
reference	O
]	O
(	O
right	O
)	O
,	O
the	O
first	O
two	O
rows	O
show	O
one	O
time	O
-	O
scale	O
update	Method
rules	Method
.	O
The	O
large	O
learning	Metric
rate	Metric
in	O
the	O
first	O
row	O
diverges	O
and	O
has	O
large	O
fluctuations	O
.	O
The	O
smaller	O
learning	Metric
rate	Metric
in	O
the	O
second	O
row	O
converges	O
but	O
slower	O
than	O
the	O
TTUR	Method
in	O
the	O
third	O
row	O
which	O
has	O
slow	O
-	O
updates	O
.	O
TTUR	Method
with	O
slow	O
-	O
updates	O
in	O
the	O
fourth	O
row	O
also	O
converges	O
but	O
slower	O
.	O
paragraph	O
:	O
DCGAN	Method
on	O
Image	Material
Data	Material
.	O
We	O
test	O
TTUR	Method
for	O
the	O
deep	Method
convolutional	Method
GAN	Method
(	O
DCGAN	Method
)	O
at	O
the	O
CelebA	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
SVHN	Material
and	O
LSUN	Material
Bedrooms	Material
dataset	Material
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
FID	Metric
during	O
learning	Task
with	O
the	O
original	O
learning	Method
method	Method
(	O
orig	Method
)	O
and	O
with	O
TTUR	Method
.	O
The	O
original	O
training	Method
method	Method
is	O
faster	O
at	O
the	O
beginning	O
,	O
but	O
TTUR	Method
eventually	O
achieves	O
better	O
performance	O
.	O
DCGAN	Method
trained	O
TTUR	Method
reaches	O
constantly	O
a	O
lower	O
FID	Metric
than	O
the	O
original	O
method	O
and	O
for	O
CelebA	Material
and	O
LSUN	Material
Bedrooms	Material
all	O
one	O
time	O
-	O
scale	O
runs	O
diverge	O
.	O
For	O
DCGAN	Method
the	O
learning	Metric
rate	Metric
of	O
the	O
generator	Method
is	O
larger	O
then	O
that	O
of	O
the	O
discriminator	Method
,	O
which	O
,	O
however	O
,	O
does	O
not	O
contradict	O
the	O
TTUR	Method
theory	O
(	O
see	O
the	O
Appendix	O
Section	O
[	O
reference	O
]	O
)	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
report	O
the	O
best	O
FID	Metric
with	O
TTUR	Method
and	O
one	O
time	Method
-	Method
scale	Method
training	Method
for	O
optimized	O
number	O
of	O
updates	O
and	O
learning	Metric
rates	Metric
.	O
TTUR	Method
constantly	O
outperforms	O
standard	O
training	Method
and	O
is	O
more	O
stable	O
.	O
paragraph	O
:	O
WGAN	Method
-	Method
GP	Method
on	O
Image	Material
Data	Material
.	O
We	O
used	O
the	O
WGAN	Method
-	Method
GP	Method
image	O
model	O
to	O
test	O
TTUR	Method
with	O
the	O
CIFAR	Material
-	Material
10	Material
and	O
LSUN	Material
Bedrooms	Material
datasets	Material
.	O
In	O
contrast	O
to	O
the	O
original	O
code	O
where	O
the	O
discriminator	Method
is	O
trained	O
five	O
times	O
for	O
each	O
generator	Method
update	Method
,	O
TTUR	Method
updates	O
the	O
discriminator	O
only	O
once	O
,	O
therefore	O
we	O
align	O
the	O
training	O
progress	O
with	O
wall	O
-	O
clock	O
time	O
.	O
The	O
learning	Metric
rate	Metric
for	O
the	O
original	O
training	O
was	O
optimized	O
to	O
be	O
large	O
but	O
leads	O
to	O
stable	O
learning	Task
.	O
TTUR	Method
can	O
use	O
a	O
higher	O
learning	Metric
rate	Metric
for	O
the	O
discriminator	Method
since	O
TTUR	Method
stabilizes	O
learning	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
FID	Metric
during	O
learning	Task
with	O
the	O
original	O
learning	Method
method	Method
and	O
with	O
TTUR	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
best	O
FID	Metric
with	O
TTUR	Method
and	O
one	O
time	Method
-	Method
scale	Method
training	Method
for	O
optimized	O
number	O
of	O
iterations	O
and	O
learning	Metric
rates	Metric
.	O
Again	O
TTUR	Method
reaches	O
lower	O
FIDs	Metric
than	O
one	O
time	Method
-	Method
scale	Method
training	Method
.	O
paragraph	O
:	O
WGAN	Method
-	Method
GP	Method
on	O
Language	O
Data	O
.	O
Finally	O
the	O
One	Material
Billion	Material
Word	Material
Benchmark	Material
serves	O
to	O
evaluate	O
TTUR	Method
on	O
WGAN	Method
-	Method
GP	Method
.	O
The	O
character	Method
-	Method
level	Method
generative	Method
language	Method
model	Method
is	O
a	O
1D	Method
convolutional	Method
neural	Method
network	Method
(	Method
CNN	Method
)	O
which	O
maps	O
a	O
latent	O
vector	O
to	O
a	O
sequence	O
of	O
one	O
-	O
hot	O
character	O
vectors	O
of	O
dimension	O
32	O
given	O
by	O
the	O
maximum	O
of	O
a	O
softmax	O
output	O
.	O
The	O
discriminator	O
is	O
also	O
a	O
1D	Method
CNN	Method
applied	O
to	O
sequences	O
of	O
one	O
-	O
hot	O
vectors	O
of	O
32	O
characters	O
.	O
Since	O
the	O
FID	Metric
criterium	O
only	O
works	O
for	O
images	O
,	O
we	O
measured	O
the	O
performance	O
by	O
the	O
Jensen	Metric
-	Metric
Shannon	Metric
-	Metric
divergence	Metric
(	O
JSD	Metric
)	O
between	O
the	O
model	O
and	O
the	O
real	O
world	O
distribution	O
as	O
has	O
been	O
done	O
previously	O
.	O
In	O
contrast	O
to	O
the	O
original	O
code	O
where	O
the	O
critic	Method
is	O
trained	O
ten	O
times	O
for	O
each	O
generator	Method
update	Method
,	O
TTUR	Method
updates	O
the	O
discriminator	Method
only	O
once	O
,	O
therefore	O
we	O
align	O
the	O
training	O
progress	O
with	O
wall	O
-	O
clock	O
time	O
.	O
The	O
learning	Metric
rate	Metric
for	O
the	O
original	O
training	O
was	O
optimized	O
to	O
be	O
large	O
but	O
leads	O
to	O
stable	O
learning	Task
.	O
TTUR	Method
can	O
use	O
a	O
higher	O
learning	Metric
rate	Metric
for	O
the	O
discriminator	Method
since	O
TTUR	Method
stabilizes	O
learning	O
.	O
We	O
report	O
for	O
the	O
4	Task
and	O
6	Task
-	Task
gram	Task
word	Task
evaluation	Task
the	O
normalized	O
mean	O
JSD	Metric
for	O
ten	O
runs	O
for	O
original	Task
training	Task
and	O
TTUR	Method
training	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
report	O
the	O
best	O
JSD	Metric
at	O
an	O
optimal	O
time	O
-	O
step	O
where	O
TTUR	Method
outperforms	O
the	O
standard	O
training	Method
for	O
both	O
measures	O
.	O
The	O
improvement	O
of	O
TTUR	Method
on	O
the	O
6	Metric
-	Metric
gram	Metric
statistics	Metric
over	O
original	O
training	O
shows	O
that	O
TTUR	Method
enables	O
to	O
learn	O
to	O
generate	O
more	O
subtle	O
pseudo	O
-	O
words	O
which	O
better	O
resembles	O
real	O
words	O
.	O
section	O
:	O
Conclusion	O
For	O
learning	Task
GANs	Task
,	O
we	O
have	O
introduced	O
the	O
two	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
(	O
TTUR	Method
)	O
,	O
which	O
we	O
have	O
proved	O
to	O
converge	O
to	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
.	O
Then	O
we	O
described	O
Adam	Method
stochastic	Method
optimization	Method
as	O
a	O
heavy	Method
ball	Method
with	Method
friction	Method
(	O
HBF	Method
)	O
dynamics	O
,	O
which	O
shows	O
that	O
Adam	Method
converges	Method
and	O
that	O
Adam	Method
tends	O
to	O
find	O
flat	O
minima	O
while	O
avoiding	O
small	O
local	O
minima	O
.	O
A	O
second	Method
order	Method
differential	Method
equation	Method
describes	O
the	O
learning	Method
dynamics	Method
of	Method
Adam	Method
as	O
an	O
HBF	Method
system	O
.	O
Via	O
this	O
differential	Method
equation	Method
,	O
the	O
convergence	Method
of	Method
GANs	Method
trained	O
with	O
TTUR	Method
to	O
a	O
stationary	O
local	O
Nash	O
equilibrium	O
can	O
be	O
extended	O
to	O
Adam	Method
.	O
Finally	O
,	O
to	O
evaluate	O
GANs	Method
,	O
we	O
introduced	O
the	O
‘	O
Fréchet	Metric
Inception	Metric
Distance	Metric
’	O
’	O
(	O
FID	Metric
)	O
which	O
captures	O
the	O
similarity	O
of	O
generated	O
images	O
to	O
real	O
ones	O
better	O
than	O
the	O
Inception	Metric
Score	Metric
.	O
In	O
experiments	O
we	O
have	O
compared	O
GANs	Method
trained	O
with	O
TTUR	Method
to	O
conventional	O
GAN	Method
training	Method
with	O
a	O
one	O
time	Method
-	Method
scale	Method
update	Method
rule	Method
on	O
CelebA	Material
,	O
CIFAR	Material
-	Material
10	Material
,	O
SVHN	Material
,	O
LSUN	Material
Bedrooms	Material
,	O
and	O
the	O
One	Material
Billion	Material
Word	Material
Benchmark	Material
.	O
TTUR	Method
outperforms	O
conventional	O
GAN	Method
training	Method
consistently	O
in	O
all	O
experiments	O
.	O
section	O
:	O
Acknowledgment	O
This	O
work	O
was	O
supported	O
by	O
NVIDIA	O
Corporation	O
,	O
Bayer	O
AG	O
with	O
Research	O
Agreement	O
09	O
/	O
2017	O
,	O
Zalando	O
SE	O
with	O
Research	O
Agreement	O
01	O
/	O
2016	O
,	O
Audi	O
.	O
JKU	Method
Deep	Method
Learning	Method
Center	Method
,	O
Audi	O
Electronic	O
Venture	O
GmbH	O
,	O
IWT	O
research	O
grant	O
IWT150865	O
(	O
Exaptation	Method
)	O
,	O
H2020	O
project	O
grant	O
671555	O
(	O
ExCAPE	Method
)	O
and	O
FWF	O
grant	O
P	O
28660	O
-	O
N31	O
.	O
section	O
:	O
References	O
The	O
references	O
are	O
provided	O
after	O
Section	O
[	O
reference	O
]	O
.	O
section	O
:	O
Appendix	O
section	O
:	O
Fréchet	Metric
Inception	Metric
Distance	Metric
(	O
FID	Metric
)	O
We	O
improve	O
the	O
Inception	Metric
score	Metric
for	O
comparing	O
the	O
results	O
of	O
GANs	Method
.	O
The	O
Inception	Method
score	Method
has	O
the	O
disadvantage	O
that	O
it	O
does	O
not	O
use	O
the	O
statistics	O
of	O
real	O
world	O
samples	O
and	O
compare	O
it	O
to	O
the	O
statistics	O
of	O
synthetic	O
samples	O
.	O
Let	O
be	O
the	O
distribution	O
of	O
model	O
samples	O
and	O
the	O
distribution	O
of	O
the	O
samples	O
from	O
real	O
world	O
.	O
The	O
equality	O
holds	O
except	O
for	O
a	O
non	O
-	O
measurable	O
set	O
if	O
and	O
only	O
if	O
for	O
a	O
basis	O
spanning	O
the	O
function	O
space	O
in	O
which	O
and	O
live	O
.	O
These	O
equalities	O
of	O
expectations	O
are	O
used	O
to	O
describe	O
distributions	O
by	O
moments	O
or	O
cumulants	Method
,	O
where	O
are	O
polynomials	O
of	O
the	O
data	O
.	O
We	O
replacing	O
by	O
the	O
coding	Method
layer	Method
of	O
an	O
Inception	Method
model	Method
in	O
order	O
to	O
obtain	O
vision	O
-	O
relevant	O
features	O
and	O
consider	O
polynomials	O
of	O
the	O
coding	Method
unit	Method
functions	Method
.	O
For	O
practical	O
reasons	O
we	O
only	O
consider	O
the	O
first	O
two	O
polynomials	O
,	O
that	O
is	O
,	O
the	O
first	O
two	O
moments	O
:	O
mean	O
and	O
covariance	O
.	O
The	O
Gaussian	Method
is	O
the	O
maximum	Method
entropy	Method
distribution	Method
for	O
given	O
mean	O
and	O
covariance	O
,	O
therefore	O
we	O
assume	O
the	O
coding	Method
units	Method
to	O
follow	O
a	O
multidimensional	O
Gaussian	O
.	O
The	O
difference	O
of	O
two	O
Gaussians	O
is	O
measured	O
by	O
the	O
Fréchet	O
distance	O
also	O
known	O
as	O
Wasserstein	O
-	O
2	O
distance	O
.	O
The	O
Fréchet	O
distance	O
between	O
the	O
Gaussian	O
with	O
mean	O
and	O
covariance	O
obtained	O
from	O
and	O
the	O
Gaussian	O
obtained	O
from	O
is	O
called	O
the	O
‘	O
‘	O
Fréchet	Metric
Inception	Metric
Distance	Metric
’	O
’	O
(	O
FID	Metric
)	O
,	O
which	O
is	O
given	O
by	O
:	O
Next	O
we	O
show	O
that	O
the	O
FID	Metric
is	O
consistent	O
with	O
increasing	O
disturbances	O
and	O
human	O
judgment	O
on	O
the	O
CelebA	Material
dataset	O
.	O
We	O
computed	O
the	O
on	O
all	O
CelebA	Material
images	O
,	O
while	O
for	O
computing	O
we	O
used	O
50	O
,	O
000	O
randomly	O
selected	O
samples	O
.	O
We	O
considered	O
following	O
disturbances	O
of	O
the	O
image	O
:	O
Gaussian	O
noise	O
:	O
We	O
constructed	O
a	O
matrix	O
with	O
Gaussian	O
noise	O
scaled	O
to	O
.	O
The	O
noisy	O
image	O
is	O
computed	O
as	O
for	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
noise	O
added	O
to	O
the	O
image	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
of	O
the	O
image	O
.	O
Gaussian	Task
blur	Task
:	O
The	O
image	O
is	O
convolved	O
with	O
a	O
Gaussian	Method
kernel	Method
with	O
standard	O
deviation	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
of	O
the	O
image	O
,	O
that	O
is	O
,	O
the	O
more	O
the	O
image	O
is	O
smoothed	O
.	O
Black	O
rectangles	O
:	O
To	O
an	O
image	O
five	O
black	O
rectangles	O
are	O
are	O
added	O
at	O
randomly	O
chosen	O
locations	O
.	O
The	O
rectangles	O
cover	O
parts	O
of	O
the	O
image	O
.	O
The	O
size	O
of	O
the	O
rectangles	O
is	O
with	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
of	O
the	O
image	O
,	O
that	O
is	O
,	O
the	O
more	O
of	O
the	O
image	O
is	O
covered	O
by	O
black	O
rectangles	O
.	O
Swirl	Method
:	O
Parts	O
of	O
the	O
image	O
are	O
transformed	O
as	O
a	O
spiral	O
,	O
that	O
is	O
,	O
as	O
a	O
swirl	O
(	O
whirlpool	O
effect	O
)	O
.	O
Consider	O
the	O
coordinate	O
in	O
the	O
noisy	O
(	O
swirled	O
)	O
image	O
for	O
which	O
we	O
want	O
to	O
find	O
the	O
color	O
.	O
Towards	O
this	O
end	O
we	O
need	O
the	O
reverse	Task
mapping	Task
for	O
the	O
swirl	Task
transformation	Task
which	O
gives	O
the	O
location	O
which	O
is	O
mapped	O
to	O
.	O
We	O
first	O
compute	O
polar	O
coordinates	O
relative	O
to	O
a	O
center	O
given	O
by	O
the	O
angle	O
and	O
the	O
radius	O
.	O
We	O
transform	O
them	O
according	O
to	O
.	O
Here	O
is	O
a	O
parameter	O
for	O
the	O
amount	O
of	O
swirl	O
and	O
indicates	O
the	O
swirl	O
extent	O
in	O
pixels	O
.	O
The	O
original	O
coordinates	O
,	O
where	O
the	O
color	O
for	O
can	O
be	O
found	O
,	O
are	O
and	O
.	O
We	O
set	O
to	O
the	O
center	O
of	O
the	O
image	O
and	O
.	O
The	O
disturbance	O
level	O
is	O
given	O
by	O
the	O
amount	O
of	O
swirl	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
of	O
the	O
image	O
via	O
the	O
amount	O
of	O
swirl	O
.	O
Salt	O
and	O
pepper	O
noise	O
:	O
Some	O
pixels	O
of	O
the	O
image	O
are	O
set	O
to	O
black	O
or	O
white	O
,	O
where	O
black	O
is	O
chosen	O
with	O
50	O
%	O
probability	O
(	O
same	O
for	O
white	O
)	O
.	O
Pixels	O
are	O
randomly	O
chosen	O
for	O
being	O
flipped	O
to	O
white	O
or	O
black	O
,	O
where	O
the	O
ratio	O
of	O
pixel	O
flipped	O
to	O
white	O
or	O
black	O
is	O
given	O
by	O
the	O
noise	O
level	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
noise	O
added	O
to	O
the	O
image	O
via	O
flipping	O
pixels	O
to	O
white	O
or	O
black	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
level	O
.	O
ImageNet	Task
contamination	Task
:	O
From	O
each	O
of	O
the	O
1	O
,	O
000	O
ImageNet	Material
classes	Material
,	O
5	O
images	O
are	O
randomly	O
chosen	O
,	O
which	O
gives	O
5	O
,	O
000	O
ImageNet	Material
images	Material
.	O
The	O
images	O
are	O
ensured	O
to	O
be	O
RGB	O
and	O
to	O
have	O
a	O
minimal	O
size	O
of	O
256x256	O
.	O
A	O
percentage	O
of	O
of	O
the	O
CelebA	Material
images	O
has	O
been	O
replaced	O
by	O
ImageNet	Material
images	Material
.	O
means	O
all	O
images	O
are	O
from	O
CelebA	Material
,	O
means	O
that	O
75	O
%	O
of	O
the	O
images	O
are	O
from	O
CelebA	Material
and	O
25	O
%	O
from	O
ImageNet	Material
etc	O
.	O
The	O
larger	O
is	O
,	O
the	O
larger	O
is	O
the	O
disturbance	O
of	O
the	O
CelebA	Material
dataset	O
by	O
contaminating	O
it	O
by	O
ImageNet	Material
images	Material
.	O
The	O
larger	O
the	O
disturbance	O
level	O
is	O
,	O
the	O
more	O
the	O
dataset	O
deviates	O
from	O
the	O
reference	O
real	O
world	O
dataset	O
.	O
We	O
compare	O
the	O
Inception	Metric
Score	Metric
with	O
the	O
FID	Metric
.	O
The	O
Inception	Metric
Score	Metric
with	O
samples	O
and	O
classes	O
is	O
The	O
FID	Metric
is	O
a	O
distance	O
,	O
while	O
the	O
Inception	Metric
Score	Metric
is	O
a	O
score	O
.	O
To	O
compare	O
FID	Metric
and	O
Inception	Metric
Score	Metric
,	O
we	O
transform	O
the	O
Inception	Metric
Score	Metric
to	O
a	O
distance	O
,	O
which	O
we	O
call	O
‘	O
‘	O
Inception	O
Distance	O
’	O
’	O
(	O
IND	O
)	O
.	O
This	O
transformation	O
to	O
a	O
distance	O
is	O
possible	O
since	O
the	O
Inception	Metric
Score	Metric
has	O
a	O
maximal	O
value	O
.	O
For	O
zero	O
probability	O
,	O
we	O
set	O
the	O
value	O
.	O
We	O
can	O
bound	O
the	O
-	O
term	O
by	O
Using	O
this	O
bound	O
,	O
we	O
obtain	O
an	O
upper	O
bound	O
on	O
the	O
Inception	Metric
Score	Metric
:	O
The	O
upper	O
bound	O
is	O
tight	O
and	O
achieved	O
if	O
and	O
every	O
sample	O
is	O
from	O
a	O
different	O
class	O
and	O
the	O
sample	O
is	O
classified	O
correctly	O
with	O
probability	O
1	O
.	O
The	O
IND	Method
is	O
computed	O
‘	O
‘	O
IND	O
=	O
-	O
Inception	Metric
Score	Metric
’	O
’	O
,	O
therefore	O
the	O
IND	O
is	O
zero	O
for	O
a	O
perfect	O
subset	O
of	O
the	O
ImageNet	Material
with	O
samples	O
,	O
where	O
each	O
sample	O
stems	O
from	O
a	O
different	O
class	O
.	O
Therefore	O
both	O
distances	O
should	O
increase	O
with	O
increasing	O
disturbance	O
level	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
present	O
the	O
evaluation	O
for	O
each	O
kind	O
of	O
disturbance	O
.	O
The	O
larger	O
the	O
disturbance	O
level	O
is	O
,	O
the	O
larger	O
the	O
FID	Metric
and	O
IND	O
should	O
be	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
[	O
reference	O
]	O
,	O
and	O
[	O
reference	O
]	O
we	O
show	O
examples	O
of	O
images	O
generated	O
with	O
DCGAN	Method
trained	O
on	O
CelebA	Material
with	O
FIDs	Material
500	O
,	O
300	O
,	O
133	O
,	O
100	O
,	O
45	O
,	O
13	O
,	O
and	O
FID	Metric
3	O
achieved	O
with	O
WGAN	Method
-	Method
GP	Method
on	O
CelebA.	Material
section	O
:	O
Two	O
Time	Method
-	Method
Scale	Method
Stochastic	Method
Approximation	Method
Algorithms	Method
Stochastic	Method
approximation	Method
algorithms	Method
are	O
iterative	Method
procedures	Method
to	O
find	O
a	O
root	O
or	O
a	O
stationary	O
point	O
(	O
minimum	Metric
,	O
maximum	O
,	O
saddle	O
point	O
)	O
of	O
a	O
function	O
when	O
only	O
noisy	O
observations	O
of	O
its	O
values	O
or	O
its	O
derivatives	O
are	O
provided	O
.	O
Two	O
time	Method
-	Method
scale	Method
stochastic	Method
approximation	Method
algorithms	Method
are	O
two	O
coupled	Method
iterations	Method
with	O
different	O
step	O
sizes	O
.	O
For	O
proving	O
convergence	O
of	O
these	O
interwoven	O
iterates	O
it	O
is	O
assumed	O
that	O
one	O
step	O
size	O
is	O
considerably	O
smaller	O
than	O
the	O
other	O
.	O
The	O
slower	O
iterate	O
(	O
the	O
one	O
with	O
smaller	O
step	O
size	O
)	O
is	O
assumed	O
to	O
be	O
slow	O
enough	O
to	O
allow	O
the	O
fast	O
iterate	O
converge	O
while	O
being	O
perturbed	O
by	O
the	O
the	O
slower	O
.	O
The	O
perturbations	O
of	O
the	O
slow	O
should	O
be	O
small	O
enough	O
to	O
ensure	O
convergence	O
of	O
the	O
faster	O
.	O
The	O
iterates	O
map	O
at	O
time	O
step	O
the	O
fast	O
variable	O
and	O
the	O
slow	O
variable	O
to	O
their	O
new	O
values	O
:	O
The	O
iterates	O
use	O
:	O
mapping	Method
for	O
the	O
slow	O
iterate	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
mapping	O
for	O
the	O
fast	Task
iterate	Task
Eq	Task
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
step	O
size	O
for	O
the	O
slow	O
iterate	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
step	O
size	O
for	O
the	O
fast	Method
iterate	Method
Eq	Method
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
additive	Method
random	Method
Markov	Method
process	Method
for	O
the	O
slow	Task
iterate	Task
Eq	Task
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
additive	Method
random	Method
Markov	Method
process	Method
for	O
the	O
fast	Task
iterate	Task
Eq	Task
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
random	Method
Markov	Method
process	Method
for	O
the	O
slow	O
iterate	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
,	O
:	O
random	Method
Markov	Method
process	Method
for	O
the	O
fast	Task
iterate	Task
Eq	Task
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Convergence	O
of	O
Two	O
Time	Method
-	Method
Scale	Method
Stochastic	Method
Approximation	Method
Algorithms	Method
subsubsection	Method
:	O
Additive	Method
Noise	Method
The	O
first	O
result	O
is	O
from	O
Borkar	O
1997	O
which	O
was	O
generalized	O
in	O
Konda	O
and	O
Borkar	O
1999	O
.	O
Borkar	O
considered	O
the	O
iterates	O
:	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Assumptions	O
on	O
the	O
update	O
functions	O
:	O
The	O
functions	O
and	O
are	O
Lipschitz	O
.	O
Assumptions	O
on	O
the	O
learning	O
rates	O
:	O
Assumptions	O
on	O
the	O
noise	O
:	O
For	O
the	O
increasing	O
-	O
field	O
the	O
sequences	O
of	O
random	O
variables	O
and	O
satisfy	O
Assumption	O
on	O
the	O
existence	O
of	O
a	O
solution	O
of	O
the	O
fast	O
iterate	O
:	O
For	O
each	O
,	O
the	O
ODE	Method
has	O
a	O
unique	O
global	O
asymptotically	O
stable	O
equilibrium	O
such	O
that	O
is	O
Lipschitz	O
.	O
Assumption	O
on	O
the	O
existence	O
of	O
a	O
solution	O
of	O
the	O
slow	O
iterate	O
:	O
The	O
ODE	Method
has	O
a	O
unique	O
global	O
asymptotically	O
stable	O
equilibrium	O
.	O
Assumption	O
of	O
bounded	O
iterates	O
:	O
paragraph	O
:	O
Convergence	O
Theorem	O
The	O
next	O
theorem	O
is	O
from	O
Borkar	O
1997	O
.	O
theorem	O
:	O
(	O
Borkar	O
)	O
.	O
If	O
the	O
assumptions	O
are	O
satisfied	O
,	O
then	O
the	O
iterates	O
Eq	O
.	O
(	O
)	O
and	O
Eq	O
.	O
(	O
)	O
converge	O
to	O
(	O
θ*	O
,	O
⁢λ	O
(	O
θ	O
*	O
)	O
)	O
a.s	O
.	O
paragraph	O
:	O
Comments	O
According	O
to	O
Lemma	O
2	O
in	O
Assumption	O
(	O
A3	O
)	O
is	O
fulfilled	O
if	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
w.r.t	O
with	O
and	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
w.r.t	O
with	O
where	O
and	O
are	O
positive	O
deterministic	O
constants	O
.	O
Assumption	O
(	O
A3	O
)	O
holds	O
for	O
mini	Task
-	Task
batch	Task
learning	Task
which	O
is	O
the	O
most	O
frequent	O
case	O
of	O
stochastic	Method
gradient	Method
.	O
The	O
batch	Method
gradient	Method
is	O
and	O
the	O
mini	Method
-	Method
batch	Method
gradient	Method
for	O
batch	O
size	O
is	O
,	O
where	O
the	O
indexes	O
are	O
randomly	O
and	O
uniformly	O
chosen	O
.	O
For	O
the	O
noise	O
we	O
have	O
.	O
Since	O
the	O
indexes	O
are	O
chosen	O
without	O
knowing	O
past	O
events	O
,	O
we	O
have	O
a	O
martingale	O
difference	O
sequence	O
.	O
For	O
bounded	O
gradients	O
we	O
have	O
bounded	O
.	O
We	O
address	O
assumption	O
(	O
A4	O
)	O
with	O
weight	O
decay	O
in	O
two	O
ways	O
:	O
(	O
I	O
)	O
Weight	Method
decay	Method
avoids	O
problems	O
with	O
a	O
discriminator	Method
that	O
is	O
region	O
-	O
wise	O
constant	O
and	O
,	O
therefore	O
,	O
does	O
not	O
have	O
a	O
locally	Method
stable	Method
generator	Method
.	O
If	O
the	O
generator	Method
is	O
perfect	O
,	O
then	O
the	O
discriminator	Method
is	O
0.5	O
everywhere	O
.	O
For	O
generator	Method
with	Method
mode	Method
collapse	Method
,	O
(	O
i	O
)	O
the	O
discriminator	Method
is	O
1	O
in	O
regions	O
without	O
generator	O
examples	O
,	O
(	O
ii	O
)	O
0	O
in	O
regions	O
with	O
generator	O
examples	O
only	O
,	O
(	O
iii	O
)	O
is	O
equal	O
to	O
the	O
local	O
ratio	O
of	O
real	O
world	O
examples	O
for	O
regions	O
with	O
generator	O
and	O
real	O
world	O
examples	O
.	O
Since	O
the	O
discriminator	Method
is	O
locally	O
constant	O
,	O
the	O
generator	Method
has	O
gradient	O
zero	O
and	O
can	O
not	O
improve	O
.	O
Also	O
the	O
discriminator	Method
can	O
not	O
improve	O
,	O
since	O
it	O
has	O
minimal	O
error	O
given	O
the	O
current	Method
generator	Method
.	O
However	O
,	O
without	O
weight	O
decay	O
the	O
Nash	O
Equilibrium	O
is	O
not	O
stable	O
since	O
the	O
second	O
order	O
derivatives	O
are	O
zero	O
,	O
too	O
.	O
(	O
II	O
)	O
Weight	Method
decay	Method
avoids	O
that	O
the	O
generator	Method
is	O
driven	O
to	O
infinity	O
with	O
unbounded	O
weights	O
.	O
For	O
example	O
a	O
linear	Method
discriminator	O
can	O
supply	O
a	O
gradient	O
for	O
the	O
generator	O
outside	O
each	O
bounded	O
region	O
.	O
The	O
main	O
result	O
used	O
in	O
the	O
proof	O
of	O
the	O
theorem	O
relies	O
on	O
work	O
on	O
perturbations	Method
of	Method
ODEs	Method
according	O
to	O
Hirsch	O
1989	O
.	O
Konda	O
and	O
Borkar	O
1999	O
generalized	O
the	O
convergence	Method
proof	Method
to	O
distributed	O
asynchronous	O
update	Method
rules	Method
.	O
Tadić	O
relaxed	O
the	O
assumptions	O
for	O
showing	O
convergence	Task
.	O
In	O
particular	O
the	O
noise	O
assumptions	O
(	O
Assumptions	O
A2	O
in	O
)	O
do	O
not	O
have	O
to	O
be	O
martingale	O
difference	O
sequences	O
and	O
are	O
more	O
general	O
than	O
in	O
.	O
In	O
another	O
result	O
the	O
assumption	O
of	O
bounded	O
iterates	O
is	O
not	O
necessary	O
if	O
other	O
assumptions	O
are	O
ensured	O
.	O
Finally	O
,	O
Tadić	Method
considers	O
the	O
case	O
of	O
non	O
-	O
additive	O
noise	O
.	O
Tadić	Method
does	O
not	O
provide	O
proofs	O
for	O
his	O
results	O
.	O
We	O
were	O
not	O
able	O
to	O
find	O
such	O
proofs	O
even	O
in	O
other	O
publications	O
of	O
Tadić	O
.	O
subsubsection	Method
:	O
Linear	Method
Update	Method
,	O
Additive	Method
Noise	Method
,	O
and	O
Markov	Method
Chain	Method
In	O
contrast	O
to	O
the	O
previous	O
subsection	O
,	O
we	O
assume	O
that	O
an	O
additional	O
Markov	O
chain	O
influences	O
the	O
iterates	O
.	O
The	O
Markov	Method
chain	Method
allows	O
applications	O
in	O
reinforcement	Task
learning	Task
,	O
in	O
particular	O
in	O
actor	Task
-	Task
critic	Task
setting	Task
where	O
the	O
Markov	Method
chain	Method
is	O
used	O
to	O
model	O
the	O
environment	O
.	O
The	O
slow	O
iterate	O
is	O
the	O
actor	Method
update	Method
while	O
the	O
fast	O
iterate	O
is	O
the	O
critic	Method
update	Method
.	O
For	O
reinforcement	Task
learning	Task
both	O
the	O
actor	O
and	O
the	O
critic	O
observe	O
the	O
environment	O
which	O
is	O
driven	O
by	O
the	O
actor	O
actions	O
.	O
The	O
environment	O
observations	O
are	O
assumed	O
to	O
be	O
a	O
Markov	Method
chain	Method
.	O
The	O
Markov	Method
chain	Method
can	O
include	O
eligibility	O
traces	O
which	O
are	O
modeled	O
as	O
explicit	O
states	O
in	O
order	O
to	O
keep	O
the	O
Markov	O
assumption	O
.	O
The	O
Markov	Method
chain	Method
is	O
the	O
sequence	O
of	O
observations	O
of	O
the	O
environment	O
which	O
progresses	O
via	O
transition	O
probabilities	O
.	O
The	O
transitions	O
are	O
not	O
affected	O
by	O
the	O
critic	O
but	O
by	O
the	O
actor	O
.	O
Konda	O
et	O
al	O
.	O
considered	O
the	O
iterates	Method
:	O
is	O
a	O
random	Method
process	Method
that	O
drives	O
the	O
changes	O
of	O
.	O
We	O
assume	O
that	O
is	O
a	O
slow	O
enough	O
process	O
.	O
We	O
have	O
a	O
linear	Method
update	O
rule	O
for	O
the	O
fast	O
iterate	O
using	O
the	O
vector	O
function	O
and	O
the	O
matrix	Method
function	Method
.	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Assumptions	O
on	O
the	O
Markov	Method
process	Method
,	O
that	O
is	O
,	O
the	O
transition	O
kernel	O
:	O
The	O
stochastic	Method
process	Method
takes	O
values	O
in	O
a	O
Polish	O
(	O
complete	O
,	O
separable	O
,	O
metric	O
)	O
space	O
with	O
the	O
Borel	O
-	O
field	O
For	O
every	O
measurable	O
set	O
and	O
the	O
parametrized	O
transition	O
kernel	O
we	O
have	O
:	O
We	O
define	O
for	O
every	O
measurable	O
function	O
Assumptions	O
on	O
the	O
learning	O
rates	O
:	O
for	O
some	O
.	O
Assumptions	O
on	O
the	O
noise	O
:	O
The	O
sequence	O
is	O
a	O
-	O
matrix	O
valued	O
-	O
martingale	O
difference	O
with	O
bounded	O
moments	O
:	O
We	O
assume	O
slowly	O
changing	O
,	O
therefore	O
the	O
random	Method
process	Method
satisfies	O
Assumption	O
on	O
the	O
existence	O
of	O
a	O
solution	O
of	O
the	O
fast	O
iterate	O
:	O
We	O
assume	O
the	O
existence	O
of	O
a	O
solution	O
to	O
the	O
Poisson	Method
equation	Method
for	O
the	O
fast	Method
iterate	Method
.	O
For	O
each	O
,	O
there	O
exist	O
functions	O
,	O
,	O
,	O
and	O
that	O
satisfy	O
the	O
Poisson	O
equations	O
:	O
Assumptions	O
on	O
the	O
update	O
functions	O
and	O
solutions	O
to	O
the	O
Poisson	Method
equation	Method
:	O
Boundedness	O
of	O
solutions	O
:	O
For	O
some	O
constant	O
and	O
for	O
all	O
:	O
Boundedness	O
in	O
expectation	O
:	O
All	O
moments	O
are	O
bounded	O
.	O
For	O
any	O
,	O
there	O
exists	O
such	O
that	O
Lipschitz	O
continuity	O
of	O
solutions	O
:	O
For	O
some	O
constant	O
and	O
for	O
all	O
,	O
:	O
Lipschitz	O
continuity	O
in	O
expectation	O
:	O
There	O
exists	O
a	O
positive	O
measurable	O
function	O
on	O
such	O
that	O
Function	O
gives	O
the	O
Lipschitz	O
constant	O
for	O
every	O
:	O
Uniform	O
positive	O
definiteness	O
:	O
There	O
exists	O
some	O
such	O
that	O
for	O
all	O
and	O
:	O
paragraph	O
:	O
Convergence	Metric
Theorem	Metric
.	O
We	O
report	O
Theorem	O
3.2	O
(	O
see	O
also	O
Theorem	O
7	O
in	O
)	O
and	O
Theorem	O
3.13	O
from	O
:	O
theorem	O
:	O
(	O
Konda	O
&	O
Tsitsiklis	O
)	O
.	O
If	O
the	O
assumptions	O
are	O
satisfied	O
,	O
then	O
for	O
the	O
iterates	O
Eq	O
.	O
(	O
)	O
and	O
Eq	O
.	O
(	O
)	O
holds	O
:	O
paragraph	O
:	O
Comments	O
.	O
The	O
proofs	O
only	O
use	O
the	O
boundedness	O
of	O
the	O
moments	O
of	O
,	O
therefore	O
may	O
depend	O
on	O
.	O
In	O
his	O
PhD	O
thesis	O
,	O
Vijaymohan	O
Konda	O
used	O
this	O
framework	O
for	O
the	O
actor	Method
-	Method
critic	Method
learning	Method
,	O
where	O
drives	O
the	O
updates	O
of	O
the	O
actor	O
parameters	O
.	O
However	O
,	O
the	O
actor	Method
updates	Method
are	O
based	O
on	O
the	O
current	O
parameters	O
of	O
the	O
critic	O
.	O
The	O
random	Method
process	Method
can	O
affect	O
as	O
long	O
as	O
boundedness	O
is	O
ensured	O
.	O
Nonlinear	Method
update	Method
rule	Method
.	O
can	O
be	O
viewed	O
as	O
a	O
linear	Method
approximation	O
of	O
a	O
nonlinear	Method
update	Method
rule	Method
.	O
The	O
nonlinear	Task
case	Task
has	O
been	O
considered	O
in	O
where	O
additional	O
approximation	O
errors	O
due	O
to	O
linearization	Task
were	O
addressed	O
.	O
These	O
errors	O
are	O
treated	O
in	O
the	O
given	O
framework	O
.	O
subsubsection	O
:	O
Additive	O
Noise	O
and	O
Controlled	Method
Markov	Method
Processes	Method
The	O
most	O
general	O
iterates	O
use	O
nonlinear	O
update	O
functions	O
and	O
,	O
have	O
additive	O
noise	O
,	O
and	O
have	O
controlled	Method
Markov	Method
processes	Method
.	O
paragraph	O
:	O
Required	O
Definitions	O
.	O
Marchaud	Method
Map	Method
:	O
A	O
set	Method
-	Method
valued	Method
map	Method
}	O
is	O
called	O
a	O
Marchaud	Method
map	Method
if	O
it	O
satisfies	O
the	O
following	O
properties	O
:	O
For	O
each	O
,	O
is	O
convex	O
and	O
compact	O
.	O
(	O
point	O
-	O
wise	O
boundedness	O
)	O
For	O
each	O
,	O
for	O
some	O
.	O
is	O
an	O
upper	O
-	O
semicontinuous	O
map	O
.	O
We	O
say	O
that	O
is	O
upper	O
-	O
semicontinuous	O
,	O
if	O
given	O
sequences	O
(	O
in	O
)	O
and	O
(	O
in	O
)	O
with	O
,	O
and	O
.	O
In	O
other	O
words	O
,	O
the	O
graph	O
of	O
,	O
is	O
closed	O
in	O
.	O
If	O
the	O
set	Method
-	Method
valued	Method
map	Method
is	O
Marchaud	O
,	O
then	O
the	O
differential	O
inclusion	O
(	O
DI	O
)	O
given	O
by	O
is	O
guaranteed	O
to	O
have	O
at	O
least	O
one	O
solution	O
that	O
is	O
absolutely	O
continuous	O
.	O
If	O
is	O
an	O
absolutely	O
continuous	O
map	O
satisfying	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
then	O
we	O
say	O
that	O
.	O
Invariant	O
Set	O
:	O
is	O
invariant	O
if	O
for	O
every	O
there	O
exists	O
a	O
trajectory	O
,	O
,	O
entirely	O
in	O
with	O
.	O
In	O
other	O
words	O
,	O
with	O
,	O
for	O
all	O
.	O
Internally	O
Chain	O
Transitive	O
Set	O
:	O
is	O
said	O
to	O
be	O
internally	O
chain	O
transitive	O
if	O
is	O
compact	O
and	O
for	O
every	O
,	O
and	O
we	O
have	O
the	O
following	O
:	O
There	O
exist	O
that	O
are	O
solutions	O
to	O
the	O
differential	O
inclusion	O
,	O
a	O
sequence	O
and	O
real	O
numbers	O
greater	O
than	O
such	O
that	O
:	O
where	O
is	O
the	O
open	O
-	O
neighborhood	O
of	O
and	O
for	O
.	O
The	O
sequence	O
is	O
called	O
an	O
chain	O
in	O
from	O
to	O
.	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Assumptions	O
on	O
the	O
controlled	Method
Markov	Method
processes	Method
:	O
The	O
controlled	Method
Markov	Method
process	Method
takes	O
values	O
in	O
a	O
compact	O
metric	O
space	O
.	O
The	O
controlled	Method
Markov	Method
process	Method
takes	O
values	O
in	O
a	O
compact	O
metric	O
space	O
.	O
Both	O
processes	O
are	O
controlled	O
by	O
the	O
iterate	O
sequences	O
and	O
.	O
Furthermore	O
is	O
additionally	O
controlled	O
by	O
a	O
random	Method
process	Method
taking	O
values	O
in	O
a	O
compact	O
metric	O
space	O
and	O
is	O
additionally	O
controlled	O
by	O
a	O
random	Method
process	Method
taking	O
values	O
in	O
a	O
compact	O
metric	O
space	O
.	O
The	O
dynamics	O
is	O
for	O
Borel	O
in	O
.	O
The	O
dynamics	O
is	O
for	O
Borel	O
in	O
.	O
Assumptions	O
on	O
the	O
update	O
functions	O
:	O
is	O
jointly	O
continuous	O
as	O
well	O
as	O
Lipschitz	O
in	O
its	O
first	O
two	O
arguments	O
uniformly	O
w.r.t	O
.	O
the	O
third	O
.	O
The	O
latter	O
condition	O
means	O
that	O
Note	O
that	O
the	O
Lipschitz	O
constant	O
does	O
not	O
depend	O
on	O
.	O
is	O
jointly	O
continuous	O
as	O
well	O
as	O
Lipschitz	O
in	O
its	O
first	O
two	O
arguments	O
uniformly	O
w.r.t	O
.	O
the	O
third	O
.	O
The	O
latter	O
condition	O
means	O
that	O
Note	O
that	O
the	O
Lipschitz	O
constant	O
does	O
not	O
depend	O
on	O
.	O
Assumptions	O
on	O
the	O
additive	O
noise	O
:	O
and	O
are	O
martingale	Method
difference	Method
sequence	Method
with	O
second	O
moments	O
bounded	O
by	O
.	O
More	O
precisely	O
,	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
w.r.t	Method
.	O
increasing	O
-	O
fields	O
satisfying	O
for	O
and	O
a	O
given	O
constant	O
.	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
w.r.t	O
.	O
increasing	O
-	O
fields	O
satisfying	O
for	O
and	O
a	O
given	O
constant	O
.	O
Assumptions	O
on	O
the	O
learning	Metric
rates	Metric
:	O
Furthermore	O
,	O
are	O
non	O
-	O
increasing	O
.	O
Assumptions	O
on	O
the	O
controlled	Method
Markov	Method
processes	Method
,	O
that	O
is	O
,	O
the	O
transition	O
kernels	O
:	O
The	O
state	Method
-	Method
action	Method
map	Method
and	O
the	O
state	Method
-	Method
action	Method
map	Method
are	O
continuous	O
.	O
Assumptions	O
on	O
the	O
existence	O
of	O
a	O
solution	O
:	O
We	O
consider	O
occupation	Method
measures	Method
which	O
give	O
for	O
the	O
controlled	Method
Markov	Method
process	Method
the	O
probability	O
or	O
density	O
to	O
observe	O
a	O
particular	O
state	O
-	O
action	O
pair	O
from	O
for	O
given	O
and	O
a	O
given	O
control	Method
policy	Method
.	O
We	O
denote	O
by	O
the	O
set	O
of	O
all	O
ergodic	Metric
occupation	Metric
measures	Metric
for	O
the	O
prescribed	O
and	O
on	O
state	O
-	O
action	O
space	O
for	O
the	O
controlled	Method
Markov	Method
process	Method
with	O
policy	Method
.	O
Analogously	O
we	O
denote	O
,	O
by	O
the	O
set	O
of	O
all	O
ergodic	Metric
occupation	Metric
measures	Metric
for	O
the	O
prescribed	O
and	O
on	O
state	O
-	O
action	O
space	O
for	O
the	O
controlled	Method
Markov	Method
process	Method
with	O
policy	Method
.	O
Define	O
for	O
a	O
measure	O
on	O
and	O
the	O
Marchaud	Method
map	Method
We	O
assume	O
that	O
the	O
set	O
is	O
singleton	O
,	O
that	O
is	O
,	O
contains	O
a	O
single	O
function	O
and	O
we	O
use	O
the	O
same	O
notation	O
for	O
the	O
set	O
and	O
its	O
single	O
element	O
.	O
If	O
the	O
set	O
is	O
not	O
a	O
singleton	O
,	O
the	O
assumption	O
of	O
a	O
solution	O
can	O
be	O
expressed	O
by	O
the	O
differential	O
inclusion	O
.	O
,	O
the	O
ODE	Method
has	O
an	O
asymptotically	O
stable	O
equilibrium	O
with	O
domain	O
of	O
attraction	O
where	O
is	O
a	O
Lipschitz	O
map	O
with	O
constant	O
.	O
Moreover	O
,	O
the	O
function	O
is	O
continuously	O
differentiable	O
where	O
is	O
the	O
Lyapunov	O
function	O
for	O
and	O
.	O
This	O
extra	O
condition	O
is	O
needed	O
so	O
that	O
the	O
set	O
becomes	O
an	O
asymptotically	O
stable	O
set	O
of	O
the	O
coupled	Method
ODE	Method
Assumption	Method
of	Method
bounded	Method
iterates	Method
:	O
paragraph	O
:	O
Convergence	Metric
Theorem	Metric
.	O
The	O
following	O
theorem	O
is	O
from	O
Karmakar	O
&	O
Bhatnagar	O
:	O
theorem	O
:	O
(	O
Karmakar	O
&	O
Bhatnagar	O
)	O
.	O
Under	O
above	O
assumptions	O
if	O
for	O
all	O
∈θRm	O
,	O
with	O
probability	O
1	O
,	O
{	O
wn	O
}	O
belongs	O
to	O
a	O
compact	O
subset	O
Qθ	O
(	O
depending	O
on	O
the	O
sample	O
point	O
)	O
of	O
Gθ	O
‘	O
‘	O
eventually	O
’	O
’	O
,	O
then	O
where	O
=	O
A0∩⩾t0¯{⁢¯θ	O
(	O
s	O
)	O
:⩾st	O
}	O
which	O
is	O
almost	O
everywhere	O
an	O
internally	O
chain	O
transitive	O
set	O
of	O
the	O
differential	O
inclusion	O
where	O
=	O
⁢^h	O
(	O
θ	O
)	O
{⁢~h	O
(	O
θ	O
,	O
⁢λ	O
(	O
θ	O
),	O
ν	O
)	O
:∈ν⁢D	O
(	O
w	O
)(	O
θ	O
,	O
⁢λ	O
(	O
θ	O
))	O
}.	O
paragraph	O
:	O
Comments	O
.	O
This	O
framework	O
allows	O
to	O
show	O
convergence	O
for	O
gradient	Method
descent	Method
methods	Method
beyond	O
stochastic	Method
gradient	Method
like	O
for	O
the	O
ADAM	Method
procedure	Method
where	O
current	O
learning	O
parameters	O
are	O
memorized	O
and	O
updated	O
.	O
The	O
random	Method
processes	Method
and	O
may	O
track	O
the	O
current	O
learning	O
status	O
for	O
the	O
fast	O
and	O
slow	O
iterate	O
,	O
respectively	O
.	O
Stochastic	Method
regularization	Method
like	O
dropout	Method
is	O
covered	O
via	O
the	O
random	Method
processes	Method
and	O
.	O
subsection	O
:	O
Rate	Metric
of	Metric
Convergence	Metric
of	O
Two	O
Time	Method
-	Method
Scale	Method
Stochastic	Method
Approximation	Method
Algorithms	Method
subsubsection	Method
:	O
Linear	Method
Update	Method
Rules	Method
First	O
we	O
consider	O
linear	Method
iterates	O
according	O
to	O
the	O
PhD	O
thesis	O
of	O
Konda	O
and	O
Konda	O
&	O
Tsitsiklis	O
.	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
The	O
random	O
variables	O
,	O
are	O
independent	O
of	O
and	O
of	O
each	O
other	O
.	O
The	O
have	O
zero	O
mean	O
:	O
and	O
.	O
The	O
covariance	O
is	O
The	O
learning	O
rates	O
are	O
deterministic	O
,	O
positive	O
,	O
nondecreasing	O
and	O
satisfy	O
with	O
:	O
We	O
often	O
consider	O
the	O
case	O
.	O
Convergence	O
of	O
the	O
iterates	O
:	O
We	O
define	O
A	O
matrix	O
is	O
Hurwitz	O
if	O
the	O
real	O
part	O
of	O
each	O
eigenvalue	O
is	O
strictly	O
negative	O
.	O
We	O
assume	O
that	O
the	O
matrices	O
and	O
are	O
Hurwitz	O
.	O
Convergence	Metric
rate	Metric
remains	O
simple	O
:	O
There	O
exists	O
a	O
constant	O
such	O
that	O
If	O
,	O
then	O
The	O
matrix	O
is	O
Hurwitz	O
.	O
paragraph	O
:	O
Rate	Metric
of	Metric
Convergence	Metric
Theorem	Metric
.	O
The	O
next	O
theorem	O
is	O
taken	O
from	O
Konda	O
and	O
Konda	O
&	O
Tsitsiklis	O
.	O
Let	O
and	O
be	O
the	O
unique	O
solution	O
to	O
the	O
system	O
of	O
linear	Method
equations	O
For	O
each	O
,	O
let	O
theorem	O
:	O
(	O
Konda	O
&	O
Tsitsiklis	O
)	O
.	O
Under	O
above	O
assumptions	O
and	O
when	O
the	O
constant	O
ϵ	O
is	O
sufficiently	O
small	O
,	O
the	O
limit	O
matrices	O
exist	O
.	O
Furthermore	O
,	O
the	O
matrix	O
is	O
the	O
unique	O
solution	O
to	O
the	O
following	O
system	O
of	O
equations	O
Finally	O
,	O
The	O
next	O
theorems	O
shows	O
that	O
the	O
asymptotic	Metric
covariance	Metric
matrix	Metric
of	O
is	O
the	O
same	O
as	O
that	O
of	O
,	O
where	O
evolves	O
according	O
to	O
the	O
single	O
time	Method
-	Method
scale	Method
stochastic	Method
iteration	Method
:	O
The	O
next	O
theorem	O
combines	O
Theorem	O
2.8	O
of	O
Konda	O
&	O
Tsitsiklis	O
and	O
Theorem	O
4.1	O
of	O
Konda	O
&	O
Tsitsiklis	O
:	O
theorem	O
:	O
(	O
Konda	O
&	O
Tsitsiklis	O
2nd	O
)	O
.	O
Under	O
above	O
assumptions	O
If	O
the	O
assumptions	O
hold	O
with	O
=	O
ϵ0	O
,	O
then	O
⁢a	O
(	O
n	O
)-/	O
12^θn	O
converges	O
in	O
distribution	O
to	O
⁢N	O
(	O
0	O
,	O
Σ11	O
(	O
0	O
)	O
)	O
.	O
paragraph	O
:	O
Comments	O
.	O
In	O
his	O
PhD	O
thesis	O
Konda	O
extended	O
the	O
analysis	O
to	O
the	O
nonlinear	Task
case	Task
.	O
Konda	Method
makes	O
a	O
linearization	Method
of	Method
the	Method
nonlinear	Method
function	Method
and	O
with	O
There	O
are	O
additional	O
errors	O
due	O
to	O
linearization	Task
which	O
have	O
to	O
be	O
considered	O
.	O
However	O
,	O
only	O
a	O
sketch	O
of	O
a	O
proof	O
is	O
provided	O
but	O
not	O
a	O
complete	O
proof	O
.	O
Theorem	O
4.1	O
of	O
Konda	O
&	O
Tsitsiklis	O
is	O
important	O
to	O
generalize	O
to	O
the	O
nonlinear	Task
case	Task
.	O
The	O
convergence	Metric
rate	Metric
is	O
governed	O
by	O
for	O
the	O
fast	O
and	O
for	O
the	O
slow	O
iterate	O
.	O
in	O
turn	O
is	O
affected	O
by	O
the	O
interaction	O
effects	O
captured	O
by	O
and	O
together	O
with	O
the	O
inverse	O
of	O
.	O
subsubsection	O
:	O
Nonlinear	Method
Update	Method
Rules	Method
The	O
rate	Metric
of	Metric
convergence	Metric
for	O
nonlinear	O
update	Method
rules	Method
according	O
to	O
Mokkadem	Method
&	O
Pelletier	O
is	O
considered	O
.	O
The	O
iterates	O
are	O
with	O
the	O
increasing	O
-	O
fields	O
The	O
terms	O
and	O
can	O
be	O
used	O
to	O
address	O
the	O
error	O
through	O
linearization	Task
,	O
that	O
is	O
,	O
the	O
difference	O
of	O
the	O
nonlinear	O
functions	O
to	O
their	O
linear	Method
approximation	O
.	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Convergence	Task
is	O
ensured	O
:	O
Linear	Method
approximation	Method
and	O
Hurwitz	Method
:	O
There	O
exists	O
a	O
neighborhood	O
of	O
such	O
that	O
,	O
for	O
all	O
We	O
define	O
A	O
matrix	O
is	O
Hurwitz	O
if	O
the	O
real	O
part	O
of	O
each	O
eigenvalue	O
is	O
strictly	O
negative	O
.	O
We	O
assume	O
that	O
the	O
matrices	O
and	O
are	O
Hurwitz	O
.	O
Assumptions	O
on	O
the	O
learning	Metric
rates	Metric
:	O
where	O
and	O
and	O
.	O
If	O
,	O
then	O
with	O
as	O
the	O
absolute	O
value	O
of	O
the	O
largest	O
eigenvalue	O
of	O
(	O
the	O
eigenvalue	O
closest	O
to	O
0	O
)	O
.	O
Assumptions	O
on	O
the	O
noise	O
and	O
error	O
:	O
martingale	O
difference	O
sequences	O
:	O
existing	O
second	O
moments	O
:	O
bounded	O
moments	O
:	O
There	O
exist	O
such	O
that	O
bounded	O
error	O
:	O
with	O
paragraph	O
:	O
Rate	Metric
of	Metric
Convergence	Metric
Theorem	Metric
.	O
We	O
report	O
a	O
theorem	O
and	O
a	O
proposition	O
from	O
Mokkadem	O
&	O
Pelletier	O
.	O
However	O
,	O
first	O
we	O
have	O
to	O
define	O
the	O
covariance	O
matrices	O
and	O
which	O
govern	O
the	O
rate	Metric
of	Metric
convergence	Metric
.	O
First	O
we	O
define	O
We	O
now	O
define	O
the	O
asymptotic	O
covariance	O
matrices	O
and	O
:	O
and	O
are	O
solutions	O
of	O
the	O
Lyapunov	Method
equations	Method
:	O
theorem	O
:	O
(	O
Mokkadem	O
&	O
Pelletier	O
:	O
Joint	Task
weak	Task
convergence	Task
)	O
.	O
Under	O
above	O
assumptions	O
:	O
theorem	O
:	O
(	O
Mokkadem	O
&	O
Pelletier	O
:	O
Strong	Task
convergence	Task
)	O
.	O
Under	O
above	O
assumptions	O
:	O
paragraph	O
:	O
Comments	O
.	O
Besides	O
the	O
learning	O
steps	O
and	O
,	O
the	O
convergence	Metric
rate	Metric
is	O
governed	O
by	O
for	O
the	O
fast	O
and	O
for	O
the	O
slow	O
iterate	O
.	O
in	O
turn	O
is	O
affected	O
by	O
interaction	O
effects	O
which	O
are	O
captured	O
by	O
and	O
together	O
with	O
the	O
inverse	O
of	O
.	O
subsection	O
:	O
Equal	Method
Time	Method
-	Method
Scale	Method
Stochastic	Method
Approximation	Method
Algorithms	Method
In	O
this	O
subsection	O
we	O
consider	O
the	O
case	O
when	O
the	O
learning	O
rates	O
have	O
equal	O
time	O
-	O
scale	O
.	O
subsubsection	Method
:	O
Equal	O
Time	O
-	O
Scale	O
for	O
Saddle	Method
Point	Method
Iterates	Method
If	O
equal	O
time	O
-	O
scales	O
assumed	O
then	O
the	O
iterates	O
revisit	O
infinite	O
often	O
an	O
environment	O
of	O
the	O
solution	O
.	O
In	O
Zhang	O
2007	O
,	O
the	O
functions	O
of	O
the	O
iterates	O
are	O
the	O
derivatives	O
of	O
a	O
Lagrangian	O
with	O
respect	O
to	O
the	O
dual	O
and	O
primal	O
variables	O
.	O
The	O
iterates	O
are	O
with	O
the	O
increasing	O
-	O
fields	O
The	O
terms	O
and	O
subsum	Metric
biased	Metric
estimation	Metric
errors	Metric
.	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Assumptions	O
on	O
update	O
function	O
:	O
and	O
are	O
continuous	O
,	O
differentiable	O
,	O
and	O
bounded	O
.	O
The	O
Jacobians	O
are	O
Hurwitz	O
.	O
A	O
matrix	O
is	O
Hurwitz	O
if	O
the	O
real	O
part	O
of	O
each	O
eigenvalue	O
is	O
strictly	O
negative	O
.	O
This	O
assumptions	O
corresponds	O
to	O
the	O
assumption	O
in	O
that	O
the	O
Lagrangian	O
is	O
concave	O
in	O
and	O
convex	O
in	O
.	O
Assumptions	O
on	O
noise	O
:	O
and	O
are	O
a	O
martingale	Method
difference	Method
sequences	Method
w.r.t	O
.	O
the	O
increasing	O
-	O
fields	O
.	O
Furthermore	O
they	O
are	O
mutually	O
independent	O
.	O
Bounded	O
second	O
moment	O
:	O
Assumptions	O
on	O
the	O
learning	Metric
rate	Metric
:	O
Assumption	O
on	O
the	O
biased	Metric
error	Metric
:	O
Boundedness	O
:	O
paragraph	O
:	O
Theorem	O
.	O
Define	O
the	O
‘	O
‘	O
contraction	O
region	O
’	O
’	O
as	O
follows	O
:	O
theorem	O
:	O
(	O
Zhang	O
)	O
.	O
Under	O
above	O
assumptions	O
the	O
iterates	O
return	O
to	O
Aη	O
infinitely	O
often	O
with	O
probability	O
one	O
(	O
a.s	O
.	O
)	O
.	O
paragraph	O
:	O
Comments	O
.	O
The	O
proof	O
of	O
the	O
theorem	O
in	O
does	O
not	O
use	O
the	O
saddle	O
point	O
condition	O
and	O
not	O
the	O
fact	O
that	O
the	O
functions	O
of	O
the	O
iterates	O
are	O
derivatives	O
of	O
the	O
same	O
function	O
.	O
For	O
the	O
unbiased	Task
case	Task
,	O
Zhang	O
showed	O
in	O
Theorem	O
3.1	O
of	O
that	O
the	O
iterates	O
converge	O
.	O
However	O
,	O
he	O
used	O
the	O
saddle	O
point	O
condition	O
of	O
the	O
Lagrangian	O
.	O
He	O
considered	O
iterates	O
with	O
functions	O
that	O
are	O
the	O
derivatives	O
of	O
a	O
Lagrangian	O
with	O
respect	O
to	O
the	O
dual	O
and	O
primal	O
variables	O
.	O
subsubsection	Method
:	O
Equal	Method
Time	Method
Step	Method
for	O
Actor	Method
-	Method
Critic	Method
Method	Method
If	O
equal	O
time	O
-	O
scales	O
assumed	O
then	O
the	O
iterates	O
revisit	O
infinite	O
often	O
an	O
environment	O
of	O
the	O
solution	O
of	O
DiCastro	O
&	O
Meir	O
.	O
The	O
iterates	O
of	O
DiCastro	Method
&	O
Meir	Method
are	O
derived	O
for	O
actor	Method
-	Method
critic	Method
learning	Method
.	O
To	O
present	O
the	O
actor	Method
-	Method
critic	Method
update	Method
iterates	Method
,	O
we	O
have	O
to	O
define	O
some	O
functions	O
and	O
terms	O
.	O
is	O
the	O
policy	Method
function	Method
parametrized	O
by	O
with	O
observations	O
and	O
actions	O
.	O
A	O
Markov	Method
chain	Method
given	O
by	O
gives	O
the	O
next	O
observation	O
using	O
the	O
observation	O
and	O
the	O
action	O
.	O
In	O
each	O
state	O
the	O
agent	O
receives	O
a	O
reward	O
.	O
The	O
average	O
reward	O
per	O
stage	O
is	O
for	O
the	O
recurrent	O
state	O
:	O
The	O
estimate	O
of	O
is	O
denoted	O
by	O
.	O
The	O
differential	O
value	O
function	O
is	O
The	O
temporal	O
difference	O
is	O
The	O
estimate	O
of	O
is	O
denoted	O
by	O
.	O
The	O
likelihood	O
ratio	O
derivative	O
is	O
The	O
value	O
function	O
is	O
approximated	O
by	O
where	O
.	O
We	O
define	O
and	O
For	O
TD	Task
(	Task
)	O
we	O
have	O
an	O
eligibility	O
trace	O
:	O
We	O
define	O
the	O
approximation	Metric
error	Metric
with	O
optimal	O
parameter	O
:	O
where	O
is	O
an	O
projection	O
operator	O
into	O
the	O
span	O
of	O
.	O
We	O
bound	O
this	O
error	O
by	O
We	O
denoted	O
by	O
,	O
,	O
and	O
the	O
exact	O
functions	O
and	O
used	O
for	O
their	O
approximation	O
,	O
,	O
and	O
,	O
respectively	O
.	O
We	O
have	O
learning	O
rate	O
adjustments	O
and	O
for	O
the	O
critic	Method
.	O
The	O
update	Method
rules	Method
are	O
:	O
Critic	O
:	O
Actor	O
:	O
paragraph	O
:	O
Assumptions	O
.	O
We	O
make	O
the	O
following	O
assumptions	O
:	O
Assumption	O
on	O
rewards	O
:	O
The	O
rewards	O
are	O
uniformly	O
bounded	O
by	O
a	O
finite	O
constant	O
.	O
Assumption	O
on	O
the	O
Markov	Method
chain	Method
:	O
Each	O
Markov	Method
chain	Method
for	O
each	O
is	O
aperiodic	O
,	O
recurrent	O
,	O
and	O
irreducible	O
.	O
Assumptions	O
on	O
the	O
policy	O
function	O
:	O
The	O
conditional	Method
probability	Method
function	Method
is	O
twice	O
differentiable	O
.	O
Moreover	O
,	O
there	O
exist	O
positive	O
constants	O
,	O
and	O
,	O
such	O
that	O
for	O
all	O
,	O
,	O
and	O
we	O
have	O
Assumption	O
on	O
the	O
likelihood	O
ratio	O
derivative	O
:	O
For	O
all	O
,	O
,	O
and	O
,	O
there	O
exists	O
a	O
positive	O
constant	O
,	O
such	O
that	O
where	O
is	O
the	O
Euclidean	O
norm	O
.	O
Assumptions	O
on	O
the	O
approximation	O
space	O
given	O
by	O
:	O
The	O
columns	O
of	O
the	O
matrix	O
are	O
independent	O
,	O
that	O
is	O
,	O
the	O
form	O
a	O
basis	O
of	O
dimension	O
.	O
The	O
norms	O
of	O
the	O
columns	O
vectors	O
of	O
the	O
matrix	O
are	O
bounded	O
above	O
by	O
,	O
that	O
is	O
,	O
for	O
.	O
Assumptions	O
on	O
the	O
learning	Metric
rate	Metric
:	O
paragraph	O
:	O
Theorem	O
.	O
The	O
algorithm	O
converged	O
if	O
,	O
since	O
the	O
actor	O
reached	O
a	O
stationary	O
point	O
where	O
the	O
updates	O
are	O
zero	O
.	O
We	O
assume	O
that	O
hints	O
at	O
how	O
close	O
we	O
are	O
to	O
the	O
convergence	O
point	O
.	O
The	O
next	O
theorem	O
from	O
DiCastro	O
&	O
Meir	O
implies	O
that	O
the	O
trajectory	O
visits	O
a	O
neighborhood	O
of	O
a	O
local	O
maximum	O
infinitely	O
often	O
.	O
Although	O
it	O
may	O
leave	O
the	O
local	O
vicinity	O
of	O
the	O
maximum	O
,	O
it	O
is	O
guaranteed	O
to	O
return	O
to	O
it	O
infinitely	O
often	O
.	O
theorem	O
:	O
(	O
DiCastro	O
&	O
Meir	O
)	O
.	O
Define	O
where	O
B⁢Δtd1	O
,	O
B⁢Δtd2	O
,	O
and	O
B⁢Δtd3	O
are	O
finite	O
constants	O
depending	O
on	O
the	O
Markov	O
decision	O
process	O
and	O
the	O
agent	O
parameters	O
.	O
Under	O
above	O
assumptions	O
The	O
trajectory	O
visits	O
a	O
neighborhood	O
of	O
a	O
local	O
maximum	O
infinitely	O
often	O
.	O
paragraph	O
:	O
Comments	O
.	O
The	O
larger	O
the	O
critic	Metric
learning	Metric
rates	Metric
and	O
are	O
,	O
the	O
smaller	O
is	O
the	O
region	O
around	O
the	O
local	O
maximum	O
.	O
The	O
results	O
are	O
in	O
agreement	O
with	O
those	O
of	O
Zhang	O
2007	O
.	O
Even	O
if	O
the	O
results	O
are	O
derived	O
for	O
a	O
special	O
actor	Method
-	Method
critic	Method
setting	Method
,	O
they	O
carry	O
over	O
to	O
a	O
more	O
general	O
setting	O
of	O
the	O
iterates	O
.	O
section	O
:	O
ADAM	Task
Optimization	Task
as	O
Stochastic	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
The	O
Nesterov	Method
Accelerated	Method
Gradient	Method
Descent	Method
(	O
NAGD	Method
)	O
has	O
raised	O
considerable	O
interest	O
due	O
to	O
its	O
numerical	Metric
simplicity	Metric
and	O
its	O
low	Metric
complexity	Metric
.	O
Previous	O
to	O
NAGD	Method
and	O
its	O
derived	O
methods	O
there	O
was	O
Polyak	Method
’s	Method
Heavy	Method
Ball	Method
method	Method
.	O
The	O
idea	O
of	O
the	O
Heavy	O
Ball	O
is	O
a	O
ball	O
that	O
evolves	O
over	O
the	O
graph	O
of	O
a	O
function	O
with	O
damping	O
(	O
due	O
to	O
friction	O
)	O
and	O
acceleration	O
.	O
Therefore	O
,	O
this	O
second	Method
-	Method
order	Method
dynamical	Method
system	Method
can	O
be	O
described	O
by	O
the	O
ODE	Method
for	O
the	O
Heavy	Method
Ball	Method
with	Method
Friction	Method
(	O
HBF	Method
)	O
:	O
where	O
is	O
the	O
damping	O
coefficient	O
with	O
for	O
.	O
This	O
ODE	O
is	O
equivalent	O
to	O
the	O
integro	Method
-	Method
differential	Method
equation	Method
where	O
and	O
are	O
two	O
memory	O
functions	O
related	O
to	O
.	O
For	O
polynomially	O
memoried	O
HBF	Method
we	O
have	O
and	O
for	O
some	O
positive	O
,	O
and	O
for	O
exponentially	O
memoried	O
HBF	Method
we	O
have	O
and	O
.	O
For	O
the	O
sum	O
of	O
the	O
learning	Metric
rates	Metric
,	O
we	O
obtain	O
where	O
is	O
the	O
Euler	O
-	O
Mascheroni	O
constant	O
.	O
Gadat	O
et	O
al	O
.	O
derived	O
a	O
discrete	O
and	O
stochastic	O
version	O
of	O
the	O
HBF	Method
:	O
where	O
This	O
recursion	O
can	O
be	O
rewritten	O
as	O
The	O
recursion	O
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
is	O
the	O
first	Method
moment	Method
update	Method
of	Method
ADAM	Method
.	O
For	O
the	O
term	O
we	O
obtain	O
for	O
the	O
polynomial	O
memory	O
the	O
approximations	O
Gadat	O
et	O
al	O
.	O
showed	O
that	O
the	O
recursion	Method
Eq	Method
.	O
(	O
[	O
reference	O
]	O
)	O
converges	O
for	O
functions	O
with	O
at	O
most	O
quadratic	O
grow	O
.	O
The	O
authors	O
mention	O
that	O
convergence	O
can	O
be	O
proofed	O
for	O
functions	O
that	O
are	O
-	O
smooth	O
,	O
that	O
is	O
,	O
the	O
gradient	O
is	O
-	O
Lipschitz	O
.	O
Kingma	O
et	O
al	O
.	O
state	O
in	O
Theorem	O
4.1	O
convergence	Metric
of	O
ADAM	Method
while	O
assuming	O
that	O
,	O
the	O
first	Metric
moment	Metric
running	Metric
average	Metric
coefficient	Metric
,	O
decays	O
exponentially	O
.	O
Furthermore	O
they	O
assume	O
that	O
and	O
the	O
learning	O
rate	O
decays	O
with	O
.	O
ADAM	O
divides	O
of	O
the	O
recursion	Method
Eq	Method
.	O
(	O
[	O
reference	O
]	O
)	O
by	O
the	O
bias	Method
-	Method
corrected	Method
second	Method
raw	Method
moment	Method
estimate	Method
.	O
Since	O
the	O
bias	Method
-	Method
corrected	Method
second	Method
raw	Method
moment	Method
estimate	Method
changes	O
slowly	O
,	O
we	O
consider	O
it	O
as	O
an	O
error	O
.	O
ADAM	Method
assumes	O
the	O
second	O
moment	O
to	O
be	O
stationary	O
with	O
its	O
approximation	O
:	O
Therefore	O
We	O
are	O
interested	O
in	O
the	O
difference	O
of	O
actual	O
stochastic	O
to	O
the	O
true	O
stationary	O
:	O
For	O
a	O
stationary	O
second	O
moment	O
of	O
and	O
,	O
we	O
have	O
.	O
We	O
use	O
a	O
linear	Method
approximation	O
to	O
ADAM	Method
’s	Method
second	Method
moment	Method
normalization	Method
.	O
If	O
we	O
set	O
,	O
then	O
and	O
,	O
since	O
.	O
For	O
a	O
stationary	O
second	O
moment	O
of	O
,	O
is	O
a	O
martingale	Method
difference	Method
sequence	Method
with	O
a	O
bounded	O
second	O
moment	O
.	O
Therefore	O
can	O
be	O
subsumed	O
into	O
in	O
update	Method
rules	Method
Eq	O
.	O
(	O
[	O
reference	O
]	O
)	O
.	O
The	O
factor	O
can	O
be	O
incorporated	O
into	O
and	O
.	O
section	O
:	O
Experiments	O
:	O
Additional	O
Information	O
subsection	O
:	O
WGAN	Method
-	Method
GP	Method
on	O
Image	O
Data	O
.	O
subsection	O
:	O
WGAN	Method
-	Method
GP	Method
on	O
the	O
One	Material
Billion	Material
Word	Material
Benchmark	Material
.	O
subsection	O
:	O
BEGAN	Method
The	O
Boundary	Method
Equilibrium	Method
GAN	Method
(	O
BEGAN	Method
)	O
maintains	O
an	O
equilibrium	O
between	O
the	O
discriminator	O
and	O
generator	O
loss	O
(	O
cf	O
.	O
Section	O
3.3	O
in	O
)	O
which	O
,	O
in	O
turn	O
,	O
also	O
leads	O
to	O
a	O
fixed	O
relation	O
between	O
the	O
two	O
gradients	O
,	O
therefore	O
,	O
a	O
two	O
time	O
-	O
scale	O
update	O
is	O
not	O
ensured	O
by	O
solely	O
adjusting	O
the	O
learning	Metric
rates	Metric
.	O
Indeed	O
,	O
for	O
stable	Metric
learning	Metric
rates	Metric
,	O
we	O
see	O
no	O
differences	O
in	O
the	O
learning	Metric
progress	Metric
between	O
orig	O
and	O
TTUR	Method
as	O
depicted	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
section	O
:	O
Discriminator	Method
vs.	O
Generator	Method
Learning	Method
Rate	Method
The	O
convergence	Task
proof	Task
for	O
learning	O
GANs	Method
with	O
TTUR	Method
assumes	O
that	O
the	O
generator	Metric
learning	Metric
rate	Metric
will	O
eventually	O
become	O
small	O
enough	O
to	O
ensure	O
convergence	O
of	O
the	O
discriminator	Method
learning	Method
.	O
At	O
some	O
time	O
point	O
,	O
the	O
perturbations	O
of	O
the	O
discriminator	Method
updates	Method
by	O
updates	O
of	O
the	O
generator	O
parameters	O
are	O
sufficient	O
small	O
to	O
assure	O
that	O
the	O
discriminator	Method
converges	O
.	O
Crucial	O
for	O
discriminator	Task
convergence	Task
is	O
the	O
magnitude	O
of	O
the	O
perturbations	O
which	O
the	O
generator	Method
induces	O
into	O
the	O
discriminator	Method
updates	Method
.	O
These	O
perturbations	O
are	O
not	O
only	O
determined	O
by	O
the	O
generator	Metric
learning	Metric
rate	Metric
but	O
also	O
by	O
its	O
loss	O
function	O
,	O
current	O
value	O
of	O
the	O
loss	O
function	O
,	O
optimization	Method
method	Method
,	O
size	O
of	O
the	O
error	O
signals	O
that	O
reach	O
the	O
generator	O
(	O
vanishing	O
or	O
exploding	O
gradient	O
)	O
,	O
complexity	O
of	O
generator	Method
’s	Method
learning	Method
task	Method
,	O
architecture	O
of	O
the	O
generator	Method
,	O
regularization	Method
,	O
and	O
others	O
.	O
Consequently	O
,	O
the	O
size	O
of	O
generator	Metric
learning	Metric
rate	Metric
does	O
not	O
solely	O
determine	O
how	O
large	O
the	O
perturbations	O
of	O
the	O
discriminator	O
updates	O
are	O
but	O
serve	O
to	O
modulate	O
them	O
.	O
Thus	O
,	O
the	O
generator	Metric
learning	Metric
rate	Metric
may	O
be	O
much	O
larger	O
than	O
the	O
discriminator	Metric
learning	Metric
rate	Metric
without	O
inducing	O
large	O
perturbation	O
into	O
the	O
discriminator	Method
learning	Method
.	O
Even	O
the	O
learning	O
dynamics	O
of	O
the	O
generator	Method
is	O
different	O
from	O
the	O
learning	O
dynamics	O
of	O
the	O
discriminator	Method
,	O
though	O
they	O
both	O
have	O
the	O
same	O
learning	Metric
rate	Metric
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
loss	O
of	O
the	O
generator	Method
and	O
the	O
discriminator	Method
for	O
an	O
experiment	O
with	O
DCGAN	Method
on	O
CelebA	Material
,	O
where	O
the	O
learning	Metric
rate	Metric
was	O
0.0005	O
for	O
both	O
the	O
discriminator	Method
and	O
the	O
generator	Method
.	O
However	O
,	O
the	O
discriminator	Metric
loss	Metric
is	O
decreasing	O
while	O
the	O
generator	Metric
loss	Metric
is	O
increasing	O
.	O
This	O
example	O
shows	O
that	O
the	O
learning	Metric
rate	Metric
neither	O
determines	O
the	O
perturbations	O
nor	O
the	O
progress	O
in	O
learning	Task
for	O
two	O
coupled	Method
update	Method
rules	Method
.	O
The	O
choice	O
of	O
the	O
learning	O
rate	O
for	O
the	O
generator	Method
should	O
be	O
independent	O
from	O
choice	O
for	O
the	O
discriminator	Method
.	O
Also	O
the	O
search	O
ranges	O
of	O
discriminator	O
and	O
generator	O
learning	O
rates	O
should	O
be	O
independent	O
from	O
each	O
other	O
,	O
but	O
adjusted	O
to	O
the	O
corresponding	O
architecture	O
,	O
task	O
,	O
etc	O
.	O
section	O
:	O
Used	O
Software	O
,	O
Datasets	O
,	O
Pretrained	Method
Models	Method
,	O
and	O
Implementations	O
We	O
used	O
the	O
following	O
datasets	O
to	O
evaluate	O
GANs	Method
:	O
The	O
Large	Material
-	Material
scale	Material
CelebFaces	Material
Attributes	Material
(	O
CelebA	Material
)	O
dataset	O
,	O
aligned	O
and	O
cropped	O
,	O
the	O
training	O
dataset	O
of	O
the	O
bedrooms	Material
category	Material
of	O
the	O
large	Material
scale	Material
image	Material
database	Material
(	O
LSUN	Material
)	O
,	O
the	O
CIFAR	Material
-	Material
10	Material
training	Material
dataset	Material
,	O
the	O
Street	Material
View	Material
House	Material
Numbers	Material
training	Material
dataset	Material
(	O
SVHN	Material
)	O
,	O
and	O
the	O
One	Material
Billion	Material
Word	Material
Benchmark	Material
.	O
All	O
experiments	O
rely	O
on	O
the	O
respective	O
reference	O
implementations	O
for	O
the	O
corresponding	O
GAN	Method
model	Method
.	O
The	O
software	O
framework	O
for	O
our	O
experiments	O
was	O
Tensorflow	O
1.3	O
and	O
Python	O
3.6	O
.	O
We	O
used	O
following	O
software	O
,	O
datasets	O
and	O
pretrained	Method
models	Method
:	O
BEGAN	Method
in	O
Tensorflow	Method
,	O
,	O
Fixed	O
random	O
seeds	O
removed	O
.	O
Accessed	O
:	O
2017	O
-	O
05	O
-	O
30	O
DCGAN	Method
in	O
Tensorflow	Material
,	O
,	O
Fixed	O
random	O
seeds	O
removed	O
.	O
Accessed	O
:	O
2017	O
-	O
04	O
-	O
03	O
Improved	O
Training	O
of	O
Wasserstein	Method
GANs	Method
,	O
image	Method
model	Method
,	O
,	O
Accessed	O
:	O
2017	O
-	O
06	O
-	O
12	O
Improved	O
Training	O
of	O
Wasserstein	Method
GANs	Method
,	O
language	Method
model	Method
,	O
,	O
Accessed	O
:	O
2017	O
-	O
06	O
-	O
12	O
Inception	O
-	O
v3	O
pretrained	O
,	O
,	O
Accessed	O
:	O
2017	O
-	O
05	O
-	O
02	O
Implementations	O
are	O
available	O
at	O
bibliography	O
:	O
References	O
