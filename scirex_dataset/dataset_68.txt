document O
: O
Invariant Method
Information Method
Clustering Method
for O
Unsupervised Task
Image Task
Classification Task
and O
Segmentation Task
We O
present O
a O
novel O
clustering Method
objective Method
that O
learns O
a O
neural Method
network Method
classifier Method
from O
scratch O
, O
given O
only O
unlabelled O
data O
samples O
. O
The O
model O
discovers O
clusters O
that O
accurately O
match O
semantic O
classes O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
in O
eight O
unsupervised Task
clustering O
benchmarks O
spanning O
image Task
classification Task
and Task
segmentation Task
. O
These O
include O
STL10 O
, O
an O
unsupervised Task
variant O
of O
ImageNet O
, O
and O
CIFAR10 O
, O
where O
we O
significantly O
beat O
the O
accuracy Metric
of O
our O
closest O
competitors O
by O
8 O
and O
9.5 O
absolute O
percentage O
points O
respectively O
. O
The O
method O
is O
not O
specialised O
to O
computer Task
vision Task
and O
operates O
on O
any O
paired O
dataset O
samples O
; O
in O
our O
experiments O
we O
use O
random Method
transforms Method
to O
obtain O
a O
pair O
from O
each O
image O
. O
The O
trained O
network O
directly O
outputs O
semantic O
labels O
, O
rather O
than O
high Method
dimensional Method
representations Method
that O
need O
external O
processing O
to O
be O
usable O
for O
semantic Task
clustering Task
. O
The O
objective O
is O
simply O
to O
maximise O
mutual O
information O
between O
the O
class O
assignments O
of O
each O
pair O
. O
It O
is O
easy O
to O
implement O
and O
rigorously O
grounded O
in O
information Method
theory Method
, O
meaning O
we O
effortlessly O
avoid O
degenerate O
solutions O
that O
other O
clustering Method
methods Method
are O
susceptible O
to O
. O
In O
addition O
to O
the O
fully O
unsupervised Task
mode O
, O
we O
also O
test O
two O
semi O
- O
supervised Task
settings O
. O
The O
first O
achieves O
88.8 O
% O
accuracy Metric
on O
STL10 Task
classification Task
, O
setting O
a O
new O
global O
state O
- O
of O
- O
the O
- O
art O
over O
all O
existing O
methods O
( O
whether O
supervised Task
, O
semi Task
- Task
supervised Task
or O
unsupervised Task
) O
. O
The O
second O
shows O
robustness O
to O
90 O
% O
reductions O
in O
label Metric
coverage Metric
, O
of O
relevance O
to O
applications O
that O
wish O
to O
make O
use O
of O
small O
amounts O
of O
labels O
. O
capbtabboxtable O
[ O
] O
[ O
] O
[ O
table O
] O
capposition O
= O
bottom O
, O
captionskip=0.2em O
[ O
figure O
] O
capposition O
= O
bottom O
, O
captionskip=0.2em O
section O
: O
Introduction O
Most O
supervised Task
deep O
learning O
methods O
require O
large O
quantities O
of O
manually O
labelled O
data O
, O
limiting O
their O
applicability O
in O
many O
scenarios O
. O
This O
is O
true O
for O
large Task
- Task
scale Task
image Task
classification Task
and O
even O
more O
for O
segmentation Task
( O
pixel Task
- Task
wise Task
classification Task
) O
where O
the O
annotation Metric
cost Metric
per O
image O
is O
very O
high O
. O
Unsupervised Method
clustering Method
, O
on O
the O
other O
hand O
, O
aims O
to O
group O
data O
points O
into O
classes O
entirely O
without O
labels O
. O
Many O
authors O
have O
sought O
to O
combine O
mature O
clustering Method
algorithms Method
with O
deep Method
learning Method
, O
for O
example O
by O
bootstrapping Method
network Method
training Method
with O
k Method
- Method
means Method
style Method
objectives Method
. O
However O
, O
trivially O
combining O
clustering Method
and Method
representation Method
learning Method
methods Method
often O
leads O
to O
degenerate O
solutions O
. O
It O
is O
precisely O
to O
prevent O
such O
degeneracy O
that O
cumbersome O
pipelines O
— O
involving O
pre Method
- Method
training Method
, O
feature Method
post Method
- Method
processing Method
( O
whitening Method
or O
PCA Method
) O
, O
clustering Method
mechanisms Method
external O
to O
the O
network O
— O
have O
evolved O
. O
In O
this O
paper O
, O
we O
introduce O
Invariant Method
Information Method
Clustering Method
( O
IIC Method
) O
, O
a O
method O
that O
addresses O
this O
issue O
in O
a O
more O
principled O
manner O
. O
IIC Method
is O
a O
generic O
clustering Method
algorithm Method
that O
directly O
trains O
a O
randomly Method
initialised Method
neural Method
network Method
into O
a O
classification Method
function Method
, O
end O
- O
to O
- O
end O
and O
without O
any O
labels O
. O
It O
involves O
a O
simple O
objective Method
function Method
, O
which O
is O
the O
mutual O
information O
between O
the O
function O
’s O
classifications O
for O
paired O
data O
samples O
. O
The O
input O
data O
can O
be O
of O
any O
modality O
and O
, O
since O
the O
clustering O
space O
is O
discrete O
, O
mutual O
information O
can O
be O
computed O
exactly O
. O
Despite O
its O
simplicity O
, O
IIC Method
is O
intrinsically O
robust O
to O
two O
issues O
that O
affect O
other O
methods O
. O
The O
first O
is O
clustering Metric
degeneracy Metric
, O
which O
is O
the O
tendency O
for O
a O
single O
cluster O
to O
dominate O
the O
predictions O
or O
for O
clusters O
to O
disappear O
( O
which O
can O
be O
observed O
with O
k Method
- Method
means Method
, O
especially O
when O
combined O
with O
representation Method
learning Method
) O
. O
Due O
to O
the O
entropy Method
maximisation Method
component Method
within O
mutual O
information O
, O
the O
loss O
is O
not O
minimised O
if O
all O
images O
are O
assigned O
to O
the O
same O
class O
. O
At O
the O
same O
time O
, O
it O
is O
optimal O
for O
the O
model O
to O
predict O
for O
each O
image O
a O
single O
class O
with O
certainty O
( O
i.e. O
one O
- O
hot O
) O
due O
to O
the O
conditional Method
entropy Method
minimisation Method
( O
f O
: O
mnist_dots Method
) O
. O
The O
second O
issue O
is O
noisy O
data O
with O
unknown O
or O
distractor O
classes O
( O
present O
in O
STL10 O
for O
example O
) O
. O
IIC Method
addresses O
this O
issue O
by O
employing O
an O
auxiliary Method
output Method
layer Method
that O
is O
parallel O
to O
the O
main O
output Method
layer Method
, O
trained O
to O
produce O
an O
overclustering O
( O
i.e. O
same O
loss O
function O
but O
greater O
number O
of O
clusters O
than O
the O
ground O
truth O
) O
that O
is O
ignored O
at O
test O
time O
. O
Auxiliary Method
overclustering Method
is O
a O
general O
technique O
that O
could O
be O
useful O
for O
other O
algorithms O
. O
These O
two O
features O
of O
IIC Method
contribute O
to O
making O
it O
the O
only O
method O
amongst O
our O
unsupervised Task
baselines O
that O
is O
robust O
enough O
to O
make O
use O
of O
the O
noisy O
unlabelled O
subset O
of O
STL10 O
, O
a O
version O
of O
ImageNet O
specifically O
designed O
as O
a O
benchmark O
for O
unsupervised Task
clustering O
. O
In O
the O
rest O
of O
the O
paper O
, O
we O
begin O
by O
explaining O
the O
difference O
between O
semantic Method
clustering Method
and O
intermediate Method
representation Method
learning Method
( O
s O
: O
related O
) O
, O
which O
separates O
our O
method O
from O
the O
majority O
of O
work O
in O
unsupervised Task
deep O
learning O
. O
We O
then O
describe O
the O
theoretical O
foundations O
of O
IIC Method
in O
statistical Method
learning Method
( O
s Method
: Method
method Method
) O
, O
demonstrating O
that O
maximising O
mutual O
information O
between O
pairs O
of O
samples O
under O
a O
bottleneck O
is O
a O
principled Method
clustering Method
objective Method
which O
is O
equivalent O
to O
distilling O
their O
shared O
abstract O
content O
( O
co Method
- Method
clustering Method
) O
. O
We O
propose O
that O
for O
static O
images O
, O
an O
easy O
way O
to O
generate O
pairs O
with O
shared O
abstract O
content O
from O
unlabelled O
data O
is O
to O
take O
each O
image O
and O
its O
random O
transformation O
, O
or O
each O
patch O
and O
a O
neighbour O
. O
We O
show O
that O
maximising Task
MI Task
automatically O
avoids O
degenerate O
solutions O
and O
can O
be O
written O
as O
a O
convolution Method
in O
the O
case O
of O
segmentation Task
, O
allowing O
for O
efficient O
implementation O
with O
any O
deep Method
learning Method
library Method
. O
We O
perform O
experiments O
on O
a O
large O
number O
of O
datasets O
( O
s O
: O
experiments O
) O
including O
STL O
, O
CIFAR O
, O
MNIST Material
, O
COCO O
- O
Stuff O
and O
Potsdam O
, O
setting O
a O
new O
state O
- O
of O
- O
the O
- O
art O
on O
unsupervised Task
clustering O
and O
segmentation Task
in O
all O
cases O
, O
with O
results O
of O
61.0 O
% O
, O
61.7 O
% O
and O
72.3 O
% O
on O
STL10 O
, O
CIFAR10 O
and O
COCO O
- O
Stuff O
- O
3 O
beating O
the O
closest O
competitors O
( O
53.0 O
% O
, O
52.2 O
% O
, O
54.0 O
% O
) O
with O
significant O
margins O
. O
Note O
that O
training O
deep Method
neural Method
networks Method
to O
perform O
large Task
scale Task
, Task
real Task
- Task
world Task
segmentations Task
from O
scratch O
, O
without O
labels O
or O
heuristics O
, O
is O
a O
highly O
challenging O
task O
with O
negligible O
precedent O
. O
We O
also O
perform O
an O
ablation O
study O
and O
additionally O
test O
two O
semi O
- O
supervised Task
modes O
, O
setting O
a O
new O
global O
state O
- O
of O
- O
the O
- O
art O
of O
88.8 O
% O
on O
STL10 O
over O
all O
supervised Task
, O
semi Task
- Task
supervised Task
and O
unsupervised Task
methods O
, O
and O
demonstrating O
the O
robustness Metric
in O
semi O
- O
supervised Task
accuracy Metric
when O
90 O
% O
of O
labels O
are O
removed O
. O
section O
: O
Related O
work O
paragraph O
: O
Co Method
- Method
clustering Method
and O
mutual O
information O
. O
The O
use O
of O
information O
as O
a O
criterion O
to O
learn O
representations Task
is O
not O
new O
. O
One O
of O
the O
earliest O
works O
to O
do O
so O
is O
by O
Becker O
and O
Hinton O
. O
More O
generally O
, O
learning Task
from O
paired O
data O
has O
also O
been O
explored O
in O
co Task
- Task
clustering Task
and O
in O
other O
works O
that O
build O
on O
the O
information Method
bottleneck Method
principle Method
. O
Several O
recent O
papers O
have O
used O
information O
as O
a O
tool O
to O
train O
deep Method
networks Method
in O
particular O
, O
albeit O
not O
for O
discrete Task
clustering Task
. O
IMSAT Method
maximises O
mutual O
information O
between O
data O
and O
its O
representation O
and O
DeepINFOMAX Method
maximizes O
information O
between O
spatially O
- O
preserved O
features O
and O
compact O
features O
. O
However O
, O
IMSAT Method
and O
DeepINFOMAX Method
combine O
information O
with O
other O
criteria O
, O
whereas O
in O
our O
method O
information O
is O
the O
only O
criterion O
used O
. O
Furthermore O
, O
both O
IMSAT Method
and O
DeepINFOMAX Method
compute O
mutual O
information O
over O
continuous O
random O
variables O
, O
which O
requires O
complex O
estimators O
, O
whereas O
IIC Method
does O
so O
for O
discrete O
variables O
with O
simple O
and O
exact O
computations O
. O
Finally O
, O
DeepINFOMAX Method
considers O
the O
information O
between O
the O
features O
and O
a O
deterministic O
function O
of O
it O
, O
which O
is O
in O
principle O
the O
same O
as O
the O
entropy O
; O
in O
contrast O
, O
in O
IIC Method
information O
does O
not O
trivially O
reduce O
to O
entropy O
. O
paragraph O
: O
Semantic Method
clustering Method
versus O
intermediate Method
representation Method
learning Method
. O
In O
semantic Task
clustering Task
, O
the O
learned O
function O
directly O
outputs O
discrete O
assignments O
for O
high O
level O
( O
i.e. O
semantic O
) O
clusters O
. O
Intermediate Method
representation Method
learners Method
, O
on O
the O
other O
hand O
, O
produce O
continuous Task
, Task
distributed Task
, Task
high Task
- Task
dimensional Task
representations Task
that O
must O
be O
post O
- O
processed O
, O
for O
example O
by O
k Method
- Method
means Method
, O
to O
obtain O
the O
discrete O
low O
- O
cardinality O
assignments O
required O
for O
unsupervised Task
semantic O
clustering O
. O
The O
latter O
includes O
objectives O
such O
as O
generative Task
autoencoder Task
image Task
reconstruction Task
, O
triplets Task
and O
spatial Task
- Task
temporal Task
order Task
or O
context Task
prediction Task
, O
for O
example O
predicting Task
patch Task
proximity Task
, O
solving O
jigsaw Task
puzzles Task
and O
inpainting Task
. O
Note O
it O
also O
includes O
a O
number O
of O
clustering Method
methods Method
( O
DeepCluster Method
, O
exemplars Method
) O
where O
the O
clustering Method
is O
only O
auxiliary O
; O
a O
clustering Method
- Method
style Method
objective Method
is O
used O
but O
does O
not O
produce O
groups O
with O
semantic O
correspondence O
. O
For O
example O
, O
DeepCluster Method
is O
a O
state O
- O
of O
- O
the O
- O
art O
method O
for O
learning Task
highly Task
- Task
transferable Task
intermediate Task
features Task
using O
overclustering Method
as O
a O
proxy Task
task Task
, O
but O
does O
not O
automatically O
find O
semantically O
meaningful O
clusters O
. O
As O
these O
methods O
use O
auxiliary O
objectives O
divorced O
from O
the O
semantic Method
clustering Method
objective Method
, O
it O
is O
unsurprising O
that O
they O
perform O
worse O
than O
IIC Method
( O
s O
: O
experiments O
) O
, O
which O
directly O
optimises O
for O
it O
, O
training O
the O
network O
end O
- O
to O
- O
end O
with O
the O
final O
clusterer O
implicitly O
wrapped O
inside O
. O
paragraph O
: O
Optimising Task
image Task
- Task
to Task
- Task
image Task
distance Task
. O
Many O
approaches O
to O
deep Task
clustering Task
, O
whether O
semantic O
or O
auxiliary O
, O
utilise O
a O
distance O
function O
between O
input O
images O
that O
approximates O
a O
given O
grouping Metric
criterion Metric
. O
Agglomerative Method
clustering Method
and O
partially O
ordered O
sets O
of O
HOG O
features O
have O
been O
used O
to O
group O
images O
, O
and O
exemplars O
define O
a O
group O
as O
a O
set O
of O
random O
transformations O
applied O
to O
a O
single O
image O
. O
Note O
the O
latter O
does O
not O
scale O
easily O
, O
in O
particular O
to O
image Task
segmentation Task
where O
a O
single O
image O
would O
call O
for O
40k O
classes O
. O
DAC Method
, O
JULE Method
, O
DeepCluster Method
, O
ADC Method
and O
DEC Method
rely O
on O
the O
inherent O
visual O
consistency O
and O
disentangling O
properties O
of O
CNNs Method
to O
produce O
cluster O
assignments O
, O
which O
are O
processed O
and O
reinforced O
in O
each O
iteration O
. O
The O
latter O
three O
are O
based O
on O
k Method
- Method
means Method
style Method
mechanisms Method
to O
refine O
feature O
centroids O
, O
which O
is O
prone O
to O
degenerate O
solutions O
and O
thus O
needs O
explicit O
prevention Method
mechanisms Method
such O
as O
pre Method
- Method
training Method
, O
cluster Method
- Method
reassignment Method
or O
feature Method
cleaning Method
via O
PCA Method
and O
whitening Method
. O
paragraph O
: O
Invariance O
as O
a O
training O
objective O
. O
Optimising O
for O
function O
outputs O
to O
be O
persistent O
through O
spatio O
- O
temporal O
or O
non O
- O
material O
distortion O
is O
an O
idea O
shared O
by O
IIC Method
with O
several O
works O
, O
including O
exemplars Method
, O
IMSAT Method
, O
proximity Method
prediction Method
, O
the O
denoising Metric
objective Metric
of O
Tagger Method
, O
temporal O
slowness O
constraints O
, O
and O
optimising O
for O
features O
to O
be O
invariant O
to O
local O
image O
transformations O
. O
More O
broadly O
, O
the O
problem O
of O
modelling Task
data Task
transformation Task
has O
received O
significant O
attention O
in O
deep Task
learning Task
, O
one O
example O
being O
the O
transforming Method
autoencoder Method
. O
section O
: O
Method O
First O
we O
introduce O
a O
generic O
objective O
, O
Invariant Method
Information Method
Clustering Method
, O
which O
can O
be O
used O
to O
cluster O
any O
kind O
of O
unlabelled O
paired O
data O
by O
training O
a O
network O
to O
predict O
cluster O
identities O
( O
s O
: O
generic_clustering O
) O
. O
We O
then O
apply O
it O
to O
image Task
clustering Task
( O
s O
: O
image_clustering O
, O
f O
: O
overview O
and O
f O
: O
mnist_dots Method
) O
and O
segmentation Task
( O
s O
: O
image_segmentation Task
) Task
, O
by O
generating O
the O
required O
paired O
data O
using O
random O
transformations O
and O
spatial O
proximity O
. O
subsection O
: O
Invariant Method
Information Method
Clustering Method
Let O
be O
a O
paired O
data O
sample O
from O
a O
joint O
probability O
distribution O
. O
For O
example O
, O
and O
could O
be O
different O
images O
containing O
the O
same O
object O
. O
The O
goal O
of O
Invariant Method
Information Method
Clustering Method
( O
IIC Method
) O
is O
to O
learn O
a O
representation O
that O
preserves O
what O
is O
in O
common O
between O
and O
while O
discarding O
instance O
- O
specific O
details O
. O
The O
former O
can O
be O
achieved O
by O
maximizing O
the O
mutual O
information O
between O
encoded O
variables O
: O
which O
is O
equivalent O
to O
maximising O
the O
predictability O
of O
from O
and O
vice O
versa O
. O
An O
effect O
of O
equation O
e O
: O
info_orig Method
, O
in O
general O
, O
is O
to O
make O
representations O
of O
paired O
samples O
the O
same O
. O
However O
, O
it O
is O
not O
the O
same O
as O
merely O
minimising O
representation O
distance O
, O
as O
done O
for O
example O
in O
methods O
based O
on O
k Method
- Method
means Method
: O
the O
presence O
of O
entropy O
within O
allows O
us O
to O
avoid O
degeneracy O
, O
as O
discussed O
in O
detail O
below O
. O
If O
is O
a O
neural Method
network Method
with O
a O
small O
output O
capacity O
( O
often O
called O
a O
‘ O
‘ O
bottleneck O
’ O
’ O
) O
, O
e O
: O
info_orig Method
also O
has O
the O
effect O
of O
discarding O
instance O
- O
specific O
details O
from O
the O
data O
. O
Clustering Method
imposes O
a O
natural O
bottleneck O
, O
since O
the O
representation O
space O
is O
, O
a O
finite O
set O
of O
class O
indices O
( O
as O
opposed O
to O
an O
infinite O
vector O
space O
) O
. O
Without O
a O
bottleneck O
, O
i.e. O
assuming O
unbounded O
capacity O
, O
e O
: O
info_orig O
is O
trivially O
solved O
by O
setting O
to O
the O
identity O
function O
because O
of O
the O
data O
processing O
inequality O
, O
i.e. O
. O
Since O
our O
goal O
is O
to O
learn O
the O
representation O
with O
a O
deep Method
neural Method
network Method
, O
we O
consider O
soft O
rather O
than O
hard Task
clustering Task
, O
meaning O
the O
neural Method
network Method
is O
terminated O
by O
a O
( O
differentiable Method
) Method
softmax Method
layer Method
. O
Then O
the O
output O
can O
be O
interpreted O
as O
the O
distribution O
of O
a O
discrete O
random O
variable O
over O
classes O
, O
formally O
given O
by O
. O
Making O
the O
output O
probabilistic O
amounts O
to O
allowing O
for O
uncertainty O
in O
the O
cluster O
assigned O
to O
an O
input O
. O
Consider O
now O
a O
pair O
of O
such O
cluster O
assignment O
variables O
and O
for O
two O
inputs O
and O
respectively O
. O
Their O
conditional O
joint O
distribution O
is O
given O
by O
This O
equation O
states O
that O
and O
are O
independent O
when O
conditioned O
on O
specific O
inputs O
and O
; O
however O
, O
in O
general O
they O
are O
not O
independent O
after O
marginalization O
over O
a O
dataset O
of O
input O
pairs O
, O
. O
For O
example O
, O
for O
a O
trained O
classification Method
network Method
and O
a O
dataset O
of O
image O
pairs O
where O
each O
image O
contains O
the O
same O
object O
of O
its O
pair O
but O
in O
a O
randomly O
different O
position O
, O
the O
random O
variable O
constituted O
by O
the O
class O
of O
the O
first O
of O
each O
pair O
, O
, O
will O
have O
a O
strong O
statistical O
relationship O
with O
the O
random O
variable O
for O
the O
class O
of O
the O
second O
of O
each O
pair O
, O
; O
one O
is O
predictive O
of O
the O
other O
( O
in O
fact O
identical O
to O
it O
, O
in O
this O
case O
) O
so O
they O
are O
highly O
dependent O
. O
After O
marginalization O
over O
the O
dataset O
( O
or O
batch O
, O
in O
practice O
) O
, O
the O
joint O
probability O
distribution O
is O
given O
by O
the O
matrix O
, O
where O
each O
element O
at O
row O
and O
column O
constitutes O
: O
The O
marginals O
and O
can O
be O
obtained O
by O
summing O
over O
the O
rows O
and O
columns O
of O
this O
matrix O
. O
As O
we O
generally O
consider O
symmetric Task
problems Task
, O
where O
for O
each O
we O
also O
have O
, O
is O
symmetrized O
using O
. O
Now O
the O
objective Metric
function Metric
e O
: O
info_orig O
can O
be O
computed O
by O
plugging O
the O
matrix O
into O
the O
expression O
for O
mutual O
information O
, O
which O
results O
in O
the O
formula O
: O
paragraph O
: O
Why O
degenerate O
solutions O
are O
avoided O
. O
Mutual O
information O
( O
[ O
reference O
] O
) O
expands O
to O
. O
Hence O
, O
maximizing O
this O
quantity O
trades O
- O
off O
minimizing O
the O
conditional O
cluster O
assignment O
entropy O
and O
maximising O
individual O
cluster O
assignments O
entropy O
. O
The O
smallest O
value O
of O
is O
0 O
, O
obtained O
when O
the O
cluster O
assignments O
are O
exactly O
predictable O
from O
each O
other O
. O
The O
largest O
value O
of O
is O
, O
obtained O
when O
all O
clusters O
are O
equally O
likely O
to O
be O
picked O
. O
This O
occurs O
when O
the O
data O
is O
assigned O
evenly O
between O
the O
clusters O
, O
equalizing O
their O
mass O
. O
Therefore O
the O
loss O
is O
not O
minimised O
if O
all O
samples O
are O
assigned O
to O
a O
single O
cluster O
( O
i.e. O
output O
class O
is O
identical O
for O
all O
samples O
) O
. O
Thus O
as O
maximising O
mutual O
information O
naturally O
balances O
reinforcement Task
of Task
predictions Task
with O
mass Method
equalization Method
, O
it O
avoids O
the O
tendency O
for O
degenerate O
solutions O
that O
algorithms O
which O
combine O
k Method
- Method
means Method
with O
representation Method
learning Method
are O
susceptible O
to O
. O
For O
further O
discussion O
of O
entropy Task
maximisation Task
, O
and O
optionally O
how O
to O
prioritise O
it O
with O
an O
entropy O
coefficient O
, O
see O
supplementary O
material O
. O
paragraph O
: O
Meaning O
of O
mutual O
information O
. O
The O
reader O
may O
now O
wonder O
what O
are O
the O
benefits O
of O
maximising O
mutual O
information O
, O
as O
opposed O
to O
merely O
maximising O
entropy O
. O
Firstly O
, O
due O
to O
the O
soft Method
clustering Method
, O
entropy O
alone O
could O
be O
maximised O
trivially O
by O
setting O
all O
prediction O
vectors O
to O
uniform O
distributions O
, O
resulting O
in O
no O
clustering Method
. O
This O
is O
corrected O
by O
the O
conditional Method
entropy Method
component Method
, O
which O
encourages O
deterministic Task
one Task
- Task
hot Task
predictions Task
. O
For O
example O
, O
even O
for O
the O
degenerate O
case O
of O
identical O
pairs O
, O
the O
IIC Method
objective O
encourages O
a O
deterministic Method
clustering Method
function Method
( O
i.e. O
is O
a O
one O
- O
hot O
vector O
) O
as O
this O
results O
in O
null O
conditional O
entropy O
. O
Secondly O
, O
the O
objective O
of O
IIC Method
is O
to O
find O
what O
is O
common O
between O
two O
data O
points O
that O
share O
redundancy O
, O
such O
as O
different O
images O
of O
the O
same O
object O
, O
explicitly O
encouraging O
distillation O
of O
the O
common O
part O
while O
ignoring O
the O
rest O
, O
i.e. O
instance O
details O
specific O
to O
one O
of O
the O
samples O
. O
This O
would O
not O
be O
possible O
without O
pairing O
samples O
. O
subsection O
: O
Image Task
clustering Task
IIC Method
requires O
a O
source O
of O
paired O
samples O
, O
which O
are O
often O
unavailable O
in O
unsupervised Task
image O
clustering O
applications O
. O
In O
this O
case O
, O
we O
propose O
to O
use O
generated O
image O
pairs O
, O
consisting O
of O
image O
and O
its O
randomly O
perturbed O
version O
. O
The O
objective O
e O
: O
info_orig O
can O
thus O
be O
written O
as O
: O
where O
both O
image O
and O
transformation O
are O
random O
variables O
. O
Useful O
could O
include O
scaling O
, O
skewing O
, O
rotation O
or O
flipping O
( O
geometric O
) O
, O
changing O
contrast O
and O
colour O
saturation O
( O
photometric O
) O
, O
or O
any O
other O
perturbation O
that O
is O
likely O
to O
leave O
the O
content O
of O
the O
image O
intact O
. O
IIC Method
can O
then O
be O
used O
to O
recover O
the O
factor O
which O
is O
invariant O
to O
which O
of O
the O
pair O
is O
picked O
. O
The O
effect O
is O
to O
learn O
a O
function O
that O
partitions O
the O
data O
such O
that O
clusters O
are O
closed O
to O
the O
perturbations O
, O
without O
dropping O
clusters O
. O
The O
objective O
is O
simple O
enough O
to O
be O
written O
in O
six O
lines O
of O
PyTorch O
code O
( O
f O
: O
code O
) O
. O
paragraph O
: O
Auxiliary Method
overclustering Method
. O
For O
certain O
datasets O
( O
e.g. O
STL10 O
) O
, O
training O
data O
comes O
in O
two O
types O
: O
one O
known O
to O
contain O
only O
relevant O
classes O
and O
the O
other O
known O
to O
contain O
irrelevant O
or O
distractor O
classes O
. O
It O
is O
desirable O
to O
train O
a O
clusterer Method
specialised O
for O
the O
relevant O
classes O
, O
that O
still O
benefits O
from O
the O
context O
provided O
by O
the O
distractor O
classes O
, O
since O
the O
latter O
is O
often O
much O
larger O
( O
for O
example O
100 O
K O
compared O
to O
13 O
K O
for O
STL10 O
) O
. O
Our O
solution O
is O
to O
add O
an O
auxiliary Method
overclustering Method
head Method
to O
the O
network O
( O
f O
: O
overview Method
) O
that O
is O
trained O
with O
the O
full O
dataset O
, O
whilst O
the O
main O
output O
head O
is O
trained O
with O
the O
subset O
containing O
only O
relevant O
classes O
. O
This O
allows O
us O
to O
make O
use O
of O
the O
noisy O
unlabelled O
subset O
despite O
being O
an O
unsupervised Task
clustering O
method O
. O
Other O
methods O
are O
generally O
not O
robust O
enough O
to O
do O
so O
and O
thus O
avoid O
the O
100k O
- O
samples O
unlabelled O
subset O
of O
STL10 O
when O
training O
for O
unsupervised Task
clustering O
( O
) O
. O
Since O
the O
auxiliary Method
overclustering Method
head Method
outputs O
predictions O
over O
a O
larger O
number O
of O
clusters O
than O
the O
ground O
truth O
, O
whilst O
still O
maintaining O
a O
predictor O
that O
is O
matched O
to O
ground O
truth O
number O
of O
clusters O
( O
the O
main O
head O
) O
, O
it O
can O
be O
useful O
in O
general O
for O
increasing O
expressivity O
in O
the O
learned O
feature Method
representation Method
, O
even O
for O
datasets O
where O
there O
are O
no O
distractor O
classes O
. O
subsection O
: O
Image Task
segmentation Task
IIC Method
can O
be O
applied O
to O
image Task
segmentation Task
identically O
to O
image Task
clustering Task
, O
except O
for O
two O
modifications O
. O
Firstly O
, O
since O
predictions O
are O
made O
for O
each O
pixel O
densely O
, O
clustering Method
is O
applied O
to O
image O
patches O
( O
defined O
by O
the O
receptive O
field O
of O
the O
neural Method
network Method
for O
each O
output O
pixel O
) O
rather O
than O
whole O
images O
. O
Secondly O
, O
unlike O
with O
whole O
images O
, O
one O
has O
access O
to O
the O
spatial O
relationships O
between O
patches O
. O
Thus O
, O
we O
can O
add O
local O
spatial O
invariance O
to O
the O
list O
of O
geometric O
and O
photometric O
invariances O
in O
s O
: O
image_clustering O
, O
meaning O
we O
form O
pairs O
of O
patches O
not O
only O
via O
synthetic O
perturbations O
, O
but O
also O
by O
extracting O
pairs O
of O
adjacent O
patches O
in O
the O
image O
. O
In O
detail O
, O
let O
the O
RGB O
image O
be O
a O
tensor O
, O
a O
pixel O
location O
, O
and O
a O
patch O
centered O
at O
. O
We O
can O
form O
a O
pair O
of O
patches O
by O
looking O
at O
location O
and O
its O
neighbour O
at O
some O
small O
displacement O
. O
The O
cluster O
probability O
vectors O
for O
all O
patches O
can O
be O
read O
off O
as O
the O
column O
vectors O
of O
the O
tensor O
, O
computed O
by O
a O
single O
application O
of O
the O
convolutional Method
network Method
. O
Then O
, O
to O
apply O
IIC Method
, O
one O
simply O
substitutes O
pairs O
, O
in O
the O
calculation O
of O
the O
joint O
probability O
matrix O
( O
[ O
reference O
] O
) O
. O
The O
geometric O
and O
photometric O
perturbations O
used O
before O
for O
whole Task
image Task
clustering Task
can O
be O
applied O
to O
individual O
patches O
too O
. O
Rather O
than O
transforming O
patches O
individually O
, O
however O
, O
it O
is O
much O
more O
efficient O
to O
transform O
all O
of O
them O
in O
parallel O
by O
perturbing O
the O
entire O
image O
. O
Any O
number O
or O
combination O
of O
these O
invariances O
can O
be O
chained O
and O
learned O
simultaneously O
; O
the O
only O
detail O
is O
to O
ensure O
indices O
of O
the O
original O
image O
and O
transformed O
image O
class O
probability O
tensors O
line O
up O
, O
meaning O
that O
predictions O
from O
patches O
which O
are O
intended O
to O
be O
paired O
together O
do O
so O
. O
Formally O
, O
if O
the O
image Task
transformation Task
is O
a O
geometric O
transformation O
, O
the O
vector O
of O
cluster O
probabilities O
will O
not O
correspond O
to O
; O
rather O
, O
it O
will O
correspond O
to O
because O
patch O
is O
sent O
to O
patch O
by O
the O
transformation O
. O
All O
vectors O
can O
be O
paired O
at O
once O
by O
applying O
the O
reverse Method
transformation Method
to O
the O
tensor O
, O
as O
For O
example O
, O
flipping O
the O
input O
image O
will O
require O
flipping O
the O
resulting O
probability O
tensor O
back O
. O
In O
general O
, O
the O
perturbation O
can O
incorporate O
geometric O
and O
photometric O
transformations O
, O
and O
only O
needs O
to O
undo O
geometric O
ones O
. O
The O
segmentation Task
objective Task
is O
thus O
: O
Hence O
the O
goal O
is O
to O
maximize O
the O
information O
between O
each O
patch O
label O
and O
the O
patch O
label O
of O
its O
transformed O
neighbour O
patch O
, O
in O
expectation O
over O
images O
, O
patches O
within O
each O
image O
, O
and O
perturbations O
. O
Information O
is O
in O
turn O
averaged O
over O
all O
neighbour O
displacements O
( O
which O
was O
found O
to O
perform O
slightly O
better O
than O
averaging O
over O
before O
computing O
information O
; O
see O
supplementary O
material O
) O
. O
paragraph O
: O
Implementation O
. O
The O
joint O
distribution O
of O
e O
: O
info_seg O
for O
all O
displacements O
can O
be O
computed O
in O
a O
simple O
and O
highly O
efficient O
way O
. O
Given O
two O
network O
outputs O
for O
one O
batch O
of O
image O
pairs O
where O
, O
we O
first O
bring O
back O
into O
the O
coordinate O
- O
space O
of O
by O
using O
a O
bilinear Method
resampler Method
, O
which O
inverts O
any O
geometrical O
transforms O
in O
, O
. O
Then O
, O
the O
inner O
summation O
in O
e O
: O
info_seg O
reduces O
to O
the O
convolution O
of O
the O
two O
tensors O
. O
Using O
any O
standard O
deep Method
learning Method
framework Method
, O
this O
can O
be O
achieved O
by O
swapping O
the O
first O
two O
dimensions O
of O
each O
of O
and O
, O
computing O
( O
a O
2D Method
convolution Method
with O
padding O
in O
both O
dimensions O
) O
, O
and O
normalising O
the O
result O
to O
produce O
. O
section O
: O
Experiments O
We O
apply O
IIC Method
to O
fully O
unsupervised Task
image O
clustering O
and O
segmentation O
, O
as O
well O
as O
two O
semi O
- O
supervised Task
settings O
. O
Existing O
baselines O
are O
outperformed O
in O
all O
cases O
. O
We O
also O
conduct O
an O
analysis O
of O
our O
method O
via O
ablation Task
studies Task
. O
For O
minor O
details O
see O
supplementary O
material O
. O
subsection O
: O
Image Task
clustering Task
[ O
] O
table O
[ O
0.27 O
] O
[ O
] O
figure O
[ O
0.68 O
] O
paragraph O
: O
Datasets O
. O
We O
test O
on O
STL10 O
, O
which O
is O
ImageNet O
adapted O
for O
unsupervised Task
classification O
, O
as O
well O
as O
CIFAR10 O
, O
CIFAR100 O
- O
20 O
and O
MNIST Material
. O
The O
main O
setting O
is O
pure O
unsupervised Task
clustering O
( O
IIC Method
) O
but O
we O
also O
test O
two O
semi O
- O
supervised Task
settings O
: O
finetuning Method
and O
overclustering Method
. O
For O
unsupervised Task
clustering O
, O
following O
previous O
work O
, O
we O
train O
on O
the O
full O
dataset O
and O
test O
on O
the O
labelled O
part O
; O
for O
the O
semi O
- O
supervised Task
settings O
, O
train O
and O
test O
sets O
are O
separate O
. O
As O
for O
DeepCluster Method
, O
we O
found O
Sobel Method
filtering Method
to O
be O
beneficial O
, O
as O
it O
discourages O
clustering Task
based O
on O
trivial O
cues O
such O
as O
colour O
and O
encourages O
using O
more O
meaningful O
cues O
such O
as O
shape O
. O
Additionally O
, O
for O
data Task
augmentation Task
, O
we O
repeat O
images O
within O
each O
batch O
times O
; O
this O
means O
that O
multiple O
image O
pairs O
within O
a O
batch O
contain O
the O
same O
original O
image O
, O
each O
paired O
with O
a O
different O
transformation O
, O
which O
encourages O
greater O
distillation O
since O
there O
are O
more O
examples O
of O
which O
visual O
details O
to O
ignore O
( O
s O
: O
equalization O
) O
. O
We O
set O
for O
all O
experiments O
. O
Images O
are O
rescaled O
and O
cropped O
for O
training O
( O
prior O
to O
applying O
transforms O
, O
consisting O
of O
random O
additive O
and O
multiplicative O
colour O
transformations O
and O
horizontal O
flipping O
) O
and O
a O
single O
center O
crop O
is O
used O
at O
test O
time O
for O
all O
experiments O
except O
semi O
- O
supervised Task
finetuning O
, O
where O
10 O
crops O
are O
used O
. O
paragraph O
: O
Architecture O
. O
All O
networks O
are O
randomly O
initialised O
and O
consist O
of O
a O
ResNet Method
or O
VGG11 O
- O
like O
base O
( O
see O
sup O
. O
mat O
. O
) O
, O
followed O
by O
one O
or O
more O
heads O
( O
linear O
predictors O
) O
. O
Let O
the O
number O
of O
ground O
truth O
clusters O
be O
and O
the O
output O
channels O
of O
a O
head O
be O
. O
For O
IIC Method
, O
there O
is O
a O
main O
output O
head O
with O
and O
an O
auxiliary Method
overclustering Method
head Method
( O
f O
: O
overview O
) O
with O
. O
For O
semi Method
- Method
supervised Method
overclustering Method
there O
is O
one O
output O
head O
with O
. O
For O
increased O
robustness O
, O
each O
head O
is O
duplicated O
times O
with O
a O
different O
random O
initialisation O
, O
and O
we O
call O
these O
concrete O
instantiations O
sub O
- O
heads O
. O
Each O
sub O
- O
head O
takes O
features O
from O
and O
outputs O
a O
probability Method
distribution Method
for O
each O
batch O
element O
over O
the O
relevant O
number O
of O
clusters O
. O
For O
semi O
- O
supervised Task
finetuning O
( O
t O
: O
iid_imgclus_semisup O
) O
, O
the O
base O
is O
copied O
from O
a O
semi O
- O
supervised Task
overclustering O
network O
and O
combined O
with O
a O
single O
randomly Method
initialised Method
linear Method
layer Method
where O
. O
paragraph O
: O
Training O
. O
We O
use O
the O
Adam Method
optimiser Method
with O
learning Method
rate Method
. O
For O
IIC Method
, O
the O
main O
and O
auxiliary O
heads O
are O
trained O
by O
maximising O
e O
: O
loss_expanded O
in O
alternate O
epochs O
. O
For O
semi Method
- Method
supervised Method
overclustering Method
, O
the O
single O
head O
is O
trained O
by O
maximising O
e O
: O
loss_expanded Metric
. O
Semi O
- O
supervised Task
finetuning O
uses O
a O
standard O
logistic Method
loss Method
. O
paragraph O
: O
Evaluation O
. O
We O
evaluate O
based O
on O
accuracy Metric
( O
true Metric
positives Metric
divided O
by O
sample O
size O
) O
. O
For O
IIC Method
we O
follow O
the O
standard O
protocol O
of O
finding O
the O
best O
one O
- O
to O
- O
one O
permutation O
mapping O
between O
learned O
and O
ground O
- O
truth O
clusters O
( O
from O
the O
main O
output O
head O
; O
auxiliary Method
overclustering Method
head Method
is O
ignored O
) O
using O
linear Method
assignment Method
. O
While O
this O
step O
uses O
labels O
, O
it O
does O
not O
constitute O
learning Task
as O
it O
merely O
makes O
the O
metric O
invariant O
to O
the O
order O
of O
the O
clusters O
. O
For O
semi Method
- Method
supervised Method
overclustering Method
, O
each O
ground O
- O
truth O
cluster O
may O
correspond O
to O
the O
union O
of O
several O
predicted O
clusters O
. O
Evaluation Task
thus O
requires O
a O
many O
- O
to O
- O
one O
discrete O
map O
from O
to O
, O
since O
. O
This O
extracts O
some O
information O
from O
the O
labels O
and O
thus O
requires O
separated O
training O
and O
test O
set O
. O
Note O
this O
mapping O
is O
found O
using O
the O
training O
set O
( O
accuracy Metric
is O
computed O
on O
the O
test O
set O
) O
and O
does O
not O
affect O
the O
network O
parameters O
as O
it O
is O
used O
for O
evaluation Task
only O
. O
For O
semi O
- O
supervised Task
finetuning O
, O
output O
channel O
order O
matches O
ground O
truth O
so O
no O
mapping O
is O
required O
. O
The O
performance O
of O
each O
sub O
- O
head O
is O
assessed O
independently O
, O
and O
best O
and O
average O
performances O
are O
reported O
. O
paragraph O
: O
Unsupervised Method
learning Method
analysis Method
. O
IIC Method
is O
highly O
capable O
of O
discovering O
clusters O
in O
unlabelled O
data O
that O
accurately O
correspond O
to O
the O
underlying O
semantic O
classes O
, O
and O
outperforms O
all O
competing O
baselines O
at O
this O
task O
( O
t O
: O
img_clus_iid O
) O
, O
with O
significant O
margins O
of O
and O
in O
the O
case O
of O
STL10 O
and O
CIFAR10 O
. O
As O
mentioned O
in O
s O
: O
related O
, O
this O
underlines O
the O
advantages O
of O
end Task
- Task
to Task
- Task
end Task
optimisation Task
instead O
of O
using O
a O
fixed O
external Method
procedure Method
like O
k Method
- Method
means Method
as O
with O
many O
baselines O
. O
The O
clusters O
found O
by O
IIC Method
are O
highly O
discriminative O
( O
f O
: O
images_img_clus O
) O
, O
although O
note O
some O
failure O
cases O
; O
as O
IIC Method
distills O
purely O
visual O
correspondences O
within O
images O
, O
it O
can O
be O
confused O
by O
instances O
that O
combine O
classes O
, O
such O
as O
a O
deer O
with O
the O
coat O
pattern O
of O
a O
cat O
. O
Our O
ablations Method
( O
t O
: O
iid_imgclus_ablation Method
) O
illustrate O
the O
contributions O
of O
various O
implementation O
details O
, O
and O
in O
particular O
the O
accuracy Metric
gain O
from O
using O
auxiliary Method
overclustering Method
. O
paragraph O
: O
Semi O
- O
supervised Task
learning O
analysis O
. O
For O
semi O
- O
supervised Task
learning O
, O
we O
establish O
a O
new O
state O
- O
of O
- O
the O
- O
art O
on O
STL10 O
out O
of O
all O
reported O
methods O
by O
finetuning O
a O
network O
trained O
in O
an O
entirely O
unsupervised Task
fashion O
with O
the O
IIC Method
objective O
( O
recall O
labels O
in O
semi Method
- Method
supervised Method
overclustering Method
are O
used O
for O
evaluation O
and O
do O
not O
influence O
the O
network O
parameters O
) O
. O
This O
explicitly O
validates O
the O
quality O
of O
our O
unsupervised Task
learning O
method O
, O
as O
we O
beat O
even O
the O
supervised Task
state O
- O
of O
- O
the O
- O
art O
( O
t O
: O
iid_imgclus_semisup Method
) O
. O
Given O
that O
the O
bulk O
of O
parameters O
within O
semi Method
- Method
supervised Method
overclustering Method
are O
trained O
unsupervised Task
( O
i.e. O
all O
network O
parameters O
) O
, O
it O
is O
unsurprising O
that O
f O
: O
imgclus_variation Method
shows O
a O
90 O
% O
drop O
in O
the O
number O
of O
available O
labels O
for O
STL10 O
( O
decreasing O
the O
amount O
of O
labelled O
data O
available O
from O
5000 O
to O
500 O
over O
10 O
classes O
) O
barely O
impacts O
performance O
, O
costing O
just O
10 O
% O
drop O
in O
accuracy Metric
. O
This O
setting O
has O
lower O
label O
requirements O
than O
finetuning O
because O
whereas O
the O
latter O
learns O
all O
network O
parameters O
, O
the O
former O
only O
needs O
to O
learn O
a O
discrete O
map O
between O
and O
, O
making O
it O
an O
important O
practical O
setting O
for O
applications O
with O
small O
amounts O
of O
labelled O
data O
. O
subsection O
: O
Segmentation Task
1pt O
40 O
paragraph O
: O
Datasets O
. O
Large Task
scale Task
segmentation Task
on O
real O
- O
world O
data O
using O
deep Method
neural Method
networks Method
is O
extremely O
difficult O
without O
labels O
or O
heuristics O
, O
and O
has O
negligible O
precedent O
. O
We O
establish O
new O
baselines O
on O
scene O
and O
satellite O
images O
to O
highlight O
performance O
on O
textural Task
classes Task
, O
where O
the O
assumption O
of O
spatially O
proximal O
invariance O
( O
s O
: O
image_segmentation O
) O
is O
most O
valid O
. O
COCO O
- O
Stuff O
is O
a O
challenging O
and O
diverse O
segmentation O
dataset O
containing O
‘ O
‘ O
stuff O
’ O
’ O
classes O
ranging O
from O
buildings O
to O
bodies O
of O
water O
. O
We O
use O
the O
15 O
coarse O
labels O
and O
164k O
images O
variant O
, O
reduced O
to O
52k O
by O
taking O
only O
images O
with O
at O
least O
75 O
% O
stuff O
pixels O
. O
COCO O
- O
Stuff O
- O
3 O
is O
a O
subset O
of O
COCO O
- O
Stuff O
with O
only O
sky O
, O
ground O
and O
plants O
labelled O
. O
For O
both O
COCO O
datasets O
, O
input O
images O
are O
shrunk O
by O
two O
thirds O
and O
cropped O
to O
pixels O
, O
Sobel Method
preprocessing Method
is O
applied O
for O
data Task
augmentation Task
, O
and O
predictions O
for O
non O
- O
stuff O
pixels O
are O
ignored O
. O
Potsdam O
is O
divided O
into O
8550 O
RGBIR O
px O
satellite O
images O
, O
of O
which O
3150 O
are O
unlabelled O
. O
We O
test O
both O
the O
6 Method
- Method
label Method
variant Method
( O
roads O
and O
cars O
, O
vegetation O
and O
trees O
, O
buildings O
and O
clutter O
) O
and O
a O
3 Method
- Method
label Method
variant Method
( O
Potsdam O
- O
3 O
) O
formed O
by O
merging O
each O
of O
the O
3 O
pairs O
. O
All O
segmentation O
training O
and O
testing O
sets O
will O
be O
released O
with O
our O
code O
. O
paragraph O
: O
Architecture O
. O
All O
networks O
are O
randomly O
initialised O
and O
consist O
of O
a O
base O
CNN Method
( O
see O
sup O
. O
mat O
. O
) O
followed O
by O
head O
( O
s O
) O
, O
which O
are O
convolution Method
layers Method
. O
Similar O
to O
s O
: O
exp_img_clus Method
, O
overclustering Method
uses O
3 O
- O
5 O
times O
higher O
than O
. O
Since O
segmentation Task
is O
much O
more O
expensive O
than O
image Task
clustering Task
( O
e.g. O
a O
single O
Potsdam O
image O
contains O
40 O
, O
000 O
predictions O
) O
, O
all O
segmentation Task
experiments O
were O
run O
with O
and O
( O
sec O
. O
[ O
reference O
] O
) O
. O
paragraph O
: O
Training O
. O
The O
convolutional Method
implementation Method
of O
IIC Method
( O
e O
: O
info_seg Method
) O
was O
used O
with O
. O
For O
Potsdam O
- O
3 O
and O
COCO O
- O
Stuff O
- O
3 O
, O
the O
optional Metric
entropy Metric
coefficient Metric
( O
s O
: O
equalization O
and O
sup O
. O
mat O
. O
) O
was O
used O
and O
set O
to O
1.5 O
. O
Using O
the O
coefficient O
made O
slight O
improvements O
of O
1.2% O
- O
3.2 O
% O
on O
performance O
. O
These O
two O
datasets O
are O
balanced O
in O
nature O
with O
very O
large O
sample O
volume O
( O
e.g. O
predictions O
per O
batch O
for O
Potsdam O
- O
3 O
) O
resulting O
in O
stable O
and O
balanced O
batches O
, O
justifying O
prioritisation O
of O
equalisation Method
. O
Other O
training O
details O
are O
the O
same O
as O
s O
: O
exp_img_clus O
. O
paragraph O
: O
Evaluation O
. O
Evaluation Metric
uses O
accuracy Metric
as O
in O
s O
: O
exp_img_clus O
, O
computed O
per O
- O
pixel O
. O
For O
the O
baselines O
, O
the O
original O
authors O
’ O
code O
was O
adapted O
from O
image Method
clustering Method
where O
available O
, O
and O
the O
architectures O
are O
shared O
with O
IIC Method
for O
fairness O
. O
For O
baselines O
that O
required O
application O
of O
k Method
- Method
means Method
to O
produce O
per Task
- Task
pixel Task
predictions Task
( O
t O
: O
iid_seg O
) O
, O
k Method
- Method
means Method
was O
trained O
with O
randomly O
sampled O
pixel O
features O
from O
the O
training O
set O
( O
10 O
M O
for O
Potsdam O
, O
Potsdam O
- O
3 O
; O
50 O
M O
for O
COCO O
- O
Stuff O
, O
COCO O
- O
Stuff O
- O
3 O
) O
and O
tested O
on O
the O
full O
test O
set O
to O
obtain O
accuracy Metric
. O
paragraph O
: O
Analysis O
. O
Without O
labels O
or O
heuristics O
to O
learn O
from O
, O
and O
given O
just O
the O
cluster O
cardinality O
( O
3 O
) O
, O
IIC Method
automatically O
partitions O
COCO O
- O
Stuff O
- O
3 O
into O
clusters O
that O
are O
recognisable O
as O
sky O
, O
vegetation O
and O
ground O
, O
and O
learns O
to O
classify O
vegetation O
, O
roads O
and O
buildings O
for O
Potsdam O
- O
3 O
( O
f O
: O
images_img_seg O
) O
. O
The O
segmentations Method
are O
notably O
intricate O
, O
capturing O
fine O
detail O
, O
but O
are O
at O
the O
same O
time O
locally O
consistent O
and O
coherent O
across O
all O
images O
. O
Since O
spatial O
smoothness O
is O
built O
into O
the O
loss O
( O
s O
: O
image_segmentation O
) O
, O
all O
our O
results O
are O
able O
to O
use O
raw O
network O
outputs O
without O
post O
- O
processing O
( O
avoiding O
e.g. O
CRF Method
smoothing Method
) O
. O
Quantitatively O
, O
we O
outperform O
all O
baselines O
( O
t O
: O
iid_seg Method
) O
, O
notably O
by O
in O
the O
case O
of O
COCO O
- O
Stuff O
- O
3 O
. O
The O
efficient O
convolutional Method
formulation Method
of O
the O
loss O
( O
e O
: O
info_seg O
) O
allows O
us O
to O
optimise O
over O
all O
pixels O
in O
all O
batch O
images O
in O
parallel O
, O
converging O
in O
fewer O
epochs O
( O
passes O
of O
the O
dataset O
) O
without O
paying O
the O
price O
of O
reduced O
computational Metric
speed Metric
for O
dense Method
sampling Method
. O
This O
is O
in O
contrast O
to O
our O
baselines O
which O
, O
being O
not O
natively O
adapted O
for O
segmentation Task
, O
required O
sampling O
a O
subset O
of O
pixels O
within O
each O
batch O
, O
resulting O
in O
increased O
loss Metric
volatility Metric
and O
training Metric
speeds Metric
that O
were O
up O
to O
3.3 O
slower O
than O
IIC Method
. O
section O
: O
Conclusions O
We O
have O
shown O
that O
it O
is O
possible O
to O
train O
neural Method
networks Method
into O
semantic Method
clusterers Method
without O
using O
labels O
or O
heuristics O
. O
The O
novel O
objective O
presented O
relies O
on O
statistical Method
learning Method
, O
by O
optimising O
mutual O
information O
between O
related O
pairs O
- O
a O
relationship O
that O
can O
be O
generated O
by O
random Method
transforms Method
- O
and O
naturally O
avoids O
degenerate O
solutions O
. O
The O
resulting O
models O
classify O
and O
segment O
images O
with O
state O
- O
of O
- O
the O
- O
art O
levels O
of O
semantic O
accuracy Metric
. O
Being O
not O
specific O
to O
vision Task
, O
the O
method O
opens O
up O
many O
interesting O
research O
directions O
, O
including O
optimising O
information O
in O
datastreams O
over O
time O
. O
bibliography O
: O
References O
