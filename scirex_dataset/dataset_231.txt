document O
: O
Cutting O
the O
Error O
by O
Half O
: O
Investigation O
of O
Very O
Deep Method
CNN Method
and O
Advanced O
Training Method
Strategies Method
for O
Document Task
Image Task
Classification Task
We O
present O
an O
exhaustive O
investigation O
of O
recent O
Deep Method
Learning Method
architectures Method
, O
algorithms O
, O
and O
strategies O
for O
the O
task O
of O
document Task
image Task
classification Task
to O
finally O
reduce O
the O
error Metric
by O
more O
than O
half O
. O
Existing O
approaches O
, O
such O
as O
the O
DeepDocClassifier Method
, O
apply O
standard O
Convolutional Method
Network Method
architectures Method
with O
transfer Method
learning Method
from O
the O
object Task
recognition Task
domain Task
. O
The O
contribution O
of O
the O
paper O
is O
threefold O
: O
First O
, O
it O
investigates O
recently O
introduced O
very O
deep Method
neural Method
network Method
architectures Method
( O
GoogLeNet Method
, O
VGG Method
, O
ResNet Method
) O
using O
transfer Method
learning Method
( O
from O
real O
images O
) O
. O
Second O
, O
it O
proposes O
transfer Method
learning Method
from O
a O
huge O
set O
of O
document O
images O
, O
i.e. O
documents O
. O
Third O
, O
it O
analyzes O
the O
impact O
of O
the O
amount O
of O
training O
data O
( O
document O
images O
) O
and O
other O
parameters O
to O
the O
classification Metric
abilities Metric
. O
We O
use O
two O
datasets O
, O
the O
Tobacco Material
- Material
3482 Material
and O
the O
large Material
- Material
scale Material
RVL Material
- Material
CDIP Material
dataset Material
. O
We O
achieve O
an O
accuracy Metric
of O
for O
the O
Tobacco Material
- Material
3482 Material
dataset Material
while O
earlier O
approaches O
reach O
only O
. O
Thus O
, O
a O
relative Metric
error Metric
reduction Metric
of O
more O
than O
is O
achieved O
. O
For O
the O
large O
dataset O
RVL Material
- Material
CDIP Material
, O
an O
accuracy Metric
of O
is O
achieved O
, O
corresponding O
to O
a O
relative Metric
error Metric
reduction Metric
of O
. O
Document Task
Image Task
Classification Task
, O
Deep Method
CNN Method
, O
Convolutional Method
Neural Method
Network Method
, O
Transfer Method
Learning Method
section O
: O
Introduction O
An O
important O
step O
of O
the O
is O
the O
classification Task
of Task
the Task
documents Task
. O
An O
early O
classification Task
of Task
documents Task
helps O
to O
process O
the O
subsequent O
processes O
in O
such O
as O
information Task
extraction Task
, O
text Task
recognition Task
etc O
. O
Due O
to O
its O
fundamental O
importance O
, O
this O
area O
has O
been O
explored O
extensively O
. O
Earlier O
methods O
that O
have O
been O
dealing O
with O
document Task
classification Task
focused O
mainly O
on O
either O
exploiting O
the O
structural O
similarity O
constraints O
or O
extracting O
features O
from O
the O
documents O
that O
may O
be O
able O
to O
help O
for O
document Task
classification Task
. O
Some O
of O
the O
methods O
combine O
both O
of O
the O
features O
. O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
.12 O
Deep Method
Learning Method
has O
been O
used O
for O
many O
document Task
analysis Task
tasks Task
such O
as O
binarization Task
, O
layout Task
analysis Task
, O
etc O
. O
Recently O
, O
deep Method
learning Method
methods Method
have O
also O
been O
exploited O
for O
document Task
image Task
classification Task
. O
Deep Method
learning Method
methods Method
do O
not O
require O
any O
manual Method
feature Method
extraction Method
. O
However O
, O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
methods O
do O
transfer Method
learning Method
. O
Fig O
. O
[ O
reference O
] O
shows O
the O
sample O
images O
both O
real O
and O
document O
images O
from O
the O
ImageNet Material
and O
Tobacco Material
- Material
3482 Material
datasets Material
respectively O
. O
While O
the O
images O
are O
visually O
very O
different O
, O
the O
visual O
queues O
are O
generic O
and O
thus O
, O
transfer Method
learning Method
helps O
to O
boost O
the O
performance O
of O
the O
document Task
image Task
classification Task
. O
The O
networks O
that O
are O
not O
using O
transfer Method
learning Method
( O
i.e. O
, O
they O
are O
randomly O
initialized O
) O
are O
under O
- O
performing O
. O
The O
performance O
evaluation O
for O
the O
deep Method
neural Method
networks Method
was O
only O
performed O
using O
Tobacco Material
- Material
3482 Material
images Material
. O
Another O
dataset O
introduced O
by O
Harley O
et O
al O
. O
consist O
of O
images O
that O
are O
divided O
into O
classes O
. O
Representative O
images O
from O
each O
of O
the O
classes O
are O
shown O
in O
Fig O
. O
[ O
reference O
] O
. O
Although O
now O
we O
have O
a O
large O
dataset O
available O
for O
training O
document O
images O
, O
there O
is O
no O
study O
that O
shows O
the O
performance O
of O
very O
deep Method
networks Method
for O
large O
datasets O
of O
document O
images O
. O
Furthermore O
, O
the O
potential O
of O
pretraining Task
using O
document O
images O
only O
is O
not O
explored O
either O
. O
Therefore O
, O
in O
this O
work O
, O
we O
evaluate O
deep Method
neural Method
networks Method
both O
on O
the O
small O
and O
big O
dataset O
. O
An O
exhaustive O
evaluation O
of O
the O
deep Method
neural Method
networks Method
is O
performed O
to O
show O
the O
impact O
of O
the O
amount O
of O
data O
for O
training O
in O
combination O
with O
very O
deep O
. O
Furthermore O
, O
an O
evaluation O
is O
performed O
for O
transfer Method
learning Method
when O
the O
weights O
are O
adapted O
both O
from O
natural O
images O
and O
document O
images O
. O
The O
proposed O
approach O
shows O
a O
significant O
improvement O
over O
the O
current O
state O
- O
of O
- O
the O
- O
art O
by O
reducing O
the O
error Metric
by O
more O
than O
half O
. O
section O
: O
Related O
Work O
Over O
the O
years O
, O
different O
methods O
have O
been O
proposed O
for O
document Task
image Task
classification Task
. O
The O
overall O
classification Method
methods Method
can O
be O
divided O
into O
three O
distinct O
categories O
. O
The O
first O
category O
exploits O
structure O
and O
layout O
similarities O
, O
while O
the O
second O
focuses O
on O
developing O
local O
and O
global O
features O
that O
could O
be O
used O
for O
document Task
classification Task
. O
The O
third O
category O
is O
based O
on O
deep Method
s Method
that O
extract O
the O
features O
automatically O
for O
document Task
classification Task
. O
This O
section O
provides O
a O
summary O
of O
the O
important O
related O
work O
regarding O
the O
above O
mentioned O
three O
categories O
. O
0.22 O
AlexNet Method
0.21 O
VGG Method
- Method
16 Method
0.36 O
GoogLeNet O
0.19 O
ResNet O
- O
50 O
Dengel O
and O
Dubiel O
used O
layout O
structure O
printed O
documents O
. O
They O
used O
top Method
- Method
down Method
induction Method
in O
decision Method
trees Method
to O
convert O
printed O
documents O
into O
a O
complementary O
logical O
structure O
. O
Bagdanov Method
and O
Worring O
classify O
machine O
- O
printed O
documents O
by O
using O
the O
Attributed Method
Relational Method
Graphs Method
( O
ARGs Method
) O
. O
Byun O
and O
Lee O
used O
parts O
of O
the O
documents O
for O
the O
recognition Task
. O
They O
reasoned O
that O
processing O
complete O
documents O
is O
time O
- O
consuming O
. O
The O
document Task
classification Task
was O
performed O
on O
parts O
of O
the O
documents O
using O
DP Method
algorithm Method
. O
Their O
approach O
was O
fast O
but O
the O
applicability O
is O
limited O
to O
forms O
. O
Shin O
and O
Doermann O
proposed O
an O
approach O
that O
used O
layout O
structural O
similarity O
for O
full Task
or Task
partial Task
image Task
matching Task
for O
retrieval Task
. O
Kevyn O
and O
Nickolov O
used O
both O
the O
layout O
and O
the O
text O
features O
for O
matching O
the O
documents O
for O
retrieval Task
. O
Jayant O
et O
al O
. O
propose O
a O
method O
that O
relies O
on O
the O
patch O
code O
words O
derived O
from O
the O
document O
images O
. O
The O
code O
book O
is O
learned O
independently O
of O
the O
class O
labels O
of O
the O
documents O
. O
In O
the O
first O
step O
, O
the O
images O
are O
recursively O
partitioned O
both O
in O
horizontal O
and O
vertical O
direction O
for O
modeling O
spatial O
relationships O
. O
Subsequently O
, O
a O
histogram O
for O
each O
partition O
is O
calculated O
that O
is O
used O
for O
the O
classification Task
. O
Following O
the O
same O
idea O
of O
developing O
the O
code Method
book Method
, O
another O
work O
presented O
by O
Jayant O
et O
al O
. O
build O
a O
codebook Method
of Method
SURF Method
descriptors Method
extracted O
from O
training O
images O
. O
Then O
, O
histograms O
of O
codewords O
are O
created O
similar O
to O
. O
A O
Random Method
Forest Method
classifier Method
is O
used O
for O
classification Task
. O
The O
applicability O
of O
the O
approach O
is O
shown O
in O
the O
presence O
of O
limited O
data O
. O
Chen O
et O
al O
. O
propose O
a O
method O
based O
on O
low O
- O
level O
image O
features O
to O
classify O
documents O
. O
The O
approach O
is O
limited O
to O
structured O
documents O
. O
An O
important O
point O
is O
that O
one O
could O
obtain O
the O
registration O
of O
two O
images O
by O
matching O
the O
feature O
points O
. O
Joutel O
et O
al O
. O
proposed O
a O
method O
that O
used O
curvelet Method
transformation Method
for O
indexing Task
and O
querying O
the O
documents O
at O
different O
image O
scales O
. O
Their O
method O
is O
designed O
particularly O
for O
large O
databases O
of O
handwritten O
manuscripts O
. O
Kochi O
and O
Saitoh O
used O
textual O
descriptions O
of O
document O
images O
for O
information Task
extraction Task
from O
documents O
. O
The O
method O
is O
limited O
to O
semi O
- O
structured O
documents O
and O
assumes O
a O
pre O
- O
defined O
knowledge O
is O
available O
for O
the O
document O
classes O
. O
Reddy O
and O
Govindaraju O
used O
binary O
images O
for O
the O
classification Task
of Task
the Task
documents Task
. O
They O
use O
pixel O
information O
and O
calculate O
pixel O
densities O
. O
They O
used O
k Method
- Method
means Method
clustering Method
supported O
by O
adaptive Method
boosting Method
. O
The O
method O
is O
evaluated O
on O
the O
benchmark O
NIST O
scanned O
special O
tax O
form O
databases O
and O
. O
The O
pioneering O
work O
that O
performed O
document Task
classification Task
using O
s Method
used O
a O
rather O
shallow Method
network Method
for O
classification Task
. O
Nevertheless O
, O
the O
proposed O
approach O
outperformed O
structural Method
similarity Method
based Method
methods Method
and O
shows O
the O
potential O
of O
automatic Method
feature Method
learning Method
for O
document Task
classification Task
using O
s. O
The O
reason O
may O
be O
that O
deep Method
networks Method
require O
a O
lot O
of O
data O
for O
training O
and O
at O
that O
time O
the O
standard O
challenging O
dataset O
consisted O
of O
only O
images O
. O
Afzal O
et O
. O
al O
. O
and O
Harley O
et O
. O
al O
. O
provided O
a O
breakthrough O
when O
they O
showed O
that O
it O
is O
possible O
to O
use O
transfer Method
learning Method
and O
the O
features O
that O
are O
learned O
from O
general O
( O
daily O
life O
) O
images O
can O
be O
used O
for O
the O
classification Task
of Task
document Task
images Task
. O
They O
achieved O
a O
significant O
improvement O
over O
based O
methods O
that O
were O
the O
state O
- O
of O
- O
the O
- O
art O
at O
that O
time O
. O
Another O
notable O
contribution O
by O
Harley O
et O
. O
al O
. O
was O
that O
they O
introduced O
a O
dataset O
consisting O
of O
documents O
divided O
into O
classes O
. O
This O
allowed O
for O
the O
evaluation O
of O
deep Method
neural Method
networks Method
using O
a O
significant O
amount O
of O
data O
. O
The O
state O
- O
of O
- O
the O
- O
art O
in O
deep Task
s Task
has O
advanced O
significantly O
in O
recent O
years O
and O
there O
has O
been O
no O
comprehensive O
study O
regarding O
the O
impact O
of O
deep Method
architectures Method
for O
document Task
classification Task
. O
Moreover O
, O
there O
is O
no O
study O
that O
explores O
transfer Method
learning Method
from O
document O
images O
and O
also O
there O
is O
no O
report O
of O
the O
impact O
of O
the O
amount O
of O
training O
images O
. O
The O
presented O
work O
takes O
into O
account O
these O
issues O
and O
performs O
a O
comprehensive O
set O
of O
experiments O
to O
fill O
the O
gaps O
that O
exist O
. O
Eventually O
, O
this O
study O
leads O
to O
an O
approach O
that O
can O
reduce O
the O
error Metric
by O
more O
than O
half O
and O
therefore O
provides O
another O
leap O
forward O
in O
the O
domain O
of O
document Task
image Task
classification Task
. O
section O
: O
Deep Method
Convolutional Method
Neural Method
Networks Method
This O
section O
briefly O
presents O
the O
deep Method
architectures Method
used O
in O
this O
work O
. O
Furthermore O
, O
the O
image Task
preprocessing Task
and O
training O
details O
are O
described O
. O
subsection O
: O
Network Method
Architectures Method
The O
deep Method
architectures Method
used O
in O
this O
paper O
are O
well O
known O
in O
the O
domain O
of O
object Task
recognition Task
but O
are O
not O
used O
frequently O
for O
document Task
image Task
classification Task
. O
The O
networks O
are O
of O
very O
different O
nature O
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
. O
subsubsection O
: O
AlexNet Method
AlexNet Method
is O
the O
eight O
- O
layer O
that O
won O
the O
ImageNet Material
Large O
Scale O
Visual O
Recognition O
Challenge O
( O
ILSVRC Task
) O
in O
2012 O
by O
a O
large O
margin O
. O
It O
employs O
five O
convolutional Method
layers O
with O
optional Method
pooling Method
and O
local Method
response Method
normalization Method
. O
These O
are O
then O
followed O
by O
three O
fully Method
- Method
connected Method
layers Method
and O
a O
softmax Method
classifier Method
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
. O
subsubsection O
: O
VGG Method
- Method
16 Method
VGG O
- O
16 O
, O
as O
the O
name O
suggests O
is O
a O
16 O
- O
layer O
. O
Unlike O
AlexNet Method
, O
it O
uses O
only O
convolutional Method
filters O
of O
size O
. O
Just O
like O
AlexNet Method
, O
it O
has O
a O
straightforward O
architecture O
, O
but O
with O
convolutional Method
layers O
and O
fully O
connected O
layers O
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
it O
is O
quite O
a O
bit O
deeper O
and O
has O
a O
repetitive O
pattern O
of O
layers O
. O
This O
architecture O
has O
won O
the O
localization Task
category Task
of O
the O
ILSVRC Task
2014 Task
. O
subsubsection O
: O
GoogLeNet O
GoogLeNet Method
, O
just O
like O
VGG Method
- Method
16 Method
, O
won O
a O
category O
of O
the O
ILSVRC Task
2014 Task
, O
namely O
the O
classification Task
category Task
. O
The O
architecture O
of O
this O
network O
, O
however O
, O
is O
a O
bit O
more O
sophisticated O
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
. O
Unlike O
AlexNet Method
and O
VGG Method
- Method
16 Method
, O
it O
is O
not O
just O
a O
stack Method
of Method
Convolution Method
layers Method
and O
Pooling Method
layers Method
, O
but O
rather O
a O
stack Method
of Method
building Method
blocks Method
, O
which O
themselves O
consist O
of O
Convolution Method
and Method
Pooling Method
layers Method
. O
It O
is O
therefore O
a O
Network Method
- Method
in Method
- Method
Network Method
approach Method
. O
Due O
to O
its O
high O
depth O
, O
the O
network O
employs O
three O
softmax Method
classifiers Method
during O
training O
, O
to O
enable O
efficient O
backpropagation O
of O
the O
error O
. O
At O
test O
time O
, O
the O
two O
auxiliary Method
classifiers Method
are O
discarded O
. O
subsubsection O
: O
Resnet O
- O
50 O
ResNets Method
are O
a O
family O
of O
very Method
deep Method
architectures Method
which O
make O
use O
of O
residual O
connections O
to O
overcome O
the O
challenge O
of O
efficient O
error Task
backpropagation Task
. O
ResNet Method
- Method
50 Method
is O
a O
variant O
of O
the O
network O
with O
layers O
, O
which O
, O
as O
in O
GoogLeNet Method
, O
are O
grouped O
in O
building O
blocks O
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
. O
An O
even O
deeper O
variant O
with O
layers O
won O
the O
ILSVRC Task
classification Task
task Task
in O
2015 O
. O
Interestingly O
, O
despite O
its O
increased O
depth O
, O
the O
network O
has O
fewer O
parameters O
to O
fit O
than O
VGG Method
- Method
16 Method
. O
subsection O
: O
Preprocessing O
As O
the O
networks O
used O
in O
this O
paper O
require O
images O
of O
a O
fixed O
size O
as O
input O
, O
we O
first O
downscale O
all O
images O
to O
the O
expected O
input O
size O
of O
the O
networks O
. O
For O
AlexNet Method
, O
the O
images O
are O
resized O
to O
pixels O
, O
for O
the O
other O
networks O
the O
images O
are O
resized O
to O
pixels O
. O
Typically O
, O
when O
training Task
s Task
, O
the O
training O
data O
is O
augmented O
by O
resizing O
the O
images O
to O
a O
larger O
size O
, O
e.g. O
pixels O
and O
then O
cropping O
random O
patches O
of O
these O
images O
in O
the O
size O
of O
the O
network O
input O
. O
This O
approach O
has O
shown O
to O
be O
effective O
for O
real Task
- Task
world Task
image Task
classification Task
. O
In O
real O
- O
world O
images O
, O
the O
objects O
are O
typically O
close O
to O
the O
center O
of O
the O
image O
and O
therefore O
always O
contained O
in O
the O
random O
crops O
. O
However O
, O
the O
most O
discriminative O
parts O
of O
document O
images O
are O
not O
always O
close O
to O
the O
center O
of O
the O
image O
but O
reside O
in O
the O
outer O
regions O
, O
e.g. O
the O
head O
of O
a O
letter O
. O
Therefore O
, O
we O
do O
not O
enlarge O
our O
training O
dataset O
in O
this O
way O
but O
train O
solely O
with O
images O
containing O
the O
entire O
document O
. O
After O
resizing O
the O
images O
, O
we O
compute O
the O
mean O
pixel O
values O
of O
the O
training O
images O
and O
subtract O
them O
from O
all O
images O
to O
center O
the O
training O
data O
. O
As O
a O
last O
preprocessing O
step O
, O
we O
convert O
the O
grayscale O
images O
to O
RGB O
images O
by O
simply O
copying O
the O
pixel O
values O
of O
the O
single O
- O
channel O
images O
to O
three O
channels O
. O
subsection O
: O
Training O
Details O
We O
train O
all O
networks O
using O
stochastic Method
gradient Method
descent Method
with O
a O
momentum O
of O
and O
a O
learning Metric
rate Metric
that O
is O
updated O
every O
iteration O
to O
The O
initial O
learning Metric
rate Metric
is O
set O
to O
a O
value O
between O
and O
depending O
on O
the O
network Method
architecture Method
, O
the O
training O
dataset O
and O
the O
weight Method
initialization Method
. O
The O
number O
of O
training O
epochs O
depends O
on O
the O
task O
and O
ranges O
between O
and O
epochs O
. O
section O
: O
Experiments O
and O
Results O
subsection O
: O
Datasets O
To O
evaluate O
the O
performance O
of O
the O
deep Method
neural Method
networks Method
presented O
in O
section O
[ O
reference O
] O
, O
two O
datasets O
are O
used O
. O
First O
, O
we O
train O
a O
variety O
of O
networks O
on O
the O
Ryerson Task
Vision Task
Lab Task
Complex Task
Document Task
Information Task
Processing Task
( O
RVL Material
- Material
CDIP Material
) O
dataset O
. O
This O
dataset O
consists O
of O
labeled O
document O
images O
from O
16 O
classes O
. O
The O
dataset O
is O
already O
split O
into O
a O
training O
dataset O
which O
contains O
images O
and O
a O
validation O
and O
a O
test O
dataset O
which O
each O
contain O
images O
. O
Secondly O
, O
we O
use O
the O
Tobacco Material
- Material
3482 Material
dataset Material
to O
evaluate O
the O
performance O
of O
the O
deep Method
s Method
and O
to O
investigate O
to O
which O
extent O
transfer Method
learning Method
from O
the O
first O
dataset O
is O
applicable O
. O
The O
Tobacco Material
- Material
3482 Material
dataset Material
contains O
images O
from O
ten O
document O
classes O
. O
Both O
datasets O
are O
quite O
similar O
and O
there O
even O
exists O
some O
overlap O
. O
Therefore O
, O
at O
the O
transfer Method
learning Method
experiments O
, O
we O
pretrain O
the O
networks O
not O
on O
the O
full O
RVL Material
- Material
CDIP Material
dataset O
, O
but O
only O
on O
the O
images O
that O
are O
not O
contained O
in O
the O
Tobacco Material
- Material
3482 Material
dataset Material
. O
Thus O
, O
the O
networks O
are O
pretrained O
on O
only O
training O
images O
. O
subsection O
: O
Evaluation O
For O
the O
RVL Material
- Material
CDIP Material
dataset O
, O
we O
just O
train O
the O
networks O
and O
report O
the O
top O
- O
1 O
accuracy Metric
achieved O
on O
the O
test O
set O
. O
Since O
the O
Tobacco Material
- Material
3482 Material
dataset Material
is O
so O
small O
, O
we O
use O
a O
slightly O
more O
sophisticated O
evaluation O
technique O
to O
get O
an O
expected O
accuracy Metric
and O
to O
avoid O
unrepresentative O
results O
due O
to O
random O
initialization O
or O
a O
specific O
dataset O
split O
. O
To O
come O
up O
with O
a O
robust O
estimate O
of O
how O
well O
the O
networks O
perform O
, O
we O
split O
the O
dataset O
such O
, O
that O
to O
images O
per O
class O
are O
used O
for O
training O
while O
the O
rest O
are O
for O
testing O
. O
The O
training O
dataset O
is O
again O
split O
with O
an O
ratio O
, O
so O
that O
of O
the O
training O
data O
are O
used O
for O
validation Task
. O
For O
each O
split O
size O
, O
we O
randomly O
create O
ten O
dataset O
partitions O
and O
report O
the O
median O
accuracy Metric
achieved O
by O
the O
networks O
. O
This O
is O
similar O
to O
the O
evaluation O
scheme O
that O
was O
also O
used O
by O
Kang O
et O
al O
. O
and O
Kumar O
et O
al O
. O
and O
allows O
for O
a O
fair O
comparison O
with O
their O
approaches O
. O
subsection O
: O
Results O
on O
Tobacco Material
- Material
3482 Material
We O
have O
trained O
the O
four O
deep Method
s Method
described O
in O
Section O
[ O
reference O
] O
on O
the O
two O
datasets O
with O
different O
weight O
initializations O
to O
investigate O
the O
benefits O
of O
transfer Method
learning Method
. O
As O
shown O
in O
Table O
[ O
reference O
] O
and O
Fig O
. O
[ O
reference O
] O
which O
correspond O
to O
the O
achieved O
performance O
on O
the O
Tobacco Material
- Material
3482 Material
dataset Material
, O
transfer Method
learning Method
does O
improve O
the O
classification Task
performance O
significantly O
. O
When O
the O
networks O
are O
pretrained O
on O
a O
similar O
dataset O
, O
the O
accuracy Metric
achieved O
on O
the O
final O
dataset O
is O
higher O
than O
and O
already O
with O
as O
little O
training O
data O
as O
samples O
per O
class O
, O
we O
could O
outperform O
the O
current O
state O
- O
of O
- O
the O
- O
art O
which O
achieves O
only O
. O
subsection O
: O
Analysis O
We O
also O
compare O
the O
networks O
with O
ImageNet Material
initialization O
against O
randomly Method
initialized Method
networks Method
and O
find O
that O
even O
though O
the O
images O
are O
substantially O
different O
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
, O
it O
helps O
to O
use O
pretrained Method
models Method
. O
Fortunately O
, O
there O
are O
models O
which O
are O
pretrained O
on O
ImageNet Material
available O
online O
for O
many O
architectures O
, O
including O
the O
four O
networks O
used O
in O
this O
work O
. O
So O
, O
in O
case O
there O
is O
no O
large O
document O
dataset O
available O
for O
pretraining Task
, O
one O
can O
and O
should O
always O
resort O
to O
using O
an O
ImageNet Material
pretrained O
model O
for O
finetuning Task
. O
Depending O
on O
the O
amount O
of O
available O
training O
data O
, O
AlexNet Method
and O
VGG Method
- Method
16 Method
are O
the O
best O
choices O
when O
finetuning O
the O
networks O
from O
models O
that O
were O
pretrained O
on O
ImageNet Material
( O
cf O
. O
Fig O
. O
[ O
reference O
] O
) O
. O
When O
pretrained O
on O
the O
RVL Material
- Material
CDIP Material
dataset O
, O
GoogLeNet Material
is O
significantly O
worse O
than O
the O
other O
networks O
, O
especially O
for O
a O
small O
amount O
of O
training O
data O
. O
subsection O
: O
Results O
on O
RVL Material
- Material
CDIP Material
On O
the O
large Material
- Material
scale Material
RVL Material
- Material
CDIP Material
dataset Material
, O
all O
networks O
achieve O
very O
good O
results O
( O
cf O
. O
Table O
[ O
reference O
] O
) O
with O
the O
VGG Method
- Method
16 Method
performing O
best O
at O
an O
accuracy Metric
of O
. O
The O
current O
state O
- O
of O
- O
the O
- O
art O
on O
this O
dataset O
only O
achieves O
an O
accuracy Metric
of O
, O
thus O
we O
could O
decrease O
the O
relative Metric
error Metric
by O
more O
than O
by O
simply O
using O
a O
different O
network Method
architecture Method
. O
Note O
, O
that O
even O
though O
the O
training O
dataset O
is O
quite O
large O
, O
all O
of O
the O
networks O
still O
benefit O
from O
Imagenet Method
pretraining Method
. O
On O
average O
, O
VGG Method
- Method
16 Method
performs O
very O
well O
on O
all O
experiments O
performed O
in O
this O
work O
. O
As O
can O
be O
seen O
in O
Fig O
. O
[ O
reference O
] O
which O
shows O
the O
confusion Metric
matrix Metric
of O
a O
trained O
VGG Method
- Method
16 Method
network Method
, O
even O
the O
classes O
that O
were O
pointed O
out O
to O
be O
hard O
by O
Afzal O
et O
al O
. O
, O
get O
significant O
performance O
boosts O
. O
section O
: O
Conclusion O
and O
Future O
Work O
The O
outcome O
of O
the O
study O
brings O
insights O
both O
for O
the O
deep Method
neural Method
network Method
architectures Method
and O
the O
amount O
of O
required O
training O
data O
. O
The O
proposed O
approach O
of O
training O
on O
document O
images O
and O
then O
finetuning O
for O
a O
document O
based O
dataset O
improved O
the O
error Metric
by O
. O
We O
show O
that O
the O
random Method
initialization Method
performs O
worst O
and O
initialization Method
based O
on O
document O
images O
performs O
best O
. O
Furthermore O
, O
on O
the O
large Material
- Material
scale Material
RVL Material
- Material
CDIP Material
dataset Material
, O
VGG Method
- Method
16 Method
outperforms O
the O
other O
networks O
. O
Finally O
, O
a O
relative Metric
error Metric
reduction Metric
of O
compared O
to O
the O
state O
- O
of O
- O
the O
- O
art O
is O
achieved O
. O
Future O
work O
may O
evaluate O
recurrent Method
neural Method
networks Method
or O
a O
combination O
of O
convolutional Method
and O
recurrent Method
neural Method
networks Method
to O
improve O
the O
performance O
further O
. O
bibliography O
: O
References O
