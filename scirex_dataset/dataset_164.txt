document	O
:	O
Gradient	Method
Harmonized	Method
Single	Method
-	Method
stage	Method
Detector	Method
Despite	O
the	O
great	O
success	O
of	O
two	Method
-	Method
stage	Method
detectors	Method
,	O
single	Method
-	Method
stage	Method
detector	Method
is	O
still	O
a	O
more	O
elegant	O
and	O
efficient	O
way	O
,	O
yet	O
suffers	O
from	O
the	O
two	O
well	O
-	O
known	O
disharmonies	O
during	O
training	O
,	O
i.e.	O
the	O
huge	O
difference	O
in	O
quantity	O
between	O
positive	O
and	O
negative	O
examples	O
as	O
well	O
as	O
between	O
easy	O
and	O
hard	O
examples	O
.	O
In	O
this	O
work	O
,	O
we	O
first	O
point	O
out	O
that	O
the	O
essential	O
effect	O
of	O
the	O
two	O
disharmonies	O
can	O
be	O
summarized	O
in	O
term	O
of	O
the	O
gradient	O
.	O
Further	O
,	O
we	O
propose	O
a	O
novel	O
gradient	Method
harmonizing	Method
mechanism	Method
(	O
GHM	Method
)	O
to	O
be	O
a	O
hedging	O
for	O
the	O
disharmonies	O
.	O
The	O
philosophy	O
behind	O
GHM	Method
can	O
be	O
easily	O
embedded	O
into	O
both	O
classification	Metric
loss	Metric
function	Metric
like	O
cross	Method
-	Method
entropy	Method
(	Method
CE	Method
)	O
and	O
regression	Method
loss	Method
function	Method
like	O
smooth	Method
-	Method
(	Method
)	Method
loss	Method
.	O
To	O
this	O
end	O
,	O
two	O
novel	O
loss	Method
functions	Method
called	O
GHM	Method
-	Method
C	Method
and	O
GHM	Method
-	Method
R	Method
are	O
designed	O
to	O
balancing	O
the	O
gradient	O
flow	O
for	O
anchor	Task
classification	Task
and	O
bounding	Task
box	Task
refinement	Task
,	O
respectively	O
.	O
Ablation	O
study	O
on	O
MS	Material
COCO	Material
demonstrates	O
that	O
without	O
laborious	O
hyper	Method
-	Method
parameter	Method
tuning	Method
,	O
both	O
GHM	Method
-	Method
C	Method
and	O
GHM	Method
-	Method
R	Method
can	O
bring	O
substantial	O
improvement	O
for	O
single	Method
-	Method
stage	Method
detector	Method
.	O
Without	O
any	O
whistles	O
and	O
bells	O
,	O
the	O
proposed	O
model	O
achieves	O
41.6	O
mAP	Metric
on	O
COCO	Material
test	Material
-	Material
dev	Material
set	Material
which	O
surpass	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
,	O
Focal	Method
Loss	Method
(	O
FL	Method
)	O
+	O
,	O
by	O
0.8	O
.	O
The	O
code	O
is	O
released	O
to	O
facilitate	O
future	O
research	O
.	O
section	O
:	O
Introduction	O
One	Method
-	Method
stage	Method
approach	Method
is	O
the	O
most	O
efficient	O
and	O
elegant	O
framework	O
for	O
object	Task
detection	Task
.	O
But	O
for	O
a	O
long	O
time	O
,	O
the	O
performance	O
of	O
one	Method
-	Method
stage	Method
detectors	Method
has	O
a	O
large	O
gap	O
from	O
that	O
of	O
two	Method
-	Method
stage	Method
detectors	Method
.	O
The	O
most	O
challenging	O
problem	O
for	O
the	O
training	O
of	O
one	Method
-	Method
stage	Method
detector	Method
is	O
the	O
serious	O
imbalance	O
between	O
easy	O
and	O
hard	O
examples	O
as	O
well	O
as	O
that	O
between	O
positive	O
and	O
negative	O
examples	O
.	O
The	O
huge	O
number	O
of	O
easy	O
and	O
background	O
examples	O
tend	O
to	O
overwhelm	O
the	O
training	O
.	O
But	O
these	O
problems	O
are	O
not	O
existed	O
for	O
two	Method
-	Method
stage	Method
detectors	Method
,	O
owing	O
to	O
the	O
proposal	Method
-	Method
driven	Method
mechanism	Method
.	O
To	O
handle	O
the	O
former	O
imbalance	Task
problem	Task
,	O
example	Method
mining	Method
based	Method
methods	Method
such	O
as	O
OHEM	Method
are	O
in	O
common	O
use	O
,	O
but	O
they	O
directly	O
abandon	O
most	O
examples	O
and	O
the	O
training	O
is	O
inefficient	O
.	O
For	O
the	O
latter	O
imbalance	O
,	O
the	O
recent	O
work	O
,	O
Focal	Method
Loss	Method
,	O
has	O
tried	O
to	O
address	O
it	O
by	O
rectifying	O
the	O
cross	Method
-	Method
entropy	Method
loss	Method
function	Method
to	O
a	O
elaborately	O
designed	O
form	O
.	O
However	O
,	O
Focal	Method
Loss	Method
adopts	O
two	O
hyper	O
-	O
parameters	O
which	O
should	O
be	O
tuned	O
with	O
a	O
lot	O
of	O
efforts	O
.	O
And	O
it	O
is	O
a	O
static	O
loss	O
which	O
is	O
not	O
adaptive	O
for	O
the	O
changing	O
of	O
data	O
distribution	O
,	O
which	O
varies	O
along	O
with	O
the	O
training	Task
process	Task
.	O
In	O
this	O
work	O
,	O
we	O
first	O
point	O
out	O
that	O
the	O
class	O
imbalance	O
can	O
be	O
summarized	O
to	O
the	O
imbalance	Metric
in	Metric
difficulty	Metric
and	O
the	O
imbalance	Metric
in	Metric
difficulty	Metric
can	O
be	O
summarized	O
to	O
the	O
imbalance	O
in	O
gradient	O
norm	O
distribution	O
.	O
If	O
a	O
positive	O
example	O
is	O
well	O
-	O
classified	O
,	O
it	O
is	O
an	O
easy	O
example	O
and	O
the	O
model	O
benefit	O
little	O
from	O
it	O
,	O
i.e.	O
a	O
little	O
magnitude	O
of	O
gradient	O
will	O
be	O
produced	O
by	O
this	O
sample	O
.	O
And	O
a	O
misclassified	O
example	O
should	O
attract	O
attention	O
of	O
the	O
model	O
no	O
matter	O
which	O
class	O
it	O
belongs	O
to	O
.	O
So	O
if	O
viewed	O
globally	O
,	O
the	O
large	O
amount	O
of	O
negative	O
examples	O
tends	O
to	O
be	O
easy	O
to	O
classify	O
and	O
the	O
hard	O
examples	O
are	O
usually	O
positive	O
.	O
So	O
the	O
two	O
kind	O
of	O
imbalance	O
can	O
be	O
roughly	O
summed	O
up	O
as	O
attribute	O
imbalance	O
.	O
Moreover	O
,	O
we	O
claim	O
that	O
the	O
imbalance	O
of	O
examples	O
with	O
different	O
attributes	O
(	O
hard	O
/	O
easy	O
and	O
pos	O
/	O
neg	O
)	O
can	O
be	O
implied	O
by	O
the	O
distribution	O
of	O
gradient	O
norm	O
.	O
The	O
density	O
of	O
examples	O
w.r.t	O
.	O
gradient	O
norm	O
,	O
which	O
we	O
call	O
as	O
gradient	O
density	O
for	O
convenient	O
,	O
varies	O
largely	O
as	O
showed	O
in	O
the	O
left	O
of	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
The	O
examples	O
with	O
very	O
small	O
gradient	O
norm	O
have	O
a	O
quite	O
large	O
density	O
which	O
is	O
corresponding	O
to	O
the	O
large	O
amount	O
of	O
easy	O
negative	O
examples	O
.	O
Although	O
one	O
easy	O
example	O
has	O
less	O
contribution	O
on	O
the	O
global	O
gradient	O
than	O
a	O
hard	O
example	O
,	O
the	O
total	O
contribution	O
of	O
the	O
huge	O
amount	O
of	O
easy	O
examples	O
can	O
overwhelm	O
the	O
contribution	O
of	O
the	O
minority	O
of	O
hard	O
examples	O
and	O
the	O
training	Method
process	Method
will	O
be	O
inefficient	O
.	O
Besides	O
,	O
we	O
also	O
discover	O
that	O
the	O
density	O
of	O
examples	O
with	O
very	O
large	O
gradient	O
norm	O
(	O
very	O
hard	O
examples	O
)	O
is	O
slightly	O
larger	O
than	O
the	O
density	O
of	O
the	O
medium	O
examples	O
.	O
And	O
we	O
consider	O
these	O
very	O
hard	O
examples	O
mostly	O
as	O
outliers	O
since	O
they	O
exist	O
stably	O
even	O
when	O
the	O
model	O
is	O
converged	O
.	O
The	O
outliers	O
may	O
affect	O
the	O
stability	O
of	O
model	O
since	O
their	O
gradients	O
may	O
have	O
a	O
large	O
discrepancy	O
from	O
the	O
other	O
common	O
examples	O
.	O
Inspired	O
by	O
the	O
analysis	O
of	O
gradient	Method
norm	Method
distribution	Method
,	O
we	O
propose	O
a	O
gradient	Method
harmonizing	Method
mechanism	Method
(	O
GHM	Method
)	O
to	O
train	O
the	O
one	O
-	O
stage	O
object	Task
detection	Task
model	O
in	O
an	O
efficient	O
,	O
which	O
focuses	O
on	O
the	O
harmony	O
of	O
gradient	O
contribution	O
of	O
different	O
examples	O
.	O
The	O
GHM	Method
first	O
performs	O
statistics	O
on	O
the	O
number	O
of	O
examples	O
with	O
similar	O
attributes	O
w.r.t	O
their	O
gradient	O
density	O
and	O
then	O
attach	O
a	O
harmonizing	O
parameter	O
to	O
the	O
gradient	O
of	O
each	O
example	O
according	O
to	O
the	O
density	O
.	O
The	O
effect	O
of	O
GHM	Method
compared	O
with	O
CE	Method
and	O
FL	Method
is	O
illustrated	O
in	O
the	O
right	O
of	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
Training	O
with	O
GHM	Method
,	O
the	O
huge	O
amount	O
of	O
cumulated	O
gradient	O
produced	O
by	O
easy	O
examples	O
can	O
be	O
largely	O
down	O
-	O
weighted	O
and	O
the	O
outliers	O
can	O
be	O
relatively	O
down	O
-	O
weighted	O
as	O
well	O
.	O
In	O
the	O
end	O
,	O
the	O
contribution	O
of	O
each	O
kind	O
of	O
examples	O
will	O
be	O
balanced	O
and	O
the	O
training	O
can	O
be	O
more	O
efficient	O
and	O
stable	O
.	O
In	O
practice	O
,	O
the	O
modification	O
of	O
gradient	O
can	O
be	O
equivalently	O
implemented	O
by	O
reformulating	O
the	O
loss	O
function	O
,	O
we	O
embed	O
the	O
GHM	Method
into	O
the	O
classification	O
loss	O
,	O
which	O
is	O
denoted	O
as	O
GHM	Method
-	O
C	O
loss	O
.	O
This	O
loss	Method
function	Method
is	O
elegantly	O
formulated	O
without	O
many	O
hyper	O
-	O
parameters	O
to	O
tune	O
.	O
Since	O
the	O
gradient	O
density	O
is	O
a	O
statistical	O
variable	O
depending	O
on	O
the	O
examples	O
distribution	O
in	O
a	O
mini	O
-	O
batch	O
,	O
GHM	Method
-	Method
C	Method
is	O
a	O
dynamic	Method
loss	Method
that	O
can	O
adapt	O
to	O
the	O
change	O
of	O
data	O
distribution	O
in	O
each	O
batch	O
as	O
well	O
as	O
to	O
the	O
updating	Method
of	Method
model	Method
.	O
To	O
showcase	O
the	O
generality	O
of	O
GHM	Method
,	O
we	O
also	O
adopt	O
it	O
in	O
the	O
box	Method
regression	Method
branch	Method
as	O
the	O
form	O
of	O
GHM	Method
-	O
R	O
loss	O
.	O
Experiments	O
on	O
the	O
bounding	Task
box	Task
detection	Task
track	Task
of	O
the	O
challenging	O
COCO	Material
benchmark	O
show	O
that	O
the	O
GHM	Method
-	O
C	O
loss	O
has	O
a	O
large	O
gain	O
compared	O
to	O
the	O
traditional	O
cross	Method
-	Method
entropy	Method
loss	Method
and	O
slightly	O
surpasses	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Focal	Method
Loss	Method
.	O
And	O
the	O
GHM	Method
-	O
R	O
loss	O
also	O
has	O
better	O
performance	O
than	O
the	O
commonly	O
used	O
smooth	Method
loss	Method
.	O
The	O
combination	O
of	O
GHM	Method
-	Method
C	Method
and	O
GHM	Method
-	Method
R	Method
attains	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
COCO	Material
tes	O
-	O
dev	O
set	O
.	O
Our	O
main	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
reveal	O
the	O
essential	O
principle	O
behind	O
the	O
significant	O
example	O
imbalance	O
in	O
one	Method
-	Method
stage	Method
detector	Method
in	O
term	O
of	O
gradient	O
norm	O
distribution	O
,	O
and	O
propose	O
a	O
novel	O
gradient	Method
harmonizing	Method
mechanism	Method
(	O
GHM	Method
)	O
to	O
handle	O
it	O
.	O
We	O
embed	O
the	O
GHM	Method
into	O
the	O
loss	Method
for	O
classification	Task
and	Task
regression	Task
as	O
GHM	Method
-	Method
C	Method
and	O
GHM	Method
-	Method
R	Method
respectively	O
,	O
which	O
rectify	O
the	O
gradient	O
contribution	O
of	O
examples	O
with	O
different	O
attributes	O
and	O
is	O
robust	O
to	O
hyper	O
-	O
parameters	O
.	O
Collaborating	O
with	O
GHM	Method
,	O
we	O
can	O
easily	O
train	O
a	O
single	Method
stage	Method
detector	Method
without	O
any	O
data	Method
sampling	Method
strategy	Method
and	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
on	O
COCO	Material
benchmark	O
.	O
section	O
:	O
Related	O
Work	O
subsubsection	O
:	O
Object	Task
Detection	Task
:	O
Object	Task
detection	Task
is	O
one	O
of	O
the	O
most	O
basic	O
and	O
important	O
task	O
in	O
the	O
field	O
of	O
computer	Task
vision	Task
.	O
Deep	Method
convolutional	Method
neural	Method
network	Method
(	O
CNN	Method
)	O
based	O
methods	O
,	O
e.g.	O
,	O
have	O
become	O
more	O
and	O
more	O
developed	O
and	O
achieved	O
great	O
success	O
in	O
recent	O
years	O
,	O
owing	O
to	O
the	O
significant	O
progress	O
of	O
network	Method
architecture	Method
such	O
as	O
.	O
Advanced	O
object	Task
detection	Task
frameworks	O
can	O
be	O
divided	O
into	O
two	O
categories	O
:	O
one	Method
-	Method
stage	Method
detector	Method
and	O
two	Method
-	Method
stage	Method
detector	Method
.	O
Most	O
state	O
of	O
the	O
art	O
methods	O
use	O
two	Method
-	Method
stage	Method
detectors	Method
,	O
e.g.	O
.	O
They	O
are	O
mainly	O
based	O
on	O
the	O
Region	Method
CNN	Method
(	O
R	Method
-	Method
CNN	Method
)	O
architecture	O
.	O
These	O
approaches	O
first	O
obtain	O
a	O
manageable	O
number	O
of	O
region	O
proposals	O
called	O
region	O
of	O
interest	O
(	O
RoI	O
)	O
from	O
the	O
nearly	O
infinite	O
candidate	O
regions	O
and	O
then	O
use	O
the	O
network	O
to	O
evaluate	O
each	O
RoI.	Method
One	Method
-	Method
stage	Method
detectors	Method
have	O
the	O
advantage	O
of	O
simple	O
structures	O
and	O
high	O
speed	O
.	O
SSD	Method
,	O
YOLO	Method
for	O
generic	O
object	Task
detection	Task
and	O
RSA	Method
for	O
face	Task
detection	Task
have	O
achieved	O
good	O
speed	Metric
/	Metric
accuracy	Metric
trade	O
-	O
off	O
.	O
However	O
,	O
they	O
can	O
hardly	O
surpass	O
the	O
accuracy	Metric
of	O
two	Method
-	Method
stage	Method
detectors	Method
.	O
RetinaNet	Method
is	O
the	O
state	O
of	O
the	O
art	O
one	Method
-	Method
stage	Method
object	Method
detector	Method
that	O
achieve	O
comparable	O
performance	O
to	O
two	Method
-	Method
stage	Method
detectors	Method
.	O
It	O
adopts	O
an	O
architecture	O
modified	O
from	O
RPN	Method
and	O
focuses	O
on	O
addressing	O
the	O
class	O
imbalance	O
during	O
training	Task
.	O
subsubsection	O
:	O
Object	Method
Functions	Method
for	O
Object	Task
Detector	Task
:	O
Most	O
detection	Method
models	Method
use	O
cross	Method
entropy	Method
based	Method
loss	Method
function	Method
for	O
classification	Task
.	O
While	O
one	Method
-	Method
stage	Method
detectors	Method
face	O
a	O
problem	O
of	O
extreme	Task
class	Task
imbalance	Task
that	O
two	Method
-	Method
stage	Method
detectors	Method
do	O
not	O
have	O
.	O
Earlier	O
methods	O
try	O
to	O
use	O
hard	Method
example	Method
mining	Method
methods	Method
,	O
e.g.	O
,	O
but	O
they	O
discard	O
most	O
examples	O
and	O
can	O
not	O
handle	O
the	O
problem	O
well	O
.	O
Recently	O
the	O
work	O
reformulate	O
the	O
cross	O
-	O
entropy	O
loss	O
so	O
that	O
easy	O
negatives	O
are	O
down	O
-	O
weighted	O
and	O
the	O
hard	O
examples	O
are	O
unaffected	O
or	O
even	O
up	O
-	O
weighted	O
.	O
For	O
stable	Task
training	Task
of	Task
box	Task
regression	Task
,	O
Fast	O
R	O
-	O
CNN	Method
introduces	O
the	O
smooth	O
loss	O
.	O
This	O
loss	O
reduces	O
the	O
impact	O
of	O
outliers	O
so	O
that	O
the	O
training	O
of	O
model	O
can	O
be	O
more	O
stable	O
.	O
Almost	O
all	O
the	O
following	O
works	O
take	O
the	O
smooth	Method
loss	Method
as	O
a	O
default	O
for	O
box	Task
regression	Task
.	O
The	O
work	O
tries	O
to	O
improve	O
regression	Task
performance	O
by	O
changing	O
the	O
target	O
to	O
a	O
distribution	O
and	O
using	O
a	O
histogram	Method
loss	Method
to	O
calculate	O
the	O
K	Metric
-	Metric
L	Metric
divergence	Metric
of	Metric
prediction	Metric
and	O
target	O
.	O
The	O
work	O
balances	O
multi	Task
-	Task
task	Task
losses	Task
by	O
dynamically	O
tuning	O
gradient	O
magnitude	O
of	O
different	O
task	O
branches	O
.	O
Our	O
GHM	Method
based	O
loss	O
harmonizes	O
the	O
contribution	O
of	O
examples	O
on	O
the	O
basis	O
of	O
the	O
distribution	O
of	O
their	O
gradient	O
,	O
so	O
that	O
it	O
can	O
handle	O
both	O
the	O
class	Task
imbalance	Task
and	O
the	O
outliers	Task
problem	Task
well	O
.	O
It	O
can	O
also	O
adapt	O
the	O
weights	O
to	O
the	O
changing	O
of	O
data	O
distribution	O
in	O
each	O
mini	O
-	O
batch	O
.	O
section	O
:	O
Gradient	Method
Harmonizing	Method
Mechanism	Method
subsection	O
:	O
Problem	O
Description	O
Similar	O
to	O
,	O
our	O
efforts	O
here	O
are	O
focused	O
on	O
classification	Task
in	O
one	O
-	O
stage	O
object	Task
detection	Task
where	O
the	O
classes	O
(	O
foreground	O
/	O
background	O
)	O
of	O
examples	O
are	O
quite	O
imbalanced	O
.	O
For	O
a	O
candidate	O
box	O
,	O
let	O
be	O
the	O
probability	O
predicted	O
by	O
the	O
model	O
and	O
be	O
its	O
ground	O
-	O
truth	O
label	O
for	O
a	O
certain	O
class	O
.	O
Consider	O
the	O
binary	Metric
cross	Metric
entropy	Metric
loss	Metric
:	O
Let	O
x	O
be	O
the	O
direct	O
output	O
of	O
the	O
model	O
such	O
that	O
,	O
we	O
have	O
the	O
gradient	O
with	O
regard	O
to	O
x	O
:	O
We	O
define	O
as	O
follows	O
:	O
equals	O
to	O
the	O
norm	O
of	O
gradient	O
w.r.t	O
.	O
The	O
value	O
of	O
represents	O
attribute	O
(	O
e.g.	O
easy	O
or	O
hard	O
)	O
of	O
an	O
example	O
and	O
implies	O
the	O
example	O
’s	O
impact	O
on	O
the	O
global	O
gradient	O
.	O
Although	O
the	O
strict	O
definition	O
of	O
gradient	O
is	O
on	O
the	O
whole	O
parameter	O
space	O
,	O
which	O
means	O
is	O
a	O
relative	O
norm	O
of	O
an	O
example	O
’s	O
gradient	O
,	O
we	O
call	O
as	O
gradient	O
norm	O
in	O
this	O
paper	O
for	O
convenience	O
.	O
Fig	O
.	O
[	O
reference	O
]	O
shows	O
the	O
distribution	O
of	O
from	O
a	O
converged	O
one	Method
-	Method
stage	Method
detection	Method
model	Method
.	O
Since	O
the	O
easy	O
negatives	O
have	O
a	O
dominant	O
number	O
,	O
we	O
use	O
log	O
axis	O
to	O
display	O
the	O
fraction	O
of	O
examples	O
to	O
demonstrate	O
the	O
details	O
of	O
the	O
variance	O
of	O
examples	O
with	O
different	O
attributes	O
.	O
It	O
can	O
be	O
seen	O
that	O
the	O
number	O
of	O
very	O
easy	O
examples	O
is	O
extremely	O
large	O
,	O
which	O
have	O
a	O
great	O
impact	O
on	O
the	O
global	O
gradient	O
.	O
Moreover	O
,	O
we	O
can	O
see	O
that	O
a	O
converged	Method
model	Method
still	O
ca	O
n’t	O
handle	O
some	O
very	O
hard	O
examples	O
whose	O
number	O
is	O
even	O
larger	O
than	O
the	O
examples	O
with	O
medium	O
difficulty	O
.	O
These	O
very	O
hard	O
examples	O
can	O
be	O
regarded	O
as	O
outliers	O
since	O
their	O
gradient	O
directions	O
tends	O
to	O
vary	O
largely	O
from	O
the	O
gradient	O
directions	O
of	O
the	O
large	O
amount	O
of	O
other	O
examples	O
.	O
That	O
is	O
,	O
if	O
the	O
converged	Method
model	Method
is	O
forced	O
to	O
learn	O
to	O
classify	O
these	O
outliers	O
better	O
,	O
the	O
classification	Task
of	O
the	O
large	O
number	O
of	O
other	O
examples	O
tends	O
to	O
be	O
less	O
accurate	O
.	O
subsection	O
:	O
Gradient	Method
Density	Method
To	O
handle	O
the	O
problem	O
of	O
the	O
disharmony	O
of	O
gradient	O
norm	O
distribution	O
,	O
we	O
introduce	O
a	O
harmonizing	Method
approach	Method
with	O
regard	O
to	O
gradient	O
density	O
.	O
Gradient	O
density	O
function	O
of	O
training	O
examples	O
is	O
formulated	O
as	O
Equation	O
.	O
[	O
reference	O
]	O
:	O
where	O
is	O
the	O
gradient	O
norm	O
of	O
the	O
k	O
-	O
th	O
example	O
.	O
And	O
The	O
gradient	O
density	O
of	O
denotes	O
the	O
number	O
of	O
examples	O
lying	O
in	O
the	O
region	O
centered	O
at	O
with	O
a	O
length	O
of	O
and	O
normalized	O
by	O
the	O
valid	O
length	O
of	O
the	O
region	O
.	O
Now	O
we	O
define	O
the	O
gradient	O
density	O
harmonizing	O
parameter	O
as	O
:	O
where	O
is	O
the	O
total	O
number	O
of	O
examples	O
.	O
To	O
better	O
comprehend	O
the	O
gradient	O
density	O
harmonizing	O
parameter	O
,	O
we	O
can	O
rewrite	O
it	O
as	O
.	O
The	O
denominator	O
is	O
a	O
normalizer	O
indicating	O
the	O
fraction	O
of	O
examples	O
with	O
neighborhood	O
gradients	O
to	O
the	O
i	O
-	O
th	O
example	O
.	O
If	O
the	O
examples	O
are	O
uniformly	O
distributed	O
with	O
regard	O
to	O
gradient	O
,	O
for	O
any	O
and	O
each	O
example	O
will	O
have	O
the	O
same	O
,	O
which	O
means	O
nothing	O
is	O
changed	O
.	O
Otherwise	O
,	O
the	O
examples	O
with	O
large	O
density	O
will	O
be	O
relatively	O
down	O
-	O
weighted	O
by	O
the	O
normalizer	Method
.	O
subsection	O
:	O
GHM	Method
-	O
C	O
Loss	O
We	O
embed	O
the	O
GHM	Method
into	O
classification	Task
loss	Task
by	O
regarding	O
as	O
the	O
loss	O
weight	O
of	O
the	O
i	O
-	O
th	O
example	O
and	O
the	O
gradient	O
density	O
harmonized	O
form	O
of	O
loss	O
function	O
is	O
:	O
Fig	O
.	O
[	O
reference	O
]	O
illustrates	O
the	O
reformulated	O
gradient	O
norm	O
of	O
different	O
losses	O
.	O
Here	O
we	O
take	O
the	O
original	O
gradient	O
norm	O
of	O
CE	O
,	O
i.e.	O
,	O
as	O
the	O
x	O
-	O
axis	O
for	O
convenient	O
view	O
since	O
the	O
density	O
is	O
calculated	O
according	O
to	O
.	O
We	O
can	O
see	O
that	O
the	O
curves	O
of	O
Focal	Method
Loss	Method
and	O
GHM	Method
-	O
C	O
loss	O
have	O
similar	O
trend	O
,	O
which	O
implies	O
that	O
Focal	Method
Loss	Method
with	O
the	O
best	O
hyper	O
-	O
parameters	O
is	O
similar	O
with	O
uniform	Method
gradient	Method
harmonizing	Method
.	O
Furthermore	O
,	O
GHM	Method
-	Method
C	Method
has	O
one	O
more	O
merit	O
that	O
Focal	Method
loss	Method
ignores	O
:	O
down	O
-	O
weighting	O
the	O
gradient	O
contribution	O
of	O
outliers	O
.	O
With	O
our	O
GHM	Method
-	O
C	O
loss	O
,	O
the	O
huge	O
number	O
of	O
very	O
easy	O
examples	O
are	O
largely	O
down	O
-	O
weighted	O
and	O
the	O
outliers	O
are	O
slightly	O
down	O
-	O
weighted	O
as	O
well	O
,	O
which	O
simultaneously	O
addresses	O
the	O
attribute	Task
imbalance	Task
problem	Task
and	O
the	O
outliers	Task
problem	Task
.	O
From	O
the	O
right	O
figure	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
we	O
can	O
better	O
see	O
that	O
GHM	Method
-	Method
C	Method
harmonizes	O
the	O
total	O
gradient	O
contribution	O
of	O
different	O
group	O
of	O
examples	O
.	O
Since	O
the	O
gradient	O
density	O
is	O
calculated	O
every	O
iteration	O
,	O
the	O
weights	O
of	O
examples	O
are	O
not	O
fixed	O
w.r.t	O
.	O
(	O
or	O
)	O
like	O
focal	Method
loss	Method
but	O
adaptive	O
to	O
current	O
state	O
of	O
model	O
and	O
mini	O
-	O
batch	O
of	O
data	O
.	O
The	O
dynamic	O
property	O
of	O
GHM	Method
-	O
C	O
loss	O
makes	O
the	O
training	Method
more	O
efficient	O
and	O
robust	O
.	O
subsection	O
:	O
Unit	Method
Region	Method
Approximation	Method
subsubsection	O
:	O
Complexity	Method
Analysis	Method
:	O
The	O
naive	Method
algorithm	Method
to	O
calculate	O
the	O
gradient	O
density	O
values	O
of	O
all	O
examples	O
has	O
a	O
time	Metric
complexity	Metric
of	O
,	O
which	O
can	O
be	O
easily	O
attained	O
from	O
Equations	O
[	O
reference	O
]	O
and	O
[	O
reference	O
]	O
.	O
Even	O
parallel	O
computed	O
,	O
each	O
computing	O
unit	O
still	O
bears	O
a	O
computation	O
of	O
.	O
And	O
as	O
far	O
as	O
we	O
know	O
,	O
the	O
best	O
algorithm	O
first	O
sort	O
the	O
examples	O
by	O
gradient	O
norm	O
with	O
a	O
complexity	O
of	O
and	O
then	O
use	O
a	O
queue	O
to	O
scan	O
the	O
examples	O
and	O
get	O
their	O
density	O
with	O
.	O
This	O
sorting	Method
based	Method
method	Method
can	O
not	O
gain	O
much	O
from	O
parallel	Task
computing	Task
.	O
Since	O
of	O
an	O
image	O
in	O
one	O
-	O
stage	Method
detector	Method
can	O
be	O
or	O
even	O
,	O
to	O
directly	O
calculate	O
the	O
gradient	O
density	O
is	O
quite	O
time	O
consuming	O
.	O
So	O
we	O
introduce	O
an	O
alternative	O
approach	O
to	O
approximately	O
attain	O
the	O
gradient	O
density	O
of	O
examples	O
.	O
subsubsection	Method
:	O
Unit	O
Region	O
:	O
We	O
divide	O
the	O
range	O
space	O
of	O
into	O
individual	O
unit	O
regions	O
with	O
a	O
length	O
of	O
,	O
and	O
there	O
are	O
unit	O
regions	O
.	O
Let	O
be	O
the	O
unit	O
region	O
with	O
index	O
j	O
so	O
that	O
.	O
Let	O
denote	O
the	O
number	O
of	O
examples	O
lying	O
in	O
.	O
We	O
define	O
s.t	O
.	O
,	O
which	O
is	O
the	O
index	O
function	O
to	O
the	O
unit	O
region	O
that	O
lies	O
in	O
.	O
Then	O
we	O
define	O
the	O
approximate	Method
gradient	Method
density	Method
function	Method
as	O
:	O
Then	O
we	O
have	O
the	O
approximate	Method
gradient	Method
density	Method
harmonizing	Method
parameter	Method
:	O
Consider	O
the	O
special	O
case	O
where	O
:	O
there	O
are	O
just	O
one	O
unit	O
region	O
and	O
all	O
examples	O
lie	O
in	O
it	O
,	O
so	O
obviously	O
every	O
and	O
each	O
example	O
keep	O
their	O
original	O
gradient	O
contribution	O
.	O
Finally	O
we	O
have	O
the	O
reformulated	O
loss	O
function	O
:	O
From	O
Equation	O
.	O
[	O
reference	O
]	O
we	O
can	O
see	O
that	O
the	O
examples	O
lying	O
in	O
the	O
same	O
unit	O
region	O
share	O
the	O
same	O
gradient	O
density	O
.	O
So	O
we	O
can	O
use	O
the	O
algorithm	O
of	O
histogram	O
statistics	O
and	O
the	O
computation	O
of	O
all	O
the	O
gradient	O
density	O
values	O
has	O
a	O
time	Metric
complexity	Metric
of	O
.	O
And	O
parallel	Task
computing	Task
can	O
be	O
applied	O
so	O
that	O
each	O
computing	O
unit	O
has	O
a	O
computation	O
of	O
.	O
In	O
practice	O
,	O
we	O
can	O
attain	O
good	O
performance	O
with	O
quite	O
small	O
number	O
of	O
unit	O
regions	O
.	O
That	O
is	O
is	O
fairly	O
small	O
and	O
the	O
calculation	O
of	O
loss	Task
is	O
efficient	O
.	O
subsubsection	Method
:	O
EMA	Method
:	O
Mini	Method
-	Method
batch	Method
statistics	Method
based	Method
methods	Method
usually	O
face	O
a	O
problem	O
:	O
when	O
many	O
extreme	O
data	O
are	O
just	O
sampled	O
in	O
one	O
mini	O
-	O
batch	O
,	O
the	O
statistical	O
result	O
will	O
be	O
a	O
serious	O
noise	O
and	O
the	O
training	O
will	O
be	O
unstable	O
.	O
Exponential	Method
moving	Method
average	Method
(	O
EMA	Method
)	O
is	O
a	O
common	O
used	O
method	O
to	O
address	O
this	O
problem	O
,	O
e.g.	O
,	O
SGD	Method
with	O
momentum	Method
and	O
Batch	Method
Normalization	Method
.	O
Since	O
in	O
the	O
approximation	Method
algorithm	Method
the	O
gradient	O
densities	O
come	O
from	O
the	O
numbers	O
of	O
examples	O
in	O
the	O
unit	O
regions	O
,	O
we	O
can	O
apply	O
EMA	Method
on	O
each	O
unit	O
region	O
to	O
obtain	O
more	O
stable	O
gradient	O
densities	O
for	O
examples	O
.	O
Let	O
be	O
the	O
number	O
of	O
examples	O
in	O
the	O
j	O
-	O
th	O
unit	O
region	O
in	O
the	O
t	O
-	O
th	O
iteration	O
and	O
be	O
the	O
moving	O
averaged	O
number	O
.	O
We	O
have	O
:	O
where	O
is	O
the	O
momentum	O
parameter	O
.	O
We	O
use	O
the	O
averaged	O
number	O
to	O
calculate	O
the	O
gradient	O
density	O
instead	O
of	O
:	O
With	O
EMA	Method
,	O
the	O
gradient	O
density	O
will	O
be	O
more	O
smooth	O
and	O
insensitive	O
to	O
extreme	O
data	O
.	O
subsection	O
:	O
GHM	Method
-	O
R	O
Loss	O
Consider	O
the	O
parameterized	O
offsets	O
,	O
,	O
predicted	O
by	O
box	Method
regression	Method
branch	Method
and	O
the	O
target	O
offsets	O
,	O
,	O
computed	O
from	O
ground	O
-	O
truth	O
.	O
The	O
regression	Method
loss	Method
usually	O
adopts	O
the	O
smooth	O
loss	O
function	O
:	O
where	O
where	O
is	O
the	O
division	O
point	O
between	O
the	O
quadric	O
part	O
and	O
the	O
linear	O
part	O
,	O
and	O
usually	O
set	O
to	O
in	O
practice	O
.	O
Since	O
,	O
the	O
gradient	O
of	O
smooth	O
loss	O
w.r.t	O
can	O
be	O
expressed	O
as	O
:	O
where	O
is	O
the	O
sign	O
function	O
.	O
Note	O
that	O
all	O
the	O
examples	O
with	O
larger	O
than	O
the	O
division	O
point	O
have	O
the	O
same	O
gradient	O
norm	O
,	O
which	O
makes	O
the	O
distinguishing	O
of	O
examples	O
with	O
different	O
attributes	O
impossible	O
if	O
depending	O
on	O
the	O
gradient	O
norm	O
.	O
An	O
alternative	O
choice	O
is	O
directly	O
using	O
as	O
the	O
measurement	O
of	O
different	O
attributes	O
,	O
but	O
the	O
new	O
problem	O
is	O
can	O
reach	O
to	O
infinite	O
in	O
theory	O
and	O
the	O
unit	Method
region	Method
approximation	Method
can	O
not	O
be	O
implemented	O
.	O
To	O
conveniently	O
apply	O
GHM	Method
on	O
regression	Task
loss	Task
,	O
we	O
first	O
modify	O
the	O
traditional	O
loss	Method
into	O
a	O
more	O
elegant	O
form	O
:	O
This	O
loss	O
shares	O
similar	O
property	O
with	O
loss	O
:	O
when	O
d	O
is	O
small	O
it	O
approximates	O
a	O
quadric	O
function	O
(	O
loss	Method
)	O
and	O
when	O
d	O
is	O
large	O
is	O
approximate	O
a	O
linear	O
function	O
(	O
loss	O
)	O
.	O
We	O
denote	O
the	O
modified	O
loss	O
function	O
as	O
Authentic	O
Smooth	O
(	O
)	O
loss	O
for	O
its	O
good	O
property	O
of	O
authentic	O
smoothness	O
,	O
which	O
means	O
all	O
the	O
degrees	O
of	O
derivatives	O
are	O
existed	O
and	O
continuous	O
.	O
In	O
contrast	O
,	O
the	O
second	O
derivative	O
of	O
smooth	O
loss	O
does	O
not	O
exist	O
at	O
the	O
point	O
.	O
Furthermore	O
,	O
the	O
loss	Method
has	O
an	O
elegant	O
form	O
of	O
gradient	O
w.r.t	O
:	O
The	O
range	O
of	O
the	O
gradient	O
is	O
just	O
,	O
so	O
the	O
calculation	O
of	O
density	O
in	O
unit	O
regions	O
for	O
loss	Task
in	O
regression	Task
is	O
as	O
convenient	O
as	O
CE	Task
loss	Task
in	O
classification	Task
.	O
In	O
practice	O
,	O
we	O
set	O
for	O
loss	O
to	O
keep	O
the	O
same	O
performance	O
with	O
loss	O
.	O
We	O
define	O
as	O
the	O
gradient	O
norm	O
of	O
loss	O
and	O
the	O
gradient	O
distribution	O
of	O
a	O
converged	Method
model	Method
is	O
illustrated	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
We	O
can	O
see	O
that	O
there	O
are	O
large	O
number	O
of	O
outliers	O
.	O
Note	O
that	O
the	O
regression	Method
is	O
only	O
performed	O
on	O
the	O
positive	O
examples	O
so	O
it	O
is	O
reasonable	O
for	O
the	O
different	O
distribution	O
trend	O
between	O
classification	Task
and	O
regression	Task
.	O
Above	O
all	O
,	O
we	O
can	O
apply	O
GHM	Method
on	O
regression	Task
loss	Task
:	O
The	O
reformulated	O
gradient	O
contribution	O
of	O
loss	O
,	O
loss	O
and	O
GHM	Method
-	O
R	O
loss	O
in	O
Fig	O
.	O
[	O
reference	O
]	O
.	O
The	O
x	O
-	O
axis	O
adopts	O
for	O
convenient	O
comparison	O
.	O
We	O
emphasize	O
that	O
in	O
box	Task
regression	Task
not	O
all	O
the	O
“	O
easy	O
examples	O
”	O
are	O
unimportant	O
.	O
An	O
easy	O
example	O
in	O
classification	Task
is	O
usually	O
a	O
background	O
region	O
with	O
a	O
very	O
low	O
predicted	O
probability	O
and	O
will	O
be	O
definitely	O
excluded	O
from	O
the	O
final	O
candidates	O
.	O
Thus	O
the	O
improvement	O
of	O
this	O
kind	O
of	O
examples	O
makes	O
nearly	O
no	O
contribution	O
to	O
the	O
precision	Metric
.	O
But	O
in	O
box	Task
regression	Task
,	O
an	O
easy	O
example	O
still	O
has	O
deviation	O
from	O
the	O
ground	O
truth	O
location	O
.	O
Better	O
prediction	O
of	O
any	O
example	O
will	O
directly	O
improve	O
the	O
quality	O
of	O
the	O
final	O
candidates	O
.	O
Moreover	O
,	O
advanced	O
datasets	O
care	O
more	O
about	O
the	O
localization	Metric
accuracy	Metric
.	O
For	O
example	O
,	O
COCO	Material
takes	O
the	O
average	O
AP	Metric
from	O
the	O
IoU	O
threshold	O
0.5	O
to	O
0.95	O
as	O
the	O
metric	O
to	O
evaluate	O
an	O
algorithm	O
.	O
In	O
this	O
metric	O
,	O
the	O
some	O
of	O
the	O
so	O
called	O
easy	O
examples	O
(	O
those	O
having	O
small	O
errors	O
)	O
are	O
also	O
important	O
because	O
reducing	O
the	O
errors	O
of	O
them	O
can	O
directly	O
improve	O
the	O
AP	Metric
at	O
high	O
threshold	O
(	O
e.g.	O
AP@IoU=0.75	Metric
)	O
.	O
Our	O
GHM	Method
-	O
R	O
loss	O
can	O
harmonize	O
the	O
contribution	O
of	O
easy	O
and	O
hard	O
examples	O
for	O
box	Task
regression	Task
by	O
up	O
-	O
weighting	O
the	O
important	O
part	O
of	O
easy	O
examples	O
and	O
down	O
-	O
weighting	O
the	O
outliers	O
.	O
Experiments	O
show	O
that	O
it	O
can	O
attain	O
better	O
performance	O
than	O
and	O
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
our	O
approach	O
on	O
the	O
challenging	O
COCO	Material
benchmark	O
.	O
For	O
training	O
,	O
we	O
follow	O
the	O
common	O
used	O
practice	O
to	O
divide	O
the	O
40k	O
validation	O
set	O
into	O
a	O
35k	O
subset	O
and	O
a	O
5k	O
subset	O
.	O
The	O
union	O
of	O
the	O
35k	O
validation	O
subset	O
and	O
the	O
whole	O
80k	O
training	O
set	O
are	O
used	O
for	O
training	O
together	O
and	O
denoted	O
as	O
trainval35k	O
set	O
.	O
The	O
5k	O
validation	O
subset	O
is	O
denoted	O
as	O
minival	O
set	O
and	O
our	O
ablation	O
study	O
is	O
performed	O
on	O
it	O
.	O
While	O
our	O
main	O
results	O
are	O
reported	O
on	O
the	O
test	O
-	O
dev	O
set	O
.	O
subsection	O
:	O
Implementation	O
Details	O
subsubsection	O
:	O
Network	O
Setting	O
:	O
We	O
use	O
RetinaNet	Method
as	O
network	Method
architecture	Method
and	O
all	O
the	O
experiments	O
adopt	O
ResNet	Method
as	O
backbone	O
with	O
Feature	Method
Pyramid	Method
Network	Method
(	O
FPN	Method
)	O
structure	O
.	O
Anchors	O
use	O
3	O
scales	O
and	O
3	O
aspect	O
ratios	O
for	O
convenient	O
comparison	O
with	O
focal	O
loss	O
.	O
The	O
input	O
image	O
scale	O
is	O
set	O
as	O
800	O
pixel	O
for	O
all	O
experiments	O
.	O
For	O
all	O
ablation	Task
studies	Task
,	O
ResNet	Method
-	Method
50	Method
is	O
used	O
.	O
While	O
the	O
final	O
model	O
evaluated	O
on	O
test	O
-	O
dev	O
adopts	O
ResNeXt	Method
-	Method
101	Method
.	O
In	O
contrast	O
to	O
focal	Task
loss	Task
,	O
our	O
approach	O
does	O
n’t	O
need	O
a	O
specialized	O
bias	Method
initialization	Method
.	O
subsubsection	O
:	O
Optimization	Task
:	O
All	O
the	O
models	O
are	O
optimized	O
by	O
the	O
common	O
used	O
SGD	Method
algorithm	Method
.	O
We	O
train	O
the	O
models	O
on	O
8	O
GPUs	O
with	O
2	O
images	O
on	O
each	O
GPU	O
so	O
that	O
the	O
effective	O
mini	O
-	O
batch	O
size	O
is	O
16	O
.	O
All	O
models	O
are	O
trained	O
for	O
14	O
epochs	O
with	O
an	O
initial	O
learning	Metric
rate	Metric
of	O
0.01	O
,	O
which	O
is	O
decreased	O
by	O
a	O
factor	O
0.1	O
at	O
the	O
9th	O
epoch	O
and	O
again	O
at	O
the	O
12th	O
epoch	O
.	O
We	O
also	O
use	O
a	O
weight	O
decay	O
parameter	O
of	O
0.0001	O
and	O
a	O
momentum	O
parameter	O
of	O
0.9	O
.	O
The	O
only	O
data	Method
augmentation	Method
operation	Method
is	O
horizontal	Task
image	Task
flipping	Task
.	O
For	O
the	O
EMA	Method
used	O
in	O
gradient	Task
density	Task
calculation	Task
,	O
we	O
use	O
for	O
all	O
experiments	O
since	O
the	O
results	O
are	O
insensitive	O
to	O
the	O
exact	O
value	O
of	O
.	O
subsection	O
:	O
GHM	Method
-	O
C	O
Loss	O
To	O
focus	O
on	O
the	O
effect	O
of	O
GHM	Method
-	O
C	O
loss	O
function	O
,	O
experiments	O
in	O
this	O
section	O
all	O
adopt	O
smooth	Method
loss	Method
function	Method
with	O
for	O
the	O
box	Method
regression	Method
branch	Method
.	O
subsubsection	O
:	O
Baseline	O
:	O
We	O
have	O
trained	O
a	O
model	O
with	O
the	O
standard	O
cross	Metric
entropy	Metric
loss	Metric
as	O
the	O
baseline	O
.	O
The	O
standard	O
initialization	O
will	O
lead	O
to	O
quick	O
divergence	O
,	O
so	O
we	O
follow	O
focal	O
loss	O
to	O
initialize	O
the	O
bias	O
term	O
of	O
the	O
last	O
layer	O
to	O
with	O
to	O
avoid	O
divergence	O
.	O
However	O
with	O
the	O
specialized	O
initialization	O
the	O
loss	Task
of	Task
classification	Task
is	O
very	O
small	O
,	O
so	O
we	O
up	O
-	O
weight	O
the	O
classification	Metric
loss	Metric
by	O
20	O
to	O
make	O
the	O
begging	Metric
loss	Metric
value	Metric
reasonable	O
(	O
the	O
begging	Metric
classification	Metric
loss	Metric
value	Metric
is	O
around	O
1	O
now	O
)	O
.	O
But	O
when	O
the	O
model	O
converge	O
,	O
the	O
classification	Metric
loss	Metric
is	O
still	O
very	O
small	O
and	O
we	O
finally	O
obtain	O
a	O
model	O
with	O
an	O
Average	Metric
Precision	Metric
(	O
AP	Metric
)	O
of	O
28.6	O
.	O
subsubsection	O
:	O
Number	O
of	O
Unit	O
Region	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
varying	O
which	O
is	O
the	O
number	O
of	O
unit	O
regions	O
.	O
EMA	Method
is	O
not	O
applied	O
here	O
.	O
When	O
is	O
too	O
small	O
,	O
the	O
density	O
can	O
not	O
have	O
a	O
good	O
variation	O
over	O
different	O
gradient	O
norm	O
and	O
the	O
performance	O
is	O
not	O
so	O
good	O
.	O
So	O
we	O
can	O
gain	O
more	O
when	O
increases	O
when	O
is	O
not	O
large	O
.	O
However	O
is	O
not	O
necessarily	O
very	O
large	O
,	O
when	O
,	O
the	O
GHM	Method
-	O
C	O
loss	O
yields	O
a	O
large	O
enough	O
improvement	O
over	O
baseline	O
.	O
subsubsection	O
:	O
Speed	O
:	O
Since	O
our	O
approach	O
is	O
a	O
loss	Method
function	Method
,	O
it	O
does	O
n’t	O
change	O
the	O
time	O
for	O
inference	Task
.	O
For	O
training	Task
,	O
a	O
small	O
of	O
30	O
is	O
enough	O
to	O
attain	O
good	O
performance	O
,	O
so	O
time	O
consumed	O
by	O
gradient	Method
density	Method
calculation	Method
is	O
not	O
long	O
.	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
the	O
average	O
time	O
for	O
each	O
iteration	O
during	O
training	O
as	O
well	O
as	O
average	Metric
precision	Metric
.	O
Here	O
“	O
GHM	Method
-	O
C	O
Standard	O
”	O
is	O
implemented	O
using	O
the	O
original	O
definition	Method
of	Method
gradient	Method
density	Method
and	O
“	O
GHM	Method
-	O
C	O
RU	O
”	O
represents	O
the	O
implementation	O
of	O
region	Method
unit	Method
approximation	Method
algorithm	Method
.	O
The	O
experiments	O
are	O
performed	O
on	O
1080Ti	O
GPUs	O
.	O
We	O
can	O
see	O
that	O
our	O
region	Method
unit	Method
approximation	Method
algorithm	Method
speed	O
up	O
the	O
training	Task
by	O
magnitudes	O
with	O
negligible	O
harm	O
to	O
performance	O
.	O
While	O
compared	O
with	O
CE	Method
,	O
the	O
slow	O
down	O
of	O
GHM	Method
-	O
C	O
loss	O
is	O
also	O
acceptable	O
.	O
Since	O
our	O
loss	O
is	O
not	O
fully	O
GPU	O
implemented	O
now	O
,	O
there	O
is	O
still	O
room	O
for	O
improvement	O
.	O
subsubsection	O
:	O
Comparison	O
with	O
Other	O
Methods	O
:	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
the	O
results	O
using	O
our	O
loss	O
compared	O
with	O
other	O
loss	Method
functions	Method
or	O
sampling	Method
strategy	Method
.	O
Since	O
the	O
reported	O
results	O
on	O
of	O
models	O
using	O
focal	Method
loss	Method
is	O
trained	O
with	O
the	O
input	O
image	O
scale	O
of	O
600	O
pixels	O
,	O
for	O
fair	O
comparison	O
we	O
have	O
re	O
-	O
trained	O
a	O
focal	Method
loss	Method
using	O
a	O
scale	O
of	O
800	O
pixels	O
and	O
keep	O
the	O
best	O
parameters	O
of	O
focal	Method
loss	Method
.	O
We	O
can	O
see	O
our	O
loss	O
has	O
slightly	O
better	O
performance	O
than	O
focal	Metric
loss	Metric
.	O
subsection	O
:	O
GHM	Method
-	O
R	O
Loss	O
subsubsection	O
:	O
Comparison	O
with	O
Other	O
Losses	O
:	O
The	O
experiments	O
here	O
adopt	O
the	O
best	O
configuration	O
of	O
GHM	Method
-	O
C	O
loss	O
for	O
the	O
classification	Task
branch	Task
.	O
So	O
the	O
first	O
baseline	O
is	O
the	O
model	O
(	O
trained	O
using	O
loss	Method
)	O
with	O
an	O
AP	Metric
of	O
35.8	O
showed	O
in	O
GHM	Method
-	O
C	O
loss	O
experiments	O
.	O
We	O
adopts	O
for	O
loss	O
to	O
get	O
comparable	O
results	O
with	O
loss	Metric
and	O
obtain	O
a	O
fair	O
baseline	O
for	O
GHM	Method
-	O
R	O
loss	O
.	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
the	O
baseline	Metric
and	Metric
loss	Metric
as	O
well	O
as	O
GHM	Method
-	O
R	O
loss	O
.	O
We	O
can	O
see	O
a	O
gain	O
of	O
0.7	O
mAP	Metric
based	O
on	O
the	O
loss	Metric
.	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
the	O
details	O
of	O
AP	Metric
at	O
different	O
IoU	O
thresholds	O
.	O
GHM	Method
-	O
R	O
loss	O
slightly	O
lowers	O
the	O
AP@IoU=0.5	Metric
but	O
gains	O
when	O
the	O
threshold	O
is	O
higher	O
,	O
which	O
demonstrates	O
our	O
proposition	O
that	O
the	O
so	O
called	O
easy	O
examples	O
in	O
regression	Task
is	O
important	O
for	O
accurate	Task
localization	Task
.	O
subsubsection	O
:	O
Two	O
-	O
Stage	Method
Detector	Method
:	O
GHM	Method
-	O
R	O
loss	O
for	O
regression	Task
is	O
not	O
limited	O
to	O
one	Method
-	Method
stage	Method
detectors	Method
.	O
So	O
we	O
have	O
done	O
experiments	O
to	O
verify	O
the	O
effect	O
on	O
two	Method
-	Method
stage	Method
detectors	Method
.	O
Our	O
baseline	O
method	O
is	O
faster	O
-	O
RCNN	Method
with	O
Res50	O
-	O
FPN	Method
model	O
using	O
loss	Method
for	O
box	Method
regression	Method
.	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
that	O
GHM	Method
-	O
R	O
loss	O
works	O
for	O
two	Method
-	Method
stage	Method
detector	Method
as	O
well	O
as	O
one	Method
-	Method
stage	Method
detector	Method
.	O
subsection	O
:	O
Main	O
Results	O
We	O
use	O
the	O
32x8d	O
FPN	Method
-	O
ResNext101	O
backbone	O
and	O
RetinaNet	O
model	O
with	O
GHM	Method
-	O
C	O
loss	O
for	O
classification	Task
and	O
GHM	Method
-	O
R	O
loss	O
for	O
box	Task
regression	Task
.	O
The	O
experiments	O
are	O
performed	O
on	O
test	O
-	O
dev	O
set	O
.	O
Table	O
.	O
[	O
reference	O
]	O
shows	O
our	O
main	O
result	O
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O
Our	O
approach	O
achieves	O
excellent	O
performance	O
and	O
outperforms	O
focal	Metric
loss	Metric
in	O
most	O
metrics	O
.	O
section	O
:	O
Conclusion	O
and	O
Discussion	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
the	O
two	O
imbalance	Task
problems	Task
in	O
single	Task
-	Task
stage	Task
detectors	Task
and	O
summarize	O
these	O
two	O
problems	O
to	O
the	O
disharmony	Task
in	Task
gradient	Task
density	Task
with	O
regard	O
to	O
the	O
difficulty	O
of	O
samples	O
.	O
Two	O
loss	Method
functions	Method
,	O
GHM	Method
-	Method
C	Method
and	O
GHM	Method
-	Method
R	Method
are	O
proposed	O
to	O
conquer	O
the	O
disharmony	O
in	O
classification	Task
and	O
bounding	Task
box	Task
regression	Task
respectively	O
.	O
Experiments	O
show	O
that	O
the	O
collaborate	O
with	O
GHM	Method
,	O
the	O
performance	O
of	O
single	Method
-	Method
stage	Method
detector	Method
can	O
easily	O
surpass	O
modern	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
two	Method
-	Method
stage	Method
detectors	Method
like	O
FPN	Method
and	O
Mask	Method
-	Method
RCNN	Method
with	O
the	O
same	O
network	Method
backbone	Method
.	O
Despite	O
of	O
the	O
improvement	O
of	O
select	O
uniform	O
distribution	O
to	O
be	O
the	O
target	O
,	O
we	O
still	O
hold	O
the	O
opinion	O
that	O
the	O
optimal	O
distribution	O
of	O
gradient	O
is	O
hard	O
to	O
define	O
and	O
requires	O
further	O
research	O
.	O
section	O
:	O
Acknowledgments	O
We	O
sincerely	O
appreciate	O
the	O
technical	O
and	O
GPU	O
support	O
from	O
Mr.	O
Changbao	O
Wang	O
,	O
Quanquan	O
Li	O
and	O
Junjie	O
Yan	O
at	O
Sensetime	O
Research	O
.	O
And	O
we	O
also	O
acknowledge	O
the	O
early	O
discussion	O
with	O
Prof	O
.	O
Wanli	O
Ouyang	O
from	O
University	O
of	O
Sydney	O
.	O
bibliography	O
:	O
References	O
