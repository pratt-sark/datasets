Spectral Method
Representations Method
for Method
Convolutional Method
Neural Method
Networks Method
Discrete Method
Fourier Method
transforms Method
provide O
a O
significant O
speedup O
in O
the O
computation Task
of Task
convolutions Task
in O
deep Task
learning Task
. O
In O
this O
work O
, O
we O
demonstrate O
that O
, O
beyond O
its O
advantages O
for O
efficient O
computation Task
, O
the O
spectral O
domain O
also O
provides O
a O
powerful O
representation O
in O
which O
to O
model O
and O
train O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
. O
We O
employ O
spectral Method
representations Method
to O
introduce O
a O
number O
of O
innovations O
to O
CNN Method
design O
. O
First O
, O
we O
propose O
spectral Method
pooling Method
, O
which O
performs O
dimensionality Task
reduction Task
by O
truncating O
the O
representation O
in O
the O
frequency O
domain O
. O
This O
approach O
preserves O
considerably O
more O
information O
per O
parameter O
than O
other O
pooling Method
strategies Method
and O
enables O
flexibility O
in O
the O
choice O
of O
pooling O
output O
dimensionality O
. O
This O
representation O
also O
enables O
a O
new O
form O
of O
stochastic Method
regularization Method
by O
randomized Method
modification Method
of Method
resolution Method
. O
We O
show O
that O
these O
methods O
achieve O
competitive O
results O
on O
classification Task
and O
approximation Task
tasks Task
, O
without O
using O
any O
dropout Method
or Method
max Method
- Method
pooling Method
. O
Finally O
, O
we O
demonstrate O
the O
effectiveness O
of O
complex Method
- Method
coefficient Method
spectral Method
parameterization Method
of Method
convolutional Method
filters Method
. O
While O
this O
leaves O
the O
underlying O
model O
unchanged O
, O
it O
results O
in O
a O
representation O
that O
greatly O
facilitates O
optimization Task
. O
We O
observe O
on O
a O
variety O
of O
popular O
CNN Method
configurations O
that O
this O
leads O
to O
significantly O
faster O
convergence Metric
during O
training Task
. O
Spectral Method
Representations Method
for Method
Convolutional Method
Neural Method
Networks Method
Oren O
Rippel O
, O
Jasper O
Snoek O
, O
Ryan O
P. O
Adams O
Reading O
Group O
Presenter O
: O
David O
Carlson O
July O
17 O
, O
2015 O
David O
Carlson O
Spectral O
Pooling O
July O
17 O
, O
2015 O
1 O
/ O
11 O
Introduction O
Convolutional Method
Neural Method
Nets Method
( O
CNNs Method
) O
have O
been O
wildly O
successful O
for O
image Task
classification Task
tasks Task
However O
: O
They O
are O
computationally O
expensive O
Any O
pooling Method
step Method
reduces O
the O
dimensionality Metric
by O
at O
least O
4 O
Previous O
work O
suggests O
using O
FFTs Method
to O
compute O
the O
convolutional O
mask O
– O
even O
for O
small O
filter O
sizes O
– O
to O
help O
with O
computational O
time O
This O
work O
suggests O
using O
FFTs Method
, O
and O
then O
performing O
pooling Method
and Method
learning Method
in O
the O
Fourier Method
Transform Method
domain Method
Introduced O
spectral Method
pooling Method
can O
reduce O
dimensionality Metric
by O
an O
user O
- O
defined O
amount O
( O
reduces O
slower O
than O
traditional O
pooling Method
steps Method
) O
Spectral Method
parameterization Method
defines O
the O
CNN Method
filters O
in O
the O
frequency O
domains O
, O
which O
empirically O
converges O
2 O
- O
5 O
times O
faster O
than O
the O
standard O
spatial Method
representation Method
with O
the O
same O
result O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
2 O
/ O
11 O
Reminder O
of O
Fourier O
Properties O
Convolution Method
using O
DFT Method
: O
F O
( O
x O
∗ O
f O
) O
= O
F O
( O
x O
) O
F O
( O
f O
) O
Parseval Method
’s Method
Theorem Method
: O
||x O
− O
x̂ O
||22 O
= O
||F O
( O
x O
) O
−F O
( O
x̂ O
) O
||22 O
Conjugate O
symmetry O
forces O
R O
representation O
– O
if O
x O
∈ O
RM×N O
, O
then O
y O
= O
F O
( O
x O
) O
∈ O
CM×N O
has O
: O
ymn O
= O
y O
( O
M−m O
) O
mod O
( O
M O
),( O
N−n O
) O
mod O
( O
N O
) O
This O
adds O
constraints O
on O
conjugate O
symmetry O
for O
filters O
Differentiation Task
is O
straightforward O
because O
the O
Fourier Method
transform Method
is O
an O
( O
orthonormal O
) O
linear Method
operator Method
δR O
δx O
= O
F−1 O
( O
δR O
δy O
) O
David O
Carlson O
Spectral O
Pooling O
July O
17 O
, O
2015 O
3 O
/ O
11 O
( O
a O
) O
DFT Method
basis Method
functions Method
. O
( O
b O
) O
Examples O
of O
input O
- O
transform O
pairs O
. O
( O
c O
) O
Conjugate O
Symm O
. O
Given O
an O
input O
x O
2 O
CM O
⇥ O
N O
( O
we O
address O
the O
constraint O
of O
real O
inputs O
in O
Subsection O
2.1 O
) O
, O
its O
2D O
DFT O
F O
( O
x O
) O
2 O
CM O
⇥ O
N O
is O
given O
by O
F O
( O
x O
) O
hw O
= O
1p O
MN O
M O
1X O
m=0 O
N O
1X O
n=0 O
xmne O
2 O
⇡ O
i O
( O
mhM O
+ O
nwN O
) O
8h O
2 O
{ O
0 O
, O
. O
. O
. O
, O
M O
1 O
} O
, O
8w O
2 O
{ O
0 O
, O
. O
. O
. O
, O
N O
1 O
} O
. O
The O
DFT Method
is O
linear O
and O
unitary O
, O
and O
so O
its O
inverse Method
transform Method
is O
given O
by O
F O
1 O
( O
· O
) O
= O
F O
( O
· O
) O
⇤ O
, O
namely O
the O
conjugate O
of O
the O
transform O
itself O
. O
Intuitively O
, O
the O
DFT O
coefficients O
resulting O
from O
projections O
onto O
the O
different O
frequencies O
can O
be O
thought O
of O
as O
measures O
of O
correlation O
of O
the O
input O
with O
basis O
functions O
of O
various O
length O
- O
scales O
. O
See O
Figure O
1 O
( O
a O
) O
for O
a O
visualization O
of O
the O
DFT Method
basis Method
functions Method
, O
and O
Figure O
1 O
( O
b O
) O
for O
examples O
of O
inputfrequency O
map O
pairs O
. O
The O
widespread O
deployment O
of O
the O
DFT Method
can O
be O
partially O
attributed O
to O
the O
development O
of O
the O
Fast Method
Fourier Method
Transform Method
( Method
FFT Method
) Method
, O
a O
mainstay O
of O
signal Method
processing Method
and O
a O
standard O
component O
of O
most O
math Method
libraries Method
. O
The O
FFT Method
is O
an O
efficient O
implementation O
of O
the O
DFT Method
with O
time Metric
complexity Metric
O O
( O
MN O
log O
( O
MN O
) O
) O
. O
Convolution Method
using O
DFT Method
One O
powerful O
property O
of O
frequency Task
analysis Task
is O
the O
operator O
duality O
between O
convolution Method
in O
the O
spatial O
domain O
and O
element Method
- Method
wise Method
multiplication Method
in O
the O
spectral O
domain O
. O
Namely O
, O
given O
two O
inputs O
x O
, O
f O
2 O
RM O
⇥ O
N O
, O
we O
may O
write O
F O
( O
x O
⇤ O
f O
) O
= O
F O
( O
x O
) O
F O
( O
f O
) O
( O
1 O
) O
where O
by O
⇤ O
we O
denote O
a O
convolution Method
and O
by O
an O
element O
- O
wise O
product O
. O
Approximation Metric
error Metric
The O
unitarity O
of O
the O
Fourier Method
basis Method
makes O
it O
convenient O
for O
the O
analysis Task
of Task
approximation Task
loss Task
. O
More O
specifically O
, O
Parseval Method
’s Method
Theorem Method
links O
the O
` O
2 O
loss O
between O
any O
input O
x O
and O
its O
approximation O
x̂ O
to O
the O
corresponding O
loss O
in O
the O
frequency O
domain O
: O
kx O
x̂k22 O
= O
kF O
( O
x O
) O
F O
( O
x̂ O
) O
k22 O
. O
( O
2 O
) O
An O
equivalent O
statement O
also O
holds O
for O
the O
inverse Method
DFT Method
operator Method
. O
This O
allows O
us O
to O
quickly O
assess O
how O
an O
input O
is O
affected O
by O
any O
distortion O
we O
might O
make O
to O
its O
frequency Method
representation Method
. O
2.1 O
Conjugate O
symmetry O
constraints O
In O
the O
following O
sections O
of O
the O
paper O
, O
we O
will O
propagate O
signals O
and O
their O
gradients O
through O
DFT Method
and Method
inverse Method
DFT Method
layers Method
. O
In O
these O
layers O
, O
we O
will O
represent O
the O
frequency O
domain O
in O
the O
complex O
field O
. O
3 O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
4 O
/ O
11 O
Spectral Method
Pooling Method
The O
first O
proposed O
idea O
is O
spectral Method
pooling Method
: O
However O
, O
for O
all O
layers O
apart O
from O
these O
, O
we O
would O
like O
to O
ensure O
that O
both O
the O
signal O
and O
its O
gradient O
are O
constrained O
to O
the O
reals O
. O
A O
necessary O
and O
sufficient O
condition O
to O
achieve O
this O
is O
conjugate O
symmetry O
in O
the O
frequency O
domain O
. O
Namely O
, O
for O
any O
transform O
y O
= O
F O
( O
x O
) O
of O
some O
input O
x O
, O
it O
must O
hold O
that O
ymn O
= O
y O
⇤ O
( O
M O
m O
) O
modM O
,( O
N O
n O
) O
modN O
8 O
m O
2 O
{ O
0 O
, O
. O
. O
. O
, O
M O
1 O
} O
, O
8n O
2 O
{ O
0 O
, O
. O
. O
. O
, O
N O
1 O
} O
. O
( O
3 O
) O
Thus O
, O
intuitively O
, O
given O
the O
left O
half O
of O
our O
frequency Method
map Method
, O
the O
diminished O
number O
of O
degrees O
of O
freedom O
allows O
us O
to O
reconstruct O
the O
right O
. O
In O
effect O
, O
this O
allows O
us O
to O
store O
approximately O
half O
the O
parameters O
that O
would O
otherwise O
be O
necessary O
. O
Note O
, O
however O
, O
that O
this O
does O
not O
reduce O
the O
effective Metric
dimensionality Metric
, O
since O
each O
element O
consists O
of O
real O
and O
imaginary O
components O
. O
The O
conjugate O
symmetry O
constraints O
are O
visualized O
in O
Figure O
1 O
( O
c O
) O
. O
Given O
a O
real O
input O
, O
its O
DFT Method
will O
necessarily O
meet O
these O
. O
This O
symmetry O
can O
be O
observed O
in O
the O
frequency Method
representations Method
of O
the O
examples O
in O
Figure O
1 O
( O
b O
) O
. O
However O
, O
since O
we O
seek O
to O
optimize O
over O
parameters O
embedded O
directly O
in O
the O
frequency O
domain O
, O
we O
need O
to O
pay O
close O
attention O
to O
ensure O
the O
conjugate O
symmetry O
constraints O
are O
enforced O
upon O
inversion O
back O
to O
the O
spatial O
domain O
( O
see O
Subsection O
2.2 O
) O
. O
2.2 O
Differentiation Task
Here O
we O
discuss O
how O
to O
propagate O
the O
gradient O
through O
a O
Fourier Method
transform Method
layer Method
. O
This O
analysis O
can O
be O
similarly O
applied O
to O
the O
inverse Method
DFT Method
layer Method
. O
Define O
x O
2 O
RM O
⇥ O
N O
and O
y O
= O
F O
( O
x O
) O
to O
be O
the O
input O
and O
output O
of O
a O
DFT O
layer O
respectively O
, O
and O
R O
: O
RM O
⇥ O
N O
! O
R O
a O
real O
- O
valued O
loss O
function O
applied O
to O
y O
which O
can O
be O
considered O
as O
the O
remainder O
of O
the O
forward O
pass O
. O
Since O
the O
DFT Method
is O
a O
linear Method
operator Method
, O
its O
gradient O
is O
simply O
the O
transformation O
matrix O
itself O
. O
During O
back Task
- Task
propagation Task
, O
then O
, O
this O
gradient O
is O
conjugated O
, O
and O
this O
, O
by O
DFT Method
unitarity Method
, O
corresponds O
to O
the O
application O
of O
the O
inverse Method
transform Method
: O
@R O
@x O
= O
F O
1 O
✓ O
@R O
@y O
◆ O
. O
( O
4 O
) O
There O
is O
an O
intricacy O
that O
makes O
matters O
a O
bit O
more O
complicated O
. O
Namely O
, O
the O
conjugate O
symmetry O
condition O
discussed O
in O
Subsection O
2.1 O
introduces O
redundancy O
. O
Inspecting O
the O
conjugate O
symmetry O
constraints O
in O
Equation O
( O
3 O
) O
, O
we O
note O
their O
enforcement O
of O
the O
special O
case O
y00 O
2 O
R O
for O
N O
odd O
, O
and O
y00 O
, O
yN O
2 O
, O
0 O
, O
y0 O
, O
N2 O
, O
yN O
2 O
, O
N O
2 O
2 O
R O
for O
N O
even O
. O
For O
all O
other O
indices O
they O
enforce O
conjugate O
equality O
of O
pairs O
of O
distinct O
elements O
. O
These O
conditions O
imply O
that O
the O
number O
of O
unconstrained O
parameters O
is O
about O
half O
the O
map O
in O
its O
entirety O
. O
3 O
Spectral Method
Pooling Method
The O
choice O
of O
a O
pooling Method
technique Method
boils O
down O
to O
the O
selection O
of O
an O
appropriate O
set O
of O
basis O
functions O
to O
project O
onto O
, O
and O
some O
truncation O
of O
this O
representation O
to O
establish O
a O
lower Method
- Method
dimensionality Method
approximation Method
to O
the O
original O
input O
. O
The O
idea O
behind O
spectral Method
pooling Method
stems O
from O
the O
observation O
that O
the O
frequency O
domain O
provides O
an O
ideal O
basis O
for O
inputs O
with O
spatial O
structure O
. O
We O
first O
discuss O
the O
technical O
details O
of O
this O
approach O
, O
and O
then O
its O
advantages O
. O
Spectral Method
pooling Method
is O
straightforward O
to O
understand O
and O
to O
implement O
. O
We O
assume O
we O
are O
given O
an O
input O
x O
2 O
RM O
⇥ O
N O
, O
and O
some O
desired O
output O
map O
dimensionality O
H O
⇥ O
W O
. O
First O
, O
we O
compute O
the O
discrete Method
Fourier Method
transform Method
of O
the O
input O
into O
the O
frequency O
domain O
as O
y O
= O
F O
( O
x O
) O
2 O
CM O
⇥ O
N O
, O
and O
assume O
that O
the O
DC O
component O
has O
been O
shifted O
to O
the O
center O
of O
the O
domain O
as O
is O
standard O
practice O
. O
We O
then O
crop O
the O
frequency Method
representation Method
by O
maintaining O
only O
the O
central O
H O
⇥ O
W O
submatrix O
of O
frequencies O
, O
which O
we O
denote O
as O
ŷ O
2 O
CH O
⇥ O
W O
. O
Finally O
, O
we O
map O
this O
approximation O
back O
into O
the O
spatial O
domain O
by O
taking O
Algorithm O
1 O
: O
Spectral Method
pooling Method
Input O
: O
Map O
x O
2 O
RM O
⇥ O
N O
, O
output O
size O
H O
⇥ O
W O
Output O
: O
Pooled O
map O
x̂ O
2 O
RH O
⇥ O
W O
1 O
: O
y O
F O
( O
x O
) O
2 O
: O
ŷ O
CROPSPECTRUM O
( O
y O
, O
H O
⇥ O
W O
) O
3 O
: O
ŷ O
TREATCORNERCASES O
( O
ŷ O
) O
4 O
: O
x̂ O
F O
1 O
( O
ŷ O
) O
Algorithm O
2 O
: O
Spectral Method
pooling Method
back Method
- Method
propagation Method
Input O
: O
Gradient O
w.r.t O
output O
@R O
@x̂ O
Output O
: O
Gradient O
w.r.t O
input O
@R O
@x O
1 O
: O
ẑ O
F O
@R O
@x̂ O
2 O
: O
ẑ O
REMOVEREDUNDANCY O
( O
ẑ O
) O
3 O
: O
z O
PADSPECTRUM O
( O
ẑ O
, O
M O
⇥ O
N O
) O
4 O
: O
z O
RECOVERMAP O
( O
z O
) O
5 O
: O
@R O
@x O
F O
1 O
( O
z O
) O
4 O
Very O
simple O
to O
understand O
, O
not O
as O
obvious O
why O
this O
is O
a O
good O
idea O
David O
Carlson O
Spectral O
Pooling O
July O
17 O
, O
2015 O
5 O
/ O
11 O
its O
inverse Method
DFT Method
as O
x̂ O
= O
F O
1 O
( O
ŷ O
) O
2 O
RH O
⇥ O
W O
. O
These O
steps O
are O
listed O
in O
Algorithm O
1 O
. O
Note O
that O
some O
of O
the O
conjugate O
symmetry O
special O
cases O
described O
in O
Subsection O
2.2 O
might O
be O
broken O
by O
this O
truncation O
. O
As O
such O
, O
to O
ensure O
that O
x̂ O
is O
real O
- O
valued O
, O
we O
must O
treat O
these O
individually O
with O
TREATCORNERCASES O
, O
which O
can O
be O
found O
in O
the O
supplementary O
material O
. O
Figure O
2 O
demonstrates O
the O
effect O
of O
this O
pooling O
for O
various O
choices O
of O
H O
⇥ O
W O
. O
The O
backpropagation Method
procedure Method
is O
quite O
intuitive O
, O
and O
can O
be O
found O
in O
Algorithm O
2 O
( O
REMOVEREDUNDANCY O
and O
RECOVERMAP Task
can O
be O
found O
in O
the O
supplementary O
material O
) O
. O
In O
Subsection O
2.2 O
, O
we O
addressed O
the O
nuances O
of O
differentiating O
through O
DFT Method
and Method
inverse Method
DFT Method
layers Method
. O
Apart O
from O
these O
, O
the O
last O
component O
left O
undiscussed O
is O
differentiation O
through O
the O
truncation Method
of Method
the Method
frequency Method
matrix Method
, O
but O
this O
corresponds O
to O
a O
simple O
zero Method
- Method
padding Method
of Method
the Method
gradient Method
maps Method
to O
the O
appropriate O
dimensions O
. O
In O
practice O
, O
the O
DFTs Method
are O
the O
computational Method
bottlenecks Method
of O
spectral Method
pooling Method
. O
However O
, O
we O
note O
that O
in O
convolutional Method
neural Method
networks Method
that O
employ O
FFTs Method
for O
convolution Method
computation O
, O
spectral Method
pooling Method
can O
be O
implemented O
at O
a O
negligible O
additional O
computational Metric
cost Metric
, O
since O
the O
DFT Method
is O
performed O
regardless O
. O
We O
proceed O
to O
discuss O
a O
number O
of O
properties O
of O
spectral Method
pooling Method
, O
which O
we O
then O
test O
comprehensively O
in O
Section O
5 O
. O
3.1 O
Information Method
preservation Method
Spectral Method
pooling Method
can O
significantly O
increase O
the O
amount O
of O
retained O
information O
relative O
to O
max Method
- Method
pooling Method
in O
two O
distinct O
ways O
. O
First O
, O
its O
representation O
maintains O
more O
information O
for O
the O
same O
number O
of O
degrees O
of O
freedom O
. O
Spectral Method
pooling Method
reduces O
the O
information Metric
capacity Metric
by O
tuning O
the O
resolution O
of O
the O
input O
precisely O
to O
match O
the O
desired O
output O
dimensionality O
. O
This O
operation O
can O
also O
be O
viewed O
as O
linear Method
low Method
- Method
pass Method
filtering Method
and O
it O
exploits O
the O
non O
- O
uniformity O
of O
the O
spectral O
density O
of O
the O
data O
with O
respect O
to O
frequency O
. O
That O
is O
, O
that O
the O
power O
spectra O
of O
inputs O
with O
spatial O
structure O
, O
such O
as O
natural O
images O
, O
carry O
most O
of O
their O
mass O
on O
lower O
frequencies O
. O
As O
such O
, O
since O
the O
amplitudes O
of O
the O
higher O
frequencies O
tend O
to O
be O
small O
, O
Parseval O
’s O
theorem O
from O
Section O
2 O
informs O
us O
that O
their O
elimination O
will O
result O
in O
a O
representation O
that O
minimizes O
the O
` Metric
2 Metric
distortion Metric
after O
reconstruction Task
. O
Second O
, O
spectral Method
pooling Method
does O
not O
suffer O
from O
the O
sharp O
reduction Metric
in Metric
output Metric
dimensionality Metric
exhibited O
by O
other O
pooling Method
techniques Method
. O
More O
specifically O
, O
for O
stride Method
- Method
based Method
pooling Method
strategies Method
such O
as O
max Method
pooling Method
, O
the O
number O
of O
degrees O
of O
freedom O
of O
two O
- O
dimensional O
inputs O
is O
reduced O
by O
at O
least O
75 O
% O
as O
a O
function O
of O
stride O
. O
In O
contrast O
, O
spectral Method
pooling Method
allows O
us O
to O
specify O
any O
arbitrary O
output O
dimensionality O
, O
and O
thus O
allows O
us O
to O
reduce O
the O
map O
size O
gradually O
as O
a O
function O
of O
layer O
. O
5 O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
6 O
/ O
11 O
Spectral Method
Parameterization Method
of Method
filters Method
Seek O
to O
learn O
a O
filter O
f O
∈ O
CH×W O
that O
is O
parameterized O
in O
the O
frequency O
space O
If O
conjugate O
symmetry O
is O
upheld O
, O
then O
F−1 O
( O
f O
) O
∈ O
RH×W O
Because O
the O
Fourier Method
transform Method
is O
an O
( O
invertible O
) O
linear Method
operator Method
, O
the O
local O
minima O
and O
gradients O
are O
the O
same O
– O
only O
a O
change O
of O
basis O
However O
, O
the O
recent O
optimization Method
methods Method
( O
ADAgrad Method
, O
RMSprop Method
, O
ADAM Method
) O
use O
diagonal Method
preconditioners Method
, O
so O
a O
different O
basis O
can O
give O
vastly O
different O
performance O
However O
, O
for O
all O
layers O
apart O
from O
these O
, O
we O
would O
like O
to O
ensure O
that O
both O
the O
signal O
and O
its O
gradient O
are O
constrained O
to O
the O
reals O
. O
A O
necessary O
and O
sufficient O
condition O
to O
achieve O
this O
is O
conjugate O
symmetry O
in O
the O
frequency O
domain O
. O
Namely O
, O
for O
any O
transform O
y O
= O
F O
( O
x O
) O
of O
some O
input O
x O
, O
it O
must O
hold O
that O
ymn O
= O
y O
⇤ O
( O
M O
m O
) O
modM O
,( O
N O
n O
) O
modN O
8 O
m O
2 O
{ O
0 O
, O
. O
. O
. O
, O
M O
1 O
} O
, O
8n O
2 O
{ O
0 O
, O
. O
. O
. O
, O
N O
1 O
} O
. O
( O
3 O
) O
Thus O
, O
intuitively O
, O
given O
the O
left O
half O
of O
our O
frequency Method
map Method
, O
the O
diminished O
number O
of O
degrees O
of O
freedom O
allows O
us O
to O
reconstruct O
the O
right O
. O
In O
effect O
, O
this O
allows O
us O
to O
store O
approximately O
half O
the O
parameters O
that O
would O
otherwise O
be O
necessary O
. O
Note O
, O
however O
, O
that O
this O
does O
not O
reduce O
the O
effective Metric
dimensionality Metric
, O
since O
each O
element O
consists O
of O
real O
and O
imaginary O
components O
. O
The O
conjugate O
symmetry O
constraints O
are O
visualized O
in O
Figure O
1 O
( O
c O
) O
. O
Given O
a O
real O
input O
, O
its O
DFT Method
will O
necessarily O
meet O
these O
. O
This O
symmetry O
can O
be O
observed O
in O
the O
frequency Method
representations Method
of O
the O
examples O
in O
Figure O
1 O
( O
b O
) O
. O
However O
, O
since O
we O
seek O
to O
optimize O
over O
parameters O
embedded O
directly O
in O
the O
frequency O
domain O
, O
we O
need O
to O
pay O
close O
attention O
to O
ensure O
the O
conjugate O
symmetry O
constraints O
are O
enforced O
upon O
inversion O
back O
to O
the O
spatial O
domain O
( O
see O
Subsection O
2.2 O
) O
. O
2.2 O
Differentiation Task
Here O
we O
discuss O
how O
to O
propagate O
the O
gradient O
through O
a O
Fourier Method
transform Method
layer Method
. O
This O
analysis O
can O
be O
similarly O
applied O
to O
the O
inverse Method
DFT Method
layer Method
. O
Define O
x O
2 O
RM O
⇥ O
N O
and O
y O
= O
F O
( O
x O
) O
to O
be O
the O
input O
and O
output O
of O
a O
DFT O
layer O
respectively O
, O
and O
R O
: O
RM O
⇥ O
N O
! O
R O
a O
real O
- O
valued O
loss O
function O
applied O
to O
y O
which O
can O
be O
considered O
as O
the O
remainder O
of O
the O
forward O
pass O
. O
Since O
the O
DFT Method
is O
a O
linear Method
operator Method
, O
its O
gradient O
is O
simply O
the O
transformation O
matrix O
itself O
. O
During O
back Task
- Task
propagation Task
, O
then O
, O
this O
gradient O
is O
conjugated O
, O
and O
this O
, O
by O
DFT Method
unitarity Method
, O
corresponds O
to O
the O
application O
of O
the O
inverse Method
transform Method
: O
@R O
@x O
= O
F O
1 O
✓ O
@R O
@y O
◆ O
. O
( O
4 O
) O
There O
is O
an O
intricacy O
that O
makes O
matters O
a O
bit O
more O
complicated O
. O
Namely O
, O
the O
conjugate O
symmetry O
condition O
discussed O
in O
Subsection O
2.1 O
introduces O
redundancy O
. O
Inspecting O
the O
conjugate O
symmetry O
constraints O
in O
Equation O
( O
3 O
) O
, O
we O
note O
their O
enforcement O
of O
the O
special O
case O
y00 O
2 O
R O
for O
N O
odd O
, O
and O
y00 O
, O
yN O
2 O
, O
0 O
, O
y0 O
, O
N2 O
, O
yN O
2 O
, O
N O
2 O
2 O
R O
for O
N O
even O
. O
For O
all O
other O
indices O
they O
enforce O
conjugate O
equality O
of O
pairs O
of O
distinct O
elements O
. O
These O
conditions O
imply O
that O
the O
number O
of O
unconstrained O
parameters O
is O
about O
half O
the O
map O
in O
its O
entirety O
. O
3 O
Spectral Method
Pooling Method
The O
choice O
of O
a O
pooling Method
technique Method
boils O
down O
to O
the O
selection O
of O
an O
appropriate O
set O
of O
basis O
functions O
to O
project O
onto O
, O
and O
some O
truncation O
of O
this O
representation O
to O
establish O
a O
lower Method
- Method
dimensionality Method
approximation Method
to O
the O
original O
input O
. O
The O
idea O
behind O
spectral Method
pooling Method
stems O
from O
the O
observation O
that O
the O
frequency O
domain O
provid O
s O
an O
ideal O
basis O
for O
inputs O
with O
spatial O
structure O
. O
We O
first O
discuss O
the O
technical O
details O
of O
this O
approach O
, O
and O
then O
its O
advantages O
. O
Spectral Method
pooling Method
is O
straightforward O
to O
understand O
and O
to O
implement O
. O
We O
assume O
we O
are O
given O
an O
input O
x O
2 O
RM O
⇥ O
N O
, O
and O
some O
desired O
output O
map O
dimensionality O
H O
⇥ O
W O
. O
First O
, O
we O
compute O
the O
d Method
screte Method
Fourier Method
transform Method
of O
the O
input O
into O
the O
frequency O
domain O
as O
y O
= O
F O
( O
x O
) O
2 O
CM O
⇥ O
N O
, O
and O
assume O
that O
the O
DC O
component O
has O
been O
shifted O
to O
the O
center O
of O
the O
domain O
as O
is O
standard O
practice O
. O
We O
then O
crop O
the O
frequency Method
representation Method
by O
maintaining O
only O
the O
central O
H O
⇥ O
W O
submatrix O
of O
frequencies O
, O
which O
we O
denote O
as O
ŷ O
2 O
CH O
⇥ O
W O
. O
Finally O
, O
we O
map O
thi O
approximation Method
ba Method
k Method
into O
the O
spatial O
domain O
by O
taking O
Algorithm O
1 O
: O
Spectral Method
pooling Method
Input O
: O
Map O
x O
2 O
RM O
⇥ O
N O
, O
output O
size O
H O
⇥ O
W O
Output O
: O
Pooled O
map O
x̂ O
2 O
RH O
⇥ O
W O
1 O
: O
y O
F O
( O
x O
) O
2 O
: O
ŷ O
CROPSPECTRUM O
( O
y O
, O
H O
⇥ O
W O
) O
3 O
: O
ŷ O
TREATCORNERCASES O
( O
ŷ O
) O
4 O
: O
x̂ O
F O
1 O
( O
ŷ O
) O
Algorithm O
2 O
: O
Spectral Method
pooling Method
back Method
- Method
propagation Method
Input O
: O
Gradient O
w.r.t O
output O
@R O
@x̂ O
Output O
: O
Gradient O
w.r.t O
input O
@R O
@x O
1 O
: O
ẑ O
F O
@R O
@x̂ O
2 O
: O
ẑ O
REMOVEREDUNDANCY O
( O
ẑ O
) O
3 O
: O
z O
PADSPECTRUM O
( O
ẑ O
, O
M O
⇥ O
N O
) O
4 O
: O
z O
RECOVERMAP O
( O
z O
) O
5 O
: O
@R O
@x O
F O
1 O
( O
z O
) O
4David O
Carlson O
Spectral O
Pooling O
July O
17 O
, O
2015 O
7 O
/ O
11 O
( O
a O
) O
Filters O
over O
time O
. O
( O
b O
) O
Sparsity O
patterns O
. O
10 O
4 O
10 O
3 O
10 O
2 O
10 O
1 O
Element O
momentum O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.0 O
N O
or O
m O
al O
iz O
ed O
co O
un O
t O
Spatial O
Spectral O
( O
c O
) O
Momenta O
distributions O
. O
3.2 O
Regularization Task
via O
resolution Method
corruption Method
We O
note O
that O
the O
low O
- O
pass O
filtering O
radii O
, O
say O
RH O
and O
RW O
, O
can O
be O
chosen O
to O
be O
smaller O
than O
the O
output O
map O
dimensionalities O
H O
, O
W O
. O
Namely O
, O
while O
we O
truncate O
our O
input O
frequency O
map O
to O
size O
H O
⇥ O
W O
, O
we O
can O
further O
zero O
- O
out O
all O
frequencies O
outside O
the O
central O
RH O
⇥ O
RW O
square O
. O
While O
this O
maintains O
the O
output O
dimensionality O
H O
⇥ O
W O
of O
the O
input O
domain O
after O
applying O
the O
inverse Method
DFT Method
, O
it O
effectively O
reduces O
the O
resolution O
of O
the O
output O
. O
This O
can O
be O
seen O
in O
Figure O
2 O
. O
This O
allows O
us O
to O
introduce O
regularization O
in O
the O
form O
of O
random Method
resolution Method
reduction Method
. O
We O
apply O
this O
stochastically O
by O
assigning O
a O
distribution O
pR O
( O
· O
) O
on O
the O
frequency O
truncation O
radius O
( O
for O
simplicity O
we O
apply O
the O
same O
truncation O
on O
both O
axes O
) O
, O
sampling O
from O
this O
a O
random O
radius O
at O
each O
iteration O
, O
and O
wiping O
out O
all O
frequencies O
outside O
the O
square O
of O
that O
size O
. O
Note O
that O
this O
can O
be O
regarded O
as O
an O
application O
of O
nested Method
dropout Method
( O
Rippel O
et O
al O
. O
, O
2014 O
) O
on O
both O
dimensions O
of O
the O
frequency Method
decomposition Method
of O
our O
input O
. O
In O
practice O
, O
we O
have O
had O
success O
choosing O
pR O
( O
· O
) O
= O
U O
[ O
Hmin O
, O
H O
] O
( O
· O
) O
, O
i.e. O
, O
a O
uniform O
distribution O
stretching O
from O
some O
minimum O
value O
all O
the O
way O
up O
to O
the O
highest O
possible O
resolution O
. O
4 O
Spectral O
Parametrization O
of O
CNNs Method
Here O
we O
demonstrate O
how O
to O
learn O
the O
filters O
of O
CNNs Method
directly O
in O
their O
frequency Method
domain Method
representations Method
. O
This O
offers O
significant O
advantages O
over O
the O
traditional O
spatial Method
representation Method
, O
which O
we O
show O
empirically O
in O
Section O
5 O
. O
Let O
us O
assume O
that O
for O
some O
layer O
of O
our O
convolutional Method
neural Method
network Method
we O
seek O
to O
learn O
filters O
of O
size O
H O
⇥ O
W O
. O
To O
do O
this O
, O
we O
parametrize O
each O
filter O
f O
2 O
CH O
⇥ O
W O
in O
our O
network O
directly O
in O
the O
frequency O
domain O
. O
To O
attain O
its O
spatial Method
representation Method
, O
we O
simply O
compute O
its O
inverse Method
DFT Method
as O
F O
1 O
( O
f O
) O
2 O
RH O
⇥ O
W O
. O
From O
this O
point O
on O
, O
we O
proceed O
as O
we O
would O
for O
any O
standard O
CNN Method
by O
computing O
the O
convolution Method
of O
the O
filter O
with O
inputs O
in O
our O
mini O
- O
batch O
, O
and O
so O
on O
. O
The O
back Method
- Method
propagation Method
through O
the O
inverse Method
DFT Method
is O
virtually O
identical O
to O
the O
one O
of O
spectral Method
pooling Method
described O
in O
Section O
3 O
. O
We O
compute O
the O
gradient O
as O
outlined O
in O
Subsection O
2.2 O
, O
being O
careful O
to O
obey O
the O
conjugate O
symmetry O
constraints O
discussed O
in O
Subsection O
2.1 O
. O
We O
emphasize O
that O
this O
approach O
does O
not O
change O
the O
underlying O
CNN Method
model O
in O
any O
way O
— O
only O
the O
way O
in O
which O
it O
is O
parametrized O
. O
Hence O
, O
this O
only O
affects O
the O
way O
the O
solution O
space O
is O
explored O
by O
the O
optimization Method
procedure Method
. O
4.1 O
Leveraging O
filter Method
structure Method
This O
idea O
exploits O
the O
observation O
that O
CNN Method
filters O
have O
a O
very O
characteristic O
structure O
that O
reappears O
across O
data O
sets O
and O
problem O
domains O
. O
That O
is O
, O
CNN Method
weights O
can O
typically O
be O
captured O
with O
a O
small O
6 O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
8 O
/ O
11 O
Experiments O
Used O
CIFAR Material
- Material
10 Material
, O
CIFAR Material
- Material
100 Material
, O
and O
ImageNet O
Used O
the O
network O
: O
e O
2 O
e O
1 O
e0 O
e1 O
Si O
ze O
5 O
e O
1 O
e0 O
e1 O
e O
2 O
e O
1 O
e0 O
e1 O
Spatial O
Spectral O
0 O
40 O
80 O
120 O
160 O
200 O
Deep O
e O
2 O
e O
1 O
e0 O
e1 O
Si O
ze O
3 O
0 O
40 O
80 O
120 O
160 O
200 O
Generic O
e O
1 O
e0 O
e1 O
0 O
30 O
60 O
90 O
120 O
150 O
Sp O
. O
Pooling O
e O
2 O
e O
1 O
e0 O
e1 O
( O
a O
) O
Training O
curves O
. O
Architecture O
Filtersize O
Speedup O
factor O
Deep O
( O
7 O
) O
3 O
⇥ O
3 O
2.2 O
Deep O
( O
7 O
) O
5 O
⇥ O
5 O
4.8 O
Generic O
( O
6 O
) O
3 O
⇥ O
3 O
2.2 O
Generic O
( O
6 O
) O
5 O
⇥ O
5 O
5.1 O
Sp O
. O
Pooling O
( O
5 O
) O
3 O
⇥ O
3 O
2.4 O
Sp O
. O
Pooling O
( O
5 O
) O
5 O
⇥ O
5 O
4.8 O
( O
b O
) O
Speedup O
factors O
. O
Figure O
5 O
: O
Optimization O
of O
CNNs Method
via O
spectral Method
parametrization Method
. O
All O
experiments O
include O
data Task
augmentation Task
. O
( O
a O
) O
Training O
curves O
for O
the O
various O
experiments O
. O
The O
remainder O
of O
the O
optimization Task
past O
the O
matching O
point O
is O
marked O
in O
light O
blue O
. O
The O
red O
diamonds O
indicate O
the O
relative O
epochs O
in O
which O
the O
asymptotic Metric
error Metric
rate Metric
of O
the O
spatial Method
approach Method
is O
achieved O
. O
( O
b O
) O
Speedup O
factors O
for O
different O
architectures O
and O
filter O
sizes O
. O
A O
non O
- O
negligible O
speedup O
is O
observed O
even O
for O
tiny O
3 O
⇥ O
3 O
filters O
. O
Classification Method
with O
convolutional Method
neural Method
networks Method
We O
test O
spectral Method
pooling Method
on O
different O
classification Task
tasks O
. O
We O
hyperparametrize Method
and O
optimize O
the O
following O
CNN Method
architecture O
: O
C96 O
+ O
32m3 O
⇥ O
3 O
! O
SP#b O
Hmc O
⇥ O
b O
Hmc O
M O
m=1 O
! O
C96 O
+ O
32M1 O
⇥ O
1 O
! O
C O
10 O
/ O
100 Material
1 O
⇥ O
1 O
! O
GA O
! O
Softmax Method
( O
5 O
) O
Here O
, O
by O
CFS Method
we O
denote O
a O
convolutional Method
layer Method
with O
F O
filters O
each O
of O
size O
S O
, O
by O
SP#S Method
a O
spectral Method
pooling Method
layer Method
with O
output O
dimensionality O
S O
, O
and O
GA Method
the O
global Method
averaging Method
layer Method
described O
in O
Lin O
et O
al O
. O
( O
2013 O
) O
. O
We O
upper O
- O
bound O
the O
number O
of O
filters O
per O
layer O
as O
288 O
. O
Every O
convolution Method
and O
pooling O
layer O
is O
followed O
by O
a O
ReLU Method
nonlinearity Method
. O
We O
let O
Hm O
be O
the O
height O
of O
the O
map O
of O
layer O
m. O
Hence O
, O
each O
spectral Method
pooling Method
layer Method
reduces O
each O
output O
map O
dimension O
by O
factor O
2 O
( O
0 O
, O
1 O
) O
. O
We O
assign O
frequency O
dropout O
distribution O
pR O
( O
· O
; O
m O
, O
↵ O
, O
) O
= O
U O
[ O
bcmHmc O
, O
Hm O
] O
( O
· O
) O
for O
layer O
m O
, O
total O
layers O
M O
and O
with O
cm O
( O
↵ O
, O
) O
= O
↵ O
+ O
m O
M O
( O
↵ O
) O
for O
some O
constants O
↵ O
, O
2 O
R. O
This O
parametrization O
can O
be O
thought O
of O
as O
some O
linear Method
parametrization Method
of Method
the Method
dropout Method
rate Method
as O
a O
function O
of O
the O
layer O
. O
We O
perform O
hyperparameter Method
optimization Method
on O
the O
dimensionality Metric
decay Metric
rate Metric
2 O
[ O
0.25 O
, O
0.85 O
] O
, O
number O
of O
layers O
M O
2 O
{ O
1 O
, O
. O
. O
. O
, O
15 O
} O
, O
resolution O
randomization O
hyperparameters O
↵ O
, O
2 O
[ O
0 O
, O
0.8 O
] O
, O
weight O
decay O
rate O
in O
[ O
10 O
5 O
, O
10 O
2 O
] O
, O
momentum O
in O
[ O
1 O
0.10.5 O
, O
1 O
0.12 O
] O
and O
initial O
learning Metric
rate Metric
in O
[ O
0.14 O
, O
0.1 O
] O
. O
We O
train O
each O
model O
for O
150 O
epochs O
and O
anneal O
the O
learning Metric
rate Metric
by O
a O
factor O
of O
10 O
at O
epochs O
100 Material
and O
140 O
. O
We O
intentionally O
use O
no O
dropout Method
nor O
data Method
augmentation Method
, O
as O
these O
introduce O
a O
number O
of O
additional O
hyperparameters O
which O
we O
want O
to O
disambiguate O
as O
alternative O
factors O
for O
success O
. O
Perhaps O
unsurprisingly O
, O
the O
optimal O
hyperparameter Method
configuration Method
assigns O
the O
slowest O
possible O
layer Metric
map Metric
decay Metric
rate Metric
= O
0.85 O
. O
It O
selects O
randomized O
resolution O
reduction O
constants O
of O
about O
↵ O
⇡ O
0.30 O
, O
⇡ O
0.15 O
, O
momentum O
of O
about O
0.95 O
and O
initial O
learning Metric
rate Metric
0.0088 O
. O
These O
settings O
allow O
us O
to O
attain O
classification Task
rates O
of O
8.6 O
% O
on O
CIFAR Material
- Material
10 Material
and O
31.6 O
% O
on O
CIFAR Material
- Material
100 Material
. O
These O
are O
competitive O
results O
among O
approaches O
that O
do O
not O
employ O
data Method
augmentation Method
: O
a O
comparison O
to O
state O
- O
of O
- O
the O
- O
art O
approaches O
from O
the O
literature O
can O
be O
found O
in O
Table O
4 O
( O
b O
) O
. O
5.2 O
Spectral O
parametrization O
of O
CNNs Method
We O
demonstrate O
the O
effectiveness O
of O
spectral Method
parametrization Method
on O
a O
number O
of O
CNN Method
optimization O
tasks O
, O
for O
different O
architectures O
and O
for O
different O
filter O
sizes O
. O
We O
use O
the O
notation O
MPTS Method
to O
denote O
a O
max Method
pooling Method
layer Method
with O
size O
S O
and O
stride O
T O
, O
and O
FCF Method
is O
a O
fully Method
- Method
connected Method
layer Method
with O
F O
filters O
. O
The O
first O
architecture O
is O
the O
generic O
one O
used O
in O
a O
variety O
of O
deep Task
learning Task
papers Task
, O
such O
as O
Krizhevsky O
et O
al O
. O
( O
2012 O
) O
; O
Snoek O
et O
al O
. O
( O
2012 O
) O
; O
Krizhevsky O
( O
2009 O
) O
; O
Kingma O
& O
Ba O
( O
2015 O
) O
: O
C963 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
FC1024 O
! O
FC512 O
! O
Softmax Method
( O
6 O
) O
The O
second O
architecture O
we O
consider O
is O
the O
one O
employed O
in O
Snoek O
et O
al O
. O
( O
2015 O
) O
, O
which O
was O
shown O
to O
attain O
competitive O
classification Task
rates O
. O
It O
is O
deeper O
and O
more O
complex O
: O
C O
96 O
3 O
⇥ O
3 O
! O
C963 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1921 O
⇥ O
1 O
! O
C10 O
/ O
1001 O
⇥ O
1 O
! O
GA O
! O
Softmax Method
( O
7 O
) O
8 O
SP Method
is O
a O
spectral Method
pooling Method
layer Method
and O
CFS Method
has O
filters O
of O
size O
S O
with O
F O
filters O
Number O
of O
layers O
, O
penalization O
, O
nonlinearity O
type O
, O
and O
dimensionality Method
reduction Method
hyperparameters Method
were O
tuned O
using O
Some O
other O
networks O
were O
used O
as O
well O
to O
show O
comparisons O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
9 O
/ O
11 O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
Fraction O
of O
parameters O
kept O
2 O
7 O
2 O
6 O
2 O
5 O
2 O
4 O
2 O
3 O
2 O
2 O
2 O
1 O
20 O
kf O
f̂ O
k O
kf O
k O
Max Method
pooling Method
Spectral Method
pooling Method
( O
a O
) O
Approximation Method
loss Method
for O
the O
ImageNet O
validation O
set O
. O
Method O
CIFAR Material
- Material
10 Material
CIFAR Material
- Material
100 Material
Stochastic O
pooling O
15.13 O
% O
41.51 O
% O
Maxout Method
11.68 O
% O
38.57 O
% O
Network Method
- Method
in Method
- Method
network Method
10.41 O
% O
35.68 O
% O
Deeply Method
supervised Method
9.78 Method
% O
34.57 O
% O
Spectral Method
pooling Method
8.6 O
% O
31.6 O
% O
( O
b O
) O
Classification Metric
rates Metric
. O
Figure O
4 O
: O
( O
a O
) O
Average Metric
information Metric
dissipation Metric
for O
the O
ImageNet O
validation O
set O
as O
a O
function O
of O
fraction O
of O
parameters O
kept O
. O
This O
is O
measured O
in O
` Metric
2 Metric
error Metric
normalized O
by O
the O
input O
norm O
. O
The O
red O
horizontal O
line O
indicates O
the O
best O
error Metric
rate Metric
achievable O
by O
max Method
pooling Method
. O
( O
b O
) O
Test Metric
errors Metric
on O
CIFAR Material
- Material
10 Material
/ O
100 Material
without O
data Method
augmentation Method
of O
the O
optimal Method
spectral Method
pooling Method
architecture Method
, O
as O
compared O
to O
current O
state O
- O
of O
- O
the O
- O
art O
approaches O
: O
stochastic Method
pooling Method
( O
Zeiler O
& O
Fergus O
, O
2013 O
) O
, O
Maxout Method
( O
Goodfellow O
et O
al O
. O
, O
2013 O
) O
, O
networkin Method
- Method
network Method
( O
Lin O
et O
al O
. O
, O
2013 O
) O
, O
and O
deeply O
- O
supervised Method
nets Method
( O
Lee O
et O
al O
. O
, O
2014 O
) O
. O
number O
of O
degrees O
of O
freedom O
. O
Represented O
in O
the O
spatial O
domain O
, O
however O
, O
this O
results O
in O
significant O
redundancy O
. O
The O
frequency O
domain O
, O
on O
the O
other O
hand O
, O
provides O
an O
appealing O
basis O
for O
filter Method
representation Method
: O
characteristic Method
filters Method
( O
e.g. O
, O
Gabor Method
filters Method
) O
are O
often O
very O
localized O
in O
their O
spectral Method
representations Method
. O
This O
follows O
from O
the O
observation O
that O
filters Method
tend O
to O
feature O
very O
specific O
length O
- O
scales O
and O
orientations O
. O
Hence O
, O
they O
tend O
to O
have O
nonzero O
support O
in O
a O
narrow O
set O
of O
frequency O
components O
. O
This O
hypothesis O
can O
be O
observed O
qualitatively O
in O
Figure O
3 O
( O
a O
) O
and O
quantitatively O
in O
Figure O
3 O
( O
b O
) O
. O
Empirically O
, O
in O
Section O
5 O
we O
observe O
that O
spectral Method
representations Method
of Method
filters Method
leads O
to O
a O
convergence Metric
speedup Metric
by O
2 O
- O
5 O
times O
. O
We O
remark O
that O
, O
had O
we O
trained O
our O
network O
with O
standard O
stochastic Method
gradient Method
descent Method
, O
the O
linearity O
of O
differentiation O
and O
parameter Method
update Method
would O
have O
resulted O
in O
exactly O
the O
same O
filters O
regardless O
of O
whether O
they O
were O
represented O
in O
the O
spatial O
or O
frequency O
domain O
during O
training O
( O
this O
is O
true O
for O
any O
invertible O
linear O
transformation O
of O
the O
parameter O
space O
) O
. O
However O
, O
as O
discussed O
, O
this O
parametrization O
corresponds O
to O
a O
rotation O
to O
a O
more O
meaningful O
axis O
alignment O
, O
where O
the O
number O
of O
relevant O
elements O
has O
been O
significantly O
reduced O
. O
Since O
modern O
optimizers Method
implement O
update Method
rules Method
that O
consist O
of O
adaptive Method
element Method
- Method
wise Method
rescaling Method
, O
they O
are O
able O
to O
leverage O
this O
axis Task
alignment Task
by O
making O
large O
updates O
to O
a O
small O
number O
of O
elements O
. O
This O
can O
be O
seen O
quantitatively O
in O
Figure O
3 O
( O
c O
) O
, O
where O
the O
optimizer Method
— O
Adam Method
( O
Kingma O
& O
Ba O
, O
2015 O
) O
, O
in O
this O
case O
— O
only O
touches O
a O
small O
number O
of O
elements O
in O
its O
updates O
. O
There O
exist O
a O
number O
of O
extensions O
of O
the O
above O
approach O
we O
believe O
would O
be O
quite O
promising O
in O
future O
work O
; O
we O
elaborate O
on O
these O
in O
the O
discussion O
. O
5 O
Experiments O
We O
demonstrate O
the O
effectiveness O
of O
spectral Method
representations Method
in O
a O
number O
of O
different O
experiments O
. O
We O
ran O
all O
experiments O
on O
code O
optimized O
for O
the O
Xeon Method
Phi Method
coprocessor Method
. O
We O
used O
Spearmint Method
( O
Snoek O
et O
al O
. O
, O
2015 O
) O
for O
Bayesian Task
optimization Task
of Task
hyperparameters Task
with O
5 O
- O
20 O
concurrent O
evaluations O
. O
5.1 O
Spectral Method
pooling Method
Information Method
preservation Method
We O
test O
the O
information Metric
retainment Metric
properties Metric
of O
spectral Method
pooling Method
on O
the O
validation O
set O
of O
ImageNet O
( O
Russakovsky O
et O
al O
. O
, O
2015 O
) O
. O
For O
the O
different O
pooling Method
strategies Method
we O
plot O
the O
average Metric
approximation Metric
loss Metric
resulting O
from O
pooling Method
to O
different O
dimensionalities O
. O
This O
can O
be O
seen O
in O
Figure O
4 O
. O
We O
observe O
the O
two O
aspects O
discussed O
in O
Subsection O
3.1 O
: O
first O
, O
spectral Method
pooling Method
permits O
significantly O
better O
reconstruction Task
for O
the O
same O
number O
of O
parameters O
. O
Second O
, O
for O
max Method
pooling Method
, O
the O
only O
knob O
controlling O
the O
coarseness O
of O
approximation O
is O
the O
stride O
, O
which O
results O
in O
severe O
quantization Task
and O
a O
constraining O
lower O
bound O
on O
preserved O
information O
( O
marked O
in O
the O
figure O
as O
a O
horizontal O
red O
line O
) O
. O
In O
contrast O
, O
spectral Method
pooling Method
permits O
the O
selection O
of O
any O
output O
dimensionality O
, O
thereby O
producing O
a O
smooth O
curve O
over O
all O
frequency O
truncation O
choices O
. O
7 O
David O
Carlson O
Spectral O
Pooling O
July O
17 O
, O
2015 O
10 O
/ O
11 O
e O
2 O
e O
1 O
e0 O
e1 O
Si O
ze O
5 O
e O
1 O
e0 O
e1 O
e O
2 O
e O
1 O
e0 O
e1 O
Spatial O
Spectral O
0 O
40 O
80 O
120 O
160 O
200 O
Deep O
e O
2 O
e O
1 O
e0 O
e1 O
Si O
ze O
3 O
0 O
40 O
80 O
120 O
160 O
200 O
Generic O
e O
1 O
e0 O
e1 O
0 O
30 O
60 O
90 O
120 O
150 O
Sp O
. O
Pooling O
e O
2 O
e O
1 O
e0 O
e1 O
( O
a O
) O
Training O
curves O
. O
Architecture Method
Filtersize Method
Speedup Method
factor Method
Deep O
( O
7 O
) O
3 O
⇥ O
3 O
2.2 O
Deep O
( O
7 O
) O
5 O
⇥ O
5 O
4.8 O
Generic O
( O
6 O
) O
3 O
⇥ O
3 O
2.2 O
Generic O
( O
6 O
) O
5 O
⇥ O
5 O
5.1 O
Sp O
. O
Pooling O
( O
5 O
) O
3 O
⇥ O
3 O
2.4 O
Sp O
. O
Pooling O
( O
5 O
) O
5 O
⇥ O
5 O
4.8 O
( O
b O
) O
Speedup O
factors O
. O
Figure O
5 O
: O
Optimization O
of O
CNNs Method
via O
spectral Method
parametrization Method
. O
All O
experiments O
include O
data Task
augmentation Task
. O
( O
a O
) O
Training O
curves O
for O
the O
various O
experiments O
. O
The O
remainder O
of O
the O
optimization Task
past O
the O
matching O
point O
is O
marked O
in O
light O
blue O
. O
The O
red O
diamonds O
indicate O
the O
relative O
epochs O
in O
which O
the O
asymptotic Metric
error Metric
rate Metric
of O
the O
spatial Method
approach Method
is O
achieved O
. O
( O
b O
) O
Speedup O
factors O
for O
different O
architectures O
and O
filter O
sizes O
. O
A O
non O
- O
negligible O
speedup O
is O
observed O
even O
for O
tiny O
3 O
⇥ O
3 O
filters O
. O
Classification Method
with O
convolutional Method
neural Method
networks Method
We O
test O
spectral Method
pooling Method
on O
different O
classification Task
tasks O
. O
We O
hyperparametrize Method
and O
optimize O
the O
following O
CNN Method
architecture O
: O
C96 O
+ O
32m3 O
⇥ O
3 O
! O
SP#b O
Hmc O
⇥ O
b O
Hmc O
M O
m=1 O
! O
C96 O
+ O
32M1 O
⇥ O
1 O
! O
C O
10 O
/ O
100 Material
1 O
⇥ O
1 O
! O
GA O
! O
Softmax Method
( O
5 O
) O
Here O
, O
by O
CFS Method
we O
denote O
a O
convolutional Method
layer Method
with O
F O
filters O
each O
of O
size O
S O
, O
by O
SP#S Method
a O
spectral Method
pooling Method
layer Method
with O
output O
dimensionality O
S O
, O
and O
GA Method
the O
global Method
averaging Method
layer Method
described O
in O
Lin O
et O
al O
. O
( O
2013 O
) O
. O
We O
upper O
- O
bound O
the O
number O
of O
filters O
per O
layer O
as O
288 O
. O
Every O
convolution Method
and O
pooling O
layer O
is O
followed O
by O
a O
ReLU Method
nonlinearity Method
. O
We O
let O
Hm O
be O
the O
height O
of O
the O
map O
of O
layer O
m. O
Hence O
, O
each O
spectral Method
pooling Method
layer Method
reduces O
each O
output O
map O
dimension O
by O
factor O
2 O
( O
0 O
, O
1 O
) O
. O
We O
assign O
frequency O
dropout O
distribution O
pR O
( O
· O
; O
m O
, O
↵ O
, O
) O
= O
U O
[ O
bcmHmc O
, O
Hm O
] O
( O
· O
) O
for O
layer O
m O
, O
total O
layers O
M O
and O
with O
cm O
( O
↵ O
, O
) O
= O
↵ O
+ O
m O
M O
( O
↵ O
) O
for O
some O
constants O
↵ O
, O
2 O
R. O
This O
parametrization O
can O
be O
thought O
of O
as O
some O
linear Method
parametrization Method
of Method
the Method
dropout Method
rate Method
as O
a O
function O
of O
the O
layer O
. O
We O
perform O
hyperparameter Method
optimization Method
on O
the O
dimensionality Metric
decay Metric
rate Metric
2 O
[ O
0.25 O
, O
0.85 O
] O
, O
number O
of O
layers O
M O
2 O
{ O
1 O
, O
. O
. O
. O
, O
15 O
} O
, O
resolution O
randomization O
hyperparameters O
↵ O
, O
2 O
[ O
0 O
, O
0.8 O
] O
, O
weight O
decay O
rate O
in O
[ O
10 O
5 O
, O
10 O
2 O
] O
, O
momentum O
in O
[ O
1 O
0.10.5 O
, O
1 O
0.12 O
] O
and O
initial O
learning Metric
rate Metric
in O
[ O
0.14 O
, O
0.1 O
] O
. O
We O
train O
each O
model O
for O
150 O
epochs O
and O
anneal O
the O
learning Metric
rate Metric
by O
a O
factor O
of O
10 O
at O
epochs O
100 Material
and O
140 O
. O
We O
intentionally O
use O
no O
dropout Method
nor O
data Method
augmentation Method
, O
as O
these O
introduce O
a O
number O
of O
additional O
hyperparameters O
which O
we O
want O
to O
disambiguate O
as O
alternative O
factors O
for O
success O
. O
Perhaps O
unsurprisingly O
, O
the O
optimal O
hyperparameter Method
configuration Method
assigns O
the O
slowest O
possible O
layer Metric
map Metric
decay Metric
rate Metric
= O
0.85 O
. O
It O
selects O
randomized O
resolution O
reduction O
constants O
of O
about O
↵ O
⇡ O
0.30 O
, O
⇡ O
0.15 O
, O
momentum O
of O
about O
0.95 O
and O
initial O
learning Metric
rate Metric
0.0088 O
. O
These O
settings O
allow O
us O
to O
attain O
classification Task
rates O
of O
8.6 O
% O
on O
CIFAR Material
- Material
10 Material
and O
31.6 O
% O
on O
CIFAR Material
- Material
100 Material
. O
These O
are O
competitive O
results O
among O
approaches O
that O
do O
not O
employ O
data Method
augmentation Method
: O
a O
comparison O
to O
state O
- O
of O
- O
the O
- O
art O
approaches O
from O
the O
literature O
can O
be O
found O
in O
Table O
4 O
( O
b O
) O
. O
5.2 O
Spectral O
parametrization O
of O
CNNs Method
We O
demonstrate O
the O
effectiveness O
of O
spectral Method
parametrization Method
on O
a O
number O
of O
CNN Method
optimization O
tasks O
, O
for O
different O
architectures O
and O
for O
different O
filter O
sizes O
. O
We O
use O
the O
notation O
MPTS Method
to O
denote O
a O
max Method
pooling Method
layer Method
with O
size O
S O
and O
stride O
T O
, O
and O
FCF Method
is O
a O
fully Method
- Method
connected Method
layer Method
with O
F O
filters O
. O
The O
first O
architecture O
is O
the O
generic O
one O
used O
in O
a O
variety O
of O
deep Task
learning Task
papers Task
, O
such O
as O
Krizhevsky O
et O
al O
. O
( O
2012 O
) O
; O
Snoek O
et O
al O
. O
( O
2012 O
) O
; O
Krizhevsky O
( O
2009 O
) O
; O
Kingma O
& O
Ba O
( O
2015 O
) O
: O
C963 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
FC1024 O
! O
FC512 O
! O
Softmax Method
( O
6 O
) O
The O
second O
architecture O
we O
consider O
is O
the O
one O
employed O
in O
Snoek O
et O
al O
. O
( O
2015 O
) O
, O
which O
was O
shown O
to O
attain O
competitive O
classification Task
rates O
. O
It O
is O
deeper O
and O
more O
complex O
: O
C O
96 O
3 O
⇥ O
3 O
! O
C963 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
C1923 O
⇥ O
3 O
! O
MP23 O
⇥ O
3 O
! O
C1921 O
⇥ O
1 O
! O
C10 O
/ O
1001 O
⇥ O
1 O
! O
GA O
! O
Softmax Method
( O
7 O
) O
8 O
David O
Carlson O
Spectral Method
Pooling Method
July O
17 O
, O
2015 O
11 O
/ O
11 O
