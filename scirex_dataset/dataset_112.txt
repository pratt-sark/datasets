document O
: O
DeepFM Method
: O
A O
Factorization Method
- Method
Machine Method
based Method
Neural Method
Network Method
for O
CTR Task
Prediction Task
Learning O
sophisticated O
feature O
interactions O
behind O
user O
behaviors O
is O
critical O
in O
maximizing O
CTR Task
for O
recommender Task
systems Task
. O
Despite O
great O
progress O
, O
existing O
methods O
seem O
to O
have O
a O
strong O
bias O
towards O
low O
- O
or O
high O
- O
order O
interactions O
, O
or O
require O
expertise Method
feature Method
engineering Method
. O
In O
this O
paper O
, O
we O
show O
that O
it O
is O
possible O
to O
derive O
an O
end Method
- Method
to Method
- Method
end Method
learning Method
model Method
that O
emphasizes O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
. O
The O
proposed O
model O
, O
DeepFM Method
, O
combines O
the O
power O
of O
factorization Method
machines Method
for O
recommendation Task
and O
deep Method
learning Method
for O
feature Task
learning Task
in O
a O
new O
neural Method
network Method
architecture Method
. O
Compared O
to O
the O
latest O
Wide Method
& O
Deep Method
model Method
from O
Google Method
, O
DeepFM Method
has O
a O
shared O
input O
to O
its O
“ O
wide O
” O
and O
“ O
deep O
” O
parts O
, O
with O
no O
need O
of O
feature Method
engineering Method
besides O
raw O
features O
. O
Comprehensive O
experiments O
are O
conducted O
to O
demonstrate O
the O
effectiveness O
and O
efficiency O
of O
DeepFM Method
over O
the O
existing O
models O
for O
CTR Task
prediction O
, O
on O
both O
benchmark O
data O
and O
commercial O
data O
. O
section O
: O
Introduction O
The O
prediction O
of O
click Task
- Task
through Task
rate Task
( O
CTR Task
) O
is O
critical O
in O
recommender Task
system Task
, O
where O
the O
task O
is O
to O
estimate O
the O
probability O
a O
user O
will O
click O
on O
a O
recommended O
item O
. O
In O
many O
recommender Task
systems Task
the O
goal O
is O
to O
maximize O
the O
number O
of O
clicks O
, O
and O
so O
the O
items O
returned O
to O
a O
user O
can O
be O
ranked O
by O
estimated O
CTR Task
; O
while O
in O
other O
application O
scenarios O
such O
as O
online Task
advertising Task
it O
is O
also O
important O
to O
improve O
revenue O
, O
and O
so O
the O
ranking Method
strategy Method
can O
be O
adjusted O
as O
CTR Task
bid O
across O
all O
candidates O
, O
where O
“ O
bid O
” O
is O
the O
benefit O
the O
system O
receives O
if O
the O
item O
is O
clicked O
by O
a O
user O
. O
In O
either O
case O
, O
it O
is O
clear O
that O
the O
key O
is O
in O
estimating O
CTR Task
correctly O
. O
It O
is O
important O
for O
CTR Task
prediction O
to O
learn O
implicit O
feature O
interactions O
behind O
user O
click O
behaviors O
. O
By O
our O
study O
in O
a O
mainstream O
apps O
market O
, O
we O
found O
that O
people O
often O
download O
apps O
for O
food Task
delivery Task
at O
meal O
- O
time O
, O
suggesting O
that O
the O
( O
order O
- O
2 O
) O
interaction O
between O
app O
category O
and O
time O
- O
stamp O
can O
be O
used O
as O
a O
signal O
for O
CTR Task
. O
As O
a O
second O
observation O
, O
male O
teenagers O
like O
shooting O
games O
and O
RPG O
games O
, O
which O
means O
that O
the O
( O
order O
- O
3 O
) O
interaction O
of O
app O
category O
, O
user O
gender O
and O
age O
is O
another O
signal O
for O
CTR Task
. O
In O
general O
, O
such O
interactions O
of O
features O
behind O
user O
click O
behaviors O
can O
be O
highly O
sophisticated O
, O
where O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
should O
play O
important O
roles O
. O
According O
to O
the O
insights O
of O
the O
Wide Method
& O
Deep Method
model Method
from O
google Method
, O
considering O
low O
- O
and O
high O
- O
order O
feature O
interactions O
simultaneously O
brings O
additional O
improvement O
over O
the O
cases O
of O
considering O
either O
alone O
. O
The O
key O
challenge O
is O
in O
effectively O
modeling Task
feature Task
interactions Task
. O
Some O
feature O
interactions O
can O
be O
easily O
understood O
, O
thus O
can O
be O
designed O
by O
experts O
( O
like O
the O
instances O
above O
) O
. O
However O
, O
most O
other O
feature O
interactions O
are O
hidden O
in O
data O
and O
difficult O
to O
identify O
a O
priori O
( O
for O
instance O
, O
the O
classic O
association O
rule O
“ O
diaper O
and O
beer O
” O
is O
mined O
from O
data O
, O
instead O
of O
discovering O
by O
experts O
) O
, O
which O
can O
only O
be O
captured O
automatically O
by O
machine Method
learning Method
. O
Even O
for O
easy O
- O
to O
- O
understand O
interactions O
, O
it O
seems O
unlikely O
for O
experts O
to O
model O
them O
exhaustively O
, O
especially O
when O
the O
number O
of O
features O
is O
large O
. O
Despite O
their O
simplicity O
, O
generalized Method
linear Method
models Method
, O
such O
as O
FTRL Method
, O
have O
shown O
decent O
performance O
in O
practice O
. O
However O
, O
a O
linear Method
model Method
lacks O
the O
ability O
to O
learn O
feature O
interactions O
, O
and O
a O
common O
practice O
is O
to O
manually O
include O
pairwise O
feature O
interactions O
in O
its O
feature O
vector O
. O
Such O
a O
method O
is O
hard O
to O
generalize O
to O
model O
high O
- O
order O
feature O
interactions O
or O
those O
never O
or O
rarely O
appear O
in O
the O
training O
data O
. O
Factorization Method
Machines Method
( O
FM Method
) O
model O
pairwise O
feature O
interactions O
as O
inner O
product O
of O
latent O
vectors O
between O
features O
and O
show O
very O
promising O
results O
. O
While O
in O
principle O
FM Method
can O
model O
high O
- O
order O
feature O
interaction O
, O
in O
practice O
usually O
only O
order O
- O
2 O
feature O
interactions O
are O
considered O
due O
to O
high O
complexity O
. O
As O
a O
powerful O
approach O
to O
learning O
feature Task
representation Task
, O
deep Method
neural Method
networks Method
have O
the O
potential O
to O
learn O
sophisticated O
feature O
interactions O
. O
Some O
ideas O
extend O
CNN Method
and Method
RNN Method
for O
CTR Task
predition O
, O
but O
CNN Method
- Method
based Method
models Method
are O
biased O
to O
the O
interactions O
between O
neighboring O
features O
while O
RNN Method
- Method
based Method
models Method
are O
more O
suitable O
for O
click O
data O
with O
sequential O
dependency O
. O
studies O
feature Method
representations Method
and O
proposes O
Factorization Method
- Method
machine Method
supported Method
Neural Method
Network Method
( O
FNN Method
) O
. O
This O
model O
pre O
- O
trains O
FM Method
before O
applying O
DNN Method
, O
thus O
limited O
by O
the O
capability O
of O
FM Method
. O
Feature O
interaction O
is O
studied O
in O
, O
by O
introducing O
a O
product Method
layer Method
between O
embedding Method
layer Method
and O
fully Method
- Method
connected Method
layer Method
, O
and O
proposing O
the O
Product Method
- Method
based Method
Neural Method
Network Method
( O
PNN Method
) O
. O
As O
noted O
in O
, O
PNN Method
and O
FNN Method
, O
like O
other O
deep Method
models Method
, O
capture O
little O
low O
- O
order O
feature O
interactions O
, O
which O
are O
also O
essential O
for O
CTR Task
prediction O
. O
To O
model O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
, O
proposes O
an O
interesting O
hybrid Method
network Method
structure Method
( O
Wide Method
& O
Deep Method
) O
that O
combines O
a O
linear O
( O
“ O
wide O
” O
) O
model O
and O
a O
deep Method
model Method
. O
In O
this O
model O
, O
two O
different O
inputs O
are O
required O
for O
the O
“ O
wide O
part O
” O
and O
“ O
deep O
part O
” O
, O
respectively O
, O
and O
the O
input O
of O
“ O
wide O
part O
” O
still O
relies O
on O
expertise Method
feature Method
engineering Method
. O
One O
can O
see O
that O
existing O
models O
are O
biased O
to O
low O
- O
or O
high O
- O
order O
feature O
interaction O
, O
or O
rely O
on O
feature Method
engineering Method
. O
In O
this O
paper O
, O
we O
show O
it O
is O
possible O
to O
derive O
a O
learning Method
model Method
that O
is O
able O
to O
learn O
feature O
interactions O
of O
all O
orders O
in O
an O
end O
- O
to O
- O
end O
manner O
, O
without O
any O
feature Method
engineering Method
besides O
raw O
features O
. O
Our O
main O
contributions O
are O
summarized O
as O
follows O
: O
We O
propose O
a O
new O
neural O
network O
model O
DeepFM Method
( O
Figure O
[ O
reference O
] O
) O
that O
integrates O
the O
architectures O
of O
FM Method
and O
deep Method
neural Method
networks Method
( O
DNN Method
) O
. O
It O
models O
low O
- O
order O
feature O
interactions O
like O
FM Method
and O
models O
high O
- O
order O
feature O
interactions O
like O
DNN Method
. O
Unlike O
the O
wide Method
& Method
deep Method
model Method
, O
DeepFM Method
can O
be O
trained O
end O
- O
to O
- O
end O
without O
any O
feature Method
engineering Method
. O
DeepFM Method
can O
be O
trained O
efficiently O
because O
its O
wide O
part O
and O
deep O
part O
, O
unlike O
, O
share O
the O
same O
input O
and O
also O
the O
embedding O
vector O
. O
In O
, O
the O
input O
vector O
can O
be O
of O
huge O
size O
as O
it O
includes O
manually O
designed O
pairwise O
feature O
interactions O
in O
the O
input O
vector O
of O
its O
wide O
part O
, O
which O
also O
greatly O
increases O
its O
complexity Metric
. O
We O
evaluate O
DeepFM Method
on O
both O
benchmark O
data O
and O
commercial O
data O
, O
which O
shows O
consistent O
improvement O
over O
existing O
models O
for O
CTR Task
prediction O
. O
section O
: O
Our O
Approach O
Suppose O
the O
data O
set O
for O
training O
consists O
of O
instances O
, O
where O
is O
an O
- O
fields O
data O
record O
usually O
involving O
a O
pair O
of O
user O
and O
item O
, O
and O
is O
the O
associated O
label O
indicating O
user O
click O
behaviors O
( O
means O
the O
user O
clicked O
the O
item O
, O
and O
otherwise O
) O
. O
may O
include O
categorical O
fields O
( O
e.g. O
, O
gender O
, O
location O
) O
and O
continuous O
fields O
( O
e.g. O
, O
age O
) O
. O
Each O
categorical O
field O
is O
represented O
as O
a O
vector Method
of Method
one Method
- Method
hot Method
encoding Method
, O
and O
each O
continuous O
field O
is O
represented O
as O
the O
value O
itself O
, O
or O
a O
vector O
of O
one Method
- Method
hot Method
encoding Method
after O
discretization Method
. O
Then O
, O
each O
instance O
is O
converted O
to O
where O
is O
a O
- O
dimensional O
vector O
, O
with O
being O
the O
vector Method
representation Method
of O
the O
- O
th O
field O
of O
. O
Normally O
, O
is O
high O
- O
dimensional O
and O
extremely O
sparse O
. O
The O
task O
of O
CTR Task
prediction O
is O
to O
build O
a O
prediction Method
model Method
to O
estimate O
the O
probability O
of O
a O
user O
clicking O
a O
specific O
app O
in O
a O
given O
context O
. O
subsection O
: O
DeepFM Method
We O
aim O
to O
learn O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
. O
To O
this O
end O
, O
we O
propose O
a O
Factorization Method
- Method
Machine Method
based Method
neural Method
network Method
( O
DeepFM Method
) O
. O
As O
depicted O
in O
Figure O
[ O
reference O
] O
, O
DeepFM Method
consists O
of O
two O
components O
, O
FM Method
component O
and O
deep Method
component Method
, O
that O
share O
the O
same O
input O
. O
For O
feature O
, O
a O
scalar O
is O
used O
to O
weigh O
its O
order O
- O
1 O
importance O
, O
a O
latent O
vector O
is O
used O
to O
measure O
its O
impact O
of O
interactions O
with O
other O
features O
. O
is O
fed O
in O
FM Method
component O
to O
model O
order O
- O
2 O
feature O
interactions O
, O
and O
fed O
in O
deep Method
component Method
to O
model O
high O
- O
order O
feature O
interactions O
. O
All O
parameters O
, O
including O
, O
, O
and O
the O
network O
parameters O
( O
, O
below O
) O
are O
trained O
jointly O
for O
the O
combined O
prediction Method
model Method
: O
where O
is O
the O
predicted Task
CTR Task
, O
is O
the O
output O
of O
FM Method
component O
, O
and O
is O
the O
output O
of O
deep Method
component Method
. O
subsubsection O
: O
FM Method
Component O
The O
FM Method
component O
is O
a O
factorization Method
machine Method
, O
which O
is O
proposed O
in O
to O
learn O
feature Task
interactions Task
for O
recommendation Task
. O
Besides O
a O
linear O
( O
order O
- O
1 O
) O
interactions O
among O
features O
, O
FM Method
models O
pairwise O
( O
order O
- O
2 O
) O
feature O
interactions O
as O
inner O
product O
of O
respective O
feature O
latent O
vectors O
. O
It O
can O
capture O
order O
- O
2 O
feature O
interactions O
much O
more O
effectively O
than O
previous O
approaches O
especially O
when O
the O
dataset O
is O
sparse O
. O
In O
previous O
approaches O
, O
the O
parameter O
of O
an O
interaction O
of O
features O
and O
can O
be O
trained O
only O
when O
feature O
and O
feature O
both O
appear O
in O
the O
same O
data O
record O
. O
While O
in O
FM Method
, O
it O
is O
measured O
via O
the O
inner O
product O
of O
their O
latent O
vectors O
and O
. O
Thanks O
to O
this O
flexible O
design O
, O
FM Method
can O
train O
latent O
vector O
( O
) O
whenever O
( O
or O
) O
appears O
in O
a O
data O
record O
. O
Therefore O
, O
feature O
interactions O
, O
which O
are O
never O
or O
rarely O
appeared O
in O
the O
training O
data O
, O
are O
better O
learnt O
by O
FM Method
. O
As O
Figure O
[ O
reference O
] O
shows O
, O
the O
output O
of O
FM Method
is O
the O
summation O
of O
an O
Addition Method
unit Method
and O
a O
number O
of O
Inner Method
Product Method
units Method
: O
where O
and O
( O
is O
given O
) O
. O
The O
Addition O
unit O
( O
) O
reflects O
the O
importance O
of O
order O
- O
1 O
features O
, O
and O
the O
Inner O
Product O
units O
represent O
the O
impact O
of O
order O
- O
2 O
feature O
interactions O
. O
subsubsection O
: O
Deep Method
Component O
The O
deep Method
component Method
is O
a O
feed Method
- Method
forward Method
neural Method
network Method
, O
which O
is O
used O
to O
learn O
high O
- O
order O
feature O
interactions O
. O
As O
shown O
in O
Figure O
[ O
reference O
] O
, O
a O
data O
record O
( O
a O
vector O
) O
is O
fed O
into O
the O
neural Method
network Method
. O
Compared O
to O
neural Method
networks Method
with O
image O
or O
audio O
data O
as O
input O
, O
which O
is O
purely O
continuous O
and O
dense O
, O
the O
input O
of O
CTR Task
prediction O
is O
quite O
different O
, O
which O
requires O
a O
new O
network Method
architecture Method
design Method
. O
Specifically O
, O
the O
raw O
feature O
input O
vector O
for O
CTR Task
prediction O
is O
usually O
highly O
sparse O
, O
super O
high O
- O
dimensional O
, O
categorical O
- O
continuous O
- O
mixed O
, O
and O
grouped O
in O
fields O
( O
e.g. O
, O
gender O
, O
location O
, O
age O
) O
. O
This O
suggests O
an O
embedding Method
layer Method
to O
compress O
the O
input O
vector O
to O
a O
low O
- O
dimensional O
, O
dense O
real O
- O
value O
vector O
before O
further O
feeding O
into O
the O
first O
hidden Method
layer Method
, O
otherwise O
the O
network O
can O
be O
overwhelming O
to O
train O
. O
Figure O
[ O
reference O
] O
highlights O
the O
sub O
- O
network O
structure O
from O
the O
input O
layer O
to O
the O
embedding Method
layer Method
. O
We O
would O
like O
to O
point O
out O
the O
two O
interesting O
features O
of O
this O
network Method
structure Method
: O
1 O
) O
while O
the O
lengths O
of O
different O
input O
field O
vectors O
can O
be O
different O
, O
their O
embeddings O
are O
of O
the O
same O
size O
( O
) O
; O
2 O
) O
the O
latent O
feature O
vectors O
( O
) O
in O
FM Method
now O
server O
as O
network O
weights O
which O
are O
learned O
and O
used O
to O
compress O
the O
input O
field O
vectors O
to O
the O
embedding O
vectors O
. O
In O
, O
is O
pre O
- O
trained O
by O
FM Method
and O
used O
as O
initialization Task
. O
In O
this O
work O
, O
rather O
than O
using O
the O
latent O
feature O
vectors O
of O
FM Method
to O
initialize O
the O
networks O
as O
in O
, O
we O
include O
the O
FM Method
model O
as O
part O
of O
our O
overall O
learning Method
architecture Method
, O
in O
addition O
to O
the O
other O
DNN Method
model Method
. O
As O
such O
, O
we O
eliminate O
the O
need O
of O
pre Method
- Method
training Method
by O
FM Method
and O
instead O
jointly O
train O
the O
overall O
network O
in O
an O
end O
- O
to O
- O
end O
manner O
. O
Denote O
the O
output O
of O
the O
embedding Method
layer Method
as O
: O
where O
is O
the O
embedding O
of O
- O
th O
field O
and O
is O
the O
number O
of O
fields O
. O
Then O
, O
is O
fed O
into O
the O
deep Method
neural Method
network Method
, O
and O
the O
forward Method
process Method
is O
: O
where O
is O
the O
layer O
depth O
and O
is O
an O
activation O
function O
. O
, O
, O
are O
the O
output O
, O
model O
weight O
, O
and O
bias O
of O
the O
- O
th O
layer O
. O
After O
that O
, O
a O
dense O
real O
- O
value O
feature O
vector O
is O
generated O
, O
which O
is O
finally O
fed O
into O
the O
sigmoid Method
function Method
for O
CTR Task
prediction O
: O
, O
where O
is O
the O
number O
of O
hidden O
layers O
. O
It O
is O
worth O
pointing O
out O
that O
FM Method
component O
and O
deep Method
component Method
share O
the O
same O
feature Method
embedding Method
, O
which O
brings O
two O
important O
benefits O
: O
1 O
) O
it O
learns O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
from O
raw O
features O
; O
2 O
) O
there O
is O
no O
need O
for O
expertise O
feature Method
engineering Method
of O
the O
input O
, O
as O
required O
in O
Wide Method
& O
Deep Method
. O
subsection O
: O
Relationship O
with O
the O
other O
Neural Method
Networks Method
Inspired O
by O
the O
enormous O
success O
of O
deep Method
learning Method
in O
various O
applications O
, O
several O
deep Method
models Method
for O
CTR Task
prediction O
are O
developed O
recently O
. O
This O
section O
compares O
the O
proposed O
DeepFM Method
with O
existing O
deep Method
models Method
for O
CTR Task
prediction O
. O
FNN Method
: O
As O
Figure O
[ O
reference O
] O
( O
left O
) O
shows O
, O
FNN Method
is O
a O
FM Method
- O
initialized O
feed O
- O
forward O
neural O
network O
. O
The O
FM Method
pre O
- O
training O
strategy O
results O
in O
two O
limitations O
: O
1 O
) O
the O
embedding O
parameters O
might O
be O
over O
affected O
by O
FM Method
; O
2 O
) O
the O
efficiency O
is O
reduced O
by O
the O
overhead O
introduced O
by O
the O
pre Method
- Method
training Method
stage Method
. O
In O
addition O
, O
FNN Method
captures O
only O
high O
- O
order O
feature O
interactions O
. O
In O
contrast O
, O
DeepFM Method
needs O
no O
pre Method
- Method
training Method
and O
learns O
both O
high O
- O
and O
low O
- O
order O
feature O
interactions O
. O
PNN Method
: O
For O
the O
purpose O
of O
capturing Task
high Task
- Task
order Task
feature Task
interactions Task
, O
PNN Method
imposes O
a O
product Method
layer Method
between O
the O
embedding Method
layer Method
and O
the O
first Method
hidden Method
layer Method
. O
According O
to O
different O
types O
of O
product O
operation O
, O
there O
are O
three O
variants O
: O
IPNN Method
, O
OPNN Method
, O
and O
PNN Method
, O
where O
IPNN Method
is O
based O
on O
inner Method
product Method
of Method
vectors Method
, O
OPNN Method
is O
based O
on O
outer Method
product Method
, O
and O
PNN Method
is O
based O
on O
both O
inner Method
and Method
outer Method
products Method
. O
To O
make O
the O
computation O
more O
efficient O
, O
the O
authors O
proposed O
the O
approximated Method
computations Method
of O
both O
inner Method
and Method
outer Method
products Method
: O
1 O
) O
the O
inner Method
product Method
is O
approximately O
computed O
by O
eliminating O
some O
neurons O
; O
2 O
) O
the O
outer Method
product Method
is O
approximately O
computed O
by O
compressing O
- O
dimensional O
feature O
vectors O
to O
one O
- O
dimensional O
vector O
. O
However O
, O
we O
find O
that O
the O
outer Method
product Method
is O
less O
reliable O
than O
the O
inner Method
product Method
, O
since O
the O
approximated Method
computation Method
of Method
outer Method
product Method
loses O
much O
information O
that O
makes O
the O
result O
unstable O
. O
Although O
inner Method
product Method
is O
more O
reliable O
, O
it O
still O
suffers O
from O
high O
computational Metric
complexity Metric
, O
because O
the O
output O
of O
the O
product Method
layer Method
is O
connected O
to O
all O
neurons O
of O
the O
first O
hidden Method
layer Method
. O
Different O
from O
PNN Method
, O
the O
output O
of O
the O
product Method
layer Method
in O
DeepFM Method
only O
connects O
to O
the O
final O
output O
layer O
( O
one O
neuron O
) O
. O
Like O
FNN Method
, O
all O
PNNs Method
ignore O
low O
- O
order O
feature O
interactions O
. O
Wide Method
& O
Deep Method
: O
Wide Method
& O
Deep Method
( O
Figure O
[ O
reference O
] O
( O
right O
) O
) O
is O
proposed O
by O
Google Method
to O
model O
low O
- O
and O
high O
- O
order O
feature O
interactions O
simultaneously O
. O
As O
shown O
in O
, O
there O
is O
a O
need O
for O
expertise Method
feature Method
engineering Method
on O
the O
input O
to O
the O
“ O
wide O
” O
part O
( O
for O
instance O
, O
cross O
- O
product O
of O
users O
’ O
install O
apps O
and O
impression O
apps O
in O
app Task
recommendation Task
) O
. O
In O
contrast O
, O
DeepFM Method
needs O
no O
such O
expertise O
knowledge O
to O
handle O
the O
input O
by O
learning O
directly O
from O
the O
input O
raw O
features O
. O
A O
straightforward O
extension O
to O
this O
model O
is O
replacing O
LR Method
by O
FM Method
( O
we O
also O
evaluate O
this O
extension O
in O
Section O
[ O
reference O
] O
) O
. O
This O
extension O
is O
similar O
to O
DeepFM Method
, O
but O
DeepFM Method
shares O
the O
feature O
embedding O
between O
the O
FM Method
and O
deep Method
component Method
. O
The O
sharing Method
strategy Method
of Method
feature Method
embedding Method
influences O
( O
in O
back O
- O
propagate O
manner O
) O
the O
feature Method
representation Method
by O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
, O
which O
models O
the O
representation O
more O
precisely O
. O
Summarizations Task
: O
To O
summarize O
, O
the O
relationship O
between O
DeepFM Method
and O
the O
other O
deep Method
models Method
in O
four O
aspects O
is O
presented O
in O
Table O
[ O
reference O
] O
. O
As O
can O
be O
seen O
, O
DeepFM Method
is O
the O
only O
model O
that O
requires O
no O
pre O
- O
training O
and O
no O
feature Method
engineering Method
, O
and O
captures O
both O
low O
- O
and O
high O
- O
order O
feature O
interactions O
. O
section O
: O
Experiments O
In O
this O
section O
, O
we O
compare O
our O
proposed O
DeepFM Method
and O
the O
other O
state O
- O
of O
- O
the O
- O
art O
models O
empirically O
. O
The O
evaluation O
result O
indicates O
that O
our O
proposed O
DeepFM Method
is O
more O
effective O
than O
any O
other O
state O
- O
of O
- O
the O
- O
art O
model O
and O
the O
efficiency O
of O
DeepFM Method
is O
comparable O
to O
the O
best O
ones O
among O
the O
others O
. O
subsection O
: O
Experiment O
Setup O
subsubsection O
: O
Datasets O
We O
evaluate O
the O
effectiveness O
and O
efficiency O
of O
our O
proposed O
DeepFM Method
on O
the O
following O
two O
datasets O
. O
1 O
) O
Criteo Material
Dataset Material
: O
Criteo Material
dataset O
includes O
45 O
million O
users O
’ O
click O
records O
. O
There O
are O
13 O
continuous O
features O
and O
26 O
categorical O
ones O
. O
We O
split O
the O
dataset O
randomly O
into O
two O
parts O
: O
90 O
% O
is O
for O
training O
, O
while O
the O
rest O
10 O
% O
is O
for O
testing O
. O
2 O
) O
Company∗ O
Dataset O
: O
In O
order O
to O
verify O
the O
performance O
of O
DeepFM Method
in O
real O
industrial O
CTR Task
prediction O
, O
we O
conduct O
experiment O
on O
Company Material
dataset Material
. O
We O
collect O
7 O
consecutive O
days O
of O
users O
’ O
click O
records O
from O
the O
game O
center O
of O
the O
Company Material
App O
Store O
for O
training O
, O
and O
the O
next O
1 O
day O
for O
testing O
. O
There O
are O
around O
1 O
billion O
records O
in O
the O
whole O
collected O
dataset O
. O
In O
this O
dataset O
, O
there O
are O
app O
features O
( O
e.g. O
, O
identification O
, O
category O
, O
and O
etc O
) O
, O
user O
features O
( O
e.g. O
, O
user O
’s O
downloaded O
apps O
, O
and O
etc O
) O
, O
and O
context O
features O
( O
e.g. O
, O
operation O
time O
, O
and O
etc O
) O
. O
subsubsection O
: O
Evaluation Metric
Metrics Metric
We O
use O
two O
evaluation Metric
metrics Metric
in O
our O
experiments O
: O
AUC Metric
( O
Area Metric
Under Metric
ROC Metric
) O
and O
Logloss Metric
( O
cross Metric
entropy Metric
) O
. O
subsubsection O
: O
Model O
Comparison O
We O
compare O
9 O
models O
in O
our O
experiments O
: O
LR Method
, O
FM Method
, O
FNN Method
, O
PNN Method
( O
three O
variants O
) O
, O
Wide Method
& O
Deep Method
, O
and O
DeepFM Method
. O
In O
the O
Wide Method
& O
Deep Method
model O
, O
for O
the O
purpose O
of O
eliminating O
feature Task
engineering Task
effort Task
, O
we O
also O
adapt O
the O
original O
Wide Method
& O
Deep Method
model O
by O
replacing O
LR Method
by O
FM Method
as O
the O
wide O
part O
. O
In O
order O
to O
distinguish O
these O
two O
variants O
of O
Wide Method
& O
Deep Method
, O
we O
name O
them O
LR Method
& Method
DNN Method
and O
FM Method
& O
DNN O
, O
respectively O
. O
subsubsection O
: O
Parameter O
Settings O
To O
evaluate O
the O
models O
on O
Criteo Material
dataset O
, O
we O
follow O
the O
parameter O
settings O
in O
for O
FNN Method
and O
PNN Method
: O
( O
1 O
) O
dropout O
: O
0.5 O
; O
( O
2 O
) O
network O
structure O
: O
400 O
- O
400 O
- O
400 O
; O
( O
3 O
) O
optimizer O
: O
Adam Method
; O
( O
4 O
) O
activation Method
function Method
: O
tanh Method
for O
IPNN Method
, O
relu Method
for O
other O
deep Method
models Method
. O
To O
be O
fair O
, O
our O
proposed O
DeepFM Method
uses O
the O
same O
setting O
. O
The O
optimizers Method
of O
LR Method
and O
FM Method
are O
FTRL Method
and O
Adam Method
respectively O
, O
and O
the O
latent O
dimension O
of O
FM Method
is O
10 O
. O
To O
achieve O
the O
best O
performance O
for O
each O
individual O
model O
on O
Company Material
dataset Material
, O
we O
conducted O
carefully O
parameter O
study O
, O
which O
is O
discussed O
in O
Section O
[ O
reference O
] O
. O
subsection O
: O
Performance O
Evaluation O
In O
this O
section O
, O
we O
evaluate O
the O
models O
listed O
in O
Section O
[ O
reference O
] O
on O
the O
two O
datasets O
to O
compare O
their O
effectiveness O
and O
efficiency O
. O
subsubsection O
: O
Efficiency O
Comparison O
The O
efficiency O
of O
deep Method
learning Method
models Method
is O
important O
to O
real Task
- Task
world Task
applications Task
. O
We O
compare O
the O
efficiency O
of O
different O
models O
on O
Criteo Material
dataset O
by O
the O
following O
formula O
: O
. O
The O
results O
are O
shown O
in O
Figure O
[ O
reference O
] O
, O
including O
the O
tests O
on O
CPU O
( O
left O
) O
and O
GPU O
( O
right O
) O
, O
where O
we O
have O
the O
following O
observations O
: O
1 O
) O
pre Method
- Method
training Method
of O
FNN Method
makes O
it O
less O
efficient O
; O
2 O
) O
Although O
the O
speed Metric
up Metric
of O
IPNN Method
and O
PNN Method
on O
GPU Method
is O
higher O
than O
the O
other O
models O
, O
they O
are O
still O
computationally O
expensive O
because O
of O
the O
inefficient O
inner Method
product Method
operations Method
; O
3 O
) O
The O
DeepFM Method
achieves O
almost O
the O
most O
efficient O
in O
both O
tests O
. O
subsubsection O
: O
Effectiveness O
Comparison O
The O
performance O
for O
CTR Task
prediction O
of O
different O
models O
on O
Criteo Material
dataset O
and O
Company Material
dataset Material
is O
shown O
in O
Table O
[ O
reference O
] O
, O
where O
we O
have O
the O
following O
observations O
: O
Learning O
feature O
interactions O
improves O
the O
performance O
of O
CTR Task
prediction O
model O
. O
This O
observation O
is O
from O
the O
fact O
that O
LR Method
( O
which O
is O
the O
only O
model O
that O
does O
not O
consider O
feature O
interactions O
) O
performs O
worse O
than O
the O
other O
models O
. O
As O
the O
best O
model O
, O
DeepFM Method
outperforms O
LR Method
by O
0.86 O
% O
and O
4.18 O
% O
in O
terms O
of O
AUC Metric
( O
1.15 O
% O
and O
5.60 O
% O
in O
terms O
of O
Logloss Metric
) O
on O
Company Material
and O
Criteo Material
datasets O
. O
Learning O
high O
- O
and O
low O
- O
order O
feature O
interactions O
simultaneously O
and O
properly O
improves O
the O
performance O
of O
CTR Task
prediction O
model O
. O
DeepFM Method
outperforms O
the O
models O
that O
learn O
only O
low O
- O
order O
feature O
interactions O
( O
namely O
, O
FM Method
) O
or O
high O
- O
order O
feature O
interactions O
( O
namely O
, O
FNN Method
, O
IPNN Method
, O
OPNN Method
, O
PNN Method
) O
. O
Compared O
to O
the O
second O
best O
model O
, O
DeepFM Method
achieves O
more O
than O
0.37 O
% O
and O
0.25 O
% O
in O
terms O
of O
AUC Metric
( O
0.42 O
% O
and O
0.29 O
% O
in O
terms O
of O
Logloss Metric
) O
on O
Company Material
and O
Criteo Material
datasets O
. O
Learning O
high O
- O
and O
low O
- O
order O
feature O
interactions O
simultaneously O
while O
sharing O
the O
same O
feature O
embedding O
for O
high O
- O
and O
low O
- O
order O
feature O
interactions O
learning O
improves O
the O
performance O
of O
CTR Task
prediction O
model O
. O
DeepFM Method
outperforms O
the O
models O
that O
learn O
high O
- O
and O
low O
- O
order O
feature O
interactions O
using O
separate O
feature O
embeddings O
( O
namely O
, O
LR Method
& Method
DNN Method
and O
FM Method
& O
DNN O
) O
. O
Compared O
to O
these O
two O
models O
, O
DeepFM Method
achieves O
more O
than O
0.48 O
% O
and O
0.33 O
% O
in O
terms O
of O
AUC Metric
( O
0.61 O
% O
and O
0.66 O
% O
in O
terms O
of O
Logloss Metric
) O
on O
Company Material
and O
Criteo Material
datasets O
. O
Overall O
, O
our O
proposed O
DeepFM Method
model O
beats O
the O
competitors O
by O
more O
than O
0.37 O
% O
and O
0.42 O
% O
in O
terms O
of O
AUC Metric
and O
Logloss Metric
on O
Company Material
dataset Material
, O
respectively O
. O
In O
fact O
, O
a O
small O
improvement O
in O
offline Metric
AUC Metric
evaluation Metric
is O
likely O
to O
lead O
to O
a O
significant O
increase O
in O
online O
CTR Task
. O
As O
reported O
in O
, O
compared O
with O
LR Method
, O
Wide Method
& O
Deep Method
improves O
AUC Metric
by O
0.275 O
% O
( O
offline O
) O
and O
the O
improvement O
of O
online O
CTR Task
is O
3.9 O
% O
. O
The O
daily O
turnover O
of O
Company Material
’s O
App O
Store O
is O
millions O
of O
dollars O
, O
therefore O
even O
several O
percents O
lift O
in O
CTR Task
brings O
extra O
millions O
of O
dollars O
each O
year O
. O
subsection O
: O
Hyper O
- O
Parameter O
Study O
We O
study O
the O
impact O
of O
different O
hyper O
- O
parameters O
of O
different O
deep Method
models Method
, O
on O
Company Material
dataset Material
. O
The O
order O
is O
: O
1 O
) O
activation O
functions O
; O
2 O
) O
dropout Metric
rate Metric
; O
3 O
) O
number O
of O
neurons O
per O
layer O
; O
4 O
) O
number O
of O
hidden O
layers O
; O
5 O
) O
network O
shape O
. O
subsubsection O
: O
Activation Method
Function Method
According O
to O
, O
relu Method
and O
tanh Method
are O
more O
suitable O
for O
deep Method
models Method
than O
sigmoid Method
. O
In O
this O
paper O
, O
we O
compare O
the O
performance O
of O
deep Method
models Method
when O
applying O
relu Method
and Method
tanh Method
. O
As O
shown O
in O
Figure O
[ O
reference O
] O
, O
relu Method
is O
more O
appropriate O
than O
tanh Method
for O
all O
the O
deep Method
models Method
, O
except O
for O
IPNN Method
. O
Possible O
reason O
is O
that O
relu O
induces O
sparsity O
. O
subsubsection O
: O
Dropout Method
Dropout Method
refers O
to O
the O
probability O
that O
a O
neuron O
is O
kept O
in O
the O
network O
. O
Dropout Method
is O
a O
regularization Method
technique Method
to O
compromise O
the O
precision Metric
and O
the O
complexity Metric
of O
the O
neural Method
network Method
. O
We O
set O
the O
dropout O
to O
be O
1.0 O
, O
0.9 O
, O
0.8 O
, O
0.7 O
, O
0.6 O
, O
0.5 O
. O
As O
shown O
in O
Figure O
[ O
reference O
] O
, O
all O
the O
models O
are O
able O
to O
reach O
their O
own O
best O
performance O
when O
the O
dropout O
is O
properly O
set O
( O
from O
0.6 O
to O
0.9 O
) O
. O
The O
result O
shows O
that O
adding O
reasonable O
randomness O
to O
model O
can O
strengthen O
model O
’s O
robustness Metric
. O
subsubsection O
: O
Number O
of O
Neurons O
per O
Layer O
When O
other O
factors O
remain O
the O
same O
, O
increasing O
the O
number O
of O
neurons O
per O
layer O
introduces O
complexity O
. O
As O
we O
can O
observe O
from O
Figure O
[ O
reference O
] O
, O
increasing O
the O
number O
of O
neurons O
does O
not O
always O
bring O
benefit O
. O
For O
instance O
, O
DeepFM Method
performs O
stably O
when O
the O
number O
of O
neurons O
per O
layer O
is O
increased O
from O
400 O
to O
800 O
; O
even O
worse O
, O
OPNN Method
performs O
worse O
when O
we O
increase O
the O
number O
of O
neurons O
from O
400 O
to O
800 O
. O
This O
is O
because O
an O
over O
- O
complicated O
model O
is O
easy O
to O
overfit O
. O
In O
our O
dataset O
, O
200 O
or O
400 O
neurons O
per O
layer O
is O
a O
good O
choice O
. O
subsubsection O
: O
Number O
of O
Hidden O
Layers O
As O
presented O
in O
Figure O
[ O
reference O
] O
, O
increasing O
number O
of O
hidden O
layers O
improves O
the O
performance O
of O
the O
models O
at O
the O
beginning O
, O
however O
, O
their O
performance O
is O
degraded O
if O
the O
number O
of O
hidden O
layers O
keeps O
increasing O
. O
This O
phenomenon O
is O
also O
because O
of O
overfitting O
. O
subsubsection O
: O
Network Method
Shape Method
We O
test O
four O
different O
network O
shapes O
: O
constant O
, O
increasing O
, O
decreasing O
, O
and O
diamond O
. O
When O
we O
change O
the O
network O
shape O
, O
we O
fix O
the O
number O
of O
hidden O
layers O
and O
the O
total O
number O
of O
neurons O
. O
For O
instance O
, O
when O
the O
number O
of O
hidden O
layers O
is O
3 O
and O
the O
total O
number O
of O
neurons O
is O
600 O
, O
then O
four O
different O
shapes O
are O
: O
constant O
( O
200 O
- O
200 O
- O
200 O
) O
, O
increasing O
( O
100 O
- O
200 O
- O
300 O
) O
, O
decreasing O
( O
300 O
- O
200 O
- O
100 O
) O
, O
and O
diamond O
( O
150 O
- O
300 O
- O
150 O
) O
. O
As O
we O
can O
see O
from O
Figure O
[ O
reference O
] O
, O
the O
“ O
constant O
” O
network O
shape O
is O
empirically O
better O
than O
the O
other O
three O
options O
, O
which O
is O
consistent O
with O
previous O
studies O
. O
section O
: O
Related O
Work O
In O
this O
paper O
, O
a O
new O
deep Method
neural Method
network Method
is O
proposed O
for O
CTR Task
prediction O
. O
The O
most O
related O
domains O
are O
CTR Task
prediction O
and O
deep Task
learning Task
in O
recommender Task
system Task
. O
In O
this O
section O
, O
we O
discuss O
related O
work O
in O
these O
two O
domains O
. O
CTR Task
prediction O
plays O
an O
important O
role O
in O
recommender Task
system Task
. O
Besides O
generalized Method
linear Method
models Method
and O
FM Method
, O
a O
few O
other O
models O
are O
proposed O
for O
CTR Task
prediction O
, O
such O
as O
tree Method
- Method
based Method
model Method
, O
tensor Method
based Method
model Method
, O
support Method
vector Method
machine Method
, O
and O
bayesian Method
model Method
. O
The O
other O
related O
domain O
is O
deep Task
learning Task
in O
recommender Task
systems Task
. O
In O
Section O
[ O
reference O
] O
and O
Section O
[ O
reference O
] O
, O
several O
deep Method
learning Method
models Method
for O
CTR Task
prediction O
are O
already O
mentioned O
, O
thus O
we O
do O
not O
discuss O
about O
them O
here O
. O
Several O
deep Method
learning Method
models Method
are O
proposed O
in O
recommendation Task
tasks Task
other O
than O
CTR Task
prediction O
( O
e.g. O
, O
) O
. O
propose O
to O
improve O
Collaborative Task
Filtering Task
via O
deep Method
learning Method
. O
The O
authors O
of O
extract O
content O
feature O
by O
deep Method
learning Method
to O
improve O
the O
performance O
of O
music Task
recommendation Task
. O
devises O
a O
deep Method
learning Method
network Method
to O
consider O
both O
image O
feature O
and O
basic O
feature O
of O
display O
adverting O
. O
develops O
a O
two Method
- Method
stage Method
deep Method
learning Method
framework Method
for O
YouTube Task
video Task
recommendation Task
. O
section O
: O
Conclusions O
In O
this O
paper O
, O
we O
proposed O
DeepFM Method
, O
a O
Factorization Method
- Method
Machine Method
based Method
Neural Method
Network Method
for O
CTR Task
prediction O
, O
to O
overcome O
the O
shortcomings O
of O
the O
state O
- O
of O
- O
the O
- O
art O
models O
and O
to O
achieve O
better O
performance O
. O
DeepFM Method
trains O
a O
deep Method
component Method
and O
an O
FM Method
component O
jointly O
. O
It O
gains O
performance O
improvement O
from O
these O
advantages O
: O
1 O
) O
it O
does O
not O
need O
any O
pre O
- O
training O
; O
2 O
) O
it O
learns O
both O
high O
- O
and O
low O
- O
order O
feature O
interactions O
; O
3 O
) O
it O
introduces O
a O
sharing Method
strategy Method
of Method
feature Method
embedding Method
to O
avoid O
feature Method
engineering Method
. O
We O
conducted O
extensive O
experiments O
on O
two O
real O
- O
world O
datasets O
( O
Criteo Material
dataset O
and O
a O
commercial O
App O
Store O
dataset O
) O
to O
compare O
the O
effectiveness O
and O
efficiency O
of O
DeepFM Method
and O
the O
state O
- O
of O
- O
the O
- O
art Method
models Method
. O
Our O
experiment O
results O
demonstrate O
that O
1 O
) O
DeepFM Method
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
models O
in O
terms O
of O
AUC Metric
and O
Logloss Metric
on O
both O
datasets O
; O
2 O
) O
The O
efficiency O
of O
DeepFM Method
is O
comparable O
to O
the O
most O
efficient O
deep Method
model Method
in O
the O
state O
- O
of O
- O
the O
- O
art O
. O
There O
are O
two O
interesting O
directions O
for O
future O
study O
. O
One O
is O
exploring O
some O
strategies O
( O
such O
as O
introducing O
pooling O
layers O
) O
to O
strengthen O
the O
ability O
of O
learning O
most O
useful O
high O
- O
order O
feature O
interactions O
. O
The O
other O
is O
to O
train O
DeepFM Method
on O
a O
GPU Method
cluster Method
for O
large Task
- Task
scale Task
problems Task
. O
bibliography O
: O
References O
