document O
: O
Gated Method
- Method
Attention Method
Readers Method
for O
Text Task
Comprehension Task
In O
this O
paper O
we O
study O
the O
problem O
of O
answering Task
cloze Task
- Task
style Task
questions Task
over O
documents O
. O
Our O
model O
, O
the O
Gated Method
- Method
Attention Method
( O
GA Method
) O
Reader O
, O
integrates O
a O
multi Method
- Method
hop Method
architecture Method
with O
a O
novel O
attention Method
mechanism Method
, O
which O
is O
based O
on O
multiplicative O
interactions O
between O
the O
query Method
embedding Method
and O
the O
intermediate O
states O
of O
a O
recurrent Method
neural Method
network Method
document Method
reader Method
. O
This O
enables O
the O
reader O
to O
build O
query Method
- Method
specific Method
representations Method
of O
tokens O
in O
the O
document O
for O
accurate O
answer Task
selection Task
. O
The O
GA Method
Reader O
obtains O
state O
- O
of O
- O
the O
- O
art O
results O
on O
three O
benchmarks O
for O
this O
task O
– O
the O
CNN Material
& Material
Daily Material
Mail Material
news Material
stories Material
and O
the O
Who O
Did O
What O
dataset O
. O
The O
effectiveness O
of O
multiplicative O
interaction O
is O
demonstrated O
by O
an O
ablation Task
study Task
, O
and O
by O
comparing O
to O
alternative O
compositional Method
operators Method
for O
implementing O
the O
gated Method
- Method
attention Method
. O
citecolor O
= O
darkblue O
section O
: O
Introduction O
A O
recent O
trend O
to O
measure O
progress O
towards O
machine Task
reading Task
is O
to O
test O
a O
system O
’s O
ability O
to O
answer O
questions O
about O
a O
document O
it O
has O
to O
comprehend O
. O
Towards O
this O
end O
, O
several O
large O
- O
scale O
datasets O
of O
cloze O
- O
style O
questions O
over O
a O
context O
document O
have O
been O
introduced O
recently O
, O
which O
allow O
the O
training O
of O
supervised Method
machine Method
learning Method
systems Method
hermann2015teaching O
, O
hill2015goldilocks O
, O
onishi2016did O
. O
Such O
datasets O
can O
be O
easily O
constructed O
automatically O
and O
the O
unambiguous O
nature O
of O
their O
queries O
provides O
an O
objective O
benchmark O
to O
measure O
a O
system O
’s O
performance O
at O
text Task
comprehension Task
. O
Deep Method
learning Method
models Method
have O
been O
shown O
to O
outperform O
traditional O
shallow Method
approaches Method
on O
text Task
comprehension Task
tasks Task
hermann2015teaching O
. O
The O
success O
of O
many O
recent O
models O
can O
be O
attributed O
primarily O
to O
two O
factors O
: O
( O
1 O
) O
Multi Method
- Method
hop Method
architectures Method
weston2014memory O
, O
sordoni2016iterative O
, O
shen2016reasonet O
, O
allow O
a O
model O
to O
scan O
the O
document O
and O
the O
question O
iteratively O
for O
multiple O
passes O
. O
( O
2 O
) O
Attention Method
mechanisms Method
, O
chen2016thorough O
, O
hermann2015teaching O
borrowed O
from O
the O
machine Task
translation Task
literature O
bahdanau2014neural O
, O
allow O
the O
model O
to O
focus O
on O
appropriate O
subparts O
of O
the O
context O
document O
. O
Intuitively O
, O
the O
multi Method
- Method
hop Method
architecture Method
allows O
the O
reader O
to O
incrementally O
refine O
token Method
representations Method
, O
and O
the O
attention Method
mechanism Method
re O
- O
weights O
different O
parts O
in O
the O
document O
according O
to O
their O
relevance O
to O
the O
query O
. O
The O
effectiveness O
of O
multi Task
- Task
hop Task
reasoning Task
and O
attentions O
have O
been O
explored O
orthogonally O
so O
far O
in O
the O
literature O
. O
In O
this O
paper O
, O
we O
focus O
on O
combining O
both O
in O
a O
complementary O
manner O
, O
by O
designing O
a O
novel O
attention Method
mechanism Method
which O
gates O
the O
evolving O
token Method
representations Method
across O
hops O
. O
More O
specifically O
, O
unlike O
existing O
models O
where O
the O
query O
attention O
is O
applied O
either O
token Method
- Method
wise Method
hermann2015teaching O
, O
kadlec2016text O
, O
chen2016thorough O
, O
hill2015goldilocks O
or O
sentence O
- O
wise O
weston2014memory O
, O
sukhbaatar2015end O
to O
allow O
weighted Task
aggregation Task
, O
the O
Gated Method
- Method
Attention Method
( O
GA Method
) O
module O
proposed O
in O
this O
work O
allows O
the O
query O
to O
directly O
interact O
with O
each O
dimension O
of O
the O
token O
embeddings O
at O
the O
semantic O
- O
level O
, O
and O
is O
applied O
layer O
- O
wise O
as O
information Method
filters Method
during O
the O
multi Method
- Method
hop Method
representation Method
learning Method
process Method
. O
Such O
a O
fine O
- O
grained O
attention O
enables O
our O
model O
to O
learn O
conditional Method
token Method
representations Method
w.r.t O
. O
the O
given O
question O
, O
leading O
to O
accurate O
answer O
selections O
. O
We O
show O
in O
our O
experiments O
that O
the O
proposed O
GA Method
reader O
, O
despite O
its O
relative O
simplicity O
, O
consistently O
improves O
over O
a O
variety O
of O
strong O
baselines O
on O
three O
benchmark O
datasets O
. O
Our O
key O
contribution O
, O
the O
GA Method
module O
, O
provides O
a O
significant O
improvement O
for O
large O
datasets O
. O
Qualitatively O
, O
visualization O
of O
the O
attentions O
at O
intermediate O
layers O
of O
the O
GA Method
reader O
shows O
that O
in O
each O
layer O
the O
GA Method
reader O
attends O
to O
distinct O
salient O
aspects O
of O
the O
query O
which O
help O
in O
determining O
the O
answer O
. O
section O
: O
Related O
Work O
The O
cloze Task
- Task
style Task
QA Task
task Task
involves O
tuples O
of O
the O
form O
, O
where O
is O
a O
document O
( O
context O
) O
, O
is O
a O
query O
over O
the O
contents O
of O
, O
in O
which O
a O
phrase O
is O
replaced O
with O
a O
placeholder O
, O
and O
is O
the O
answer O
to O
, O
which O
comes O
from O
a O
set O
of O
candidates O
. O
In O
this O
work O
we O
consider O
datasets O
where O
each O
candidate O
has O
at O
least O
one O
token O
which O
also O
appears O
in O
the O
document O
. O
The O
task O
can O
then O
be O
described O
as O
: O
given O
a O
document Task
- Task
query Task
pair Task
, O
find O
which O
answers O
. O
Below O
we O
provide O
an O
overview O
of O
representative O
neural Method
network Method
architectures Method
which O
have O
been O
applied O
to O
this O
problem O
. O
LSTMs Method
with O
Attention Method
: O
Several O
architectures O
introduced O
in O
hermann2015teaching O
employ O
LSTM Method
units Method
to O
compute O
a O
combined Method
document Method
- Method
query Method
representation Method
, O
which O
is O
used O
to O
rank O
the O
candidate O
answers O
. O
These O
include O
the O
DeepLSTM Method
Reader Method
which O
performs O
a O
single Method
forward O
pass O
through O
the O
concatenated O
( O
document O
, O
query O
) O
pair O
to O
obtain O
; O
the O
Attentive Method
Reader Method
which O
first O
computes O
a O
document O
vector O
by O
a O
weighted O
aggregation O
of O
words O
according O
to O
attentions O
based O
on O
, O
and O
then O
combines O
and O
to O
obtain O
their O
joint Method
representation Method
; O
and O
the O
Impatient Method
Reader Method
where O
the O
document Method
representation Method
is O
built O
incrementally O
. O
The O
architecture O
of O
the O
Attentive Method
Reader Method
has O
been O
simplified O
recently O
in O
Stanford O
Attentive Method
Reader Method
, O
where O
shallower Method
recurrent Method
units Method
were O
used O
with O
a O
bilinear Method
form Method
for O
the O
query Task
- Task
document Task
attention Task
chen2016thorough O
. O
Attention O
Sum O
: O
The O
Attention Method
- Method
Sum Method
( O
AS Method
) O
Reader O
kadlec2016text O
uses O
two O
bi O
- O
directional O
GRU Method
networks O
cho2014learning O
to O
encode O
both O
and O
into O
vectors O
. O
A O
probability O
distribution O
over O
the O
entities O
in O
is O
obtained O
by O
computing O
dot O
products O
between O
and O
the O
entity O
embeddings O
and O
taking O
a O
softmax Method
. O
Then O
, O
an O
aggregation Method
scheme Method
named O
pointer Method
- Method
sum Method
attention Method
is O
further O
applied O
to O
sum O
the O
probabilities O
of O
the O
same O
entity O
, O
so O
that O
frequent O
entities O
the O
document O
will O
be O
favored O
compared O
to O
rare O
ones O
. O
Building O
on O
the O
AS Method
Reader Method
, O
the O
Attention Method
- Method
over Method
- Method
Attention Method
( O
AoA Method
) O
Reader O
cui2016attention O
introduces O
a O
two O
- O
way O
attention Method
mechanism Method
where O
the O
query O
and O
the O
document O
are O
mutually O
attentive O
to O
each O
other O
. O
Mulit Method
- Method
hop Method
Architectures Method
: O
Memory Method
Networks Method
( O
MemNets Method
) O
were O
proposed O
in O
weston2014memory O
, O
where O
each O
sentence O
in O
the O
document O
is O
encoded O
to O
a O
memory O
by O
aggregating O
nearby O
words O
. O
Attention O
over O
the O
memory O
slots O
given O
the O
query O
is O
used O
to O
compute O
an O
overall O
memory O
and O
to O
renew O
the O
query Method
representation Method
over O
multiple O
iterations O
, O
allowing O
certain O
types O
of O
reasoning O
over O
the O
salient O
facts O
in O
the O
memory O
and O
the O
query O
. O
Neural Method
Semantic Method
Encoders Method
( O
NSE Method
) O
munkhdalai2016neural O
extended Method
MemNets Method
by O
introducing O
a O
write Method
operation Method
which O
can O
evolve O
the O
memory O
over O
time O
during O
the O
course O
of O
reading Task
. O
Iterative Method
reasoning Method
has O
been O
found O
effective O
in O
several O
more O
recent O
models O
, O
including O
the O
Iterative Method
Attentive Method
Reader Method
sordoni2016iterative O
and O
ReasoNet Method
shen2016reasonet O
. O
The O
latter O
allows O
dynamic Task
reasoning Task
steps Task
and O
is O
trained O
with O
reinforcement Method
learning Method
. O
Other O
related O
works O
include O
Dynamic Method
Entity Method
Representation Method
network Method
( O
DER Method
) O
kobayashi2016dynamic O
, O
which O
builds O
dynamic Method
representations Method
of O
the O
candidate O
answers O
while O
reading O
the O
document O
, O
and O
accumulates O
the O
information O
about O
an O
entity O
by O
max Method
- Method
pooling Method
; O
EpiReader Method
trischler2016natural O
consists O
of O
two O
networks O
, O
where O
one O
proposes O
a O
small O
set O
of O
candidate O
answers O
, O
and O
the O
other O
reranks O
the O
proposed O
candidates O
conditioned O
on O
the O
query O
and O
the O
context O
; O
Bi Method
- Method
Directional Method
Attention Method
Flow Method
network Method
( O
BiDAF Method
) O
adopts O
a O
multi Method
- Method
stage Method
hierarchical Method
architecture Method
along O
with O
a O
flow Method
- Method
based Method
attention Method
mechanism Method
; O
bajgar2016embracing O
showed O
a O
10 O
% O
improvement O
on O
the O
CBT Material
corpus Material
hill2015goldilocks O
by O
training O
the O
AS Method
Reader Method
on O
an O
augmented O
training O
set O
of O
about O
14 O
million O
examples O
, O
making O
a O
case O
for O
the O
community O
to O
exploit O
data O
abundance O
. O
The O
focus O
of O
this O
paper O
, O
however O
, O
is O
on O
designing O
models O
which O
exploit O
the O
available O
data O
efficiently O
. O
section O
: O
Gated Method
- Method
Attention Method
Reader Method
Our O
proposed O
GA Method
readers O
perform O
multiple O
hops O
over O
the O
document O
( O
context O
) O
, O
similar O
to O
the O
Memory Method
Networks Method
architecture Method
sukhbaatar2015end O
. O
Multi Method
- Method
hop Method
architectures Method
mimic O
the O
multi Task
- Task
step Task
comprehension Task
process Task
of O
human O
readers O
, O
and O
have O
shown O
promising O
results O
in O
several O
recent O
models O
for O
text Task
comprehension Task
sordoni2016iterative O
, O
kumar2015ask O
, O
shen2016reasonet O
. O
The O
contextual Method
representations Method
in O
GA Method
readers O
, O
namely O
the O
embeddings O
of O
words O
in O
the O
document O
, O
are O
iteratively O
refined O
across O
hops O
until O
reaching O
a O
final O
attention Method
- Method
sum Method
module Method
kadlec2016text O
which O
maps O
the O
contextual Method
representations Method
in O
the O
last O
hop O
to O
a O
probability O
distribution O
over O
candidate O
answers O
. O
The O
attention Method
mechanism Method
has O
been O
introduced O
recently O
to O
model O
human O
focus O
, O
leading O
to O
significant O
improvement O
in O
machine Task
translation Task
and O
image Task
captioning Task
bahdanau2014neural O
, O
mnih2014recurrent O
. O
In O
reading Task
comprehension Task
tasks Task
, O
ideally O
, O
the O
semantic O
meanings O
carried O
by O
the O
contextual O
embeddings O
should O
be O
aware O
of O
the O
query O
across O
hops O
. O
As O
an O
example O
, O
human O
readers O
are O
able O
to O
keep O
the O
question O
in O
mind O
during O
multiple O
passes O
of O
reading Task
, O
to O
successively O
mask O
away O
information O
irrelevant O
to O
the O
query O
. O
However O
, O
existing O
neural Method
network Method
readers Method
are O
restricted O
to O
either O
attend O
to O
tokens O
hermann2015teaching O
, O
chen2016thorough O
or O
entire O
sentences O
weston2014memory O
, O
with O
the O
assumption O
that O
certain O
sub O
- O
parts O
of O
the O
document O
are O
more O
important O
than O
others O
. O
In O
contrast O
, O
we O
propose O
a O
finer Method
- Method
grained Method
model Method
which O
attends O
to O
components O
of O
the O
semantic Method
representation Method
being O
built O
up O
by O
the O
GRU Method
. O
The O
new O
attention Method
mechanism Method
, O
called O
gated Method
- Method
attention Method
, O
is O
implemented O
via O
multiplicative O
interactions O
between O
the O
query O
and O
the O
contextual O
embeddings O
, O
and O
is O
applied O
per O
hop O
to O
act O
as O
fine O
- O
grained Method
information Method
filters Method
during O
the O
multi Method
- Method
step Method
reasoning Method
. O
The O
filters O
weigh O
individual O
components O
of O
the O
vector Method
representation Method
of O
each O
token O
in O
the O
document O
separately O
. O
The O
design O
of O
gated Method
- Method
attention Method
layers O
is O
motivated O
by O
the O
effectiveness O
of O
multiplicative O
interaction O
among O
vector O
- O
space O
representations O
, O
e.g. O
, O
in O
various O
types O
of O
recurrent Method
units Method
hochreiter1997long O
, O
wu2016multiplicative O
and O
in O
relational Method
learning Method
yang2014learning O
, O
kiros2014multiplicative O
. O
While O
other O
types O
of O
compositional O
operators O
are O
possible O
, O
such O
as O
concatenation Method
or O
addition Method
mitchell2008vector O
, O
we O
find O
that O
multiplication Method
has O
strong O
empirical O
performance O
( O
section O
[ O
reference O
] O
) O
, O
where O
query Method
representations Method
naturally O
serve O
as O
information O
filters O
across O
hops O
. O
subsection O
: O
Model O
Details O
Several O
components O
of O
the O
model O
use O
a O
Gated Method
Recurrent Method
Unit Method
( O
GRU Method
) O
cho2014learning O
which O
maps O
an O
input O
sequence O
to O
an O
ouput O
sequence O
as O
follows O
: O
where O
denotes O
the O
Hadamard O
product O
or O
the O
element Method
- Method
wise Method
multiplication Method
. O
and O
are O
called O
the O
reset O
and O
update O
gates O
respectively O
, O
and O
the O
candidate O
output O
. O
A O
Bi Method
- Method
directional Method
GRU Method
( O
Bi Method
- Method
GRU Method
) O
processes O
the O
sequence O
in O
both O
forward O
and O
backward O
directions O
to O
produce O
two O
sequences O
and O
, O
which O
are O
concatenated O
at O
the O
output O
where O
denotes O
the O
full O
output O
of O
the O
Bi Method
- Method
GRU Method
obtained O
by O
concatenating O
each O
forward O
state O
and O
backward O
state O
at O
step O
given O
the O
input O
. O
Note O
is O
a O
matrix O
in O
where O
is O
the O
number O
of O
hidden O
units O
in O
GRU Method
. O
Let O
denote O
the O
token O
embeddings O
of O
the O
document O
, O
which O
are O
also O
inputs O
at O
layer O
1 O
for O
the O
document O
reader O
below O
, O
and O
denote O
the O
token O
embeddings O
of O
the O
query O
. O
Here O
and O
denote O
the O
document O
and O
query O
lengths O
respectively O
. O
subsubsection O
: O
Multi Method
- Method
Hop Method
Architecture Method
Fig O
. O
[ O
reference O
] O
illustrates O
the O
Gated Method
- Method
Attention Method
( O
GA Method
) O
reader O
. O
The O
model O
reads O
the O
document O
and O
the O
query O
over O
horizontal O
layers O
, O
where O
layer O
receives O
the O
contextual O
embeddings O
of O
the O
document O
from O
the O
previous O
layer O
. O
The O
document O
embeddings O
are O
transformed O
by O
taking O
the O
full O
output O
of O
a O
document O
Bi O
- O
GRU Method
( O
indicated O
in O
blue O
in O
Fig O
. O
[ O
reference O
] O
) O
: O
At O
the O
same O
time O
, O
a O
layer Method
- Method
specific Method
query Method
representation Method
is O
computed O
as O
the O
full O
output O
of O
a O
separate O
query O
Bi O
- O
GRU Method
( O
indicated O
in O
green O
in O
Figure O
[ O
reference O
] O
) O
: O
Next O
, O
Gated Method
- Method
Attention Method
is O
applied O
to O
and O
to O
compute O
inputs O
for O
the O
next O
layer O
. O
where O
GA Method
is O
defined O
in O
the O
following O
subsection O
. O
subsubsection O
: O
Gated Method
- Method
Attention Method
Module O
For O
brevity O
, O
let O
us O
drop O
the O
superscript O
in O
this O
subsection O
as O
we O
are O
focusing O
on O
a O
particular O
layer O
. O
For O
each O
token O
in O
, O
the O
GA Method
module O
forms O
a O
token Method
- Method
specific Method
representation Method
of O
the O
query O
using O
soft O
attention O
, O
and O
then O
multiplies O
the O
query Method
representation Method
element O
- O
wise O
with O
the O
document Method
token Method
representation Method
. O
Specifically O
, O
for O
: O
In O
equation O
( O
[ O
reference O
] O
) O
we O
use O
the O
multiplication O
operator O
to O
model O
the O
interactions O
between O
and O
. O
In O
the O
experiments O
section O
, O
we O
also O
report O
results O
for O
other O
choices O
of O
gating O
functions O
, O
including O
addition Method
and O
concatenation O
. O
subsubsection O
: O
Answer Task
Prediction Task
Let O
be O
an O
intermediate O
output O
of O
the O
final O
layer O
query O
Bi O
- O
GRU Method
at O
the O
location O
of O
the O
cloze O
token O
in O
the O
query O
, O
and O
be O
the O
full O
output O
of O
final O
layer O
document O
Bi O
- O
GRU Method
. O
To O
obtain O
the O
probability O
that O
a O
particular O
token O
in O
the O
document O
answers O
the O
query O
, O
we O
take O
an O
inner O
- O
product O
between O
these O
two O
, O
and O
pass O
through O
a O
softmax Method
layer Method
: O
where O
vector O
defines O
a O
probability O
distribution O
over O
the O
tokens O
in O
the O
document O
. O
The O
probability O
of O
a O
particular O
candidate O
as O
being O
the O
answer O
is O
then O
computed O
by O
aggregating O
the O
probabilities O
of O
all O
document O
tokens O
which O
appear O
in O
and O
renormalizing O
over O
the O
candidates O
: O
where O
is O
the O
set O
of O
positions O
where O
a O
token O
in O
appears O
in O
the O
document O
. O
This O
aggregation Method
operation Method
is O
the O
same O
as O
the O
pointer O
sum O
attention O
applied O
in O
the O
AS Method
Reader Method
kadlec2016text O
. O
Finally O
, O
the O
candidate O
with O
maximum O
probability O
is O
selected O
as O
the O
predicted O
answer O
: O
During O
the O
training O
phase O
, O
model O
parameters O
of O
GA Method
are O
updated O
w.r.t O
. O
a O
cross Metric
- Metric
entropy Metric
loss Metric
between O
the O
predicted O
probabilities O
and O
the O
true O
answers O
. O
subsubsection O
: O
Further O
Enhancements O
Character Task
- Task
level Task
Embeddings Task
: O
Given O
a O
token O
from O
the O
document O
or O
query O
, O
its O
vector Method
space Method
representation Method
is O
computed O
as O
. O
retrieves O
the O
word O
- O
embedding O
for O
from O
a O
lookup O
table O
, O
whose O
rows O
hold O
a O
vector O
for O
each O
unique O
token O
in O
the O
vocabulary O
. O
We O
also O
utilize O
a O
character Method
composition Method
model Method
which O
generates O
an O
orthographic Method
embedding Method
of O
the O
token O
. O
Such O
embeddings O
have O
been O
previously O
shown O
to O
be O
helpful O
for O
tasks O
like O
Named Task
Entity Task
Recognition Task
yang2016multi O
and O
dealing O
with O
OOV Task
tokens Task
at O
test O
time O
dhingra2016tweet2vec O
. O
The O
embedding O
is O
generated O
by O
taking O
the O
final O
outputs O
and O
of O
a O
Bi Method
- Method
GRU Method
applied O
to O
embeddings O
from O
a O
lookup O
table O
of O
characters O
in O
the O
token O
, O
and O
applying O
a O
linear Method
transformation Method
: O
Question O
Evidence O
Common O
Word O
Feature O
( O
qe O
- O
comm O
) O
: O
li2016dataset O
recently O
proposed O
a O
simple O
token Method
level Method
indicator Method
feature Method
which O
significantly O
boosts O
reading Task
comprehension Task
performance O
in O
some O
cases O
. O
For O
each O
token O
in O
the O
document O
we O
construct O
a O
one O
- O
hot O
vector O
indicating O
its O
presence O
in O
the O
query O
. O
It O
can O
be O
incorporated O
into O
the O
GA Method
reader O
by O
assigning O
a O
feature O
lookup O
table O
( O
we O
use O
) O
, O
taking O
the O
feature Method
embedding Method
and O
appending O
it O
to O
the O
inputs O
of O
the O
last O
layer O
document O
BiGRU O
as O
, O
for O
all O
. O
We O
conducted O
several O
experiments O
both O
with O
and O
without O
this O
feature O
and O
observed O
some O
interesting O
trends O
, O
which O
are O
discussed O
below O
. O
Henceforth O
, O
we O
refer O
to O
this O
feature O
as O
the O
qe O
- O
comm O
feature O
or O
just O
feature O
. O
section O
: O
Experiments O
and O
Results O
subsection O
: O
Datasets O
We O
evaluate O
the O
GA Method
reader O
on O
five O
large O
- O
scale O
datasets O
recently O
proposed O
in O
the O
literature O
. O
The O
first O
two O
, O
CNN Material
and O
Daily O
Mail O
news O
stories O
consist O
of O
articles O
from O
the O
popular O
CNN Material
and Material
Daily Material
Mail Material
websites Material
hermann2015teaching O
. O
A O
query O
over O
each O
article O
is O
formed O
by O
removing O
an O
entity O
from O
the O
short O
summary O
which O
follows O
the O
article O
. O
Further O
, O
entities O
within O
each O
article O
were O
anonymized O
to O
make O
the O
task O
purely O
a O
comprehension O
one O
. O
N O
- O
gram O
statistics O
, O
for O
instance O
, O
computed O
over O
the O
entire O
corpus O
are O
no O
longer O
useful O
in O
such O
an O
anonymized O
corpus O
. O
The O
next O
two O
datasets O
are O
formed O
from O
two O
different O
subsets O
of O
the O
Children Material
’s Material
Book Material
Test Material
( O
CBT Material
) O
hill2015goldilocks O
. O
Documents O
consist O
of O
20 O
contiguous O
sentences O
from O
the O
body O
of O
a O
popular O
children O
’s O
book O
, O
and O
queries O
are O
formed O
by O
deleting O
a O
token O
from O
the O
21 O
st O
sentence O
. O
We O
only O
focus O
on O
subsets O
where O
the O
deleted O
token O
is O
either O
a O
common O
noun O
( O
CN O
) O
or O
named O
entity O
( O
NE O
) O
since O
simple O
language Method
models Method
already O
give O
human O
- O
level O
performance O
on O
the O
other O
types O
( O
cf O
. O
hill2015goldilocks O
) O
. O
The O
final O
dataset O
is O
Who O
Did O
What O
( O
WDW O
) O
onishi2016did O
, O
constructed O
from O
the O
LDC O
English O
Gigaword O
newswire O
corpus O
. O
First O
, O
article O
pairs O
which O
appeared O
around O
the O
same O
time O
and O
with O
overlapping O
entities O
are O
chosen O
, O
and O
then O
one O
article O
forms O
the O
document O
and O
a O
cloze O
query O
is O
constructed O
from O
the O
other O
. O
Missing O
tokens O
are O
always O
person O
named O
entities O
. O
Questions O
which O
are O
easily O
answered O
by O
simple O
baselines O
are O
filtered O
out O
, O
to O
make O
the O
task O
more O
challenging O
. O
There O
are O
two O
versions O
of O
the O
training O
set O
— O
a O
small O
but O
focused O
“ O
Strict O
” O
version O
and O
a O
large O
but O
noisy O
“ O
Relaxed O
” O
version O
. O
We O
report O
results O
on O
both O
settings O
which O
share O
the O
same O
validation O
and O
test O
sets O
. O
Statistics O
of O
all O
the O
datasets O
used O
in O
our O
experiments O
are O
summarized O
in O
the O
Appendix O
( O
Table O
[ O
reference O
] O
) O
. O
subsection O
: O
Performance O
Comparison O
Tables O
[ O
reference O
] O
and O
[ O
reference O
] O
show O
a O
comparison O
of O
the O
performance O
of O
GA Method
Reader O
with O
previously O
published O
results O
on O
WDW O
and O
CNN Material
, Material
Daily Material
Mail Material
, O
CBT Material
datasets Material
respectively O
. O
The O
numbers O
reported O
for O
GA Method
Reader O
are O
for O
single Method
best O
models O
, O
though O
we O
compare O
to O
both O
ensembles Method
and O
single Method
models O
from O
prior O
work O
. O
GA Method
Reader– O
refers O
to O
an O
earlier O
version O
of O
the O
model O
, O
unpublished O
but O
described O
in O
a O
preprint O
, O
with O
the O
following O
differences— O
( O
1 O
) O
it O
does O
not O
utilize O
token O
- O
specific O
attentions O
within O
the O
GA Method
module O
, O
as O
described O
in O
equation O
( O
[ O
reference O
] O
) O
, O
( O
2 O
) O
it O
does O
not O
use O
a O
character Method
composition Method
model Method
, O
( O
3 O
) O
it O
is O
initialized O
with O
word O
embeddings O
pretrained O
on O
the O
corpus O
itself O
rather O
than O
GloVe O
. O
A O
detailed O
analysis O
of O
these O
differences O
is O
studied O
in O
the O
next O
section O
. O
Here O
we O
present O
4 O
variants O
of O
the O
latest O
GA Method
Reader O
, O
using O
combinations O
of O
whether O
the O
qe O
- O
comm O
feature O
is O
used O
( O
+ O
feature O
) O
or O
not O
, O
and O
whether O
the O
word O
lookup O
table O
is O
updated O
during O
training O
or O
fixed O
to O
its O
initial O
value O
. O
Other O
hyperparameters O
are O
listed O
in O
Appendix O
[ O
reference O
] O
. O
Interestingly O
, O
we O
observe O
that O
feature Method
engineering Method
leads O
to O
significant O
improvements O
for O
WDW O
and O
CBT Material
datasets Material
, O
but O
not O
for O
CNN Material
and O
Daily O
Mail O
datasets O
. O
We O
note O
that O
anonymization O
of O
the O
latter O
datasets O
means O
that O
there O
is O
already O
some O
feature Method
engineering Method
( O
it O
adds O
hints O
about O
whether O
a O
token O
is O
an O
entity O
) O
, O
and O
these O
are O
much O
larger O
than O
the O
other O
four O
. O
In O
machine Task
learning Task
it O
is O
common O
to O
see O
the O
effect O
of O
feature Method
engineering Method
diminish O
with O
increasing O
data O
size O
. O
Similarly O
, O
fixing O
the O
word O
embeddings O
provides O
an O
improvement O
for O
the O
WDW O
and O
CBT Material
, O
but O
not O
for O
CNN Material
and Material
Daily Material
Mail Material
. O
This O
is O
not O
surprising O
given O
that O
the O
latter O
datasets O
are O
larger O
and O
less O
prone O
to O
overfitting O
. O
Comparing O
with O
prior O
work O
, O
on O
the O
WDW O
dataset O
the O
basic O
version O
of O
the O
GA Method
Reader O
outperforms O
all O
previously O
published O
models O
when O
trained O
on O
the O
Strict O
setting O
. O
By O
adding O
the O
qe Method
- Method
comm Method
feature Method
the O
performance O
increases O
by O
3.2 O
% O
and O
3.5 O
% O
on O
the O
Strict O
and O
Relaxed O
settings O
respectively O
to O
set O
a O
new O
state O
of O
the O
art O
on O
this O
dataset O
. O
On O
the O
CNN Material
and O
Daily O
Mail O
datasets O
the O
GA Method
Reader O
leads O
to O
an O
improvement O
of O
3.2 O
% O
and O
4.3 O
% O
respectively O
over O
the O
best O
previous O
single Method
models O
. O
They O
also O
outperform O
previous O
ensemble Method
models Method
, O
setting O
a O
new O
state O
of O
that O
art O
for O
both O
datasets O
. O
For O
CBT Material
- O
NE O
, O
GA Method
Reader O
with O
the O
qe Method
- Method
comm Method
feature Method
outperforms O
all O
previous O
single Method
and O
ensemble Method
models Method
except O
the O
AS Method
Reader Method
trained O
on O
the O
much O
larger O
BookTest O
Corpus O
bajgar2016embracing O
. O
Lastly O
, O
on O
CBT Material
- O
CN O
the O
GA Method
Reader O
with O
the O
qe Method
- Method
comm Method
feature Method
outperforms O
all O
previously O
published O
single Method
models O
except O
the O
NSE Method
, O
and O
AS Method
Reader Method
trained O
on O
a O
larger O
corpus O
. O
For O
each O
of O
the O
4 O
datasets O
on O
which O
GA Method
achieves O
the O
top O
performance O
, O
we O
conducted O
one O
- O
sample O
proportion O
tests O
to O
test O
whether O
GA Method
is O
significantly O
better O
than O
the O
second O
- O
best O
baseline O
. O
The O
p O
- O
values O
are O
for O
CNN Material
, Material
for Material
DailyMail Material
, O
for O
CBT Material
- O
NE O
, O
and O
for O
WDW O
. O
In O
other O
words O
, O
GA Method
statistically O
significantly O
outperforms O
all O
other O
baselines O
on O
3 O
out O
of O
those O
4 O
datasets O
at O
the O
significance O
level O
. O
The O
results O
could O
be O
even O
more O
significant O
under O
paired O
tests O
, O
however O
we O
did O
not O
have O
access O
to O
the O
predictions O
from O
the O
baselines O
. O
subsection O
: O
GA Method
Reader O
Analysis O
[ O
b O
] O
0.245 O
[ O
b O
] O
0.245 O
[ O
b O
] O
0.245 O
[ O
b O
] O
0.245 O
In O
this O
section O
we O
do O
an O
ablation O
study O
to O
see O
the O
effect O
of O
Gated O
Attention O
. O
We O
compare O
the O
GA Method
Reader O
as O
described O
here O
to O
a O
model O
which O
is O
exactly O
the O
same O
in O
all O
aspects O
, O
except O
that O
it O
passes O
document O
embeddings O
in O
each O
layer O
directly O
to O
the O
inputs O
of O
the O
next O
layer O
without O
using O
the O
GA Method
module O
. O
In O
other O
words O
for O
all O
. O
This O
model O
ends O
up O
using O
only O
one O
query O
GRU Method
at O
the O
output O
layer O
for O
selecting O
the O
answer O
from O
the O
document O
. O
We O
compare O
these O
two O
variants O
both O
with O
and O
without O
the O
qe Method
- Method
comm Method
feature Method
on O
CNN Material
and O
WDW O
datasets O
for O
three O
subsets O
of O
the O
training O
data O
- O
50 O
% O
, O
75 O
% O
and O
100 O
% O
. O
Test Metric
set Metric
accuracies Metric
for O
these O
settings O
are O
shown O
in O
Figure O
[ O
reference O
] O
. O
On O
CNN Material
when O
tested O
without O
feature Method
engineering Method
, O
we O
observe O
that O
GA Method
provides O
a O
significant O
boost O
in O
performance O
compared O
to O
without O
GA Method
. O
When O
tested O
with O
the O
feature O
it O
still O
gives O
an O
improvement O
, O
but O
the O
improvement O
is O
significant O
only O
with O
100 O
% O
training O
data O
. O
On O
WDW O
- O
Strict O
, O
which O
is O
a O
third O
of O
the O
size O
of O
CNN Material
, O
without O
the O
feature O
we O
see O
an O
improvement O
when O
using O
GA Method
versus O
without O
using O
GA Method
, O
which O
becomes O
significant O
as O
the O
training O
set O
size O
increases O
. O
When O
tested O
with O
the O
feature O
on O
WDW O
, O
for O
a O
small O
data O
size O
without O
GA Method
does O
better O
than O
with O
GA Method
, O
but O
as O
the O
dataset O
size O
increases O
they O
become O
equivalent O
. O
We O
conclude O
that O
GA Method
provides O
a O
boost O
in O
the O
absence O
of O
feature Task
engineering Task
, O
or O
as O
the O
training O
set O
size O
increases O
. O
Next O
we O
look O
at O
the O
question O
of O
how O
to O
gate O
intermediate O
document O
reader O
states O
from O
the O
query O
, O
i.e. O
what O
operation O
to O
use O
in O
equation O
[ O
reference O
] O
. O
Table O
[ O
reference O
] O
( O
top O
) O
shows O
the O
performance O
on O
WDW O
dataset O
for O
three O
common O
choices O
– O
sum O
( O
) O
, O
concatenate O
( O
) O
and O
multiply O
( O
) O
. O
Empirically O
we O
find O
element Method
- Method
wise Method
multiplication Method
does O
significantly O
better O
than O
the O
other O
two O
, O
which O
justifies O
our O
motivation O
to O
“ O
filter O
” O
out O
document O
features O
which O
are O
irrelevant O
to O
the O
query O
. O
At O
the O
bottom O
of O
Table O
[ O
reference O
] O
we O
show O
the O
effect O
of O
varying O
the O
number O
of O
hops O
of O
the O
GA Method
Reader O
on O
the O
final O
performance O
. O
We O
note O
that O
for O
, O
our O
model O
is O
equivalent O
to O
the O
AS Method
Reader Method
without O
any O
GA Method
modules O
. O
We O
see O
a O
steep O
and O
steady O
rise O
in O
accuracy Metric
as O
the O
number O
of O
hops O
is O
increased O
from O
to O
, O
which O
remains O
constant O
beyond O
that O
. O
This O
is O
a O
common O
trend O
in O
machine Task
learning Task
as O
model Task
complexity Task
is O
increased O
, O
however O
we O
note O
that O
a O
multi Method
- Method
hop Method
architecture Method
is O
important O
to O
achieve O
a O
high O
performance O
for O
this O
task O
, O
and O
provide O
further O
evidence O
for O
this O
in O
the O
next O
section O
. O
subsection O
: O
Ablation Task
Study Task
for O
Model O
Components O
Table O
[ O
reference O
] O
shows O
accuracy Metric
on O
WDW O
by O
removing O
one O
component O
at O
a O
time O
. O
The O
steepest O
reduction O
is O
observed O
when O
we O
replace O
pretrained O
GloVe O
vectors O
with O
those O
pretrained O
on O
the O
corpus O
itself O
. O
GloVe Method
vectors Method
were O
trained O
on O
a O
large O
corpus O
of O
about O
6 O
billion O
tokens O
pennington2014glove O
, O
and O
provide O
an O
important O
source O
of O
prior O
knowledge O
for O
the O
model O
. O
Note O
that O
the O
strongest O
baseline O
on O
WDW O
, O
NSE Method
munkhdalai2016reasoning O
, O
also O
uses O
pretrained O
GloVe O
vectors O
, O
hence O
the O
comparison O
is O
fair O
in O
that O
respect O
. O
Next O
, O
we O
observe O
a O
substantial O
drop O
when O
removing O
token O
- O
specific O
attentions O
over O
the O
query O
in O
the O
GA Method
module O
, O
which O
allow O
gating O
individual O
tokens O
in O
the O
document O
only O
by O
parts O
of O
the O
query O
relevant O
to O
that O
token O
rather O
than O
the O
overall O
query Method
representation Method
. O
Finally O
, O
removing O
the O
character O
embeddings O
, O
which O
were O
only O
used O
for O
WDW O
and O
CBT Material
, O
leads O
to O
a O
reduction O
of O
about O
1 O
% O
in O
the O
performance O
. O
subsection O
: O
Attention Task
Visualization Task
To O
gain O
an O
insight O
into O
the O
reading Task
process Task
employed O
by O
the O
model O
we O
analyzed O
the O
attention O
distributions O
at O
intermediate O
layers O
of O
the O
reader O
. O
Figure O
[ O
reference O
] O
shows O
an O
example O
from O
the O
validation O
set O
of O
WDW O
dataset O
( O
several O
more O
are O
in O
the O
Appendix O
) O
. O
In O
each O
figure O
, O
the O
left O
and O
middle O
plots O
visualize O
attention O
over O
the O
query O
( O
equation O
[ O
reference O
] O
) O
for O
candidates O
in O
the O
document O
after O
layers O
1 O
& O
2 O
respectively O
. O
The O
right O
plot O
shows O
attention O
over O
candidates O
in O
the O
document O
of O
cloze O
placeholder O
( O
XXX O
) O
in O
the O
query O
at O
the O
final O
layer O
. O
The O
full O
document O
, O
query O
and O
correct O
answer O
are O
shown O
at O
the O
bottom O
. O
A O
generic O
pattern O
observed O
in O
these O
examples O
is O
that O
in O
intermediate O
layers O
, O
candidates O
in O
the O
document O
( O
shown O
along O
rows O
) O
tend O
to O
pick O
out O
salient O
tokens O
in O
the O
query O
which O
provide O
clues O
about O
the O
cloze O
, O
and O
in O
the O
final O
layer O
the O
candidate O
with O
the O
highest O
match O
with O
these O
tokens O
is O
selected O
as O
the O
answer O
. O
In O
Figure O
[ O
reference O
] O
there O
is O
a O
high O
attention O
of O
the O
correct O
answer O
on O
financial O
regulatory O
standards O
in O
the O
first O
layer O
, O
and O
on O
us O
president O
in O
the O
second O
layer O
. O
The O
incorrect O
answer O
, O
in O
contrast O
, O
only O
attends O
to O
one O
of O
these O
aspects O
, O
and O
hence O
receives O
a O
lower O
score O
in O
the O
final O
layer O
despite O
the O
n O
- O
gram O
overlap O
it O
has O
with O
the O
cloze O
token O
in O
the O
query O
. O
Importantly O
, O
different O
layers O
tend O
to O
focus O
on O
different O
tokens O
in O
the O
query O
, O
supporting O
the O
hypothesis O
that O
the O
multi Method
- Method
hop Method
architecture Method
of O
GA Method
Reader O
is O
able O
to O
combine O
distinct O
pieces O
of O
information O
to O
answer O
the O
query O
. O
section O
: O
Conclusion O
We O
presented O
the O
Gated Method
- Method
Attention Method
reader O
for O
answering Task
cloze Task
- Task
style Task
questions Task
over O
documents O
. O
The O
GA Method
reader O
features O
a O
novel O
multiplicative Method
gating Method
mechanism Method
, O
combined O
with O
a O
multi Method
- Method
hop Method
architecture Method
. O
Our O
model O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
several O
large O
- O
scale O
benchmark O
datasets O
with O
more O
than O
4 O
% O
improvements O
over O
competitive O
baselines O
. O
Our O
model O
design O
is O
backed O
up O
by O
an O
ablation O
study O
showing O
statistically O
significant O
improvements O
of O
using O
Gated Method
Attention Method
as O
information Method
filters Method
. O
We O
also O
showed O
empirically O
that O
multiplicative Method
gating Method
is O
superior O
to O
addition Method
and O
concatenation O
operations O
for O
implementing O
gated Method
- Method
attentions Method
, O
though O
a O
theoretical O
justification O
remains O
part O
of O
future O
research O
goals O
. O
Analysis O
of O
document Task
and Task
query Task
attentions Task
in O
intermediate O
layers O
of O
the O
reader O
further O
reveals O
that O
the O
model O
iteratively O
attends O
to O
different O
aspects O
of O
the O
query O
to O
arrive O
at O
the O
final O
answer O
. O
In O
this O
paper O
we O
have O
focused O
on O
text Task
comprehension Task
, O
but O
we O
believe O
that O
the O
Gated Method
- Method
Attention Method
mechanism O
may O
benefit O
other O
tasks O
as O
well O
where O
multiple O
sources O
of O
information O
interact O
. O
section O
: O
Acknowledgments O
This O
work O
was O
funded O
by O
NSF O
under O
CCF1414030 O
and O
Google O
Research O
. O
bibliography O
: O
References O
appendix O
: O
Implementation O
Details O
Our O
model O
was O
implemented O
using O
the O
Theano O
2016arXiv160502688short O
and O
Lasagne O
Python O
libraries O
. O
We O
used O
stochastic Method
gradient Method
descent Method
with O
ADAM Method
updates Method
for O
optimization Task
, O
which O
combines O
classical O
momentum O
and O
adaptive Method
gradients Method
kingma2014adam O
. O
The O
batch O
size O
was O
32 O
and O
the O
initial O
learning Metric
rate Metric
was O
which O
was O
halved O
every O
epoch O
after O
the O
second O
epoch O
. O
The O
same O
setting O
is O
applied O
to O
all O
models O
and O
datasets O
. O
We O
also O
used O
gradient Method
clipping Method
with O
a O
threshold O
of O
10 O
to O
stabilize O
GRU Method
training O
pascanu2012difficulty O
. O
We O
set O
the O
number O
of O
layers O
to O
be O
for O
all O
experiments O
. O
The O
number O
of O
hidden O
units O
for O
the O
character O
GRU Method
was O
set O
to O
50 O
. O
The O
remaining O
two O
hyperparameters O
— O
size O
of O
document O
and O
query O
GRUs O
, O
and O
dropout Metric
rate Metric
— O
were O
tuned O
on O
the O
validation O
set O
, O
and O
their O
optimal O
values O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
In O
general O
, O
the O
optimal O
GRU Method
size O
increases O
and O
the O
dropout Metric
rate Metric
decreases O
as O
the O
corpus O
size O
increases O
. O
The O
word O
lookup O
table O
was O
initialized O
with O
GloVe O
vectors O
pennington2014glove O
and O
OOV O
tokens O
at O
test O
time O
were O
assigned O
unique O
random O
vectors O
. O
We O
empirically O
observed O
that O
initializing O
with O
pre O
- O
trained O
embeddings O
gives O
higher O
performance O
compared O
to O
random Method
initialization Method
for O
all O
datasets O
. O
Furthermore O
, O
for O
smaller O
datasets O
( O
WDW O
and O
CBT Material
) O
we O
found O
that O
fixing O
these O
embeddings O
to O
their O
pretrained O
values O
led O
to O
higher O
test O
performance O
, O
possibly O
since O
it O
avoids O
overfitting O
. O
We O
do O
not O
use O
the O
character Method
composition Method
model Method
for O
CNN Material
and O
Daily O
Mail O
, O
since O
their O
entities O
( O
and O
hence O
candidate O
answers O
) O
are O
anonymized O
to O
generic O
tokens O
. O
For O
other O
datasets O
the O
character O
lookup O
table O
was O
randomly O
initialized O
with O
vectors O
. O
All O
other O
parameters O
were O
initialized O
to O
their O
default O
values O
as O
specified O
in O
the O
Lasagne O
library O
. O
appendix O
: O
Attention Method
Plots Method
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
[ O
b O
] O
