document O
: O
Deeply Method
- Method
Recursive Method
Convolutional Method
Network Method
for O
Image Task
Super Task
- Task
Resolution Task
We O
propose O
an O
image Task
super Task
- Task
resolution Task
method O
( O
SR Task
) O
using O
a O
deeply O
- O
recursive O
convolutional Method
network O
( O
DRCN Method
) Method
. O
Our O
network O
has O
a O
very O
deep Method
recursive Method
layer Method
( O
up O
to O
16 O
recursions O
) O
. O
Increasing O
recursion O
depth O
can O
improve O
performance O
without O
introducing O
new O
parameters O
for O
additional O
convolutions O
. O
Albeit O
advantages O
, O
learning O
a O
DRCN Method
is O
very O
hard O
with O
a O
standard O
gradient Method
descent Method
method Method
due O
to O
exploding O
/ O
vanishing O
gradients O
. O
To O
ease O
the O
difficulty O
of O
training Task
, O
we O
propose O
two O
extensions O
: O
recursive Method
- Method
supervision Method
and O
skip Method
- Method
connection Method
. O
Our O
method O
outperforms O
previous O
methods O
by O
a O
large O
margin O
. O
section O
: O
Introduction O
For O
image Task
super Task
- Task
resolution Task
( O
SR Task
) O
, O
receptive O
field O
of O
a O
convolutional Method
network O
determines O
the O
amount O
of O
contextual O
information O
that O
can O
be O
exploited O
to O
infer O
missing O
high O
- O
frequency O
components O
. O
For O
example O
, O
if O
there O
exists O
a O
pattern O
with O
smoothed O
edges O
contained O
in O
a O
receptive O
field O
, O
it O
is O
plausible O
that O
the O
pattern O
is O
recognized O
and O
edges O
are O
appropriately O
sharpened O
. O
As O
SR Task
is O
an O
ill O
- O
posed O
inverse Task
problem Task
, O
collecting O
and O
analyzing O
more O
neighbor O
pixels O
can O
possibly O
give O
more O
clues O
on O
what O
may O
be O
lost O
by O
downsampling O
. O
Deep O
convolutional Method
networks O
( O
DCN Method
) O
succeeding O
in O
various O
computer Task
vision Task
tasks Task
often O
use O
very O
large O
receptive O
fields O
( O
224x224 O
common O
in O
ImageNet Task
classification Task
) O
. O
Among O
many O
approaches O
to O
widen O
the O
receptive O
field O
, O
increasing O
network O
depth O
is O
one O
possible O
way O
: O
a O
convolutional Method
( O
conv Method
. O
) O
layer O
with O
filter O
size O
larger O
than O
a O
or O
a O
pooling O
( O
pool O
. O
) O
layer O
that O
reduces O
the O
dimension O
of O
intermediate Method
representation Method
can O
be O
used O
. O
Both O
approaches O
have O
drawbacks O
: O
a O
conv Method
. O
layer O
introduces O
more O
parameters O
and O
a O
pool O
. O
layer Method
typically O
discards O
some O
pixel O
- O
wise O
information O
. O
For O
image Task
restoration Task
problems Task
such O
as O
super Task
- Task
resolution Task
and O
denoising Task
, O
image Task
details Task
are O
very O
important O
. O
Therefore O
, O
most O
deep Method
- Method
learning Method
approaches Method
for O
such O
problems O
do O
not O
use O
pooling Method
. O
Increasing O
depth O
by O
adding O
a O
new O
weight Method
layer Method
basically O
introduces O
more O
parameters O
. O
Two O
problems O
can O
arise O
. O
First O
, O
overfitting O
is O
highly O
likely O
. O
More O
data O
are O
now O
required O
. O
Second O
, O
the O
model O
becomes O
too O
huge O
to O
be O
stored O
and O
retrieved O
. O
To O
resolve O
these O
issues O
, O
we O
use O
a O
deeply O
- O
recursive O
convolutional Method
network O
( O
DRCN Method
) Method
. O
DRCN Method
repeatedly O
applies O
the O
same O
convolutional Method
layer O
as O
many O
times O
as O
desired O
. O
The O
number O
of O
parameters O
do O
not O
increase O
while O
more O
recursions O
are O
performed O
. O
Our O
network O
has O
the O
receptive O
field O
of O
41 O
by O
41 O
and O
this O
is O
relatively O
large O
compared O
to O
SRCNN Method
( O
13 O
by O
13 O
) O
. O
While O
DRCN Method
has O
good O
properties O
, O
we O
find O
that O
DRCN Method
optimized O
with O
the O
widely O
- O
used O
stochastic Method
gradient Method
descent Method
method Method
does O
not O
easily O
converge O
. O
This O
is O
due O
to O
exploding O
/ O
vanishing O
gradients O
. O
Learning Task
long Task
- Task
range Task
dependencies Task
between O
pixels O
with O
a O
single O
weight Method
layer Method
is O
very O
difficult O
. O
We O
propose O
two O
approaches O
to O
ease O
the O
difficulty O
of O
training O
( O
Figure O
[ O
reference O
] O
( O
a O
) O
) O
. O
First O
, O
all O
recursions O
are O
supervised O
. O
Feature O
maps O
after O
each O
recursion O
are O
used O
to O
reconstruct O
the O
target O
high O
- O
resolution O
image O
( O
HR O
) O
. O
Reconstruction Method
method Method
( O
layers O
dedicated O
to O
reconstruction Task
) O
is O
the O
same O
for O
all O
recursions O
. O
As O
each O
recursion O
leads O
to O
a O
different O
HR Task
prediction Task
, O
we O
combine O
all O
predictions O
resulting O
from O
different O
levels O
of O
recursions O
to O
deliver O
a O
more O
accurate O
final O
prediction Task
. O
The O
second O
proposal O
is O
to O
use O
a O
skip O
- O
connection O
from O
input O
to O
the O
reconstruction Method
layer Method
. O
In O
SR Task
, O
a O
low O
- O
resolution O
image O
( O
input O
) O
and O
a O
high O
- O
resolution O
image O
( O
output O
) O
share O
the O
same O
information O
to O
a O
large O
extent O
. O
Exact O
copy O
of O
input O
, O
however O
, O
is O
likely O
to O
be O
attenuated O
during O
many O
forward O
passes O
. O
We O
explicitly O
connect O
the O
input O
to O
the O
layers O
for O
output Task
reconstruction Task
. O
This O
is O
particularly O
effective O
when O
input O
and O
output O
are O
highly O
correlated O
. O
Contributions O
In O
summary O
, O
we O
propose O
an O
image Task
super Task
- Task
resolution Task
method O
deeply O
recursive O
in O
nature O
. O
It O
utilizes O
a O
very O
large O
context O
compared O
to O
previous O
SR Task
methods O
with O
only O
a O
single O
recursive Method
layer Method
. O
We O
improve O
the O
simple O
recursive Method
network Method
in O
two O
ways O
: O
recursive Method
- Method
supervision Method
and O
skip Method
- Method
connection Method
. O
Our O
method O
demonstrates O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
common O
benchmarks O
. O
section O
: O
Related O
Work O
subsection O
: O
Single O
- O
Image Task
Super Task
- Task
Resolution Task
We O
apply O
DRCN Method
to O
single Task
- Task
image Task
super Task
- Task
resolution Task
( O
SR Task
) O
. O
Many O
SR Task
methods O
have O
been O
proposed O
in O
the O
computer Task
vision Task
community Task
. O
Early O
methods O
use O
very O
fast O
interpolations Method
but O
yield O
poor O
results O
. O
Some O
of O
the O
more O
powerful O
methods O
utilize O
statistical O
image O
priors O
or O
internal Method
patch Method
recurrence Method
. O
Recently O
, O
sophisticated O
learning Method
methods Method
have O
been O
widely O
used O
to O
model O
a O
mapping O
from O
LR O
to O
HR O
patches O
. O
Many O
methods O
have O
paid O
attention O
to O
find O
better O
regression Method
functions Method
from O
LR O
to O
HR O
images O
. O
This O
is O
achieved O
with O
various O
techniques O
: O
neighbor Method
embedding Method
, O
sparse Method
coding Method
, O
convolutional Method
neural O
network O
( O
CNN O
) O
and O
random Method
forest Method
. O
Among O
several O
recent O
learning Method
- O
based O
successes O
, O
convolutional Method
neural O
network O
( O
SRCNN Method
) O
demonstrated O
the O
feasibility O
of O
an O
end O
- O
to O
- O
end Method
approach Method
to O
SR Task
. O
One O
possibility O
to O
improve O
SRCNN Method
is O
to O
simply O
stack O
more O
weight O
layers O
as O
many O
times O
as O
possible O
. O
However O
, O
this O
significantly O
increases O
the O
number O
of O
parameters O
and O
requires O
more O
data O
to O
prevent O
overfitting O
. O
In O
this O
work O
, O
we O
seek O
to O
design O
a O
convolutional Method
network O
that O
models O
long O
- O
range O
pixel O
dependencies O
with O
limited O
capacity O
. O
Our O
network O
recursively O
widens O
the O
receptive O
field O
without O
increasing O
model O
capacity O
. O
subsection O
: O
Recursive Method
Neural Method
Network Method
in O
Computer Task
Vision Task
Recursive Method
neural Method
networks Method
, O
suitable O
for O
temporal O
and O
sequential O
data O
, O
have O
seen O
limited O
use O
on O
algorithms O
operating O
on O
a O
single O
static O
image O
. O
Socher O
et O
al O
. O
used O
a O
convolutional Method
network O
in O
a O
separate O
stage O
to O
first O
learn O
features O
on O
RGB O
- O
Depth O
data O
, O
prior O
to O
hierarchical Task
merging Task
. O
In O
these O
models O
, O
the O
input O
dimension O
is O
twice O
that O
of O
the O
output O
and O
recursive Method
convolutions Method
are O
applied O
only O
two O
times O
. O
Similar O
dimension Task
reduction Task
occurs O
in O
the O
recurrent O
convolutional Method
neural O
networks O
used O
for O
semantic Task
segmentation Task
. O
As O
SR Task
methods O
predict O
full O
- O
sized O
images O
, O
dimension Task
reduction Task
is O
not O
allowed O
. O
In O
Eigen O
et O
al O
. O
, O
recursive Method
layers Method
have O
the O
same O
input O
and O
output O
dimension O
, O
but O
recursive Method
convolutions Method
resulted O
in O
worse O
performances O
than O
a O
single O
convolution Method
due O
to O
overfitting O
. O
To O
overcome O
overfitting O
, O
Liang O
and O
Hu O
uses O
a O
recurrent Method
layer Method
that O
takes O
feed O
- O
forward O
inputs O
into O
all O
unfolded O
layers O
. O
They O
show O
that O
performance O
increases O
up O
to O
three O
convolutions Method
. O
Their O
network Method
structure Method
, O
designed O
for O
object Task
recognition Task
, O
is O
the O
same O
as O
the O
existing O
CNN Method
architectures Method
. O
Our O
network O
is O
similar O
to O
the O
above O
in O
the O
sense O
that O
recursive Method
or Method
recurrent Method
layers Method
are O
used O
with O
convolutions Method
. O
We O
further O
increase O
the O
recursion O
depth O
and O
demonstrate O
that O
very O
deep Method
recursions Method
can O
significantly O
boost O
the O
performance O
for O
super Task
- Task
resolution Task
. O
We O
apply O
the O
same O
convolution Method
up O
to O
16 O
times O
( O
the O
previous O
maximum O
is O
three O
) O
. O
section O
: O
Proposed O
Method O
subsection O
: O
Basic O
Model O
Our O
first O
model O
, O
outlined O
in O
Figure O
[ O
reference O
] O
, O
consists O
of O
three O
sub Method
- Method
networks Method
: O
embedding Method
, O
inference Method
and O
reconstruction Method
networks Method
. O
The O
embedding Method
net Method
is O
used O
to O
represent O
the O
given O
image O
as O
feature O
maps O
ready O
for O
inference Method
. O
Next O
, O
the O
inference Method
net O
solves O
the O
task O
. O
Once O
inference Method
is O
done O
, O
final O
feature O
maps O
in O
the O
inference Method
net O
are O
fed O
into O
the O
reconstruction Method
net Method
to O
generate O
the O
output O
image O
. O
The O
embedding Method
net Method
takes O
the O
input O
image O
( O
grayscale O
or O
RGB O
) O
and O
represents O
it O
as O
a O
set O
of O
feature O
maps O
. O
Intermediate Method
representation Method
used O
to O
pass O
information O
to O
the O
inference Method
net O
largely O
depends O
on O
how O
the O
inference Method
net O
internally O
represent O
its O
feature O
maps O
in O
its O
hidden O
layers O
. O
Learning O
this O
representation O
is O
done O
end O
- O
to O
- O
end O
altogether O
with O
learning O
other O
sub Method
- Method
networks Method
. O
Inference Method
net Method
is O
the O
main O
component O
that O
solves O
the O
task O
of O
super Task
- Task
resolution Task
. O
Analyzing O
a O
large O
image O
region O
is O
done O
by O
a O
single O
recursive Method
layer Method
. O
Each O
recursion O
applies O
the O
same O
convolution Method
followed O
by O
a O
rectified Method
linear Method
unit Method
( O
Figure O
[ O
reference O
] O
) O
. O
With O
convolution Method
filters Method
larger O
than O
, O
the O
receptive O
field O
is O
widened O
with O
every O
recursion O
. O
While O
feature O
maps O
from O
the O
final O
application O
of O
the O
recursive Method
layer Method
represent O
the O
high O
- O
resolution O
image O
, O
transforming O
them O
( O
multi O
- O
channel O
) O
back O
into O
the O
original O
image O
space O
( O
1 O
or O
3 O
- O
channel O
) O
is O
necessary O
. O
This O
is O
done O
by O
the O
reconstruction Method
net Method
. O
We O
have O
a O
single O
hidden Method
layer Method
for O
each O
sub O
- O
net O
. O
Only O
the O
layer O
for O
the O
inference Method
net O
is O
recursive O
. O
Other O
sub Method
- Method
nets Method
are O
vastly O
similar O
to O
the O
standard O
mutilayer Method
perceptrons Method
( O
MLP Method
) O
with O
a O
single O
hidden Method
layer Method
. O
For O
MLP Method
, O
full Method
connection Method
of Method
neurons Method
is O
equivalent O
to O
a O
convolution Method
with O
. O
In O
our O
sub Method
- Method
nets Method
, O
we O
use O
filters Method
. O
For O
embedding Task
net Task
, O
we O
use O
filters O
because O
image O
gradients O
are O
more O
informative O
than O
the O
raw O
intensities O
for O
super Task
- Task
resolution Task
. O
For O
inference Method
net O
, O
convolutions Method
imply O
that O
hidden O
states O
are O
passed O
to O
adjacent O
pixels O
only O
. O
Reconstruction Method
net Method
also O
takes O
direct O
neighbors O
into O
account O
. O
Mathematical Method
Formulation Method
The O
network O
takes O
an O
interpolated O
input O
image O
( O
to O
the O
desired O
size O
) O
as O
input O
and O
predicts O
the O
target O
image O
as O
in O
SRCNN Method
. O
Our O
goal O
is O
to O
learn O
a O
model O
that O
predicts O
values O
, O
where O
is O
its O
estimate O
of O
ground O
truth O
output O
. O
Let O
denote O
sub O
- O
net O
functions O
: O
embedding Task
, O
inference Method
and O
reconstruction Task
, O
respectively O
. O
Our O
model O
is O
the O
composition O
of O
three O
functions O
: O
Embedding Method
net Method
takes O
the O
input O
vector O
and O
computes O
the O
matrix O
output O
, O
which O
is O
an O
input O
to O
the O
inference Method
net O
. O
Hidden O
layer O
values O
are O
denoted O
by O
. O
The O
formula O
for O
embedding Task
net Task
is O
as O
follows O
: O
where O
the O
operator O
denotes O
a O
convolution O
and O
corresponds O
to O
a O
ReLU Method
. O
Weight O
and O
bias O
matrices O
are O
and O
. O
Inference Method
net Method
takes O
the O
input O
matrix O
and O
computes O
the O
matrix O
output O
. O
Here O
, O
we O
use O
the O
same O
weight O
and O
bias O
matrices O
and O
for O
all O
operations O
. O
Let O
denote O
the O
function O
modeled O
by O
a O
single O
recursion Method
of Method
the Method
recursive Method
layer Method
: O
. O
The O
recurrence O
relation O
is O
for O
. O
Inference Method
net Method
is O
equivalent O
to O
the O
composition O
of O
the O
same O
elementary O
function O
: O
where O
the O
operator O
denotes O
a O
function Method
composition Method
and O
denotes O
the O
- O
fold O
product O
of O
. O
Reconstruction Method
net Method
takes O
the O
input O
hidden O
state O
and O
outputs O
the O
target O
image O
( O
high O
- O
resolution O
) O
. O
Roughly O
speaking O
, O
reconstruction O
net O
is O
the O
inverse O
operation O
of O
embedding Method
net Method
. O
The O
formula O
is O
as O
follows O
: O
Model Method
Properties Method
Now O
we O
have O
all O
components O
for O
our O
model O
. O
The O
recursive Method
model Method
has O
pros O
and O
cons O
. O
While O
the O
recursive Method
model Method
is O
simple O
and O
powerful O
, O
we O
find O
training O
a O
deeply Method
- Method
recursive Method
network Method
very O
difficult O
. O
This O
is O
in O
accordance O
with O
the O
limited O
success O
of O
previous O
methods O
using O
at O
most O
three O
recursions O
so O
far O
. O
Among O
many O
reasons O
, O
two O
severe O
problems O
are O
vanishing O
and O
exploding O
gradients O
. O
Exploding O
gradients O
refer O
to O
the O
large O
increase O
in O
the O
norm O
of O
the O
gradient O
during O
training O
. O
Such O
events O
are O
due O
to O
the O
multiplicative O
nature O
of O
chained O
gradients O
. O
Long Method
term Method
components Method
can O
grow O
exponentially O
for O
deep Method
recursions Method
. O
The O
vanishing Task
gradients Task
problem Task
refers O
to O
the O
opposite O
behavior O
. O
Long Method
term Method
components Method
approach O
exponentially O
fast O
to O
the O
zero O
vector O
. O
Due O
to O
this O
, O
learning O
the O
relation O
between O
distant O
pixels O
is O
very O
hard O
. O
Another O
known O
issue O
is O
that O
storing O
an O
exact O
copy O
of O
information O
through O
many O
recursions O
is O
not O
easy O
. O
In O
SR Task
, O
output O
is O
vastly O
similar O
to O
input O
and O
recursive Method
layer Method
needs O
to O
keep O
the O
exact O
copy O
of O
input O
image O
for O
many O
recursions O
. O
These O
issues O
are O
also O
observed O
when O
we O
train O
our O
basic O
recursive Method
model Method
and O
we O
did O
not O
succeed O
in O
training O
a O
deeply Method
- Method
recursive Method
network Method
. O
In O
addition O
to O
gradient Task
problems Task
, O
there O
exists O
an O
issue O
with O
finding O
the O
optimal O
number O
of O
recursions O
. O
If O
recursions O
are O
too O
deep O
for O
a O
given O
task O
, O
we O
need O
to O
reduce O
the O
number O
of O
recursions O
. O
Finding O
the O
optimal O
number O
requires O
training O
many O
networks O
with O
different O
recursion O
depths O
. O
subsection O
: O
Advanced O
Model O
Recursive Method
- Method
Supervision Method
To O
resolve O
the O
gradient Task
and Task
optimal Task
recursion Task
issues Task
, O
we O
propose O
an O
improved O
model O
. O
We O
supervise O
all O
recursions O
in O
order O
to O
alleviate O
the O
effect O
of O
vanishing O
/ O
exploding O
gradients O
. O
As O
we O
have O
assumed O
that O
the O
same O
representation O
can O
be O
used O
again O
and O
again O
during O
convolutions Method
in O
the O
inference Method
net O
, O
the O
same O
reconstruction Method
net Method
is O
used O
to O
predict O
HR O
images O
for O
all O
recursions O
. O
Our O
reconstruction Method
net Method
now O
outputs O
predictions O
and O
all O
predictions O
are O
simultaneously O
supervised O
during O
training O
( O
Figure O
[ O
reference O
] O
( O
a O
) O
) O
. O
We O
use O
all O
intermediate O
predictions O
to O
compute O
the O
final O
output O
. O
All O
predictions O
are O
averaged O
during O
testing O
. O
The O
optimal O
weights O
are O
automatically O
learned O
during O
training O
. O
A O
similar O
but O
a O
different O
concept O
of O
supervising O
intermediate O
layers O
for O
a O
convolutional Method
network O
is O
used O
in O
Lee O
et O
al O
. O
Their O
method O
simultaneously O
minimizes O
classification Metric
error Metric
while O
improving O
the O
directness O
and O
transparency Metric
of O
the O
hidden Method
layer Method
learning Method
process Method
. O
There O
are O
two O
significant O
differences O
between O
our O
recursive Method
- Method
supervision Method
and O
deep Method
- Method
supervision Method
proposed O
in O
Lee O
et O
al O
. O
. O
They O
associate O
a O
unique O
classifier Method
for O
each O
hidden O
layer O
. O
For O
each O
additional O
layer O
, O
a O
new O
classifier Method
has O
to O
be O
introduced O
, O
as O
well O
as O
new O
parameters O
. O
If O
this O
approach O
is O
used O
, O
our O
modified Method
network Method
would O
resemble O
that O
of O
Figure O
[ O
reference O
] O
( O
b O
) O
. O
We O
would O
then O
need O
different O
reconstruction Method
networks Method
. O
This O
is O
against O
our O
original O
purpose O
of O
using O
recursive Method
networks Method
, O
which O
is O
avoid O
introducing O
new O
parameters O
while O
stacking O
more O
layers O
. O
In O
addition O
, O
using O
different O
reconstruction Method
nets Method
no O
longer O
effectively O
regularizes O
the O
network O
. O
The O
second O
difference O
is O
that O
Lee O
et O
al O
. O
discards O
all O
intermediate Method
classifiers Method
during O
testing O
. O
However O
, O
an O
ensemble O
of O
all O
intermediate O
predictions O
significantly O
boosts O
the O
performance O
. O
The O
final O
output O
from O
the O
ensemble O
is O
also O
supervised O
. O
Our O
recursive Method
- Method
supervision Method
naturally O
eases O
the O
difficulty O
of O
training O
recursive Method
networks Method
. O
Backpropagation Method
goes O
through O
a O
small O
number O
of O
layers O
if O
supervising O
signal O
goes O
directly O
from O
loss O
layer O
to O
early O
recursion O
. O
Summing O
all O
gradients O
backpropagated O
from O
different O
prediction O
losses O
gives O
a O
smoothing O
effect O
. O
The O
adversarial O
effect O
of O
vanishing O
/ O
exploding O
gradients O
along O
one O
backpropagation O
path O
is O
alleviated O
. O
Moreover O
, O
the O
importance O
of O
picking O
the O
optimal O
number O
of O
recursions O
is O
reduced O
as O
our O
supervision Method
enables O
utilizing O
predictions O
from O
all O
intermediate O
layers O
. O
If O
recursions Method
are O
too O
deep O
for O
the O
given O
task O
, O
we O
expect O
the O
weight O
for O
late O
predictions O
to O
be O
low O
while O
early O
predictions O
receive O
high O
weights O
. O
By O
looking O
at O
weights O
of O
predictions O
, O
we O
can O
figure O
out O
the O
marginal O
gain O
from O
additional O
recursions O
. O
We O
present O
an O
expanded O
CNN Method
structure Method
of O
our O
model O
for O
illustration O
purposes O
in O
Figure O
[ O
reference O
] O
( O
c O
) O
. O
If O
parameters O
are O
not O
allowed O
to O
be O
shared O
and O
CNN O
chains O
vary O
their O
depths O
, O
the O
number O
of O
free O
parameters O
grows O
fast O
( O
quadratically O
) O
. O
Skip Method
- Method
Connection Method
Now O
we O
describe O
our O
second O
extension O
: O
skip Task
- Task
connection Task
. O
For O
SR Task
, O
input O
and O
output O
images O
are O
highly O
correlated O
. O
Carrying O
most O
if O
not O
all O
of O
input O
values O
until O
the O
end O
of O
the O
network O
is O
inevitable O
but O
very O
inefficient O
. O
Due O
to O
gradient Task
problems Task
, O
exactly O
learning O
a O
simple O
linear O
relation O
between O
input O
and O
output O
is O
very O
difficult O
if O
many O
recursions O
exist O
in O
between O
them O
. O
We O
add O
a O
layer O
skip O
from O
input O
to O
the O
reconstruction Method
net Method
. O
Adding O
layer O
skips O
is O
successfully O
used O
for O
a O
semantic Task
segmentation Task
network Task
and O
we O
employ O
a O
similar O
idea O
. O
Now O
input O
image O
is O
directly O
fed O
into O
the O
reconstruction Method
net Method
whenever O
it O
is O
used O
during O
recursions O
. O
Our O
skip Method
- Method
connection Method
has O
two O
advantages O
. O
First O
, O
network O
capacity O
to O
store O
the O
input O
signal O
during O
recursions O
is O
saved O
. O
Second O
, O
the O
exact O
copy O
of O
input O
signal O
can O
be O
used O
during O
target Task
prediction Task
. O
Our O
skip Method
- Method
connection Method
is O
simple O
yet O
very O
effective O
. O
In O
super Task
- Task
resolution Task
, O
LR O
and O
HR O
images O
are O
vastly O
similar O
. O
In O
most O
regions O
, O
differences O
are O
zero O
and O
only O
small O
number O
of O
locations O
have O
non O
- O
zero O
values O
. O
For O
this O
reason O
, O
several O
super Task
- Task
resolution Task
methods O
predict O
image O
details O
only O
. O
Similarly O
, O
we O
find O
that O
this O
domain O
- O
specific O
knowledge O
significantly O
improves O
our O
learning Method
procedure Method
. O
Mathematical Method
Formulation Method
Each O
intermediate Task
prediction Task
under O
recursive Method
- Method
supervision Method
( O
Figure O
[ O
reference O
] O
( O
a O
) O
) O
is O
for O
, O
where O
now O
takes O
two O
inputs O
, O
one O
from O
skip O
- O
connection O
. O
Reconstruction Method
net Method
with O
skip O
- O
connection O
can O
take O
various O
functional O
forms O
. O
For O
example O
, O
input O
can O
be O
concatenated O
to O
the O
feature O
maps O
. O
As O
the O
input O
is O
an O
interpolated O
input O
image O
( O
roughly O
speaking O
, O
) O
, O
we O
find O
is O
enough O
for O
our O
purpose O
. O
More O
sophisticated O
functions O
for O
merging O
two O
inputs O
to O
will O
be O
explored O
in O
the O
future O
. O
Now O
, O
the O
final O
output O
is O
the O
weighted O
average O
of O
all O
intermediate O
predictions O
: O
where O
denotes O
the O
weights O
of O
predictions O
reconstructed O
from O
each O
intermediate O
hidden O
state O
during O
recursion O
. O
These O
weights O
are O
learned O
during O
training O
. O
0.5cm0.5 O
cm O
0.5cm0.5 O
cm O
0.5cm0.5 O
cm O
0.5cm0.5 O
cm O
subsection O
: O
Training Metric
Objective Metric
We O
now O
describe O
the O
training Metric
objective Metric
used O
to O
find O
optimal O
parameters O
of O
our O
model O
. O
Given O
a O
training O
dataset O
, O
our O
goal O
is O
to O
find O
the O
best O
model O
that O
accurately O
predicts O
values O
. O
In O
the O
least Method
- Method
squares Method
regression Method
setting Method
, O
typical O
in O
SR Task
, O
the O
mean Metric
squared Metric
error Metric
averaged O
over O
the O
training O
set O
is O
minimized O
. O
This O
favors O
high O
Peak Metric
Signal Metric
- Metric
to Metric
- Metric
Noise Metric
Ratio Metric
( O
PSNR Metric
) O
, O
a O
widely O
- O
used O
evaluation Metric
criteria Metric
. O
With O
recursive Method
- Method
supervision Method
, O
we O
have O
objectives O
to O
minimize O
: O
supervising O
outputs O
from O
recursions O
and O
the O
final O
output O
. O
For O
intermediate O
outputs O
, O
we O
have O
the O
loss O
function O
where O
denotes O
the O
parameter O
set O
and O
is O
the O
output O
from O
the O
- O
th O
recursion O
. O
For O
the O
final O
output O
, O
we O
have O
Now O
we O
give O
the O
final O
loss Metric
function Metric
. O
The O
training Method
is O
regularized O
by O
weight Method
decay Method
( O
penalty O
multiplied O
by O
) O
. O
where O
denotes O
the O
importance O
of O
the O
companion O
objective O
on O
the O
intermediate O
outputs O
and O
denotes O
the O
multiplier O
of O
weight Method
decay Method
. O
Setting O
high O
makes O
the O
training Method
procedure Method
stable O
as O
early O
recursions O
easily O
converge O
. O
As O
training O
progresses O
, O
decays O
to O
boost O
the O
performance O
of O
the O
final O
output O
. O
Training Task
is O
carried O
out O
by O
optimizing O
the O
regression O
objective O
using O
mini Method
- Method
batch Method
gradient Method
descent Method
based O
on O
back Method
- Method
propagation Method
( O
LeCun O
et O
al O
. O
) O
. O
We O
implement O
our O
model O
using O
the O
MatConvNet Method
␣ Method
http: Method
// Method
www.vlfeat.org Method
/ Method
matconvnet Method
/ Method
package Method
. O
section O
: O
Experimental O
Results O
In O
this O
section O
, O
we O
evaluate O
the O
performance O
of O
our O
method O
on O
several O
datasets O
. O
We O
first O
describe O
datasets O
used O
for O
training O
and O
testing O
our O
method O
. O
Next O
, O
our O
training O
setup O
is O
given O
. O
We O
give O
several O
experiments O
for O
understanding O
our O
model O
properties O
. O
The O
effect O
of O
increasing O
the O
number O
of O
recursions O
is O
investigated O
. O
Finally O
, O
we O
compare O
our O
method O
with O
several O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
subsection O
: O
Datasets O
For O
training Task
, O
we O
use O
91 O
images O
proposed O
in O
Yang O
et O
al O
. O
for O
all O
experiments O
. O
For O
testing O
, O
we O
use O
four O
datasets O
. O
Datasets O
Set5 O
and O
Set14 O
are O
often O
used O
for O
benchmark O
. O
Dataset Material
B100 Material
consists O
of O
natural O
images O
in O
the O
Berkeley Material
Segmentation Material
Dataset Material
. O
Finally O
, O
dataset O
Urban100 O
, O
urban O
images O
recently O
provided O
by O
Huang O
et O
al O
. O
, O
is O
very O
interesting O
as O
it O
contains O
many O
challenging O
images O
failed O
by O
existing O
methods O
. O
0cm O
- O
0.0 O
cm O
subsection O
: O
Training O
Setup O
We O
use O
16 O
recursions O
unless O
stated O
otherwise O
. O
When O
unfolded O
, O
the O
longest O
chain O
from O
the O
input O
to O
the O
output O
passes O
20 O
conv Method
. O
layers O
( O
receptive O
field O
of O
41 O
by O
41 O
) O
. O
We O
set O
the O
momentum O
parameter O
to O
0.9 O
and O
weight O
decay O
to O
0.0001 O
. O
We O
use O
256 O
filters O
of O
the O
size O
for O
all O
weight O
layers O
. O
Training O
images O
are O
split O
into O
41 O
by O
41 O
patches O
with O
stride O
21 O
and O
64 O
patches O
are O
used O
as O
a O
mini O
- O
batch O
for O
stochastic Method
gradient Method
descent Method
. O
For O
initializing O
weights O
in O
non Method
- Method
recursive Method
layers Method
, O
we O
use O
the O
method O
described O
in O
He O
et O
al O
. O
. O
For O
recursive Method
convolutions Method
, O
we O
set O
all O
weights O
to O
zero O
except O
self O
- O
connections O
( O
connection O
to O
the O
same O
neuron O
in O
the O
next O
layer O
) O
. O
Biases O
are O
set O
to O
zero O
. O
Learning Metric
rate Metric
is O
initially O
set O
to O
0.01 O
and O
then O
decreased O
by O
a O
factor O
of O
10 O
if O
the O
validation Metric
error Metric
does O
not O
decrease O
for O
5 O
epochs O
. O
If O
learning Metric
rate Metric
is O
less O
than O
, O
the O
procedure O
is O
terminated O
. O
Training O
roughly O
takes O
6 O
days O
on O
a O
machine O
using O
one O
Titan Method
X Method
GPU Method
. O
subsection O
: O
Study O
of O
Deep O
Recursions O
We O
study O
the O
effect O
of O
increasing O
recursion O
depth O
. O
We O
trained O
four O
models O
with O
different O
numbers O
of O
recursions O
: O
1 O
, O
6 O
, O
11 O
, O
and O
16 O
. O
Four O
models O
use O
the O
same O
number O
of O
parameters O
except O
the O
weights O
used O
for O
ensemble Task
. O
In O
Figure O
8 O
, O
it O
is O
shown O
that O
as O
more O
recursions O
are O
performed O
, O
PSNR Metric
measures O
increase O
. O
Increasing O
recursion O
depth O
with O
a O
larger O
image O
context O
and O
more O
nonlinearities O
boosts O
performance O
. O
The O
effect O
of O
ensemble O
is O
also O
investigated O
. O
We O
first O
evaluate O
intermediate O
predictions O
made O
from O
recursions O
( O
Figure O
[ O
reference O
] O
) O
. O
The O
ensemble O
output O
significantly O
improves O
performances O
of O
individual O
predictions O
. O
subsection O
: O
Comparisons O
with O
State O
- O
of O
- O
the O
- O
Art O
Methods O
We O
provide O
quantitative O
and O
qualitative O
comparisons O
. O
For O
benchmark O
, O
we O
use O
public O
code O
for O
A Method
+ Method
, O
SRCNN Method
, O
RFL Method
and O
SelfEx Method
. O
We O
deal O
with O
luminance O
components O
only O
as O
similarly O
done O
in O
other O
methods O
because O
human Task
vision Task
is O
much O
more O
sensitive O
to O
details O
in O
intensity O
than O
in O
color O
. O
As O
some O
methods O
such O
as O
A Method
+ Method
and O
RFL Method
do O
not O
predict O
image O
boundary O
, O
they O
require O
cropping O
pixels O
near O
borders O
. O
For O
our O
method O
, O
this O
procedure O
is O
unnecessary O
as O
our O
network O
predicts O
the O
full O
- O
sized O
image O
. O
For O
fair O
comparison O
, O
however O
, O
we O
also O
crop O
pixels O
to O
the O
same O
amount O
. O
PSNRs Metric
can O
be O
slightly O
different O
from O
original O
papers O
as O
existing O
methods O
use O
slightly O
different O
evaluation Method
frameworks Method
. O
We O
use O
the O
public Metric
evaluation Metric
code Metric
used O
in O
. O
In O
Table O
[ O
reference O
] O
, O
we O
provide O
a O
summary O
of O
quantitative O
evaluation O
on O
several O
datasets O
. O
Our O
method O
outperforms O
all O
existing O
methods O
in O
all O
datasets O
and O
scale O
factors O
( O
both O
PSNR Metric
and O
SSIM Metric
) O
. O
In O
Figures O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
and O
[ O
reference O
] O
, O
example O
images O
are O
given O
. O
Our O
method O
produces O
relatively O
sharp O
edges O
respective O
to O
patterns O
. O
In O
contrast O
, O
edges O
in O
other O
images O
are O
blurred O
. O
Our O
method O
takes O
a O
second O
to O
process O
a O
image O
on O
a O
GPU O
Titan O
X. O
section O
: O
Conclusion O
In O
this O
work O
, O
we O
have O
presented O
a O
super Task
- Task
resolution Task
method O
using O
a O
deeply O
- O
recursive O
convolutional Method
network O
. O
Our O
network O
efficiently O
reuses O
weight O
parameters O
while O
exploiting O
a O
large O
image O
context O
. O
To O
ease O
the O
difficulty O
of O
training O
the O
model O
, O
we O
use O
recursive Method
- Method
supervision Method
and O
skip Method
- Method
connection Method
. O
We O
have O
demonstrated O
that O
our O
method O
outperforms O
existing O
methods O
by O
a O
large O
margin O
on O
benchmarked O
images O
. O
In O
the O
future O
, O
one O
can O
try O
more O
recursions O
in O
order O
to O
use O
image O
- O
level O
context O
. O
We O
believe O
our O
approach O
is O
readily O
applicable O
to O
other O
image Task
restoration Task
problems Task
such O
as O
denoising Task
and O
compression O
artifact O
removal O
. O
bibliography O
: O
References O
