DeepWalk Method
: O
Online Task
Learning Task
of Task
Social Task
Representations Task
section O
: O
ABSTRACT O
We O
present O
DeepWalk Method
, O
a O
novel O
approach O
for O
learning Task
latent Task
representations Task
of Task
vertices Task
in Task
a Task
network Task
. O
These O
latent Method
representations Method
encode O
social O
relations O
in O
a O
continuous O
vector O
space O
, O
which O
is O
easily O
exploited O
by O
statistical Method
models Method
. O
DeepWalk Method
generalizes O
recent O
advancements O
in O
language Task
modeling Task
and O
unsupervised Task
feature Task
learning Task
( O
or O
deep Task
learning Task
) O
from O
sequences O
of O
words O
to O
graphs O
. O
DeepWalk Method
uses O
local O
information O
obtained O
from O
truncated O
random O
walks O
to O
learn O
latent Method
representations Method
by O
treating O
walks O
as O
the O
equivalent O
of O
sentences O
. O
We O
demonstrate O
DeepWalk Method
's O
latent O
representations O
on O
several O
multi Task
- Task
label Task
network Task
classification Task
tasks Task
for O
social O
networks O
such O
as O
BlogCatalog Material
, O
Flickr Material
, O
and O
YouTube Material
. O
Our O
results O
show O
that O
DeepWalk Method
outperforms O
challenging O
baselines O
which O
are O
allowed O
a O
global O
view O
of O
the O
network O
, O
especially O
in O
the O
presence O
of O
missing O
information O
. O
DeepWalk Method
's O
representations O
can O
provide O
F1 Metric
scores Metric
up O
to O
10 O
% O
higher O
than O
competing O
methods O
when O
labeled O
data O
is O
sparse O
. O
In O
some O
experiments O
, O
DeepWalk Method
's O
representations O
are O
able O
to O
outperform O
all O
baseline O
methods O
while O
using O
60 O
% O
less O
training O
data O
. O
DeepWalk Method
is O
also O
scalable O
. O
It O
is O
an O
online Method
learning Method
algorithm Method
which O
builds O
useful O
incremental O
results O
, O
and O
is O
trivially O
parallelizable O
. O
These O
qualities O
make O
it O
suitable O
for O
a O
broad O
class O
of O
real Task
world Task
applications Task
such O
as O
network Task
classification Task
, O
and O
anomaly Task
detection Task
. O
section O
: O
INTRODUCTION O
The O
sparsity O
of O
a O
network Method
representation Method
is O
both O
a O
strength O
and O
a O
weakness O
. O
Sparsity Method
enables O
the O
design O
of O
efficient O
discrete Method
algorithms Method
, O
but O
can O
make O
it O
harder O
to O
generalize O
in O
statistical Task
learning Task
. O
Machine Task
learning Task
applications Task
in O
networks Task
( O
such O
as O
network Task
classification Task
[ O
reference O
][ O
reference O
] O
, O
content O
rec O
- O
The O
learned O
representation O
encodes O
community O
structure O
so O
it O
can O
be O
easily O
exploited O
by O
standard O
classification Task
methods O
. O
Here O
, O
our O
method O
is O
used O
on O
Zachary Method
's Method
Karate Method
network Method
[ O
reference O
] O
to O
generate O
a O
latent Method
representation Method
in O
R O
2 O
. O
Note O
the O
correspondence O
between O
community O
structure O
in O
the O
input O
graph O
and O
the O
embedding O
. O
Vertex O
colors O
represent O
a O
modularity Method
- Method
based Method
clustering Method
of O
the O
input O
graph O
. O
ommendation Method
[ O
reference O
] O
, O
anomaly Task
detection Task
[ O
reference O
] O
, O
and O
missing Task
link Task
prediction Task
[ O
reference O
] O
) O
must O
be O
able O
to O
deal O
with O
this O
sparsity O
in O
order O
to O
survive O
. O
In O
this O
paper O
we O
introduce O
deep Method
learning Method
( O
unsupervised Method
feature Method
learning Method
) O
[ O
reference O
] O
techniques O
, O
which O
have O
proven O
successful O
in O
natural Task
language Task
processing Task
, O
into O
network Task
analysis Task
for O
the O
first O
time O
. O
We O
develop O
an O
algorithm O
( O
DeepWalk Method
) O
that O
learns O
social Method
representations Method
of O
a O
graph O
's O
vertices O
, O
by O
modeling O
a O
stream Method
of Method
short Method
random Method
walks Method
. O
Social Method
representations Method
are O
latent O
features O
of O
the O
vertices O
that O
capture O
neighborhood O
similarity O
and O
community O
membership O
. O
These O
latent Method
representations Method
encode O
social O
relations O
in O
a O
continuous O
vector O
space O
with O
a O
relatively O
small O
number O
of O
dimensions O
. O
DeepWalk Method
generalizes O
neural Method
language Method
models Method
to O
process O
a O
special O
language O
composed O
of O
a O
set O
of O
randomly O
- O
generated O
walks O
. O
These O
neural Method
language Method
models Method
have O
been O
used O
to O
capture O
the O
semantic O
and O
syntactic O
structure O
of O
human O
language O
[ O
reference O
] O
, O
and O
even O
logical O
analogies O
[ O
reference O
] O
. O
DeepWalk Method
takes O
a O
graph O
as O
input O
and O
produces O
a O
latent Method
representation Method
as O
an O
output O
. O
The O
result O
of O
applying O
our O
method O
to O
the O
well O
- O
studied O
Karate Task
network Task
is O
shown O
in O
Figure O
1 O
. O
The O
graph O
, O
as O
typically O
presented O
by O
force Method
- Method
directed Method
layouts Method
, O
is O
shown O
in O
Figure O
1a O
. O
Figure O
1b O
shows O
the O
output O
of O
our O
method O
with O
2 O
latent O
dimensions O
. O
Beyond O
the O
striking O
similarity O
, O
we O
note O
that O
linearly O
separable O
portions O
of O
( O
1b O
) O
correspond O
to O
clusters O
found O
through O
modularity Method
maximization Method
in O
the O
input O
graph O
( O
1a O
) O
( O
shown O
as O
vertex O
colors O
) O
. O
To O
demonstrate O
DeepWalk Method
's O
potential O
in O
real Task
world Task
sce Task
- Task
narios Task
, O
we O
evaluate O
its O
performance O
on O
challenging O
multilabel O
network Task
classification Task
problems O
in O
large O
heterogeneous O
graphs O
. O
In O
the O
relational Task
classification Task
problem Task
, O
the O
links O
between O
feature O
vectors O
violate O
the O
traditional O
i.i.d O
. O
assumption O
. O
Techniques O
to O
address O
this O
problem O
typically O
use O
approximate Method
inference Method
techniques Method
[ O
reference O
][ O
reference O
] O
to O
leverage O
the O
dependency O
information O
to O
improve O
classification Task
results O
. O
We O
distance O
ourselves O
from O
these O
approaches O
by O
learning O
labelindependent Method
representations Method
of Method
the Method
graph Method
. O
Our O
representation Metric
quality Metric
is O
not O
influenced O
by O
the O
choice O
of O
labeled O
vertices O
, O
so O
they O
can O
be O
shared O
among O
tasks O
. O
DeepWalk Method
outperforms O
other O
latent Method
representation Method
methods Method
for O
creating Task
social Task
dimensions Task
[ O
reference O
][ O
reference O
] O
, O
especially O
when O
labeled O
nodes O
are O
scarce O
. O
Strong O
performance O
with O
our O
representations O
is O
possible O
with O
very O
simple O
linear Method
classifiers Method
( O
e.g. O
logistic Method
regression Method
) O
. O
Our O
representations O
are O
general O
, O
and O
can O
be O
combined O
with O
any O
classification Task
method O
( O
including O
iterative Method
inference Method
methods Method
) O
. O
DeepWalk Method
achieves O
all O
of O
that O
while O
being O
an O
online Method
algorithm Method
that O
is O
trivially O
parallelizable O
. O
Our O
contributions O
are O
as O
follows O
: O
• O
We O
introduce O
deep Method
learning Method
as O
a O
tool O
to O
analyze O
graphs Task
, O
to O
build O
robust Method
representations Method
that O
are O
suitable O
for O
statistical Task
modeling Task
. O
DeepWalk Method
learns O
structural O
regularities O
present O
within O
short O
random O
walks O
. O
• O
We O
extensively O
evaluate O
our O
representations O
on O
multilabel Task
classification Task
tasks Task
on O
several O
social O
networks O
. O
We O
show O
significantly O
increased O
classification Task
performance O
in O
the O
presence O
of O
label O
sparsity O
, O
getting O
improvements O
5% O
- O
10 O
% O
of O
Micro Metric
F1 Metric
, O
on O
the O
sparsest Task
problems Task
we O
consider O
. O
In O
some O
cases O
, O
DeepWalk Method
's O
representations O
can O
outperform O
its O
competitors O
even O
when O
given O
60 O
% O
less O
training O
data O
. O
• O
We O
demonstrate O
the O
scalability O
of O
our O
algorithm O
by O
building O
representations Method
of Method
web Method
- Method
scale Method
graphs Method
, O
( O
such O
as O
YouTube Material
) O
using O
a O
parallel Method
implementation Method
. O
Moreover O
, O
we O
describe O
the O
minimal O
changes O
necessary O
to O
build O
a O
streaming Method
version Method
of O
our O
approach O
. O
The O
rest O
of O
the O
paper O
is O
arranged O
as O
follows O
. O
In O
Sections O
2 O
and O
3 O
, O
we O
discuss O
the O
problem Task
formulation Task
of O
classification Task
in O
data Task
networks Task
, O
and O
how O
it O
relates O
to O
our O
work O
. O
In O
Section O
4 O
we O
present O
DeepWalk Method
, O
our O
approach O
for O
Social Task
Representation Task
Learning Task
. O
We O
outline O
ours O
experiments O
in O
Section O
5 O
, O
and O
present O
their O
results O
in O
Section O
6 O
. O
We O
close O
with O
a O
discussion O
of O
related O
work O
in O
Section O
7 O
, O
and O
our O
conclusions O
. O
section O
: O
PROBLEM O
DEFINITION O
We O
consider O
the O
problem O
of O
classifying O
members O
of O
a O
social Task
network Task
into O
one O
or O
more O
categories O
. O
More O
formally O
, O
let O
G O
= O
( O
V O
, O
E O
) O
, O
where O
V O
are O
the O
members O
of O
the O
network O
, O
and O
E O
be O
its O
edges O
, O
E O
⊆ O
( O
V O
× O
V O
) O
. O
Given O
a O
partially O
labeled O
social O
network O
GL O
= O
( O
V O
, O
E O
, O
X O
, O
Y O
) O
, O
with O
attributes O
X O
∈ O
R O
section O
: O
|V O
|×S O
where O
S O
is O
the O
size O
of O
the O
feature O
space O
for O
each O
attribute O
vector O
, O
and O
Y O
∈ O
R O
|V O
|×|Y| O
, O
Y O
is O
the O
set O
of O
labels O
. O
In O
a O
traditional O
machine Task
learning Task
classification Task
setting Task
, O
we O
aim O
to O
learn O
a O
hypothesis O
H O
that O
maps O
elements O
of O
X O
to O
the O
labels O
set O
Y. O
In O
our O
case O
, O
we O
can O
utilize O
the O
significant O
information O
about O
the O
dependence O
of O
the O
examples O
embedded O
in O
the O
structure O
of O
G O
to O
achieve O
superior O
performance O
. O
In O
the O
literature O
, O
this O
is O
known O
as O
the O
relational Task
classification Task
( O
or O
the O
collective Task
classification Task
problem Task
[ O
reference O
] O
) O
. O
Traditional O
approaches O
to O
relational Task
classification Task
pose O
the O
problem O
as O
an O
inference Task
in O
an O
undirected Method
Markov Method
network Method
, O
and O
then O
use O
iterative Method
approximate Method
inference Method
algorithms Method
( O
such O
as O
the O
iterative O
classification Task
algorithm O
[ O
reference O
] O
, O
Gibbs Method
Sampling Method
[ O
reference O
] O
, O
or O
label Method
relaxation Method
[ O
reference O
] O
) O
to O
compute O
the O
posterior O
distribution O
of O
labels O
given O
the O
network O
structure O
. O
We O
propose O
a O
different O
approach O
to O
capture O
the O
network O
topology O
information O
. O
Instead O
of O
mixing O
the O
label O
space O
as O
part O
of O
the O
feature O
space O
, O
we O
propose O
an O
unsupervised Method
method Method
which O
learns O
features O
that O
capture O
the O
graph O
structure O
independent O
of O
the O
labels O
' O
distribution O
. O
This O
separation O
between O
the O
structural Method
representation Method
and O
the O
labeling Task
task Task
avoids O
cascading O
errors O
, O
which O
can O
occur O
in O
iterative Method
methods Method
[ O
reference O
] O
. O
Moreover O
, O
the O
same O
representation O
can O
be O
used O
for O
multiple O
classification Task
problems Task
concerning O
that O
network O
. O
Our O
goal O
is O
to O
learn O
XE O
∈ O
R O
|V O
|×d O
, O
where O
d O
is O
small O
number O
of O
latent O
dimensions O
. O
These O
low Method
- Method
dimensional Method
representations Method
are O
distributed O
; O
meaning O
each O
social O
phenomena O
is O
expressed O
by O
a O
subset O
of O
the O
dimensions O
and O
each O
dimension O
contributes O
to O
a O
subset O
of O
the O
social O
concepts O
expressed O
by O
the O
space O
. O
Using O
these O
structural O
features O
, O
we O
will O
augment O
the O
attributes O
space O
to O
help O
the O
classification Task
decision Task
. O
These O
features O
are O
general O
, O
and O
can O
be O
used O
with O
any O
classification Task
algorithm O
( O
including O
iterative Method
methods Method
) O
. O
However O
, O
we O
believe O
that O
the O
greatest O
utility O
of O
these O
features O
is O
their O
easy O
integration O
with O
simple O
machine Method
learning Method
algorithms Method
. O
They O
scale O
appropriately O
in O
real Task
- Task
world Task
networks Task
, O
as O
we O
will O
show O
in O
Section O
6 O
. O
section O
: O
LEARNING Task
SOCIAL Task
REPRESENTATIONS Task
We O
seek O
learning Task
social Task
representations Task
with O
the O
following O
characteristics O
: O
• O
Adaptability Task
- Task
Real Task
social Task
networks Task
are O
constantly O
evolving O
; O
new O
social O
relations O
should O
not O
require O
repeating O
the O
learning Method
process Method
all O
over O
again O
. O
• O
Community O
aware O
- O
The O
distance O
between O
latent O
dimensions O
should O
represent O
a O
metric O
for O
evaluating Task
social Task
similarity Task
between O
the O
corresponding O
members O
of O
the O
network O
. O
This O
allows O
generalization Task
in O
networks Task
with O
homophily O
. O
• O
Low Material
dimensional Material
- Material
When Material
labeled Material
data Material
is O
scarce O
, O
lowdimensional Method
models Method
generalize O
better O
, O
and O
speed O
up O
convergence Task
and O
inference Task
. O
• O
Continuous O
- O
We O
require O
latent Method
representations Method
to O
model O
partial O
community O
membership O
in O
continuous O
space O
. O
In O
addition O
to O
providing O
a O
nuanced O
view O
of O
community O
membership O
, O
a O
continuous Method
representation Method
has O
smooth O
decision O
boundaries O
between O
communities O
which O
allows O
more O
robust O
classification Task
. O
Our O
method O
for O
satisfying O
these O
requirements O
learns O
representation O
for O
vertices O
from O
a O
stream O
of O
short Method
random Method
walks Method
, O
using O
optimization Method
techniques Method
originally O
designed O
for O
language Task
modeling Task
. O
Here O
, O
we O
review O
the O
basics O
of O
both O
random Method
walks Method
and O
language Method
modeling Method
, O
and O
describe O
how O
their O
combination O
satisfies O
our O
requirements O
. O
section O
: O
Random O
Walks O
We O
denote O
a O
random O
walk O
rooted O
at O
vertex O
vi O
as O
Wv O
i O
. O
It O
is O
a O
stochastic Method
process Method
with O
random O
variables O
W O
is O
a O
vertex O
chosen O
at O
random O
from O
the O
neighbors O
of O
vertex O
v O
k O
. O
Random Method
walks Method
have O
been O
used O
as O
a O
similarity Metric
measure Metric
for O
a O
variety O
of O
problems O
in O
content Task
recommendation Task
[ O
reference O
] O
and O
community Task
detection Task
[ O
reference O
] O
. O
They O
are O
also O
the O
foundation O
of O
a O
class O
of O
output Method
sensitive Method
algorithms Method
which O
use O
them O
to O
compute O
local O
community O
structure O
information O
in O
time O
sublinear O
to O
the O
size O
of O
the O
input O
graph O
[ O
reference O
] O
. O
It O
is O
this O
connection O
to O
local O
structure O
that O
motivates O
us O
to O
use O
a O
stream Method
of Method
short Method
random Method
walks Method
as O
our O
basic O
tool O
for O
extracting Task
information Task
from O
a O
network Task
. O
In O
addition O
to O
capturing O
community O
information O
, O
using O
random O
walks O
as O
the O
basis O
for O
our O
algorithm O
gives O
us O
two O
other O
desirable O
properties O
. O
First O
, O
local Task
exploration Task
is O
easy O
to O
parallelize O
. O
Several O
random Method
walkers Method
( O
in O
different O
threads O
, O
processes O
, O
or O
machines O
) O
can O
simultaneously O
explore O
different O
parts O
of O
the O
same O
graph O
. O
Secondly O
, O
relying O
on O
information O
obtained O
from O
short O
random O
walks O
make O
it O
possible O
to O
accommodate O
small O
changes O
in O
the O
graph O
structure O
without O
the O
need O
for O
global O
recomputation O
. O
We O
can O
iteratively O
update O
the O
learned O
model O
with O
new O
random O
walks O
from O
the O
changed O
region O
in O
time O
sub O
- O
linear O
to O
the O
entire O
graph O
. O
section O
: O
Connection O
: O
Power O
laws O
Having O
chosen O
online O
random O
walks O
as O
our O
primitive O
for O
capturing Task
graph Task
structure Task
, O
we O
now O
need O
a O
suitable O
method O
to O
capture O
this O
information O
. O
If O
the O
degree O
distribution O
of O
a O
connected O
graph O
follows O
a O
power O
law O
( O
is O
scale O
- O
free O
) O
, O
we O
observe O
that O
the O
frequency O
which O
vertices O
appear O
in O
the O
short O
random O
walks O
will O
also O
follow O
a O
power Method
- Method
law Method
distribution Method
. O
Word O
frequency O
in O
natural O
language O
follows O
a O
similar O
distribution O
, O
and O
techniques O
from O
language Method
modeling Method
account O
for O
this O
distributional O
behavior O
. O
To O
emphasize O
this O
similarity O
we O
show O
two O
different O
power O
- O
law O
distributions O
in O
Figure O
2 O
. O
The O
first O
comes O
from O
a O
series O
of O
short Method
random Method
walks Method
on O
a O
scale Method
- Method
free Method
graph Method
, O
and O
the O
second O
comes O
from O
the O
text O
of O
100 O
, O
000 O
articles O
from O
the O
English Material
Wikipedia Material
. O
A O
core O
contribution O
of O
our O
work O
is O
the O
idea O
that O
techniques O
which O
have O
been O
used O
to O
model O
natural O
language O
( O
where O
the O
symbol O
frequency O
follows O
a O
power O
law O
distribution O
( O
or O
Zipf Method
's Method
law Method
) O
) O
can O
be O
re O
- O
purposed O
to O
model O
community Task
structure Task
in Task
networks Task
. O
We O
spend O
the O
rest O
of O
this O
section O
reviewing O
the O
growing O
work O
in O
language Task
modeling Task
, O
and O
transforming O
it O
to O
learn O
representations O
of O
vertices O
which O
satisfy O
our O
criteria O
. O
section O
: O
Language Method
Modeling Method
The O
goal O
of O
language Task
modeling Task
is O
estimate O
the O
likelihood O
of O
a O
specific O
sequence O
of O
words O
appearing O
in O
a O
corpus O
. O
More O
formally O
, O
given O
a O
sequence O
of O
words O
where O
wi O
∈ O
V O
( O
V O
is O
the O
vocabulary O
) O
, O
we O
would O
like O
to O
maximize O
the O
Pr O
( O
wn|w0 O
, O
w1 O
, O
· O
· O
· O
, O
wn−1 O
) O
over O
all O
the O
training O
corpus O
. O
Recent O
work O
in O
representation Task
learning Task
has O
focused O
on O
using O
probabilistic Method
neural Method
networks Method
to O
build O
general Method
representations Method
of O
words O
which O
extend O
the O
scope O
of O
language Method
modeling Method
beyond O
its O
original O
goals O
. O
In O
this O
work O
, O
we O
present O
a O
generalization Method
of Method
language Method
modeling Method
to O
explore O
the O
graph O
through O
a O
stream Method
of Method
short Method
random Method
walks Method
. O
These O
walks O
can O
be O
thought O
of O
short O
sentences O
and O
phrases O
in O
a O
special O
language O
. O
The O
direct O
analog O
is O
to O
estimate O
the O
likelihood O
of O
observing O
vertex O
vi O
given O
all O
the O
previous O
vertices O
visited O
so O
far O
in O
the O
random O
walk O
. O
Our O
goal O
is O
to O
learn O
a O
latent Method
representation Method
, O
not O
only O
a O
probability O
distribution O
of O
node O
co O
- O
occurrences O
, O
and O
so O
we O
introduce O
a O
mapping O
function O
Φ O
: O
v O
∈ O
V O
→ O
R O
|V O
|×d O
. O
This O
mapping Method
Φ Method
represents O
the O
latent Method
social Method
representation Method
associated O
with O
each O
vertex O
v O
in O
the O
graph O
. O
( O
In O
practice O
, O
we O
represent O
Φ O
by O
a O
|V O
| O
× O
d O
matrix O
of O
free O
parameters O
, O
which O
will O
serve O
later O
on O
as O
our O
XE O
. O
) O
The O
problem O
then O
, O
is O
to O
estimate O
the O
likelihood O
: O
However O
as O
the O
walk O
length O
grows O
, O
computing O
this O
objective O
function O
becomes O
unfeasible O
. O
A O
recent O
relaxation O
in O
language Task
modeling Task
[ O
reference O
][ O
reference O
] O
turns O
the O
prediction Task
problem Task
on O
its O
head O
. O
First O
, O
instead O
of O
using O
the O
context O
to O
predict O
a O
missing O
word O
, O
it O
uses O
one O
word O
to O
predict O
the O
context O
. O
Secondly O
, O
the O
context O
is O
composed O
of O
the O
words O
appearing O
to O
right O
side O
of O
the O
given O
word O
as O
well O
as O
the O
left O
side O
. O
Finally O
, O
it O
removes O
the O
ordering O
constraint O
on O
the O
problem O
. O
Instead O
, O
the O
model O
is O
required O
to O
maximize O
the O
probability O
of O
any O
word O
appearing O
in O
the O
context O
without O
the O
knowledge O
of O
its O
offset O
from O
the O
given O
word O
. O
In O
terms O
of O
vertex Method
representation Method
modeling Method
, O
this O
yields O
the O
optimization Task
problem Task
: O
We O
find O
these O
relaxations O
are O
particularly O
desirable O
for O
social Task
representation Task
learning Task
. O
First O
, O
the O
order O
independence O
assumption O
better O
captures O
a O
sense O
of O
' O
nearness O
' O
that O
is O
provided O
by O
random O
walks O
. O
Moreover O
, O
this O
relaxation O
is O
quite O
useful O
for O
speeding O
up O
the O
training Metric
time Metric
by O
building O
small Method
models Method
as O
one O
vertex O
is O
given O
at O
a O
time O
. O
Solving O
the O
optimization Task
problem Task
from O
Eq O
. O
2 O
builds O
representations O
that O
capture O
the O
shared O
similarities O
in O
local O
graph O
structure O
between O
vertices O
. O
Vertices O
which O
have O
similar O
neighborhoods O
will O
acquire O
similar O
representations O
( O
encoding O
cocitation O
similarity O
) O
, O
and O
allowing O
generalization Task
on O
machine Task
learning Task
tasks Task
. O
By O
combining O
both O
truncated Method
random Method
walks Method
and O
neural Method
language Method
models Method
we O
formulate O
a O
method O
which O
satisfies O
all O
window O
size O
w O
embedding O
size O
d O
walks O
per O
vertex O
γ O
walk O
length O
t O
Output O
: O
matrix Method
of Method
vertex Method
representations Method
Φ O
∈ O
R O
|V O
|×d O
1 O
: O
Initialization O
: O
Sample O
Φ O
from O
U O
|V O
|×d O
2 O
: O
Build O
a O
binary O
Tree O
T O
from O
V O
3 O
: O
for O
i O
= O
0 O
to O
γ O
do O
4 O
: O
for O
each O
vi O
∈ O
O O
do O
6 O
: O
end O
for O
9 O
: O
end O
for O
of O
our O
desired O
properties O
. O
This O
method O
generates O
representations Task
of Task
social Task
networks Task
that O
are O
low O
- O
dimensional O
, O
and O
exist O
in O
a O
continuous O
vector O
space O
. O
Its O
representations O
encode O
latent O
forms O
of O
community O
membership O
, O
and O
because O
the O
method O
outputs O
useful O
intermediate Method
representations Method
, O
it O
can O
adapt O
to O
changing O
network O
topology O
. O
section O
: O
METHOD O
In O
this O
section O
we O
discuss O
the O
main O
components O
of O
our O
algorithm O
. O
We O
also O
present O
several O
variants O
of O
our O
approach O
and O
discuss O
their O
merits O
. O
section O
: O
Overview O
As O
in O
any O
language Method
modeling Method
algorithm Method
, O
the O
only O
required O
input O
is O
a O
corpus O
and O
a O
vocabulary O
V. O
DeepWalk Method
considers O
a O
set O
of O
short O
truncated O
random O
walks O
its O
own O
corpus O
, O
and O
the O
graph O
vertices O
as O
its O
own O
vocabulary O
( O
V O
= O
V O
) O
. O
While O
it O
is O
beneficial O
to O
know O
the O
V O
and O
the O
frequency O
distribution O
of O
vertices O
in O
the O
random O
walks O
ahead O
of O
the O
training O
, O
it O
is O
not O
necessary O
for O
the O
algorithm O
to O
work O
as O
we O
will O
show O
in O
4.2.2 O
. O
section O
: O
Algorithm O
: O
DeepWalk Method
The O
algorithm O
consists O
of O
two O
main O
components O
; O
first O
a O
random Method
walk Method
generator Method
and O
second O
an O
update Method
procedure Method
. O
The O
random Method
walk Method
generator Method
takes O
a O
graph O
G O
and O
samples O
uniformly O
a O
random O
vertex O
vi O
as O
the O
root O
of O
the O
random Method
walk Method
Wv O
i O
. O
A O
walk O
samples O
uniformly O
from O
the O
neighbors O
of O
the O
last O
vertex O
visited O
until O
the O
maximum O
length O
( O
t O
) O
is O
reached O
. O
While O
we O
set O
the O
length O
of O
our O
random O
walks O
in O
the O
experiments O
to O
be O
fixed O
, O
there O
is O
no O
restriction O
for O
the O
random O
walks O
to O
be O
of O
the O
same O
length O
. O
These O
walks O
could O
have O
restarts O
( O
i.e. O
a O
teleport O
probability O
of O
returning O
back O
to O
their O
root O
) O
, O
but O
our O
preliminary O
results O
did O
not O
show O
any O
advantage O
of O
using O
restarts Method
. O
In O
practice O
, O
our O
implementation O
specifies O
a O
number O
of O
random O
walks O
γ O
of O
length O
t O
to O
start O
at O
each O
vertex O
. O
Lines O
3 O
- O
9 O
in O
Algorithm O
1 O
shows O
the O
core O
of O
our O
approach O
. O
The O
outer O
loop O
specifies O
the O
number O
of O
times O
, O
γ O
, O
which O
we O
should O
start O
random O
walks O
at O
each O
vertex O
. O
We O
think O
of O
each O
iteration O
as O
making O
a O
' O
pass O
' O
over O
the O
data O
and O
sample O
one O
walk O
per O
node O
during O
this O
pass O
. O
At O
the O
start O
of O
each O
pass O
we O
generate O
a O
random O
ordering O
to O
traverse O
the O
vertices O
. O
This O
is O
not O
strictly O
required O
, O
but O
is O
well O
- O
known O
to O
speed O
up O
the O
convergence O
of O
stochastic Method
gradient Method
descent Method
. O
section O
: O
Algorithm O
2 O
5 O
: O
end O
for O
6 O
: O
end O
for O
In O
the O
inner O
loop O
, O
we O
iterate O
over O
all O
the O
vertices O
of O
the O
graph O
. O
For O
each O
vertex O
vi O
we O
generate O
a O
random Method
walk Method
|Wv O
i O
| O
= O
t O
, O
and O
then O
use O
it O
to O
update O
our O
representations O
( O
Line O
7 O
) O
. O
We O
use O
the O
SkipGram Method
algorithm Method
[ O
reference O
] O
to O
update O
these O
representations O
in O
accordance O
with O
our O
objective Metric
function Metric
in O
Eq O
. O
2 O
. O
section O
: O
SkipGram Method
SkipGram Method
is O
a O
language Method
model Method
that O
maximizes O
the O
cooccurrence O
probability O
among O
the O
words O
that O
appear O
within O
a O
window O
, O
w O
, O
in O
a O
sentence O
[ O
reference O
] O
. O
Algorithm O
2 O
iterates O
over O
all O
possible O
collocations O
in O
random O
walk O
that O
appear O
within O
the O
window O
w O
( O
lines O
1 O
- O
2 O
) O
. O
For O
each O
, O
we O
map O
each O
vertex O
vj O
to O
its O
current O
representation O
vector O
Φ O
( O
vj O
) O
∈ O
R O
d O
( O
See O
Figure O
3b O
) O
. O
Given O
the O
representation O
of O
vj O
, O
we O
would O
like O
to O
maximize O
the O
probability O
of O
its O
neighbors O
in O
the O
walk O
( O
line O
3 O
) O
. O
We O
can O
learn O
such O
posterior O
distribution O
using O
several O
choices O
of O
classifiers Method
. O
For O
example O
, O
modeling O
the O
previous Task
problem Task
using O
logistic Method
regression Method
would O
result O
in O
a O
huge O
number O
of O
labels O
that O
is O
equal O
to O
|V O
| O
which O
could O
be O
in O
millions O
or O
billions O
. O
Such O
models O
require O
large O
amount O
of O
computational O
resources O
that O
could O
span O
a O
whole O
cluster O
of O
computers O
[ O
reference O
] O
. O
To O
speed O
the O
training Metric
time Metric
, O
Hierarchical O
Softmax O
[ O
reference O
][ O
reference O
] O
can O
be O
used O
to O
approximate O
the O
probability O
distribution O
. O
section O
: O
Hierarchical Method
Softmax Method
Given O
that O
u O
k O
∈ O
V O
, O
calculating O
Pr O
( O
u O
k O
| O
Φ O
( O
vj O
) O
) O
in O
line O
3 O
is O
not O
feasible O
. O
Computing O
the O
partition O
function O
( O
normalization O
factor O
) O
is O
expensive O
. O
If O
we O
assign O
the O
vertices O
to O
the O
leaves O
of O
a O
binary O
tree O
, O
the O
prediction Task
problem Task
turns O
into O
maximizing O
the O
probability O
of O
a O
specific O
path O
in O
the O
tree O
( O
See O
Figure O
3c O
) O
. O
If O
the O
path O
to O
vertex O
u O
k O
is O
identified O
by O
a O
sequence O
of O
tree O
nodes O
( O
b0 O
, O
b1 O
, O
. O
. O
. O
, O
b O
log O
|V O
| O
) O
, O
( O
b0 O
= O
root O
, O
Now O
, O
Pr O
( O
b O
l O
| O
Φ O
( O
vj O
) O
) O
could O
be O
modeled O
by O
a O
binary Method
classifier Method
that O
is O
assigned O
to O
the O
parent O
of O
the O
node O
b O
l O
. O
This O
reduces O
the O
computational Metric
complexity Metric
of O
calculating O
Pr Metric
( O
We O
can O
speed O
up O
the O
training O
process O
further O
, O
by O
assigning O
shorter O
paths O
to O
the O
frequent O
vertices O
in O
the O
random O
walks O
. O
Huffman Method
coding Method
is O
used O
to O
reduce O
the O
access O
time O
of O
frequent O
elements O
in O
the O
tree O
. O
section O
: O
Optimization Task
The O
model O
parameter O
set O
is O
{ O
Φ O
, O
T O
} O
where O
the O
size O
of O
each O
is O
O O
( O
d|V O
| O
) O
. O
Stochastic Method
gradient Method
descent Method
( O
SGD Method
) O
[ O
reference O
] O
is O
used O
to O
optimize O
these O
parameters O
( O
Line O
4 O
, O
Algorithm O
2 O
) O
. O
The O
derivatives O
are O
estimated O
using O
the O
back Method
- Method
propagation Method
algorithm Method
. O
The O
learning Metric
rate Metric
α Metric
for O
SGD Method
is O
initially O
set O
to O
2.5 O
% O
at O
the O
beginning O
of O
the O
training O
and O
then O
decreased O
linearly O
( O
v1 O
) O
) O
and O
Pr O
( O
v5 O
| O
Φ O
( O
v1 O
) O
) O
over O
sequences O
of O
probability O
distributions O
corresponding O
to O
the O
paths O
starting O
at O
the O
root O
and O
ending O
at O
v3 O
and O
v5 O
. O
The O
representation Method
Φ Method
is O
updated O
to O
maximize O
the O
probability O
of O
v1 O
co O
- O
occurring O
with O
its O
context O
{ O
v3 O
, O
v5}. O
with O
the O
number O
of O
vertices O
that O
are O
seen O
so O
far O
. O
section O
: O
Parallelizability O
As O
shown O
in O
Figure O
2 O
the O
frequency O
distribution O
of O
vertices O
in O
random O
walks O
of O
social O
network O
and O
words O
in O
a O
language O
both O
follow O
a O
power Method
law Method
. O
This O
results O
in O
a O
long O
tail O
of O
infrequent O
vertices O
, O
therefore O
, O
the O
updates O
that O
affect O
Φ O
will O
be O
sparse O
in O
nature O
. O
This O
allows O
us O
to O
use O
asynchronous Method
version Method
of Method
stochastic Method
gradient Method
descent Method
( O
ASGD Method
) Method
, O
in O
the O
multi Task
- Task
worker Task
case Task
. O
Given O
that O
our O
updates O
are O
sparse O
and O
we O
do O
not O
acquire O
a O
lock O
to O
access O
the O
model O
shared O
parameters O
, O
ASGD O
will O
achieve O
an O
optimal O
rate O
of O
convergence Metric
[ O
reference O
] O
. O
While O
we O
run O
experiments O
on O
one O
machine O
using O
multiple O
threads O
, O
it O
has O
been O
demonstrated O
that O
this O
technique O
is O
highly O
scalable O
, O
and O
can O
be O
used O
in O
very O
large Task
scale Task
machine Task
learning Task
[ O
reference O
] O
. O
Figure O
4 O
presents O
the O
effects O
of O
parallelizing O
DeepWalk Method
. O
It O
shows O
the O
speed O
up O
in O
processing O
BlogCatalog Material
and O
Flickr O
networks O
is O
consistent O
as O
we O
increase O
the O
number O
of O
workers O
to O
8 O
( O
Figure O
4a O
) O
. O
It O
also O
shows O
that O
there O
is O
no O
loss O
of O
predictive Metric
performance Metric
relative O
to O
the O
running O
DeepWalk Method
serially O
( O
Figure O
4b O
) O
. O
section O
: O
Algorithm O
Variants O
Here O
we O
discuss O
some O
variants O
of O
our O
proposed O
method O
, O
which O
we O
believe O
may O
be O
of O
interest O
. O
section O
: O
Streaming Task
One O
interesting O
variant O
of O
this O
method O
is O
a O
streaming Method
approach Method
, O
which O
could O
be O
implemented O
without O
knowledge O
of O
the O
entire O
graph O
. O
In O
this O
variant O
small O
walks O
from O
the O
graph O
are O
passed O
directly O
to O
the O
representation Method
learning Method
code Method
, O
and O
the O
model O
is O
updated O
directly O
. O
Some O
modifications O
to O
the O
learning Method
process Method
will O
also O
be O
necessary O
. O
First O
, O
using O
a O
decaying O
learning O
rate O
will O
no O
longer O
be O
possible O
. O
Instead O
, O
we O
can O
initialize O
the O
learning Metric
rate Metric
α Metric
to O
a O
small O
constant O
value O
. O
This O
will O
take O
longer O
to O
learn O
, O
but O
may O
be O
worth O
it O
in O
some O
applications O
. O
Second O
, O
we O
can O
not O
necessarily O
build O
a O
tree O
of O
parameters O
any O
more O
. O
If O
the O
cardinality O
of O
V O
is O
known O
( O
or O
can O
be O
bounded O
) O
, O
we O
can O
build O
the O
Hierarchical O
Softmax O
tree O
for O
that O
maximum O
value O
. O
Vertices O
can O
be O
assigned O
to O
one O
of O
the O
remaining O
leaves O
when O
they O
are O
first O
seen O
. O
If O
we O
have O
the O
ability O
to O
estimate O
the O
vertex O
frequency O
a O
priori O
, O
we O
can O
section O
: O
Non Task
- Task
random Task
walks Task
Some O
graphs O
are O
created O
as O
a O
by O
- O
product O
of O
agents O
interacting O
with O
a O
sequence O
of O
elements O
( O
e.g. O
users O
' O
navigation O
of O
pages O
on O
a O
website O
) O
. O
When O
a O
graph O
is O
created O
by O
such O
a O
stream O
of O
non Method
- Method
random Method
walks Method
, O
we O
can O
use O
this O
process O
to O
feed O
the O
modeling Task
phase Task
directly O
. O
Graphs O
sampled O
in O
this O
way O
will O
not O
only O
capture O
information O
related O
to O
network O
structure O
, O
but O
also O
to O
the O
frequency O
at O
which O
paths O
are O
traversed O
. O
In O
our O
view O
, O
this O
variant O
also O
encompasses O
language Method
modeling Method
. O
Sentences O
can O
be O
viewed O
as O
purposed O
walks O
through O
an O
appropriately O
designed O
language Method
network Method
, O
and O
language Method
models Method
like O
SkipGram Method
are O
designed O
to O
capture O
this O
behavior O
. O
This O
approach O
can O
be O
combined O
with O
the O
streaming Method
variant Method
( O
Section O
4.4.1 O
) O
to O
train O
features O
on O
a O
continually Method
evolving Method
network Method
without O
ever O
explicitly O
constructing O
the O
entire O
graph O
. O
Maintaining Method
representations Method
with O
this O
technique O
could O
enable O
web Task
- Task
scale Task
classification Task
without O
the O
hassles O
of O
dealing O
with O
a O
web O
- O
scale O
graph O
. O
section O
: O
EXPERIMENTAL O
DESIGN O
In O
this O
section O
we O
provide O
an O
overview O
of O
the O
datasets O
and O
methods O
which O
we O
will O
use O
in O
our O
experiments O
. O
Code O
and O
data O
to O
reproduce O
our O
results O
will O
be O
available O
at O
the O
first O
author O
's O
website O
. O
An O
overview O
of O
the O
graphs O
we O
consider O
in O
our O
experiments O
is O
given O
in O
Figure O
1 O
. O
section O
: O
Datasets O
• O
BlogCatalog Material
[ O
reference O
] O
is O
a O
network O
of O
social O
relationships O
provided O
by O
blogger O
authors O
. O
The O
labels O
represent O
the O
topic O
categories O
provided O
by O
the O
authors O
. O
• O
Flickr Material
[ O
reference O
] O
is O
a O
network O
of O
the O
contacts O
between O
users O
of O
the O
photo O
sharing O
website O
. O
The O
labels O
represent O
the O
interest O
groups O
of O
the O
users O
such O
as O
' O
black O
and O
white O
photos O
' O
. O
• O
YouTube Material
[ O
reference O
] O
is O
a O
social O
network O
between O
users O
of O
the O
popular O
video O
sharing O
website O
. O
The O
labels O
here O
represent O
groups O
of O
viewers O
that O
enjoy O
common O
video O
genres O
( O
e.g. O
anime O
and O
wrestling O
) O
. O
section O
: O
Baseline O
Methods O
To O
validate O
the O
performance O
of O
our O
approach O
we O
compare O
it O
against O
a O
number O
of O
baselines O
: O
• O
SpectralClustering Method
[ O
reference O
] O
: O
This O
method O
generates O
a O
representation O
in O
R O
d O
from O
the O
d O
- O
smallest O
eigenvectors O
of O
L O
, O
the O
normalized O
graph O
Laplacian O
of O
G. O
Utilizing O
the O
eigenvectors O
of O
L O
implicitly O
assumes O
that O
graph O
cuts O
will O
be O
useful O
for O
classification Task
. O
• O
Modularity O
[ O
reference O
] O
: O
This O
method O
generates O
a O
representation O
in O
R O
d O
from O
the O
top O
- O
d O
eigenvectors O
of O
B O
, O
the O
Modularity O
matrix O
of O
G. O
The O
eigenvectors O
of O
B O
encode O
information O
about O
modular O
graph O
partitions O
of O
G O
[ O
reference O
] O
. O
Using O
them O
as O
features O
assumes O
that O
modular O
graph O
partitions O
will O
be O
useful O
for O
classification Task
. O
• O
EdgeCluster Method
[ O
reference O
] O
: O
This O
method O
uses O
k Method
- Method
means Method
clustering Method
to O
cluster O
the O
adjacency O
matrix O
of O
G. O
Its O
has O
been O
shown O
to O
perform O
comparably O
to O
the O
Modularity Method
method Method
, O
with O
the O
added O
advantage O
of O
scaling O
to O
graphs O
which O
are O
too O
large O
for O
spectral Method
decomposition Method
. O
• O
wvRN Method
[ O
reference O
] O
: O
The O
weighted Method
- Method
vote Method
Relational Method
Neighbor Method
is O
a O
relational Method
classifier Method
. O
Given O
the O
neighborhood O
Ni O
of O
vertex O
vi O
, O
wvRN O
estimates O
Pr O
( O
yi|Ni O
) O
with O
the O
( O
appropriately O
normalized O
) O
weighted O
mean O
of O
its O
neighbors O
( O
i.e O
Pr O
( O
yi|Ni O
) O
= O
wij O
Pr O
( O
yj O
| O
Nj O
) O
) O
. O
It O
has O
shown O
surprisingly O
good O
performance O
in O
real Task
networks Task
, O
and O
has O
been O
advocated O
as O
a O
sensible O
relational O
classification Task
baseline O
[ O
reference O
] O
. O
• O
Majority O
: O
This O
naïve O
method O
simply O
chooses O
the O
most O
frequent O
labels O
in O
the O
training O
set O
. O
section O
: O
EXPERIMENTS O
In O
this O
section O
we O
present O
an O
experimental O
analysis O
of O
our O
method O
. O
We O
thoroughly O
evaluate O
it O
on O
a O
number O
of O
multilabel Task
classification Task
tasks Task
, O
and O
analyze O
its O
sensitivity O
across O
several O
parameters O
. O
section O
: O
Multi Task
- Task
Label Task
Classification Task
To O
facilitate O
the O
comparison O
between O
our O
method O
and O
the O
relevant O
baselines O
, O
we O
use O
the O
exact O
same O
datasets O
and O
experimental O
procedure O
as O
in O
[ O
reference O
][ O
reference O
] O
. O
Specifically O
, O
we O
randomly O
sample O
a O
portion O
( O
TR O
) O
of O
the O
labeled O
nodes O
, O
and O
use O
them O
as O
training O
data O
. O
The O
rest O
of O
the O
nodes O
are O
used O
as O
test O
. O
We O
repeat O
this O
process O
10 O
times O
, O
and O
report O
the O
average O
performance O
in O
terms O
of O
both O
Macro Metric
- Metric
F1 Metric
and O
Micro Metric
- Metric
F1 Metric
. O
When O
possible O
we O
report O
the O
original O
results O
[ O
reference O
][ O
reference O
] O
here O
directly O
. O
For O
all O
models O
we O
use O
a O
one Method
- Method
vs Method
- Method
rest Method
logistic Method
regression Method
implemented O
by O
LibLinear Method
[ O
reference O
] O
section O
: O
BlogCatalog Material
In O
this O
experiment O
we O
increase O
the O
training Metric
ratio Metric
( O
TR Metric
) O
on O
the O
BlogCatalog Material
network O
from O
10 O
% O
to O
90 O
% O
. O
Our O
results O
are O
presented O
in O
Table O
2 O
. O
Numbers O
in O
bold O
represent O
the O
highest O
performance O
in O
each O
column O
. O
DeepWalk Method
performs O
consistently O
better O
than O
EdgeCluster Method
, O
Modularity Method
, O
and O
wvRN Method
. O
In O
fact O
, O
when O
trained O
with O
only O
20 O
% O
of O
the O
nodes O
labeled O
, O
DeepWalk Method
performs O
better O
than O
these O
approaches O
when O
they O
are O
given O
90 O
% O
of O
the O
data O
. O
The O
performance O
of O
SpectralClustering Method
proves O
much O
more O
competitive O
, O
but O
DeepWalk Method
still O
outperforms O
when O
labeled O
data O
is O
sparse O
on O
both O
Macro Metric
- Metric
F1 Metric
( O
TR Metric
≤ O
20 O
% O
) O
and O
Micro Metric
- Metric
F1 Metric
( O
TR Metric
≤ O
60 O
% O
) O
. O
This O
strong O
performance O
when O
only O
small O
fractions O
of O
the O
graph O
are O
labeled O
is O
a O
core O
strength O
of O
our O
approach O
. O
In O
the O
following O
experiments O
, O
we O
investigate O
the O
performance O
of O
our O
representations O
on O
even O
more O
sparsely O
labeled O
graphs O
. O
section O
: O
Flickr Material
In O
this O
experiment O
we O
vary O
the O
training Metric
ratio Metric
( O
TR Metric
) O
on O
the O
Flickr Method
network Method
from O
1 O
% O
to O
10 O
% O
. O
This O
corresponds O
to O
having O
approximately O
800 O
to O
8 O
, O
000 O
nodes O
labeled O
for O
classification Task
in O
the O
entire O
network O
. O
Table O
3 O
presents O
our O
results O
, O
which O
are O
consistent O
with O
the O
previous O
experiment O
. O
DeepWalk Method
outperforms O
all O
baselines O
by O
at O
least O
3 O
% O
with O
respect O
to O
Micro Metric
- Metric
F1 Metric
. O
Additionally O
, O
its O
Micro Metric
- Metric
F1 Metric
performance O
when O
only O
3 O
% O
of O
the O
graph O
is O
labeled O
beats O
all O
other O
methods O
even O
when O
they O
have O
been O
given O
10 O
% O
of O
the O
data O
. O
In O
other O
words O
, O
DeepWalk Method
can O
outperform O
the O
baselines O
with O
60 O
% O
less O
training O
data O
. O
It O
also O
performs O
quite O
well O
in O
Macro Metric
- Metric
F1 Metric
, O
initially O
performing O
close O
to O
SpectralClustering Method
, O
but O
distancing O
itself O
to O
a O
1 O
% O
improvement O
. O
section O
: O
YouTube Material
The O
YouTube Material
network Material
is O
considerably O
larger O
than O
the O
previous O
ones O
we O
have O
experimented O
on O
, O
and O
its O
size O
prevents O
two O
of O
our O
baseline O
methods O
( O
SpectralClustering Method
and O
Modularity Method
) O
from O
running O
on O
it O
. O
It O
is O
much O
closer O
to O
a O
real O
world O
graph O
than O
those O
we O
have O
previously O
considered O
. O
The O
results O
of O
varying O
the O
training Metric
ratio Metric
( O
TR Metric
) O
from O
1 O
% O
to O
10 O
% O
are O
presented O
in O
Table O
4 O
. O
They O
show O
that O
DeepWalk Method
significantly O
outperforms O
the O
scalable O
baseline O
for O
creating Task
graph Task
representations Task
, O
EdgeCluster O
. O
When O
1 O
% O
of O
the O
labeled O
nodes O
are O
used O
for O
test O
, O
the O
Micro Metric
- Metric
F1 Metric
improves O
by O
14 O
% O
. O
The O
Macro Metric
- Metric
F1 Metric
shows O
a O
corresponding O
10 O
% O
increase O
. O
This O
lead O
narrows O
as O
the O
training O
data O
increases O
, O
but O
DeepWalk Method
ends O
with O
a O
3 O
% O
lead O
in O
Micro Metric
- Metric
F1 Metric
, O
and O
an O
impressive O
5 O
% O
improvement O
in O
Macro Metric
- Metric
F1 Metric
. O
This O
experiment O
showcases O
the O
performance O
benefits O
that O
can O
occur O
from O
using O
social Method
representation Method
learning Method
for O
multilabel Task
classification Task
. O
DeepWalk Method
, O
can O
scale O
to O
large O
graphs O
, O
and O
performs O
exceedingly O
well O
in O
such O
a O
sparsely O
labeled O
environment O
. O
section O
: O
Parameter Metric
Sensitivity Metric
In O
order O
to O
evaluate O
how O
changes O
to O
the O
parameterization O
of O
DeepWalk Method
effect O
its O
performance O
on O
classification Task
tasks Task
, O
we O
conducted O
experiments O
on O
two O
multi Task
- Task
label Task
classifications Task
tasks Task
( O
Flickr Material
, O
and O
BlogCatalog Material
) O
. O
For O
this O
test O
, O
we O
have O
fixed O
the O
window O
size O
and O
the O
walk O
length O
to O
sensible O
values O
( O
w O
= O
10 O
, O
t O
= O
40 O
) O
which O
should O
emphasize O
local O
structure O
. O
We O
then O
vary O
the O
number O
of O
latent O
dimensions O
( O
d O
) O
, O
the O
number O
of O
walks O
started O
per O
vertex O
( O
γ O
) O
, O
and O
the O
amount O
of O
training O
data O
available O
( O
TR O
) O
to O
determine O
their O
impact O
on O
the O
network Task
classification Task
performance O
. O
Figure O
5a O
shows O
the O
effects O
of O
increasing O
the O
number O
of O
latent O
dimensions O
available O
to O
our O
model O
. O
section O
: O
Effect O
of O
Dimensionality Metric
Figures O
5a1 O
and O
5a3 O
examine O
the O
effects O
of O
varying O
the O
dimensionality Metric
and O
training Metric
rate Metric
. O
The O
performance O
is O
quite O
consistent O
between O
both O
Flickr Material
and O
BlogCatalog Material
and O
show O
that O
the O
optimal O
dimensionality Metric
for O
a O
model O
is O
dependent O
on O
the O
number O
of O
training O
examples O
. O
( O
Note O
that O
1 O
% O
of O
Flickr Material
has O
approximately O
as O
many O
labeled O
examples O
as O
10 O
% O
of O
BlogCatalog Material
) O
. O
Figures O
5a2 O
and O
5a3 O
examine O
the O
effects O
of O
varying O
the O
dimensionality O
and O
number O
of O
walks O
per O
vertex O
. O
The O
relative O
performance O
between O
dimensions O
is O
relatively O
stable O
across O
different O
values O
of O
γ O
. O
These O
charts O
have O
two O
interesting O
observations O
. O
The O
first O
is O
that O
there O
is O
most O
of O
the O
benefit O
is O
accomplished O
by O
starting O
γ O
= O
30 O
walks O
per O
node O
in O
both O
graphs O
. O
The O
second O
is O
that O
the O
relative O
difference O
between O
different O
values O
of O
γ O
is O
quite O
consistent O
between O
the O
two O
graphs O
. O
Flickr Method
has O
an O
order O
of O
magnitude O
more O
edges O
than O
BlogCatalog Material
, O
and O
we O
find O
this O
behavior O
interesting O
. O
These O
experiments O
show O
that O
our O
method O
can O
make O
useful O
models O
of O
various O
sizes O
. O
They O
also O
show O
that O
the O
performance O
of O
the O
model O
depends O
on O
the O
number O
of O
random O
walks O
it O
has O
seen O
, O
and O
the O
appropriate O
dimensionality O
of O
the O
model O
depends O
on O
the O
training O
examples O
available O
. O
Figure O
5a O
shows O
the O
effects O
of O
increasing O
γ O
, O
the O
number O
of O
random O
walks O
that O
we O
start O
from O
each O
vertex O
. O
section O
: O
Effect O
of O
sampling O
frequency O
The O
results O
are O
very O
consistent O
for O
different O
dimensions O
( O
Fig O
. O
5b1 O
, O
Fig O
. O
5b3 O
) O
and O
the O
amount O
of O
training O
data O
( O
Fig O
. O
5b2 O
, O
Fig O
. O
5b4 O
) O
. O
Initially O
, O
increasing O
γ O
has O
a O
big O
effect O
in O
the O
results O
, O
but O
this O
effect O
quickly O
slows O
( O
γ O
> O
10 O
) O
. O
These O
results O
demonstrate O
that O
we O
are O
able O
to O
learn O
meaningful O
latent O
representations O
for O
vertices O
after O
only O
a O
small O
number O
of O
random O
walks O
. O
section O
: O
RELATED O
WORK O
The O
main O
differences O
between O
our O
proposed O
method O
and O
previous O
work O
can O
be O
summarized O
as O
follows O
: O
1 O
. O
We O
learn O
our O
latent Method
social Method
representations Method
, O
instead O
of O
computing O
statistics O
related O
to O
centrality O
[ O
reference O
] O
or O
partitioning O
[ O
reference O
] O
. O
2 O
. O
We O
do O
not O
attempt O
to O
extend O
the O
classification Task
procedure O
itself O
( O
through O
collective Method
inference Method
[ O
reference O
] O
or O
graph Method
kernels Method
[ O
reference O
] O
) O
. O
3 O
. O
We O
propose O
a O
scalable Method
online Method
method Method
which O
uses O
only O
local O
information O
. O
Most O
methods O
require O
global O
information O
and O
are O
offline O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
4 O
. O
We O
apply O
unsupervised Method
representation Method
learning Method
to O
graphs O
. O
In O
this O
section O
we O
discuss O
related O
work O
in O
network Task
classification Task
and O
unsupervised Task
feature Task
learning Task
. O
section O
: O
Relational Task
Learning Task
Relational O
classification Task
( O
or O
collective O
classification Task
) O
methods O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
use O
links O
between O
data O
items O
as O
part O
of O
the O
classification Task
process Task
. O
Exact Task
inference Task
in O
the O
collective Task
classification Task
problem Task
is O
NP Task
- Task
hard Task
, O
and O
solutions O
have O
focused O
on O
the O
use O
of O
approximate Method
inference Method
algorithm Method
which O
may O
not O
be O
guaranteed O
to O
converge O
[ O
reference O
] O
. O
The O
most O
relevant O
relational O
classification Task
algorithms O
to O
our O
work O
incorporate O
community O
information O
by O
learning O
clusters O
[ O
reference O
] O
, O
by O
adding O
edges O
between O
nearby O
nodes O
[ O
reference O
] O
, O
by O
using O
PageRank Method
[ O
reference O
] O
, O
or O
by O
extending O
relational Task
classification Task
to O
take O
additional O
features O
into O
account O
[ O
reference O
] O
. O
Our O
work O
takes O
a O
substantially O
different O
approach O
. O
Instead O
of O
a O
new O
approximation Method
inference Method
algorithm Method
, O
we O
propose O
a O
procedure O
which O
learns O
representations Method
of Method
network Method
structure Method
which O
can O
then O
be O
used O
by O
existing O
inference Method
procedure Method
( O
including O
iterative Method
ones Method
) O
. O
A O
number O
of O
techniques O
for O
generating Task
features Task
from O
graphs O
have O
also O
been O
proposed O
[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O
In O
contrast O
to O
these O
methods O
, O
we O
frame O
the O
feature Task
creation Task
procedure Task
as O
a O
representation Task
learning Task
problem Task
. O
Graph Method
Kernels Method
[ O
reference O
] O
have O
been O
proposed O
as O
a O
way O
to O
use O
relational O
data O
as O
part O
of O
the O
classification Task
process Task
, O
but O
are O
quite O
slow O
unless O
approximated O
[ O
reference O
] O
. O
Our O
approach O
is O
complementary O
; O
instead O
of O
encoding O
the O
structure O
as O
part O
of O
a O
kernel Method
function Method
, O
we O
learn O
a O
representation O
which O
allows O
them O
to O
be O
used O
directly O
as O
features O
for O
any O
classification Task
method O
. O
section O
: O
Unsupervised Task
Feature Task
Learning Task
Distributed Method
representations Method
have O
been O
proposed O
to O
model O
structural O
relationship O
between O
concepts O
[ O
reference O
] O
. O
These O
representations O
are O
trained O
by O
the O
back Method
- Method
propagation Method
and O
gradient Method
descent Method
. O
Computational Metric
costs Metric
and O
numerical Metric
instability Metric
led O
to O
these O
techniques O
to O
be O
abandoned O
for O
almost O
a O
decade O
. O
Recently O
, O
distributed Task
computing Task
allowed O
for O
larger O
models O
to O
be O
trained O
[ O
reference O
] O
, O
and O
the O
growth O
of O
data O
for O
unsupervised Method
learning Method
algorithms Method
to O
emerge O
[ O
reference O
] O
. O
Distributed Method
representations Method
usually O
are O
trained O
through O
neural Method
networks Method
, O
these O
networks O
have O
made O
advancements O
in O
diverse O
fields O
such O
as O
computer Task
vision Task
[ O
reference O
] O
, O
speech Task
recognition Task
[ O
reference O
] O
, O
and O
natural Task
language Task
processing Task
[ O
reference O
] O
. O
section O
: O
CONCLUSIONS O
We O
propose O
DeepWalk Method
, O
a O
novel O
approach O
for O
learning O
latent Task
social Task
representations Task
of Task
vertices Task
. O
Using O
local O
information O
from O
truncated O
random O
walks O
as O
input O
, O
our O
method O
learns O
a O
representation O
which O
encodes O
structural O
regularities O
. O
Experiments O
on O
a O
variety O
of O
different O
graphs O
illustrate O
the O
effectiveness O
of O
our O
approach O
on O
challenging O
multi Task
- Task
label Task
classification Task
tasks Task
. O
As O
an O
online Method
algorithm Method
, O
DeepWalk Method
is O
also O
scalable O
. O
Our O
results O
show O
that O
we O
can O
create O
meaningful Method
representations Method
for O
graphs Task
too O
large O
to O
run O
spectral Method
methods Method
on O
. O
On O
such O
large O
graphs O
, O
our O
method O
significantly O
outperforms O
other O
methods O
designed O
to O
operate O
for O
sparsity Task
. O
We O
also O
show O
that O
our O
approach O
is O
parallelizable O
, O
allowing O
workers O
to O
update O
different O
parts O
of O
the O
model O
concurrently O
. O
In O
addition O
to O
being O
effective O
and O
scalable O
, O
our O
approach O
is O
also O
an O
appealing O
generalization Method
of Method
language Method
modeling Method
. O
This O
connection O
is O
mutually O
beneficial O
. O
Advances O
in O
language Method
modeling Method
may O
continue O
to O
generate O
improved O
latent Method
representations Method
for O
networks Task
. O
In O
our O
view O
, O
language Task
modeling Task
is O
actually O
sampling O
from O
an O
unobservable O
language O
graph O
. O
We O
believe O
that O
insights O
obtained O
from O
modeling O
observable O
graphs O
may O
in O
turn O
yield O
improvements O
to O
modeling O
unobservable O
ones O
. O
Our O
future O
work O
in O
the O
area O
will O
focus O
on O
investigating O
this O
duality O
further O
, O
using O
our O
results O
to O
improve O
language Task
modeling Task
, O
and O
strengthening O
the O
theoretical O
justifications O
of O
the O
method O
. O
section O
: O
