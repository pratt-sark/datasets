document O
: O
Global Method
- Method
Locally Method
Self Method
- Method
Attentive Method
Dialogue Method
State Method
Tracker Method
Dialogue Task
state Task
tracking Task
, O
which O
estimates O
user O
goals O
and O
requests O
given O
the O
dialogue O
context O
, O
is O
an O
essential O
part O
of O
task O
- O
oriented O
dialogue Task
systems Task
. O
In O
this O
paper O
, O
we O
propose O
the O
Global Method
- Method
Locally Method
Self Method
- Method
Attentive Method
Dialogue Method
State Method
Tracker Method
( O
GLAD Method
) O
, O
which O
learns O
representations Method
of O
the O
user O
utterance O
and O
previous O
system O
actions O
with O
global Method
- Method
local Method
modules Method
. O
Our O
model O
uses O
global Method
modules Method
to O
share O
parameters O
between O
estimators O
for O
different O
types O
( O
called O
slots O
) O
of O
dialogue O
states O
, O
and O
uses O
local Method
modules Method
to O
learn O
slot O
- O
specific O
features O
. O
We O
show O
that O
this O
improves O
tracking Task
of Task
rare Task
states Task
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
WoZ Material
and O
DSTC2 Material
state Material
tracking Material
tasks O
. O
GLAD Method
obtains O
88.1 O
% O
joint Metric
goal Metric
accuracy Metric
and O
97.1 O
% O
request Metric
accuracy Metric
on O
WoZ Material
, O
outperforming O
prior O
work O
by O
3.7 O
% O
and O
5.5 O
% O
. O
On O
DSTC2 Material
, O
our O
model O
obtains O
74.5 O
% O
joint Metric
goal Metric
accuracy Metric
and O
97.5 O
% O
request Metric
accuracy Metric
, O
outperforming O
prior O
work O
by O
1.1 O
% O
and O
1.0 O
% O
. O
figures O
section O
: O
Introduction O
Task O
oriented O
dialogue Task
systems Task
can O
reduce O
operating Task
costs Task
by O
automating Task
processes Task
such O
as O
call Task
center Task
dispatch Task
and O
online Task
customer Task
support Task
. O
Moreover O
, O
when O
combined O
with O
automatic Method
speech Method
recognition Method
systems Method
, O
task O
- O
oriented O
dialogue Task
systems Task
provide O
the O
foundation O
of O
intelligent Task
assistants Task
such O
as O
Amazon Task
Alexa Task
, O
Apple Task
Siri Task
, O
and O
Google Task
Assistant Task
. O
In O
turn O
, O
these O
assistants O
allow O
for O
natural Task
, Task
personalized Task
interactions Task
with O
users O
by O
tailoring O
natural O
language O
system O
responses O
to O
the O
dialogue O
context O
. O
Dialogue Task
state Task
tracking Task
( O
DST Task
) Task
is O
a O
crucial O
part O
of O
dialogue Task
systems Task
young2013POMDPDialogueReview O
. O
In O
DST Task
, O
a O
dialogue Task
state Task
tracker Task
estimates O
the O
state O
of O
the O
conversation O
using O
the O
current O
user O
utterance O
and O
the O
conversation O
history O
. O
The O
dialogue Method
system Method
then O
uses O
this O
estimated O
state O
to O
plan O
the O
next O
action O
and O
respond O
to O
the O
user O
. O
A O
state O
in O
DST Task
typically O
consists O
of O
a O
set O
of O
requests O
and O
joint O
goals O
. O
Consider O
the O
task O
of O
restaurant Task
reservation Task
as O
an O
example O
. O
During O
each O
turn O
, O
the O
user O
informs O
the O
system O
of O
particular O
goals O
the O
they O
would O
like O
to O
achieve O
( O
e.g. O
inform O
( O
food O
= O
french O
) O
) O
, O
or O
request O
for O
more O
information O
from O
the O
system O
( O
e.g. O
request O
( O
address O
) O
) O
. O
The O
set O
of O
goal O
and O
request O
slot O
- O
value O
pairs O
( O
e.g. O
( O
food O
, O
french O
) O
, O
( O
request O
, O
address O
) O
) O
given O
during O
a O
turn O
are O
referred O
to O
as O
the O
turn O
goal O
and O
turn O
request O
. O
The O
joint O
goal O
is O
the O
set O
of O
accumulated O
turn O
goals O
up O
to O
the O
current O
turn O
. O
Figure O
[ O
reference O
] O
shows O
an O
example O
dialogue O
with O
annotated O
turn O
states O
, O
in O
which O
the O
user O
reserves O
a O
restaurant O
. O
Traditional O
dialogue Method
state Method
trackers Method
rely O
on O
Spoken Method
Language Method
Understanding Method
( O
SLU Method
) O
systems O
in O
order O
to O
understand O
user O
utterances O
. O
These O
trackers O
accumulate O
errors O
from O
the O
SLU Method
, O
which O
sometimes O
do O
not O
have O
the O
necessary O
dialogue O
context O
to O
interpret O
the O
user O
utterances O
. O
Subsequent O
DST Task
research O
forgo O
the O
SLU Method
and O
directly O
infer O
the O
state O
using O
the O
conversation O
history O
and O
the O
user O
utterance O
. O
These O
trackers O
rely O
on O
hand O
- O
crafted O
semantic Method
dictionaries Method
and O
delexicalization Method
— O
the O
anonymization O
of O
slots O
and O
values O
using O
generic O
tags O
— O
to O
achieve O
generalization Task
. O
Recent O
work O
by O
mrkvsic2016neural O
apply Method
representation Method
learning Method
using O
convolutional Method
neural Method
networks Method
to O
learn O
features O
relevant O
for O
each O
state O
as O
opposed O
to O
hand O
- O
crafting O
features O
. O
A O
key O
problem O
in O
DST Task
that O
is O
not O
addressed O
by O
existing O
methods O
is O
the O
extraction O
of O
rare O
slot O
- O
value O
pairs O
that O
compose O
the O
state O
during O
each O
turn O
. O
Because O
task O
oriented O
dialogues O
cover O
large O
state O
spaces O
, O
many O
slot O
- O
value O
pairs O
that O
compose O
the O
state O
rarely O
occur O
in O
the O
training O
data O
. O
Although O
the O
chance O
that O
the O
user O
specifies O
a O
particular O
rare O
slot O
- O
value O
in O
a O
turn O
is O
small O
, O
the O
chance O
that O
they O
specify O
at O
least O
one O
rare O
slot O
- O
value O
pair O
is O
large O
. O
Failure O
to O
predict O
these O
rare O
slot O
- O
value O
pairs O
results O
in O
incorrect O
turn Task
- Task
level Task
goal Task
and Task
request Task
tracking Task
. O
Accumulated O
errors O
in O
turn Task
- Task
level Task
goal Task
tracking Task
significantly O
degrade O
joint Task
goal Task
- Task
tracking Task
. O
For O
example O
, O
in O
the O
WoZ Material
state O
tracking O
dataset O
, O
slot O
- O
value O
pairs O
have O
214.9 O
training O
examples O
on O
average O
, O
while O
38.6 O
% O
of O
turns O
have O
a O
joint O
goal O
that O
contains O
a O
rare O
slot O
- O
value O
pair O
with O
less O
than O
20 O
training O
examples O
. O
In O
this O
work O
, O
we O
propose O
the O
G Task
lobal Task
- Task
L Task
ocally Task
Self Task
- Task
A O
ttentive Method
D Method
ialogue Method
State Method
Tracker Method
( O
GLAD Method
) O
, O
a O
new O
state O
- O
of O
- O
the O
- O
art O
model O
for O
dialogue Task
state Task
tracking Task
. O
In O
contrast O
to O
previous O
work O
that O
estimate O
each O
slot O
- O
value O
pair O
independently O
, O
GLAD Method
uses O
global Method
modules Method
to O
share O
parameters O
between O
estimators O
for O
each O
slot O
and O
local Method
modules Method
to O
learn O
slot Method
- Method
specific Method
feature Method
representations Method
. O
We O
show O
that O
by O
doing O
so O
, O
GLAD Method
generalizes O
on O
rare O
slot O
- O
value O
pairs O
with O
few O
training O
examples O
. O
GLAD Method
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
of O
88.1 O
% O
goal Metric
accuracy Metric
and O
97.1 O
% O
request Metric
accuracy Metric
on O
the O
WoZ Task
dialogue Task
state Task
tracking Task
task Task
wen2017NetworkBasedEndToEndDialogueSystem O
, O
outperforming O
prior O
best O
by O
3.7 O
% O
and O
5.5 O
% O
. O
On O
DSTC2 Material
dstc2 O
, O
we O
achieve O
74.5 O
% O
goal Metric
accuracy Metric
and O
97.5 O
% O
request Metric
accuracy Metric
, O
outperforming O
prior O
best O
by O
1.1 O
% O
and O
1.0 O
% O
. O
We O
release O
an O
implementation O
of O
our O
model O
along O
with O
a O
Docker O
image O
of O
our O
experiment O
setup O
for O
reproducibility O
. O
section O
: O
Global Method
- Method
Locally Method
Self Method
- Method
Attentive Method
Dialogue Method
State Method
Tracker Method
One O
formulation O
of O
state Task
tracking Task
is O
to O
predict O
the O
turn O
state O
given O
an O
user O
utterance O
and O
previous O
system O
actions O
williams2007partially O
. O
Like O
previous O
methods O
henderson2014word O
, O
wen2017NetworkBasedEndToEndDialogueSystem O
, O
mrkvsic2016neural O
, O
GLAD Method
decomposes O
the O
multi Task
- Task
label Task
state Task
prediction Task
problem Task
into O
a O
collection O
of O
binary Task
prediction Task
problems Task
by O
using O
a O
distinct O
estimator O
for O
each O
slot O
- O
value O
pair O
that O
make O
up O
the O
state O
. O
Hence O
, O
we O
describe O
GLAD Method
with O
respect O
to O
a O
slot O
- O
value O
pair O
that O
is O
being O
predicted O
by O
the O
model O
. O
Shown O
in O
Figure O
[ O
reference O
] O
, O
GLAD Method
is O
comprised O
of O
an O
encoder Method
module Method
and O
a O
scoring Method
module Method
. O
The O
encoder Method
module Method
consists O
of O
separate O
global Method
- Method
locally Method
self Method
- Method
attentive Method
encoders Method
for O
the O
user O
utterance O
, O
the O
previous O
system O
actions O
, O
and O
the O
slot O
- O
value O
pair O
under O
consideration O
. O
The O
scoring Method
module Method
consists O
of O
two O
scorers O
. O
One O
scorer O
considers O
the O
contribution O
from O
the O
utterance O
while O
the O
other O
considers O
the O
contribution O
from O
previous O
system O
actions O
. O
subsection O
: O
Global Method
- Method
Locally Method
Self Method
- Method
Attentive Method
Encoder Method
We O
begin O
by O
describing O
the O
global Method
- Method
locally Method
self Method
- Method
attentive Method
encoder Method
, O
which O
makes O
up O
the O
encoder Method
module Method
. O
DST Task
datasets O
tend O
to O
be O
small O
relative O
to O
their O
state O
space O
in O
that O
many O
slot O
- O
value O
pairs O
rarely O
occur O
in O
the O
dataset O
. O
Because O
each O
state O
is O
comprised O
of O
a O
set O
of O
slot O
- O
value O
pairs O
, O
many O
of O
them O
rare O
, O
poor O
inference O
of O
rare O
slot O
- O
value O
pairs O
subsequently O
results O
in O
poor O
turn Task
- Task
level Task
tracking Task
. O
This O
problem O
is O
amplified O
in O
joint Task
tracking Task
, O
due O
to O
the O
accumulation O
of O
turn O
- O
level O
errors O
. O
In O
developing O
this O
encoder O
, O
we O
seek O
to O
better O
model O
rare O
slot O
- O
value O
pairs O
by O
sharing O
parameters O
between O
each O
slot O
through O
global Method
modules Method
and O
learning O
slot O
- O
specific O
features O
through O
local Method
modules Method
. O
The O
global Method
- Method
locally Method
self Method
- Method
attentive Method
encoder Method
consists O
of O
a O
bidirectional Method
LSTM Method
Hochreiter1997Long O
, O
which O
captures O
temporal O
relationships O
within O
the O
sequence O
, O
followed O
by O
a O
self Method
- Method
attention Method
layer Method
to O
compute O
the O
summary O
of O
the O
sequence O
. O
Figure O
[ O
reference O
] O
illustrates O
the O
global Method
- Method
locally Method
self Method
- Method
attentive Method
encoder Method
. O
Consider O
the O
process O
of O
encoding O
a O
sequence O
with O
respect O
to O
a O
particular O
slot O
. O
Let O
denote O
the O
number O
of O
words O
in O
the O
sequence O
, O
the O
dimension O
of O
the O
embeddings O
, O
and O
the O
word O
embeddings O
corresponding O
to O
words O
in O
the O
sequence O
. O
We O
produce O
a O
global Method
encoding Method
of O
using O
a O
global O
bidirectional Method
LSTM Method
. O
where O
is O
the O
dimension O
of O
the O
LSTM O
state O
. O
We O
similarly O
produce O
a O
local Method
encoding Method
of O
, O
taking O
into O
account O
the O
slot O
, O
using O
a O
local O
bidirectional Method
LSTM Method
. O
The O
outputs O
of O
the O
two O
LSTMs Method
are O
combined O
through O
a O
mixture Method
function Method
to O
yield O
a O
global O
- O
local O
encoding O
of O
. O
Here O
, O
the O
scalar O
is O
a O
learned O
parameter O
between O
0 O
and O
1 O
that O
is O
specific O
to O
the O
slot O
. O
Next O
, O
we O
compute O
a O
global O
- O
local O
self O
- O
attention O
context O
over O
. O
Self Method
- Method
attention Method
, O
or O
intra Method
- Method
attention Method
, O
is O
an O
effective O
method O
to O
compute O
summary O
context O
over O
variable O
- O
length O
sequences O
for O
natural Task
language Task
processing Task
tasks Task
cheng2016long O
, O
Vaswani2017attention O
, O
he2017deep O
, O
lee2017end O
. O
In O
our O
case O
, O
we O
use O
a O
global Method
self Method
- Method
attention Method
module Method
to O
compute O
an O
attention O
context O
useful O
for O
general Task
- Task
purpose Task
state Task
tracking Task
, O
as O
well O
as O
a O
local Method
self Method
- Method
attention Method
module Method
to O
compute O
a O
slot O
- O
specific O
attention O
context O
. O
For O
each O
th O
element O
, O
we O
compute O
a O
scalar Metric
global Metric
self Metric
- Metric
attention Metric
score Metric
which O
is O
subsequently O
normalized O
across O
all O
elements O
using O
a O
softmax Method
function Method
. O
The O
global O
self O
- O
attention O
context O
is O
then O
the O
sum O
of O
each O
element O
, O
weighted O
by O
the O
corresponding O
normalized Metric
global Metric
self Metric
- Metric
attention Metric
score Metric
. O
We O
similarly O
compute O
the O
local O
self O
- O
attention O
context O
. O
The O
global O
- O
local O
self O
- O
attention O
context O
is O
the O
mixture O
For O
ease O
of O
exposition O
, O
we O
define O
the O
multi O
- O
value O
encode O
function O
. O
This O
function O
maps O
the O
sequence O
to O
the O
encoding O
and O
the O
self O
- O
attention O
context O
. O
subsection O
: O
Encoding Method
module Method
Having O
defined O
the O
global Method
- Method
locally Method
self Method
- Method
attentive Method
encoder Method
, O
we O
now O
build O
representations O
for O
the O
user O
utterance O
, O
the O
previous O
system O
actions O
, O
and O
the O
slot O
- O
value O
pair O
under O
consideration O
. O
Let O
denote O
word O
embeddings O
of O
the O
user O
utterance O
, O
denote O
those O
of O
the O
th O
previous O
system O
action O
( O
e.g. O
request O
( O
price O
range O
) O
, O
and O
denote O
those O
of O
the O
slot O
- O
value O
pair O
under O
consideration O
( O
e.g. O
food O
= O
french O
) O
. O
We O
have O
subsection O
: O
Scoring Method
module Method
Intuitively O
, O
we O
can O
determine O
whether O
the O
user O
has O
expressed O
the O
slot O
- O
value O
pair O
under O
consideration O
by O
examining O
two O
input O
sources O
. O
The O
first O
source O
is O
the O
user O
utterance O
, O
in O
which O
the O
user O
directly O
states O
the O
goals O
and O
requests O
. O
An O
example O
of O
this O
is O
the O
user O
saying O
“ O
how O
about O
a O
French O
restaurant O
in O
the O
centre O
of O
town O
? O
” O
, O
after O
the O
system O
asked O
“ O
how O
may O
I O
help O
you O
? O
” O
To O
handle O
these O
cases O
, O
we O
determine O
whether O
the O
utterance O
specifies O
the O
slot O
- O
value O
pair O
. O
Namely O
, O
we O
attend O
over O
the O
user O
utterance O
, O
taking O
into O
account O
the O
slot O
- O
value O
pair O
being O
considered O
, O
and O
use O
the O
resulting O
attention O
context O
to O
score O
the O
slot O
- O
value O
pair O
. O
where O
is O
the O
number O
of O
words O
in O
the O
user O
utterance O
. O
The O
score O
indicates O
the O
degree O
to O
which O
the O
value O
was O
expressed O
by O
the O
user O
utterance O
. O
The O
second O
source O
is O
the O
previous O
system O
actions O
. O
This O
source O
is O
informative O
when O
the O
user O
utterance O
does O
not O
present O
enough O
information O
and O
instead O
refers O
to O
previous O
system O
actions O
. O
An O
example O
of O
this O
is O
the O
user O
saying O
“ O
yes O
” O
, O
after O
the O
system O
asked O
“ O
would O
you O
like O
a O
restaurant O
in O
the O
centre O
of O
town O
? O
” O
To O
handle O
these O
cases O
, O
we O
examine O
previous O
actions O
after O
considering O
the O
user O
utterance O
. O
First O
, O
we O
attend O
over O
the O
previous O
action Method
representations Method
, O
taking O
into O
account O
the O
current O
user O
utterance O
. O
Here O
, O
is O
the O
number O
of O
previous O
system O
actions O
. O
Then O
, O
we O
use O
the O
similarity O
between O
the O
attention O
context O
and O
the O
slot O
- O
value O
pair O
to O
score O
the O
slot O
- O
value O
pair O
. O
In O
addition O
to O
real O
actions O
, O
we O
introduce O
a O
sentinel O
action O
to O
each O
turn O
which O
allows O
the O
attention Method
mechanism Method
to O
ignore O
previous O
system O
actions O
. O
The O
score O
indicates O
the O
degree O
to O
which O
the O
value O
was O
expressed O
by O
the O
previous O
actions O
. O
The O
final O
score O
is O
then O
a O
weighted O
sum O
between O
the O
two O
scores O
and O
, O
normalized O
by O
the O
sigmoid Method
function Method
. O
Here O
, O
the O
weight O
is O
a O
learned O
parameter O
. O
section O
: O
Experiments O
subsection O
: O
Dataset O
The O
Dialogue Task
Systems Task
Technology Task
Challenges Task
( O
DSTC Task
) O
provides O
a O
common O
framework O
for O
developing O
and O
evaluating O
dialogue Task
systems Task
and O
dialogue Task
state Task
trackers Task
. O
Under O
this O
framework O
, O
dialogue O
semantics O
such O
as O
states O
and O
actions O
are O
based O
on O
a O
task O
ontology O
such O
as O
restaurant Task
reservation Task
. O
During O
each O
turn O
, O
the O
user O
informs O
the O
system O
of O
particular O
goals O
( O
e.g. O
inform O
( O
food O
= O
french O
) O
) O
, O
or O
requests O
for O
more O
information O
from O
the O
system O
( O
e.g. O
request O
( O
address O
) O
) O
. O
For O
instance O
, O
food O
and O
area O
are O
examples O
of O
slots O
in O
the O
DSTC2 Material
task O
, O
and O
french O
and O
chinese O
are O
example O
values O
within O
the O
food O
slot O
. O
We O
train O
and O
evaluate O
our O
model O
using O
DSTC2 Material
as O
well O
as O
the O
Wizard Task
of Task
Oz Task
( O
WoZ Material
) O
restaurant Task
reservation Task
task Task
, O
which O
also O
adheres O
to O
the O
DSTC Task
framework O
and O
has O
the O
same O
ontology O
as O
DSTC2 Material
. O
For O
DSTC2 Material
, O
it O
is O
standard O
to O
evaluate O
using O
the O
N O
- O
best O
list O
of O
the O
automatic Method
speech Method
recognition Method
system Method
( O
ASR Method
) O
that O
is O
included O
with O
the O
dataset O
. O
Because O
of O
this O
, O
each O
turn O
in O
the O
DSTC2 Material
dataset O
contains O
several O
noisy O
ASR O
outputs O
instead O
of O
a O
noise O
- O
free O
user O
utterance O
. O
The O
WoZ Material
task O
does O
not O
provide O
ASR O
outputs O
, O
and O
we O
instead O
train O
and O
evaluate O
using O
the O
user O
utterance O
. O
subsection O
: O
Metrics Metric
We O
evaluate O
our O
model O
using O
turn Metric
- Metric
level Metric
request Metric
tracking Metric
accuracy Metric
as O
well O
as O
joint Metric
goal Metric
tracking Metric
accuracy Metric
. O
Our O
definition O
of O
GLAD Method
in O
the O
previous O
sections O
describes O
how O
to O
obtain O
turn O
goals O
and O
requests O
. O
To O
compute O
the O
joint O
goal O
, O
we O
simply O
accumulate O
turn O
goals O
. O
In O
the O
event O
that O
the O
current O
turn O
goal O
specifies O
a O
slot O
that O
has O
been O
specified O
before O
, O
the O
new O
specification O
takes O
precedence O
. O
For O
example O
, O
suppose O
the O
user O
specifies O
a O
food O
= O
french O
restaurant O
during O
the O
current O
turn O
. O
If O
the O
joint O
goal O
has O
no O
existing O
food O
specifications O
, O
then O
we O
simply O
add O
food O
= O
french O
to O
the O
joint O
goal O
. O
Alternatively O
, O
if O
food O
= O
thai O
had O
been O
specified O
in O
a O
previous O
turn O
, O
we O
simply O
replace O
it O
with O
food O
= O
french O
. O
subsection O
: O
Implementation O
Details O
We O
use O
fixed O
, O
pretrained O
GloVe O
embeddings O
as O
well O
as O
character Method
n Method
- Method
gram Method
embeddings Method
. O
Each O
model O
is O
trained O
using O
ADAM Method
. O
For O
regularization Task
, O
we O
apply O
dropout Method
with O
0.2 O
drop O
rate O
to O
embeddings O
and O
the O
output O
of O
each O
local Method
and Method
global Method
module Method
. O
We O
use O
the O
development O
split O
for O
hyperparameter Method
tuning Method
and O
apply O
early Method
stopping Method
using O
the O
joint Metric
goal Metric
accuracy Metric
. O
For O
the O
DSTC2 Material
task O
, O
we O
train O
using O
transcripts O
of O
user O
utterances O
and O
evaluate O
using O
the O
noisy O
ASR O
transcriptions O
. O
During O
evaluation O
, O
we O
take O
the O
sum O
of O
the O
scores O
resulting O
from O
each O
ASR O
output O
as O
the O
output O
score O
of O
a O
particular O
slot O
- O
value O
. O
We O
then O
normalize O
this O
sum O
using O
a O
sigmoid Method
function Method
as O
shown O
in O
Equation O
( O
[ O
reference O
] O
) O
. O
We O
also O
apply O
word Method
dropout Method
, O
in O
which O
the O
embeddings O
of O
a O
word O
is O
randomly O
set O
to O
zero O
with O
a O
probability O
of O
0.3 O
. O
This O
accounts O
for O
the O
poor O
quality O
of O
ASR O
outputs O
in O
DSTC2 Material
, O
which O
frequently O
miss O
words O
in O
the O
user O
utterance O
. O
We O
did O
not O
find O
word O
dropout O
to O
be O
helpful O
for O
the O
WoZ Material
task O
, O
which O
does O
not O
contain O
noisy O
ASR O
outputs O
. O
subsection O
: O
Comparison O
to O
Existing O
Methods O
Table O
[ O
reference O
] O
shows O
the O
performance O
of O
GLAD Method
compared O
to O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
. O
The O
delexicalisation Method
models Method
, O
which O
replace O
slots O
and O
values O
in O
the O
utterance O
with O
generic O
tags O
, O
are O
from O
henderson2014word O
for O
DSTC2 Material
and O
wen2017NetworkBasedEndToEndDialogueSystem O
for O
WoZ. Material
Semantic O
dictionaries O
map O
slot O
- O
value O
pairs O
to O
hand O
- O
engineered O
synonyms O
and O
phrases O
. O
The O
NBT Method
mrkvsic2016neural O
applies O
CNN Method
over O
word Method
embeddings Method
learned O
over O
a O
paraphrase O
database O
wieting2015paraphrase O
instead O
of O
delexicalised Method
n Method
- Method
gram Method
features Method
. O
On O
the O
WoZ Material
dataset Material
, O
we O
find O
that O
GLAD Method
significantly O
improves O
upon O
previous O
state O
- O
of O
- O
the O
- O
art O
performance O
by O
3.7 O
% O
on O
joint Metric
goal Metric
tracking Metric
accuracy Metric
and O
5.5 O
% O
on O
turn Metric
- Metric
level Metric
request Metric
tracking Metric
accuracy Metric
. O
On O
the O
DSTC Material
dataset Material
, O
which O
evaluates O
using O
noisy O
ASR O
outputs O
instead O
of O
user O
utterances O
, O
GLAD Method
improves O
upon O
previous O
state O
of O
the O
art O
performance O
by O
1.1 O
% O
on O
joint Metric
goal Metric
tracking Metric
accuracy Metric
and O
1.0 O
% O
on O
turn Metric
- Metric
level Metric
request Metric
tracking Metric
accuracy Metric
. O
subsection O
: O
Ablation Task
study Task
We O
perform O
ablation O
experiments O
on O
the O
development O
set O
to O
analyze O
the O
effectiveness O
of O
different O
components O
of O
GLAD Method
. O
The O
results O
of O
these O
experiments O
are O
shown O
in O
Table O
[ O
reference O
] O
. O
We O
also O
show O
turn Metric
goal Metric
accuracy Metric
in O
addition O
to O
joint Metric
goal Metric
accuracy Metric
and O
turn Metric
request Metric
accuracy Metric
for O
reference O
. O
Temporal O
order O
is O
important O
for O
state Task
tracking Task
. O
We O
experiment O
with O
an O
embedding Method
- Method
matching Method
variant Method
of O
GLAD Method
with O
self Method
- Method
attention Method
but O
without O
LSTMs Method
. O
The O
weaker O
performance O
by O
this O
model O
suggests O
that O
representations O
that O
capture O
temporal O
dependencies O
is O
helpful O
for O
understanding Task
phrases Task
for O
state Task
tracking Task
. O
Self Method
- Method
attention Method
allows O
slot O
- O
specific O
, O
robust O
feature Method
learning Method
. O
We O
observe O
a O
consistent O
drop O
in O
performance O
for O
models O
that O
use O
mean Method
- Method
pooling Method
over O
the O
temporal O
dimension O
as O
opposed O
to O
self O
- O
attention O
( O
Equations O
( O
[ O
reference O
] O
) O
to O
( O
[ O
reference O
] O
) O
) O
. O
This O
stems O
from O
the O
flexibility O
in O
the O
attention Task
context Task
computation Task
afforded O
by O
the O
self Method
- Method
attention Method
mechanism Method
, O
which O
allows O
the O
model O
to O
focus O
on O
select O
words O
relevant O
to O
the O
slot O
- O
value O
pair O
under O
consideration O
. O
Figure O
[ O
reference O
] O
illustrates O
an O
example O
in O
which O
local Method
self Method
- Method
attention Method
modules Method
focus O
on O
relevant O
parts O
of O
the O
utterance O
. O
The O
model O
attends O
to O
relevant O
phrases O
that O
n Method
- Method
gram Method
and Method
embedding Method
matching Method
techniques Method
do O
not O
capture O
( O
e.g. O
“ O
within O
5 O
miles O
” O
for O
the O
“ O
area O
” O
slot O
) O
. O
Global Task
- Task
local Task
sharing Task
improves O
goal Task
tracking Task
. O
We O
study O
the O
two O
extremes O
of O
sharing O
between O
the O
global Method
module Method
and O
the O
local Method
module Method
. O
The O
first O
uses O
only O
the O
local Method
module Method
and O
results O
in O
degradation Task
in Task
goal Task
tracking Task
but O
does O
not O
affect O
request Task
tracking Task
( O
e.g. O
) O
. O
This O
is O
because O
the O
former O
is O
a O
joint Method
prediction Method
over O
several O
slot O
- O
values O
with O
few O
training O
examples O
, O
whereas O
the O
latter O
predicts O
a O
single O
slot O
that O
has O
the O
most O
training O
examples O
. O
The O
second O
uses O
only O
the O
global Method
module Method
and O
underperforms O
in O
both O
goal Task
tracking Task
and O
request Task
tracking Task
( O
e.g. O
) O
. O
This O
model O
is O
less O
expressive O
, O
as O
it O
lacks O
slot O
- O
specific O
specializations O
except O
for O
the O
final O
scoring Method
modules Method
. O
Figure O
[ O
reference O
] O
shows O
the O
performance O
of O
GLAD Method
and O
the O
two O
sharing Method
variants Method
across O
different O
numbers O
of O
occurrences O
in O
the O
training O
data O
. O
GLAD Method
consistently O
outperforms O
both O
variants O
for O
rare O
slot O
- O
value O
pairs O
. O
For O
slot O
- O
value O
pairs O
with O
an O
abundance O
of O
training O
data O
, O
there O
is O
no O
significant O
performance O
difference O
between O
models O
as O
there O
is O
sufficient O
data O
to O
generalize O
. O
subsection O
: O
Qualitative Method
analysis Method
Table O
[ O
reference O
] O
shows O
example O
predictions O
by O
GLAD Method
. O
In O
the O
first O
example O
, O
the O
user O
explicitly O
outlines O
requests O
and O
goals O
in O
a O
single O
utterance O
. O
In O
the O
second O
example O
, O
the O
model O
previously O
prompted O
the O
user O
for O
confirmation O
of O
two O
requests O
( O
e.g. O
for O
the O
restaurant O
’s O
address O
and O
phone O
number O
) O
, O
and O
the O
user O
simply O
answers O
in O
the O
affirmative O
. O
In O
this O
case O
, O
the O
model O
obtains O
the O
correct O
result O
by O
leveraging O
the O
system O
actions O
in O
the O
previous O
turn O
. O
The O
last O
example O
demonstrates O
an O
error O
made O
by O
the O
model O
. O
Here O
, O
the O
user O
does O
not O
answer O
the O
system O
’s O
previous O
request O
for O
the O
choice O
of O
food O
and O
instead O
asks O
for O
what O
food O
is O
available O
. O
The O
model O
misinterprets O
the O
lack O
of O
response O
as O
the O
user O
not O
caring O
about O
the O
choice O
of O
food O
. O
section O
: O
Related O
Work O
Dialogue Task
State Task
Tracking Task
. O
Traditional O
dialogue Method
state Method
trackers Method
rely O
on O
a O
separate O
SLU Method
component O
that O
serves O
as O
the O
initial O
stage O
in O
the O
dialogue Task
agent Task
pipeline Task
. O
The O
downstream Method
tracker Method
then O
combines O
the O
semantics O
extracted O
by O
the O
SLU Method
with O
previous O
dialogue O
context O
in O
order O
to O
estimate O
the O
current O
dialogue O
state O
. O
Recent O
results O
in O
dialogue Task
state Task
tracking Task
show O
that O
it O
is O
beneficial O
to O
jointly O
learn O
speech Task
understanding Task
and O
dialogue Task
tracking Task
. O
These O
approaches O
directly O
take O
as O
input O
the O
N O
- O
best O
list O
produced O
by O
the O
ASR Method
system Method
. O
By O
avoiding O
the O
accumulation O
of O
errors O
from O
the O
initial O
SLU Method
component O
, O
these O
joint Method
approaches Method
typically O
achieved O
stronger O
performance O
on O
tasks O
such O
as O
DSTC2 Material
. O
One O
drawback O
to O
these O
approaches O
is O
that O
they O
rely O
on O
hand O
- O
crafted O
features O
and O
complex O
domain O
- O
specific O
lexicon O
( O
in O
addition O
to O
the O
ontology O
) O
, O
and O
consequently O
are O
difficult O
to O
extend O
and O
scale O
to O
new O
domains O
. O
The O
recent O
Neural Method
Belief Method
Tracker Method
( O
NBT Method
) O
by O
mrkvsic2016neural Method
avoids O
reliance O
on O
hand O
- O
crafted O
features O
and O
lexicon O
by O
using O
representation Method
learning Method
. O
The O
NBT Method
employs O
convolutional Method
filters Method
over O
word Method
embeddings Method
in O
lieu O
of O
previously O
- O
used O
hand O
- O
engineered O
features O
. O
Moreover O
, O
to O
outperform O
previous O
methods O
, O
the O
NBT Method
uses O
pretrained Method
embeddings Method
tailored O
to O
retain O
semantic O
relationships O
by O
injecting O
semantic O
similarity O
constraints O
from O
the O
Paraphrase Material
Database Material
. O
On O
the O
one O
hand O
, O
these O
specialized Method
embeddings Method
are O
more O
difficult O
to O
obtain O
than O
word Method
embeddings Method
from O
language Method
modeling Method
. O
On O
the O
other O
hand O
, O
these O
embeddings O
are O
not O
specific O
to O
any O
dialogue O
domain O
and O
generalize O
to O
new O
domains O
. O
Neural Method
attention Method
models Method
in O
NLP Task
. O
Attention Method
mechanisms Method
have O
led O
to O
improvements O
on O
a O
variety O
of O
natural Task
language Task
processing Task
tasks Task
. O
Bahdanau2014NeuralMT Method
propose O
attentional Method
sequence Method
to Method
sequence Method
models Method
for O
neural Task
machine Task
translation Task
. O
luong2015effective O
analyze O
various O
attention Method
techniques Method
and O
highlight O
the O
effectiveness O
of O
the O
simple O
, O
parameterless Method
dot Method
product Method
attention Method
. O
Similar O
models O
have O
also O
proven O
successful O
in O
tasks O
such O
as O
summarization Task
. O
Self Method
- Method
attention Method
, O
or O
intra Method
- Method
attention Method
, O
has O
led O
improvements O
in O
language Task
modeling Task
, O
sentiment Task
analysis Task
, O
natural Task
language Task
inference Task
, O
semantic Task
role Task
labeling Task
, O
and O
coreference Task
resolution Task
lee2017end O
. O
Deep Method
self Method
- Method
attention Method
has O
also O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
in O
machine Task
translation Task
. O
Coattention Method
, O
or O
bidirectional O
attention O
that O
codependently O
encode O
two O
sequences O
, O
have O
led O
to O
significant O
gains O
in O
question Task
answering Task
as O
well O
as O
visual Task
question Task
answering Task
. O
Parameter Task
sharing Task
between O
related O
tasks O
. O
Sharing O
parameters O
between O
related O
tasks O
to O
improve O
joint O
performance O
is O
prominent O
in O
multi Task
- Task
task Task
learning Task
. O
Early O
works O
in O
multi Task
- Task
tasking Task
use O
Gaussian Method
processes Method
whose O
covariance O
matrix O
is O
induced O
from O
shared O
kernels O
. O
Hashimoto2017joint Method
propose O
a O
progressively Method
trained Method
joint Method
model Method
for O
NLP Task
tasks Task
. O
When O
a O
new O
task O
is O
introduced O
, O
a O
new O
section O
is O
added O
to O
the O
network O
whose O
inputs O
are O
intermediate O
representations O
from O
sections O
for O
previous O
tasks O
. O
In O
this O
sense O
, O
tasks O
share O
parameters O
in O
a O
hierarchical O
manner O
. O
johnson2016google O
propose O
a O
single O
model O
that O
jointly O
learns O
to O
translate O
between O
multiple O
language O
pairs O
, O
including O
one O
- O
to O
- O
many O
, O
many Task
- Task
to Task
- Task
one Task
, O
and O
many Task
- Task
to Task
- Task
many Task
translation Task
. O
kaiser2017one O
propose O
a O
model O
that O
jointly O
learns O
multiple O
tasks O
across O
modalities O
. O
Each O
modality Method
- Method
specific Method
feature Method
extractor Method
extracts O
a O
representation O
that O
is O
fed O
into O
a O
shared Method
encoder Method
. O
section O
: O
Conclusions O
We O
introduced O
the O
Global Method
- Method
Locally Method
Self Method
- Method
Attentive Method
Dialogue Method
State Method
Tracker Method
( O
GLAD Method
) O
, O
a O
new O
state O
- O
of O
- O
the O
- O
art O
model O
for O
dialogue Task
state Task
tracking Task
. O
At O
the O
core O
of O
GLAD Method
is O
the O
global Method
- Method
locally Method
self Method
- Method
attention Method
encoder Method
, O
whose O
global Method
modules Method
allow O
parameter O
sharing O
between O
slots O
and O
local Method
modules Method
allow O
slot Method
- Method
specific Method
feature Method
learning Method
. O
This O
allows O
GLAD Method
to O
generalize O
on O
rare O
slot O
- O
value O
pairs O
with O
few O
training O
data O
. O
GLAD Method
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
of O
88.1 O
% O
goal Metric
accuracy Metric
and O
97.1 O
% O
request Metric
accuracy Metric
on O
the O
WoZ Task
dialogue Task
state Task
tracking Task
task Task
, O
as O
well O
as O
74.5 O
% O
goal Metric
accuracy Metric
and O
97.5 O
% O
request Metric
accuracy Metric
on O
DSTC2 Material
. O
section O
: O
Acknowledgement O
We O
thank O
Nikola O
Mrkšić O
for O
helpful O
discussion O
and O
for O
providing O
a O
preprocessed O
version O
of O
the O
DSTC2 Material
dataset O
. O
bibliography O
: O
References O
