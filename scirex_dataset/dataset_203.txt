document O
: O
Dilated Method
Recurrent Method
Neural Method
Networks Method
Learning Method
with O
recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
on O
long O
sequences O
is O
a O
notoriously O
difficult O
task O
. O
There O
are O
three O
major O
challenges O
: O
1 O
) O
complex O
dependencies O
, O
2 O
) O
vanishing O
and O
exploding O
gradients O
, O
and O
3 O
) O
efficient O
parallelization Task
. O
In O
this O
paper O
, O
we O
introduce O
a O
simple O
yet O
effective O
RNN Method
connection Method
structure Method
, O
the O
DilatedRNN Method
, O
which O
simultaneously O
tackles O
all O
of O
these O
challenges O
. O
The O
proposed O
architecture O
is O
characterized O
by O
multi O
- O
resolution O
dilated O
recurrent O
skip O
connections O
, O
and O
can O
be O
combined O
flexibly O
with O
diverse O
RNN O
cells O
. O
Moreover O
, O
the O
DilatedRNN Method
reduces O
the O
number O
of O
parameters O
needed O
and O
enhances O
training Metric
efficiency Metric
significantly O
, O
while O
matching O
state O
- O
of O
- O
the O
- O
art O
performance O
( O
even O
with O
standard O
RNN Method
cells Method
) O
in O
tasks O
involving O
very O
long O
- O
term O
dependencies O
. O
To O
provide O
a O
theory O
- O
based O
quantification O
of O
the O
architecture O
’s O
advantages O
, O
we O
introduce O
a O
memory Metric
capacity Metric
measure Metric
, O
the O
mean O
recurrent O
length O
, O
which O
is O
more O
suitable O
for O
RNNs Method
with O
long O
skip O
connections O
than O
existing O
measures O
. O
We O
rigorously O
prove O
the O
advantages O
of O
the O
DilatedRNN Method
over O
other O
recurrent Method
neural Method
architectures Method
. O
The O
code O
for O
our O
method O
is O
publicly O
available O
. O
section O
: O
Introduction O
Recurrent Method
neural Method
networks Method
( O
RNNs Method
) O
have O
been O
shown O
to O
have O
remarkable O
performance O
on O
many O
sequential Task
learning Task
problems Task
. O
However O
, O
long Task
sequence Task
learning Task
with O
RNNs Method
remains O
a O
challenging O
problem O
for O
the O
following O
reasons O
: O
first O
, O
memorizing O
extremely O
long O
- O
term O
dependencies O
while O
maintaining O
mid O
- O
and O
short Method
- Method
term Method
memory Method
is O
difficult O
; O
second O
, O
training O
RNNs Method
using O
back Method
- Method
propagation Method
- Method
through Method
- Method
time Method
is O
impeded O
by O
vanishing O
and O
exploding O
gradients O
; O
And O
lastly O
, O
both O
forward Method
- Method
and O
back Method
- Method
propagation Method
are O
performed O
in O
a O
sequential O
manner O
, O
which O
makes O
the O
training O
time O
- O
consuming O
. O
Many O
attempts O
have O
been O
made O
to O
overcome O
these O
difficulties O
using O
specialized O
neural O
structures O
, O
cells Method
, O
and O
optimization Method
techniques Method
. O
Long Method
short Method
- Method
term Method
memory Method
( O
LSTM Method
) O
and O
gated Method
recurrent Method
units Method
( O
GRU Method
) O
powerfully O
model O
complex O
data O
dependencies O
. O
Recent O
attempts O
have O
focused O
on O
multi Task
- Task
timescale Task
designs Task
, O
including O
clockwork Method
RNNs Method
, O
phased O
LSTM Method
, O
hierarchical Method
multi Method
- Method
scale Method
RNNs Method
, O
etc O
. O
The O
problem O
of O
vanishing O
and O
exploding O
gradients O
is O
mitigated O
by O
LSTM Method
and O
GRU Method
memory Method
gates Method
; O
other O
partial O
solutions O
include O
gradient Method
clipping Method
, O
orthogonal Method
and Method
unitary Method
weight Method
optimization Method
, O
and O
skip O
connections O
across O
multiple O
timestamps O
. O
For O
efficient O
sequential Task
training Task
, O
WaveNet Method
abandoned Method
RNN Method
structures Method
, O
proposing O
instead O
the O
dilated Method
causal Method
convolutional Method
neural Method
network Method
( O
CNN Method
) O
architecture O
, O
which O
provides O
significant O
advantages O
in O
working O
directly O
with O
raw O
audio O
waveforms O
. O
However O
, O
the O
length O
of O
dependencies O
captured O
by O
a O
dilated Method
CNN Method
is O
limited O
by O
its O
kernel Method
size Method
, O
whereas O
an O
RNN Method
’s Method
autoregressive Method
modeling Method
can O
, O
in O
theory O
, O
capture O
potentially O
infinitely O
long O
dependencies O
with O
a O
small O
number O
of O
parameters O
. O
Recently O
, O
Yu O
et O
al O
. O
proposed O
learning Method
- Method
based Method
RNNs Method
with O
the O
ability O
to O
jump O
( O
skim O
input O
text O
) O
after O
seeing O
a O
few O
timestamps O
worth O
of O
data O
; O
although O
the O
authors O
showed O
that O
the O
modified O
LSTM Method
with O
jumping Method
provides O
up O
to O
a O
six O
- O
fold O
speed Metric
increase Metric
, O
the O
efficiency O
gain O
is O
mainly O
in O
the O
testing O
phase O
. O
In O
this O
paper O
, O
we O
introduce O
the O
DilatedRNN Method
, O
a O
neural Method
connection Method
architecture Method
analogous O
to O
the O
dilated Method
CNN Method
, O
but O
under O
a O
recurrent Method
setting Method
. O
Our O
approach O
provides O
a O
simple O
yet O
useful O
solution O
that O
tries O
to O
alleviate O
all O
challenges O
simultaneously O
. O
The O
DilatedRNN Method
is O
a O
multi Method
- Method
layer Method
, Method
and Method
cell Method
- Method
independent Method
architecture Method
characterized O
by O
multi Method
- Method
resolution Method
dilated Method
recurrent Method
skip Method
connections Method
. O
The O
main O
contributions O
of O
this O
work O
are O
as O
follows O
. O
1 O
) O
We O
introduce O
a O
new O
dilated Method
recurrent Method
skip Method
connection Method
as O
the O
key O
building O
block O
of O
the O
proposed O
architecture O
. O
These O
alleviate O
gradient Task
problems Task
and O
extend O
the O
range O
of O
temporal O
dependencies O
like O
conventional O
recurrent Method
skip Method
connections Method
, O
but O
in O
the O
dilated Method
version Method
require O
fewer O
parameters O
and O
significantly O
enhance O
computational Metric
efficiency Metric
. O
2 O
) O
We O
stack O
multiple O
dilated Method
recurrent Method
layers Method
with O
hierarchical Method
dilations Method
to O
construct O
a O
DilatedRNN Method
, O
which O
learns O
temporal O
dependencies O
of O
different O
scales O
at O
different O
layers O
. O
3 O
) O
We O
present O
the O
mean O
recurrent O
length O
as O
a O
new O
neural Metric
memory Metric
capacity Metric
measure Metric
that O
reveals O
the O
performance O
difference O
between O
the O
previously O
developed O
recurrent Method
skip Method
- Method
connections Method
and O
the O
dilated Method
version Method
. O
We O
also O
verify O
the O
optimality O
of O
the O
exponentially O
increasing O
dilation Method
distribution O
used O
in O
the O
proposed O
architecture O
. O
It O
is O
worth O
mentioning O
that O
, O
the O
recent O
proposed O
Dilated Method
LSTM Method
can O
be O
viewed O
as O
a O
special O
case O
of O
our O
model O
, O
which O
contains O
only O
one O
dilated Method
recurrent Method
layer Method
with O
fixed O
dilation Method
. O
The O
main O
purpose O
of O
their O
model O
is O
to O
reduce O
the O
temporal Task
resolution Task
on Task
time Task
- Task
sensitive Task
tasks Task
. O
Thus O
, O
the O
Dilated Method
LSTM Method
is O
not O
a O
general O
solution O
for O
modeling Task
at O
multiple O
temporal O
resolutions O
. O
We O
empirically O
validate O
the O
DilatedRNN Method
in O
multiple O
RNN Method
settings Method
on O
a O
variety O
of O
sequential Task
learning Task
tasks Task
, O
including O
long Task
- Task
term Task
memorization Task
, O
pixel Task
- Task
by Task
- Task
pixel Task
classification Task
of Task
handwritten Task
digits Task
( O
with O
permutation O
and O
noise O
) O
, O
character Task
- Task
level Task
language Task
modeling Task
, O
and O
speaker Task
identification Task
with O
raw O
audio O
waveforms O
. O
The O
DilatedRNN Method
improves O
significantly O
on O
the O
performance O
of O
a O
regular Method
RNN Method
, O
LSTM Method
, O
or O
GRU Method
with O
far O
fewer O
parameters O
. O
Many O
studies O
have O
shown O
that O
vanilla Method
RNN Method
cells Method
perform O
poorly O
in O
these O
learning Task
tasks Task
. O
However O
, O
within O
the O
proposed O
structure O
, O
even O
vanilla Method
RNN Method
cells Method
outperform O
more O
sophisticated O
designs O
, O
and O
match O
the O
state O
- O
of O
- O
the O
- O
art O
. O
We O
believe O
that O
the O
DilatedRNN Method
provides O
a O
simple O
and O
generic O
approach O
to O
learning Task
on O
very O
long O
sequences O
. O
section O
: O
Dilated Method
Recurrent Method
Neural Method
Networks Method
The O
main O
ingredients O
of O
the O
DilatedRNN Method
are O
its O
dilated Method
recurrent Method
skip Method
connection Method
and O
its O
use O
of O
exponentially O
increasing O
dilation Method
; O
these O
will O
be O
discussed O
in O
the O
following O
two O
subsections O
respectively O
. O
subsection O
: O
Dilated Method
Recurrent Method
Skip Method
Connection Method
Denote O
as O
the O
cell O
in O
layer O
at O
time O
. O
The O
dilated O
skip O
connection O
can O
be O
represented O
as O
This O
is O
similar O
to O
the O
regular O
skip O
connection O
, O
which O
can O
be O
represented O
as O
is O
referred O
to O
as O
the O
skip O
length O
, O
or O
dilation Method
of O
layer O
; O
as O
the O
input O
to O
layer O
at O
time O
; O
and O
denotes O
any O
RNN O
cell O
and O
output O
operations O
, O
e.g. O
Vanilla O
RNN O
cell O
, O
LSTM Method
, O
GRU Method
etc O
. O
Both O
skip O
connections O
allow O
information O
to O
travel O
along O
fewer O
edges O
. O
The O
difference O
between O
dilated O
and O
regular O
skip O
connection O
is O
that O
the O
dependency O
on O
is O
removed O
in O
dilated O
skip O
connection O
. O
The O
left O
and O
middle O
graphs O
in O
figure O
[ O
reference O
] O
illustrate O
the O
differences O
between O
two O
architectures O
with O
dilation Method
or O
skip O
length O
, O
where O
is O
removed O
in O
the O
middle O
graph O
. O
This O
reduces O
the O
number O
of O
parameters O
. O
More O
importantly O
, O
computational Metric
efficiency Metric
of O
a O
parallel Method
implementation Method
( O
e.g. O
, O
using O
GPUs Method
) O
can O
be O
greatly O
improved O
by O
parallelizing Task
operations Task
that O
, O
in O
a O
regular Task
RNN Task
, O
would O
be O
impossible O
. O
The O
middle O
and O
right O
graphs O
in O
figure O
[ O
reference O
] O
illustrate O
the O
idea O
with O
as O
an O
example O
. O
The O
input O
subsequences O
, O
, O
and O
are O
given O
four O
different O
colors O
. O
The O
four O
cell O
chains O
, O
, O
, O
and O
, O
can O
be O
computed O
in O
parallel O
by O
feeding O
the O
four O
subsequences O
into O
a O
regular Method
RNN Method
, O
as O
shown O
in O
the O
right O
of O
figure O
[ O
reference O
] O
. O
The O
output O
can O
then O
be O
obtained O
by O
interweaving O
among O
the O
four O
output O
chains O
. O
The O
degree O
of O
parallelization O
is O
increased O
by O
times O
. O
subsection O
: O
Exponentially Method
Increasing Method
Dilation Method
To O
extract O
complex O
data O
dependencies O
, O
we O
stack O
dilated Method
recurrent Method
layers Method
to O
construct O
DilatedRNN Method
. O
Similar O
to O
settings O
that O
were O
introduced O
in O
WaveNet O
, O
the O
dilation Method
increases O
exponentially O
across O
layers O
. O
Denote O
as O
the O
dilation Method
of O
the O
- O
th O
layer O
. O
Then O
, O
The O
left O
side O
of O
figure O
[ O
reference O
] O
depicts O
an O
example O
of O
DilatedRNN Method
with O
and O
. O
On O
one O
hand O
, O
stacking O
multiple O
dilated Method
recurrent Method
layers Method
increases O
the O
model O
capacity O
. O
On O
the O
other O
hand O
, O
exponentially O
increasing O
dilation Method
brings O
two O
benefits O
. O
First O
, O
it O
makes O
different O
layers O
focus O
on O
different O
temporal O
resolutions O
. O
Second O
, O
it O
reduces O
the O
average O
length O
of O
paths O
between O
nodes O
at O
different O
timestamps O
, O
which O
improves O
the O
ability O
of O
RNNs Method
to O
extract O
long O
- O
term O
dependencies O
and O
prevents O
vanishing O
and O
exploding O
gradients O
. O
A O
formal O
proof O
of O
this O
statement O
will O
be O
given O
in O
section O
[ O
reference O
] O
. O
To O
improve O
overall O
computational Metric
efficiency Metric
, O
a O
generalization Method
of O
our O
standard O
DilatedRNN Method
is O
also O
proposed O
. O
The O
dilation Method
in O
the O
generalized O
DilatedRNN Method
does O
not O
start O
at O
one O
, O
but O
. O
Formally O
, O
where O
is O
called O
the O
starting O
dilation Method
. O
To O
compensate O
for O
the O
missing O
dependencies O
shorter O
than O
, O
a O
1 Method
- Method
by Method
- Method
convolutional Method
layer Method
is O
appended O
as O
the O
final O
layer O
. O
The O
right O
side O
of O
figure O
[ O
reference O
] O
illustrates O
an O
example O
of O
, O
i.e. O
dilations O
start O
at O
two O
. O
Without O
the O
red O
edges O
, O
there O
would O
be O
no O
edges O
connecting O
nodes O
at O
odd O
and O
even O
time O
stamps O
. O
As O
discussed O
in O
section O
[ O
reference O
] O
, O
the O
computational Metric
efficiency Metric
can O
be O
increased O
by O
by O
breaking O
the O
input O
sequence O
into O
downsampled O
subsequences O
, O
and O
feeding O
each O
into O
a O
- O
layer O
regular O
DilatedRNN Method
with O
shared O
weights O
. O
section O
: O
The O
Memory Metric
Capacity Metric
of O
DilatedRNN Method
In O
this O
section O
, O
we O
extend O
the O
analysis Method
framework Method
in O
to O
establish O
better O
measures O
of O
memory Metric
capacity Metric
and O
parameter Metric
efficiency Metric
, O
which O
will O
be O
discussed O
in O
the O
following O
two O
sections O
respectively O
. O
subsection O
: O
Memory O
Capacity O
To O
facilitate O
theoretical O
analysis O
, O
we O
apply O
the O
cyclic Method
graph Method
notation Method
introduced O
in O
. O
theorem O
: O
( O
Cyclic O
Graph O
) O
. O
The O
cyclic Method
graph Method
representation Method
of O
an O
RNN Method
structure Method
is O
a O
directed Method
multi Method
- Method
graph Method
, O
= O
GC O
( O
VC O
, O
EC O
) O
. O
Each O
edge O
is O
labeled O
as O
e= O
( O
u O
, O
v O
, O
σ O
) O
∈EC O
, O
where O
u O
is O
the O
origin O
node O
, O
v O
is O
the O
destination O
node O
, O
and O
σ O
is O
the O
number O
of O
time O
steps O
the O
edge O
travels O
. O
Each O
node O
is O
labeled O
as O
v= O
( O
i O
, O
p O
) O
∈VC O
, O
where O
i O
is O
the O
time O
index O
of O
the O
node O
modulo O
m O
, O
m O
is O
the O
period O
of O
the O
graph O
, O
and O
p O
is O
the O
node O
index O
. O
GC Method
must O
contain O
at O
least O
one O
directed O
cycle O
. O
Along O
the O
edges O
of O
any O
directed O
cycle O
, O
the O
summation O
of O
σ O
must O
not O
be O
zero O
. O
Define O
as O
the O
length O
of O
the O
shortest O
path O
from O
any O
input O
node O
at O
time O
to O
any O
output O
node O
at O
time O
. O
In O
, O
a O
measure O
of O
the O
memory Metric
capacity Metric
is O
proposed O
that O
essentially O
only O
looks O
at O
, O
where O
is O
the O
period O
of O
the O
graph O
. O
This O
is O
reasonable O
when O
the O
period O
is O
small O
. O
However O
, O
when O
the O
period O
is O
large O
, O
the O
entire O
distribution O
of O
makes O
a O
difference O
, O
not O
just O
the O
one O
at O
span O
. O
As O
a O
concrete O
example O
, O
suppose O
there O
is O
an O
RNN Method
architecture Method
of Method
period Method
, O
implemented O
using O
equation O
( O
[ O
reference O
] O
) O
with O
skip O
length O
, O
so O
that O
for O
and O
. O
This O
network O
rapidly O
memorizes O
the O
dependence O
on O
inputs O
at O
time O
of O
the O
outputs O
at O
time O
, O
but O
shorter O
dependencies O
are O
much O
harder O
to O
learn O
. O
Motivated O
by O
this O
, O
we O
proposed O
the O
following O
additional O
measure O
of O
memory Metric
capacity Metric
. O
theorem O
: O
( O
Mean O
Recurrent O
Length O
) O
. O
For O
an O
RNN Method
with O
cycle O
m O
, O
the O
mean Metric
recurrent Metric
length Metric
is O
Mean Metric
recurrent Metric
length Metric
studies O
the O
average O
dilation Method
across O
different O
time O
spans O
within O
a O
cycle O
. O
An O
architecture O
with O
good O
memory Metric
capacity Metric
should O
generally O
have O
a O
small O
recurrent O
length O
for O
all O
time O
spans O
. O
Otherwise O
the O
network O
can O
only O
selectively O
memorize O
information O
at O
a O
few O
time O
spans O
. O
Also O
, O
we O
take O
the O
maximum O
over O
, O
so O
as O
to O
punish O
networks O
that O
have O
good O
length O
only O
for O
a O
few O
starting O
times O
, O
which O
can O
only O
well O
memorize O
information O
originating O
from O
those O
specific O
times O
. O
The O
proposed O
mean O
recurrent O
length O
has O
an O
interesting O
reciprocal O
relation O
with O
the O
short Method
- Method
term Method
memory Method
( O
STM Method
) O
measure O
proposed O
in O
, O
but O
mean Method
recurrent Method
length Method
emphasizes O
more O
on O
long O
- O
term O
memory O
capacity O
, O
which O
is O
more O
suitable O
for O
our O
intended O
applications O
. O
With O
this O
, O
we O
are O
ready O
to O
illustrate O
the O
memory O
advantage O
of O
DilatedRNN Method
. O
Consider O
two O
RNN Method
architectures Method
. O
One O
is O
the O
proposed O
DilatedRNN Method
structure Method
with O
layers O
and O
( O
equation O
( O
[ O
reference O
] O
) O
) O
. O
The O
other O
is O
a O
regular Method
- Method
layer Method
RNN Method
with O
skip O
connections O
( O
equation O
( O
[ O
reference O
] O
) O
) O
. O
If O
the O
skip O
connections O
are O
of O
skip O
, O
then O
connections O
in O
the O
RNN Method
are O
a O
strict O
superset O
of O
those O
in O
the O
DilatedRNN Method
, O
and O
the O
RNN Method
accomplishes O
exactly O
the O
same O
as O
the O
DilatedRNN Method
, O
but O
with O
twice O
the O
number O
of O
trainable O
parameters O
( O
see O
section O
[ O
reference O
] O
) O
. O
Suppose O
one O
were O
to O
give O
every O
layer O
in O
the O
RNN Method
the O
largest O
possible O
skip O
for O
any O
graph O
with O
a O
period O
of O
: O
set O
in O
every O
layer O
, O
which O
is O
the O
regular Method
skip Method
RNN Method
setting Method
. O
This O
apparent O
advantage O
turns O
out O
to O
be O
a O
disadvantage O
, O
because O
time O
spans O
of O
suffer O
from O
increased O
path O
lengths O
, O
and O
therefore O
which O
grows O
linearly O
with O
. O
On O
the O
other O
hand O
, O
for O
the O
proposed O
DilatedRNN Method
, O
where O
only O
grows O
logarithmically O
with O
, O
which O
is O
much O
smaller O
than O
that O
of O
regular Method
skip Method
RNN Method
. O
It O
implies O
that O
the O
information O
in O
the O
past O
on O
average O
travels O
along O
much O
fewer O
edges O
, O
and O
thus O
undergoes O
far O
less O
attenuation O
. O
The O
derivation O
is O
given O
in O
appendix O
[ O
reference O
] O
in O
the O
supplementary O
materials O
. O
subsection O
: O
Parameter Metric
Efficiency Metric
The O
advantage O
of O
DilatedRNN Method
lies O
not O
only O
in O
the O
memory Metric
capacity Metric
but O
also O
the O
number O
of O
parameters O
that O
achieves O
such O
memory O
capacity O
. O
To O
quantify O
the O
analysis O
, O
the O
following O
measure O
is O
introduced O
. O
theorem O
: O
( O
Number O
of O
Recurrent O
Edges O
per O
Node O
) O
. O
Denote O
Card{⋅ O
} O
as O
the O
set O
cardinality O
. O
For O
an O
RNN Method
represented O
as O
= O
GC O
( O
VC O
, O
EC O
) O
, O
the O
number O
of O
recurrent O
edges O
per O
node O
, O
Nr O
, O
is O
defined O
as O
Ideally O
, O
we O
would O
want O
a O
network O
that O
has O
large O
recurrent O
skips O
while O
maintaining O
a O
small O
number O
of O
recurrent O
weights O
. O
It O
is O
easy O
to O
show O
that O
for O
DilatedRNN Method
is O
1 O
and O
that O
for O
RNNs Method
with O
regular O
skip O
connections O
is O
2 O
. O
The O
DilatedRNN Method
has O
half O
the O
recurrent Metric
complexity Metric
as O
the O
RNN Method
with Method
regular Method
skip Method
RNN Method
because O
of O
the O
removal O
of O
the O
direct O
recurrent O
edge O
. O
The O
following O
theorem O
states O
that O
the O
DilatedRNN Method
is O
able O
to O
achieve O
the O
best O
memory Metric
capacity Metric
among O
a O
class O
of O
connection O
structures O
with O
, O
and O
thus O
is O
among O
the O
most O
parameter O
efficient O
RNN Method
architectures Method
. O
theorem O
: O
( O
Parameter Metric
Efficiency Metric
of O
DilatedRNN Method
) O
. O
Consider O
a O
subset O
of O
d Method
- Method
layer Method
RNNs Method
with O
period O
= O
mM Method
- Method
d1 Method
that O
consists O
purely O
of O
dilated O
skip O
connections O
( O
hence O
= O
Nr1 O
) O
. O
For O
the O
RNNs O
in O
this O
subset O
, O
there O
are O
d O
different O
dilations O
, O
1=s1≤s2≤⋯≤sd O
= O
m O
, O
and O
where O
ni O
is O
any O
arbitrary O
positive O
integer O
. O
Among O
this O
subset O
, O
the O
d Method
- Method
layer Method
DilatedRNN Method
with O
dilation Method
rate O
{ O
M0 O
, O
⋯ O
, O
M O
- O
d1 O
} O
achieves O
the O
smallest O
¯d O
. O
The O
proof O
is O
motivated O
by O
, O
and O
is O
given O
in O
appendix O
[ O
reference O
] O
. O
subsection O
: O
Comparing O
with O
Dilated Method
CNN Method
Since O
DilatedRNN Method
is O
motivated O
by O
dilated Method
CNN Method
, O
it O
is O
useful O
to O
compare O
their O
memory Metric
capacities Metric
. O
Although O
cyclic O
graph O
, O
mean O
recurrent O
length O
and O
number O
of O
recurrent O
edges O
per O
node O
are O
designed O
for O
recurrent O
structures O
, O
they O
happen O
to O
be O
applicable O
to O
dilated Method
CNN Method
as O
well O
. O
What O
’s O
more O
, O
it O
can O
be O
easily O
shown O
that O
, O
compared O
to O
a O
DilatedRNN Method
with O
the O
same O
number O
of O
layers O
and O
dilation Method
rate O
of O
each O
layer O
, O
a O
dilated Method
CNN Method
has O
exactly O
the O
same O
number O
of O
recurrent O
edges O
per O
node O
, O
and O
a O
slightly O
smaller O
( O
by O
) O
mean O
recurrent O
length O
. O
Hence O
both O
architectures O
have O
the O
same O
model Metric
complexity Metric
, O
and O
it O
looks O
like O
a O
dilated Method
CNN Method
has O
a O
slightly O
better O
memory Metric
capacity Metric
. O
However O
, O
mean O
recurrent O
length O
only O
measures O
the O
memory O
capacity O
within O
a O
cycle O
. O
When O
going O
beyond O
a O
cycle O
, O
it O
is O
already O
shown O
that O
the O
recurrent O
length O
grows O
linearly O
with O
the O
number O
of O
cycles O
for O
RNN O
structures O
, O
including O
DilatedRNN Method
, O
whereas O
for O
a O
dilated Method
CNN Method
, O
the O
receptive O
field O
size O
is O
always O
finite O
( O
thus O
mean O
recurrent O
length O
goes O
to O
infinity O
beyond O
the O
receptive O
field O
size O
) O
. O
For O
example O
, O
with O
dilation Method
rate O
and O
layers Method
, O
a O
dilated Method
CNN Method
has O
a O
receptive O
field O
size O
of O
, O
which O
is O
two O
cycles O
. O
On O
the O
other O
hand O
, O
the O
memory O
of O
a O
DilatedRNN Method
can O
go O
far O
beyond O
two O
cycles O
, O
particularly O
with O
the O
sophisticated Method
units Method
like O
GRU Method
and O
LSTM Method
. O
Hence O
the O
memory Metric
capacity Metric
advantage Metric
of O
DilatedRNN Method
over O
a O
dilated Method
CNN Method
is O
obvious O
. O
subsection O
: O
Comparing O
with O
Clockwork Method
RNN Method
Clockwork Method
RNN Method
also O
utilizes O
the O
exponentially O
decreasing O
temporal O
resolutions O
to O
strengthen O
its O
memory O
capacity O
, O
but O
in O
a O
different O
way O
. O
Clockwork Method
RNN Method
controls O
the O
update O
rate O
of O
each O
hidden O
node O
, O
whereas O
DilatedRNN Method
updates O
all O
the O
nodes O
at O
each O
time O
step O
, O
but O
controls O
the O
data O
dependency O
. O
As O
a O
result O
, O
the O
memory Metric
capacity Metric
of O
the O
Clockwork Method
RNN Method
is O
undesirably O
time O
- O
dependent O
– O
at O
some O
output O
times O
, O
e.g. O
exponentials O
of O
2 O
, O
the O
output O
node O
has O
short O
shortest O
paths O
connecting O
to O
the O
input O
nodes O
in O
the O
past O
, O
matching O
the O
case O
of O
DilatedRNN Method
; O
at O
other O
times O
, O
the O
recurrent O
paths O
can O
be O
much O
longer O
. O
Since O
mean O
recurrent O
length O
penalizes O
such O
time O
- O
variant O
memory O
capacity O
by O
taking O
the O
worst O
case O
over O
the O
absolute O
times O
, O
a O
DilatedRNN Method
has O
a O
much O
better O
mean Metric
recurrent Metric
length Metric
than O
a O
Clockwork Method
RNN Method
with O
number O
of O
groups O
matching O
the O
number O
of O
layers O
in O
the O
DilatedRNN Method
. O
section O
: O
Experiments O
In O
this O
section O
, O
we O
evaluate O
the O
performance O
of O
DilatedRNN Method
on O
four O
different O
tasks O
, O
which O
include O
long Task
- Task
term Task
memorization Task
, O
pixel O
- O
by O
- O
pixel O
MNIST Material
classification O
, O
character Method
- Method
level Method
language Method
modeling Method
on O
the O
Penn Material
Treebank Material
, O
and O
speaker Task
identification Task
with O
raw O
waveforms O
on O
VCTK Method
. O
We O
also O
investigate O
the O
effect O
of O
dilation Method
on O
performance O
and O
computational Metric
efficiency Metric
. O
Unless O
specified O
otherwise O
, O
all O
the O
models O
are O
implemented O
with O
Tensorflow Method
. O
We O
use O
the O
default O
nonlinearities O
and O
RMSProp Method
optimizer Method
with O
learning Metric
rate Metric
0.001 O
and O
decay Metric
rate Metric
of Metric
0.9 Metric
. O
All O
weight O
matrices O
are O
initialized O
by O
the O
standard Method
normal Method
distribution Method
. O
The O
batch O
size O
is O
set O
to O
128 O
. O
Furthermore O
, O
in O
all O
the O
experiments O
, O
we O
apply O
the O
sequence Task
classification Task
setting Task
, O
where O
the O
output Method
layer Method
only O
adds O
at O
the O
end O
of O
the O
sequence O
. O
Results O
are O
reported O
for O
trained O
models O
that O
achieve O
the O
best O
validation Metric
loss Metric
. O
Unless O
stated O
otherwise O
, O
no O
tricks O
, O
such O
as O
gradient O
clipping O
, O
learning Method
rate Method
annealing Method
, O
recurrent O
weight O
dropout O
, O
recurrent O
batch O
norm O
, O
layer O
norm O
, O
etc O
. O
, O
are O
applied O
. O
All O
the O
tasks O
are O
sequence Task
level Task
classification Task
tasks Task
, O
and O
therefore O
the O
‘ O
‘ O
gridding O
’ O
’ O
problem O
is O
irrelevant O
. O
No O
‘ O
‘ O
degridded O
’ O
’ O
module O
is O
needed O
. O
Three O
RNN Method
cells Method
, O
Vanilla Method
, O
LSTM Method
and O
GRU Method
cells O
, O
are O
combined O
with O
the O
DilatedRNN Method
, O
which O
we O
refer O
to O
as O
dilated Method
Vanilla Method
, O
dilated Method
LSTM Method
and O
dilated Method
GRU Method
, O
respectively O
. O
The O
common O
baselines O
include O
single Method
- Method
layer Method
RNNs Method
( O
denoted O
as O
Vanilla Method
RNN Method
, O
LSTM Method
, O
and O
GRU Method
) O
, O
multi Method
- Method
layer Method
RNNs Method
( O
denoted O
as O
stack Method
Vanilla Method
, O
stack O
LSTM Method
, O
and O
stack O
GRU Method
) O
, O
and O
Vanilla Method
RNN Method
with O
regular Method
skip Method
connections Method
( O
denoted O
as O
Skip Method
Vanilla Method
) O
. O
Additional O
baselines O
will O
be O
specified O
in O
the O
corresponding O
subsections O
. O
subsection O
: O
Copy Task
memory Task
problem Task
This O
task O
tests O
the O
ability O
of O
recurrent Method
models Method
in O
memorizing Task
long Task
- Task
term Task
information Task
. O
We O
follow O
a O
similar O
setup O
in O
. O
Each O
input O
sequence O
is O
of O
length O
. O
The O
first O
ten O
values O
are O
randomly O
generated O
from O
integers O
0 O
to O
7 O
; O
the O
next O
values O
are O
all O
8 O
; O
the O
last O
11 O
values O
are O
all O
9 O
, O
where O
the O
first O
9 O
signals O
that O
the O
model O
needs O
to O
start O
to O
output O
the O
first O
10 O
values O
of O
the O
inputs O
. O
Different O
from O
the O
settings O
in O
, O
the O
average Metric
cross Metric
- Metric
entropy Metric
loss Metric
is O
only O
measured O
at O
the O
last O
10 O
timestamps O
. O
Therefore O
, O
the O
random Method
guess Method
yields O
an O
expected O
average Metric
cross Metric
entropy Metric
of O
. O
The O
DilatedRNN Method
uses O
9 O
layers O
with O
hidden O
state O
size O
of O
10 O
. O
The O
dilation Method
starts O
from O
one O
to O
256 O
at O
the O
last O
hidden O
layer O
. O
The O
single Method
- Method
layer Method
baselines Method
have O
256 O
hidden O
units O
. O
The O
multi Method
- Method
layer Method
baselines Method
use O
the O
same O
number O
of O
layers O
and O
hidden O
state O
size O
as O
the O
DilatedRNN Method
. O
The O
skip Method
Vanilla Method
has O
9 O
layers O
, O
and O
the O
skip O
length O
at O
each O
layer O
is O
256 O
, O
which O
matches O
the O
maximum O
dilation Method
of O
the O
DilatedRNN Method
. O
The O
convergence O
curves O
in O
two O
settings O
, O
and O
, O
are O
shown O
in O
figure O
[ O
reference O
] O
. O
In O
both O
settings O
, O
the O
DilatedRNN Method
with O
vanilla O
cells O
converges O
to O
a O
good O
optimum O
after O
about O
1 O
, O
000 O
training O
iterations O
, O
whereas O
dilated Method
LSTM Method
and O
GRU Method
converge O
slower O
. O
It O
might O
be O
because O
the O
LSTM Method
and O
GRU Method
cells Method
are O
much O
more O
complex O
than O
the O
vanilla Method
unit Method
. O
Except O
for O
the O
proposed O
models O
, O
all O
the O
other O
models O
are O
unable O
to O
do O
better O
than O
the O
random Method
guess Method
, O
including O
the O
skip Method
Vanilla Method
. O
These O
results O
suggest O
that O
the O
proposed O
structure O
as O
a O
simple O
renovation Method
is O
very O
useful O
for O
problems O
requiring O
very O
long O
memory O
. O
subsection O
: O
Pixel O
- O
by O
- O
pixel O
MNIST Material
Sequential Task
classification Task
on O
the O
MNIST Material
digits O
is O
commonly O
used O
to O
test O
the O
performance O
of O
RNNs Method
. O
We O
first O
implement O
two O
settings O
. O
In O
the O
first O
setting O
, O
called O
the O
unpermuted Task
setting Task
, O
we O
follow O
the O
same O
setups O
in O
by O
serializing O
each O
image O
into O
a O
784 O
x O
1 O
sequence O
. O
The O
second O
setting O
, O
called O
permuted O
setting O
, O
rearranges O
the O
input O
sequence O
with O
a O
fixed O
permutations O
. O
Training O
, O
validation Metric
and O
testing O
sets O
are O
the O
default O
ones O
in O
Tensorflow Method
. O
Hyperparameters Method
and O
results O
are O
reported O
in O
table O
[ O
reference O
] O
. O
In O
addition O
to O
the O
baselines O
already O
described O
, O
we O
also O
implement O
the O
dilated Method
CNN Method
. O
However O
, O
the O
receptive O
fields O
size O
of O
a O
nine Method
- Method
layer Method
dilated Method
CNN Method
is O
512 O
, O
and O
is O
insufficient O
to O
cover O
the O
sequence O
length O
of O
784 O
. O
Therefore O
, O
we O
added O
one O
more O
layer O
to O
the O
dilated Method
CNN Method
, O
which O
enlarges O
its O
receptive O
field O
size O
to O
1 O
, O
024 O
. O
It O
also O
forms O
a O
slight O
advantage O
of O
dilated Method
CNN Method
over O
the O
DilatedRNN Method
structures O
. O
In O
the O
unpermuted Task
setting Task
, O
the O
dilated Method
GRU Method
achieves O
the O
best O
evaluation Metric
accuracy Metric
of O
99.2 O
. O
However O
, O
the O
performance O
improvements O
of O
dilated O
GRU Method
and O
LSTM Method
over O
both O
the O
single O
- O
and O
multi Method
- Method
layer Method
ones Method
are O
marginal O
, O
which O
might O
be O
because O
the O
task O
is O
too O
simple O
. O
Further O
, O
we O
observe O
significant O
performance O
differences O
between O
stack Method
Vanilla Method
and O
skip Method
vanilla Method
, O
which O
is O
consistent O
with O
the O
findings O
in O
that O
RNNs Method
can O
better O
model O
long O
- O
term O
dependencies O
and O
achieves O
good O
results O
when O
recurrent O
skip O
connections O
added O
. O
Nevertheless O
, O
the O
dilated Method
vanilla Method
has O
yet O
another O
significant O
performance O
gain O
over O
the O
skip Method
Vanilla Method
, O
which O
is O
consistent O
with O
our O
argument O
in O
section O
[ O
reference O
] O
, O
that O
the O
DilatedRNN Method
has O
a O
much O
more O
balanced O
memory O
over O
a O
wide O
range O
of O
time O
periods O
than O
RNNs Method
with O
the O
regular O
skips O
. O
The O
performance O
of O
the O
dilated Method
CNN Method
is O
dominated O
by O
dilated Method
LSTM Method
and O
GRU Method
, O
even O
when O
the O
latter O
have O
fewer O
parameters O
( O
in O
the O
20 O
hidden O
units O
case O
) O
than O
the O
former O
( O
in O
the O
50 O
hidden O
units O
case O
) O
. O
In O
the O
permuted Task
setting Task
, O
almost O
all O
performances O
are O
lower O
. O
However O
, O
the O
DilatedRNN Method
models Method
maintain O
very O
high O
evaluation Metric
accuracies Metric
. O
In O
particular O
, O
dilated Method
Vanilla Method
outperforms O
the O
previous O
RNN Method
- Method
based O
state O
- O
of O
- O
the O
- O
art O
Zoneout Method
with O
a O
comparable O
number O
of O
parameters O
. O
It O
achieves O
test O
accuracy Metric
of O
96.1 O
with O
only O
44k O
parameters O
. O
Note O
that O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
utilizes O
the O
recurrent Method
batch Method
normalization Method
. O
The O
version O
without O
it O
has O
a O
much O
lower O
performance O
compared O
to O
all O
the O
dilated Method
models Method
. O
We O
believe O
the O
consistently O
high O
performance O
of O
the O
DilatedRNN Method
across O
different O
permutations O
is O
due O
to O
its O
hierarchical Method
multi Method
- Method
resolution Method
dilations Method
. O
In O
addition O
, O
the O
dilated Method
CNN Method
is O
able O
the O
achieve O
the O
best O
performance O
, O
which O
is O
in O
accordance O
with O
our O
claim O
in O
section O
[ O
reference O
] O
that O
dilated Method
CNN Method
has O
a O
slightly O
shorter O
mean Metric
recurrent Metric
length Metric
than O
DilatedRNN Method
architectures O
, O
when O
sequence O
length O
fall O
within O
its O
receptive O
field O
size O
. O
However O
, O
note O
that O
this O
is O
achieved O
by O
adding O
one O
additional O
layer O
to O
expand O
its O
receptive O
field O
size O
compared O
to O
the O
RNN Method
counterparts Method
. O
When O
the O
useful O
information O
lies O
outside O
its O
receptive O
field O
, O
the O
dilated Method
CNN Method
might O
fail O
completely O
. O
In O
addition O
to O
these O
two O
settings O
, O
we O
propose O
a O
more O
challenging O
task O
called O
the O
noisy O
MNIST Material
, O
where O
we O
pad O
the O
unpermuted O
pixel O
sequences O
with O
uniform O
random O
noise O
to O
the O
length O
of O
. O
The O
results O
with O
two O
setups O
and O
are O
shown O
in O
figure O
[ O
reference O
] O
. O
The O
dilated Method
recurrent Method
models Method
and O
skip Method
RNN Method
have O
9 O
layers O
and O
20 O
hidden O
units O
per O
layer O
. O
The O
number O
of O
skips O
at O
each O
layer O
of O
skip Method
RNN Method
is O
256 O
. O
The O
dilated Method
CNN Method
has O
10 O
layers O
and O
11 O
layers O
for O
and O
, O
respectively O
. O
This O
expands O
the O
receptive O
field O
size O
of O
the O
dilated Method
CNN Method
to O
the O
entire O
input O
sequence O
. O
The O
number O
of O
filters O
per O
layer O
is O
20 O
. O
It O
is O
worth O
mentioning O
that O
, O
in O
the O
case O
of O
, O
if O
we O
use O
a O
10 Method
- Method
layer Method
dilated Method
CNN Method
instead O
, O
it O
will O
only O
produce O
random O
guesses O
. O
This O
is O
because O
the O
output O
node O
only O
sees O
the O
last O
input O
samples O
which O
do O
not O
contain O
any O
informative O
data O
. O
All O
the O
other O
reported O
models O
have O
the O
same O
hyperparameters O
as O
shown O
in O
the O
first O
three O
row O
of O
table O
[ O
reference O
] O
. O
We O
found O
that O
none O
of O
the O
models O
without O
skip O
connections O
is O
able O
to O
learn O
. O
Although O
skip Method
Vanilla Method
remains O
learning Task
, O
its O
performance O
drops O
compared O
to O
the O
first O
unpermuted O
setup O
. O
On O
the O
contrary O
, O
the O
DilatedRNN Method
and O
dilated Method
CNN Method
models Method
obtain O
almost O
the O
same O
performances O
as O
before O
. O
It O
is O
also O
worth O
mentioning O
that O
in O
all O
three O
experiments O
, O
the O
DilatedRNN Method
models Method
are O
able O
to O
achieve O
comparable O
results O
with O
an O
extremely O
small O
number O
of O
parameters O
. O
subsection O
: O
Language Method
modeling Method
We O
further O
investigate O
the O
task O
of O
predicting O
the O
next O
character O
on O
the O
Penn Material
Treebank Material
dataset Material
. O
We O
follow O
the O
data Method
splitting Method
rule Method
with O
the O
sequence O
length O
of O
100 O
that O
are O
commonly O
used O
in O
previous O
studies O
. O
This O
corpus O
contains O
1 O
million O
words O
, O
which O
is O
small O
and O
prone O
to O
over O
- O
fitting O
. O
Therefore O
model Method
regularization Method
methods Method
have O
been O
shown O
effective O
on O
the O
validation Metric
and Metric
test Metric
set Metric
performances O
. O
Unlike O
many O
existing O
approaches O
, O
we O
apply O
no O
regularization Method
other O
than O
a O
dropout Method
on O
the O
input O
layer O
. O
Instead O
, O
we O
focus O
on O
investigating O
the O
regularization O
effect O
of O
the O
dilated O
structure O
itself O
. O
Results O
are O
shown O
in O
table O
[ O
reference O
] O
. O
Although O
Zoneout Method
, O
LayerNorm O
HM O
- O
LSTM Method
and O
HyperNetowrks O
outperform O
the O
DilatedRNN Method
models Method
, O
they O
apply O
batch Method
or Method
layer Method
normalizations Method
as O
regularization Method
. O
To O
the O
best O
of O
our O
knowledge O
, O
the O
dilated Method
GRU Method
with O
1.27 O
BPC Method
achieves O
the O
best O
result O
among O
models O
of O
similar O
sizes O
without O
layer Method
normalizations Method
. O
Also O
, O
the O
dilated Method
models Method
outperform O
their O
regular Method
counterparts Method
, O
Vanilla Method
( O
did O
n’t O
converge O
, O
omitted O
) O
, O
LSTM Method
and O
GRU Method
, O
without O
increasing O
the O
model Metric
complexity Metric
. O
subsection O
: O
Speaker Task
identification Task
from O
raw O
waveform O
We O
also O
perform O
the O
speaker Task
identification Task
task Task
using O
the O
corpus O
from O
VCTK O
. O
Learning Method
audio Method
models Method
directly O
from O
the O
raw O
waveform O
poses O
a O
difficult O
challenge O
for O
recurrent Method
models Method
because O
of O
the O
vastly O
long O
- O
term O
dependency O
. O
Recently O
the O
CLDNN Method
family Method
of Method
models Method
managed O
to O
match O
or O
surpass O
the O
log O
mel O
- O
frequency O
features O
in O
several O
speech Task
problems Task
using O
waveform O
. O
However O
, O
CLDNNs Method
coarsen O
the O
temporal O
granularity O
by O
pooling O
the O
first O
- O
layer O
CNN Method
output O
before O
feeding O
it O
into O
the O
subsequent O
RNN Method
layers Method
, O
so O
as O
to O
solve O
the O
memory Task
challenge Task
. O
Instead O
, O
the O
DilatedRNN Method
directly O
works O
on O
the O
raw O
waveform O
without O
pooling O
, O
which O
is O
considered O
more O
difficult O
. O
To O
achieve O
a O
feasible O
training Metric
time Metric
, O
we O
adopt O
the O
efficient O
generalization Method
of O
the O
DilatedRNN Method
as O
proposed O
in O
equation O
( O
[ O
reference O
] O
) O
with O
and O
. O
As O
mentioned O
before O
, O
if O
the O
dilations O
do O
not O
start O
at O
one O
, O
the O
model O
is O
equivalent O
to O
multiple O
shared Method
- Method
weight Method
networks Method
, O
each O
working O
on O
partial O
inputs O
, O
and O
the O
predictions O
are O
made O
by O
fusing O
the O
information O
using O
a O
1 Method
- Method
by Method
- Method
convolutional Method
layer Method
. O
Our O
baseline O
GRU Method
model O
follows O
the O
same O
setting O
with O
various O
resolutions O
( O
referred O
to O
as O
fused O
- O
GRU Method
) O
, O
with O
dilation Method
starting O
at O
8 O
. O
This O
baseline O
has O
8 O
share O
- O
weight O
GRU Method
networks O
, O
and O
each O
subnetwork O
works O
on O
1 O
/ O
8 O
of O
the O
subsampled O
sequences O
. O
The O
same O
fusion Method
layer Method
is O
used O
to O
obtain O
the O
final O
prediction Task
. O
Since O
most O
other O
regular O
baselines O
failed O
to O
converge O
, O
we O
also O
implemented O
the O
MFCC Method
- Method
based Method
models Method
on O
the O
same O
task O
setting O
for O
reference O
. O
The O
13 O
- O
dimensional O
log O
- O
mel O
frequency O
features O
are O
computed O
with O
25ms O
window O
and O
5ms O
shift O
. O
The O
inputs O
of O
MFCC Method
models Method
are O
of O
length O
100 O
to O
match O
the O
input O
duration O
in O
the O
waveform Method
- Method
based Method
models Method
. O
The O
MFCC Method
feature Method
has O
two O
natural O
advantages O
: O
1 O
) O
no O
information O
loss O
from O
operating O
on O
subsequences O
; O
2 O
) O
shorter O
sequence O
length O
. O
Nevertheless O
, O
our O
dilated Method
models Method
operating O
directly O
on O
the O
waveform O
still O
offer O
a O
competitive O
performance O
( O
Table O
[ O
reference O
] O
) O
. O
subsection O
: O
Discussion O
In O
this O
subsection O
, O
we O
first O
investigate O
the O
relationship O
between O
performance O
and O
the O
number O
of O
dilations O
. O
We O
compare O
the O
DilatedRNN Method
models Method
with O
different O
numbers O
of O
layers O
on O
the O
noisy O
MNIST Material
task O
. O
All O
models O
use O
vanilla Method
RNN Method
cells Method
with O
hidden O
state O
size O
20 O
. O
The O
number O
of O
dilations O
starts O
at O
one O
. O
In O
figure O
[ O
reference O
] O
, O
we O
observe O
that O
the O
classification Metric
accuracy Metric
and O
rate Metric
of Metric
convergence Metric
increases O
as O
the O
models O
become O
deeper O
. O
Recall O
the O
maximum O
skip O
is O
exponential O
in O
the O
number O
of O
layers O
. O
Thus O
, O
the O
deeper Method
model Method
has O
a O
larger O
maximum O
skip O
and O
mean O
recurrent O
length O
. O
Second O
, O
we O
consider O
maintaining O
a O
large O
maximum O
skip O
with O
a O
smaller O
number O
of O
layers O
, O
by O
increasing O
the O
dilation Method
at O
the O
bottom O
layer O
of O
DilatedRNN Method
. O
First O
, O
we O
construct O
a O
nine O
- O
layer O
DilatedRNN Method
model O
with O
vanilla Method
RNN Method
cells Method
. O
The O
number O
of O
dilations O
starts O
at O
1 O
, O
and O
hidden O
state O
size O
is O
20 O
. O
This O
architecture O
is O
referred O
to O
as O
‘ O
‘ O
starts O
at O
1 O
’ O
’ O
in O
figure O
[ O
reference O
] O
. O
Then O
, O
we O
remove O
the O
bottom O
hidden O
layers O
one O
- O
by O
- O
one O
to O
construct O
seven O
new O
models O
. O
The O
last O
created O
model O
has O
three O
layers O
, O
and O
the O
number O
of O
dilations O
starts O
at O
64 O
. O
Figure O
[ O
reference O
] O
demonstrates O
both O
the O
wall Metric
time Metric
and O
evaluation Metric
accuracy Metric
for O
50 O
, O
000 O
training O
iterations O
of O
noisy Material
MNIST Material
dataset Material
. O
The O
training Metric
time Metric
reduces O
by O
roughly O
50 O
% O
for O
every O
dropped O
layer O
( O
for O
every O
doubling O
of O
the O
minimum O
dilation Method
) O
. O
Although O
the O
testing O
performance O
decreases O
when O
the O
dilation Method
does O
not O
start O
at O
one O
, O
the O
effect O
is O
marginal O
with O
, O
and O
small O
with O
. O
Notably O
, O
the O
model O
with O
dilation Method
starting O
at O
64 O
is O
able O
to O
train O
within O
17 O
minutes O
by O
using O
a O
single O
Nvidia Method
P Method
- Method
100 Method
GPU Method
while O
maintaining O
a O
93.5 O
% O
test Metric
accuracy Metric
. O
section O
: O
Conclusion O
Our O
experiments O
with O
DilatedRNN Method
provide O
strong O
evidence O
that O
this O
simple O
multi Method
- Method
timescale Method
architectural Method
choice Method
can O
reliably O
improve O
the O
ability O
of O
recurrent Method
models Method
to O
learn O
long Task
- Task
term Task
dependency Task
in O
problems O
from O
different O
domains O
. O
We O
found O
that O
the O
DilatedRNN Method
trains Method
faster O
, O
requires O
less O
hyperparameter Method
tuning Method
, O
and O
needs O
fewer O
parameters O
to O
achieve O
the O
state O
- O
of O
- O
the O
- O
arts O
. O
In O
complement O
to O
the O
experimental O
results O
, O
we O
have O
provided O
a O
theoretical O
analysis O
showing O
the O
advantages O
of O
DilatedRNN Method
and O
proved O
its O
optimality O
under O
a O
meaningful O
architectural Metric
measure Metric
of O
RNNs Method
. O
section O
: O
Acknowledgement O
Authors O
would O
like O
to O
thank O
Tom O
Le O
Paine O
( O
paine1@illinois.edu O
) O
and O
Ryan O
Musa O
( O
ramusa@us.ibm.com O
) O
for O
their O
insightful O
discussions O
. O
bibliography O
: O
References O
section O
: O
Supplementary O
Material O
: O
Dilated Method
Recurrent Method
Neural Method
Networks Method
section O
: O
Appendix O
appendix O
: O
Mean Metric
Recurrent Metric
Length Metric
This O
appendix O
gives O
the O
detailed O
derivation O
of O
the O
conclusions O
in O
section O
[ O
reference O
] O
. O
Consider O
two O
RNN Method
architectures Method
. O
One O
is O
the O
proposed O
DilatedRNN Method
structure Method
with O
layers O
. O
The O
other O
is O
a O
regular Method
d Method
- Method
layer Method
RNN Method
with O
skip O
edges O
of O
length O
( O
hance O
) O
, O
as O
shown O
in O
figure O
[ O
reference O
] O
. O
For O
the O
regular Method
skip Method
RNN Method
, O
it O
is O
obvious O
that O
grows O
linearly O
within O
a O
cycle O
. O
and O
therefore O
which O
grows O
linearly O
with O
. O
On O
the O
other O
hand O
, O
for O
the O
proposed O
DilatedRNN Method
structure Method
, O
we O
have O
the O
following O
conclusion O
. O
theorem O
: O
. O
For O
the O
DilatedRNN Method
with O
d O
layers O
. O
where O
b0 O
, O
⋯ O
, O
b¯j O
are O
digits O
of O
the O
binary O
representation O
of O
n O
, O
and O
¯j O
is O
the O
index O
of O
the O
highest O
binary O
bit O
. O
Thus O
proof O
: O
Proof O
. O
For O
any O
path O
that O
travels O
from O
input O
to O
output O
through O
time O
steps O
consists O
of O
edges O
that O
travel O
through O
time O
and O
those O
that O
travel O
through O
layers O
. O
Therefore O
where O
is O
the O
minimum O
aggregate O
length O
of O
the O
edges O
that O
travel O
through O
time O
. O
is O
the O
minimum O
aggregate O
length O
of O
the O
edges O
that O
travel O
through O
layers O
, O
which O
is O
fixed O
. O
The O
problem O
of O
finding Task
is O
reduced O
to O
finding Task
, O
which O
can O
then O
be O
reformulated O
as O
the O
change Task
- Task
making Task
problem Task
: O
Given O
a O
set O
of O
banknotes O
valued O
and O
an O
amount O
. O
Denote O
the O
number O
of O
each O
banknote O
make O
the O
amount O
, O
such O
that O
the O
total O
number O
of O
banknotes O
used O
is O
minimized O
. O
Formally O
Since O
dilations O
’s O
are O
multiples O
of O
each O
other O
, O
the O
simple O
greedy Method
algorithm Method
suffices O
to O
find O
out O
the O
shortest O
path O
spanning O
time O
steps O
. O
That O
is O
, O
first O
use O
the O
largest O
skip O
edge O
, O
, O
times O
, O
and O
then O
use O
the O
rest O
of O
the O
dilations O
to O
fit O
the O
residuals O
. O
This O
process O
is O
analogous O
to O
converting O
into O
its O
binary Method
representation Method
. O
Hence O
the O
optimal O
solution O
to O
equation O
( O
[ O
reference O
] O
) O
, O
is O
given O
by O
For O
traversing O
through O
, O
each O
will O
be O
50 O
% O
of O
the O
time O
, O
except O
for O
, O
which O
equals O
one O
only O
once O
. O
Therefore O
∎ O
appendix O
: O
Optimality O
of O
the O
Proposed O
Skip Method
Distribution Method
This O
appendix O
provides O
the O
proof O
to O
theorem O
[ O
reference O
] O
. O
By O
analogy O
to O
the O
change Task
- Task
making Task
problem Task
, O
this O
theorem O
can O
be O
reformulated O
as O
the O
optimal Task
denomination Task
problem Task
, O
which O
involves O
finding O
the O
a O
set O
of O
banknote O
denominations O
such O
that O
the O
average O
number O
of O
banknotes O
for O
making O
the O
change O
of O
values O
ranging O
from O
to O
, O
i.e. O
, O
is O
minimized O
. O
The O
optimal Task
denomination Task
problem Task
remains O
to O
be O
an O
open O
problem O
in O
mathematics Task
, O
but O
solutions O
are O
readily O
available O
when O
the O
candidate O
denominations O
are O
confined O
to O
those O
satisfying O
equation O
( O
[ O
reference O
] O
) O
, O
as O
shown O
in O
. O
The O
proof O
here O
is O
adapted O
from O
that O
in O
. O
Proof O
to O
theorem O
: O
proof O
: O
Proof O
. O
First O
, O
it O
is O
easy O
to O
show O
that O
the O
RNN Method
architecture Method
that O
minimizes O
must O
have O
the O
same O
dilation Method
rate O
within O
the O
same O
layer O
, O
because O
1 O
) O
it O
has O
all O
the O
paths O
that O
consist O
of O
all O
the O
combinations O
of O
recurrent O
edges O
with O
different O
dilations O
, O
where O
the O
optimal O
shortest O
paths O
must O
lie O
; O
2 O
) O
in O
such O
architectures O
does O
not O
depend O
on O
, O
so O
that O
the O
maximum O
over O
in O
equation O
( O
[ O
reference O
] O
) O
does O
not O
have O
an O
effect O
. O
Now O
that O
we O
have O
confined O
the O
candidate O
set O
, O
the O
problem O
is O
reduced O
to O
finding O
a O
set O
of O
such O
that O
is O
minimized O
. O
We O
can O
apply O
equation O
( O
[ O
reference O
] O
) O
, O
Define O
as O
the O
average O
number O
of O
recurrent O
edge O
usage O
. O
Hence O
minimizing Task
is O
further O
reduced O
to O
minimizing O
. O
Since O
dilations O
’s O
are O
multiples O
of O
each O
other O
, O
the O
simple O
greedy Method
algorithm Method
suffices O
to O
find O
out O
the O
shortest O
path O
spanning O
time O
steps O
. O
That O
is O
, O
first O
use O
the O
largest O
skip O
edge O
, O
, O
times O
, O
and O
then O
use O
the O
rest O
of O
the O
skip O
lengths O
to O
fit O
the O
residuals O
. O
Therefore O
, O
to O
fit O
all O
the O
time O
spans O
ranging O
from O
to O
, O
the O
histogram O
of O
uses O
of O
recurrent O
edge O
of O
length O
per O
time O
span O
is O
distributed O
uniformly O
across O
0 O
through O
time O
. O
Formally O
, O
the O
total O
uses O
of O
recurrent O
skips O
of O
length O
is O
To O
fit O
the O
time O
span O
, O
only O
the O
edge O
will O
be O
used O
once O
. O
Hence O
Since O
the O
arithmetic O
mean O
is O
alway O
greater O
than O
or O
equal O
to O
the O
geometric O
mean O
with O
equality O
if O
and O
only O
if O
∎ O
