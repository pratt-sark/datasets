document	O
:	O
Linguistically	Task
-	Task
Informed	Task
Self	Task
-	Task
Attention	Task
for	O
Semantic	Task
Role	Task
Labeling	Task
Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
semantic	Task
role	Task
labeling	Task
(	O
SRL	Task
)	O
uses	O
a	O
deep	Method
neural	Method
network	Method
with	O
no	O
explicit	O
linguistic	O
features	O
.	O
However	O
,	O
prior	O
work	O
has	O
shown	O
that	O
gold	O
syntax	O
trees	O
can	O
dramatically	O
improve	O
SRL	Task
decoding	Task
,	O
suggesting	O
the	O
possibility	O
of	O
increased	O
accuracy	Metric
from	O
explicit	Method
modeling	Method
of	Method
syntax	Method
.	O
In	O
this	O
work	O
,	O
we	O
present	O
linguistically	Method
-	Method
informed	Method
self	Method
-	Method
attention	Method
(	O
LISA	Method
)	O
:	O
a	O
neural	Method
network	Method
model	Method
that	O
combines	O
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
with	O
multi	Method
-	Method
task	Method
learning	Method
across	O
dependency	Task
parsing	Task
,	O
part	Task
-	Task
of	Task
-	Task
speech	Task
tagging	Task
,	O
predicate	Task
detection	Task
and	O
SRL	Task
.	O
Unlike	O
previous	O
models	O
which	O
require	O
significant	O
pre	O
-	O
processing	O
to	O
prepare	O
linguistic	O
features	O
,	O
LISA	Method
can	O
incorporate	O
syntax	O
using	O
merely	O
raw	O
tokens	O
as	O
input	O
,	O
encoding	O
the	O
sequence	O
only	O
once	O
to	O
simultaneously	O
perform	O
parsing	Task
,	O
predicate	Task
detection	Task
and	O
role	Task
labeling	Task
for	O
all	O
predicates	O
.	O
Syntax	O
is	O
incorporated	O
by	O
training	O
one	O
attention	O
head	O
to	O
attend	O
to	O
syntactic	O
parents	O
for	O
each	O
token	O
.	O
Moreover	O
,	O
if	O
a	O
high	O
-	O
quality	O
syntactic	O
parse	O
is	O
already	O
available	O
,	O
it	O
can	O
be	O
beneficially	O
injected	O
at	O
test	O
time	O
without	O
re	O
-	O
training	O
our	O
SRL	Task
model	Task
.	O
In	O
experiments	O
on	O
CoNLL	Material
-	Material
2005	Material
SRL	Material
,	O
LISA	Method
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
a	O
model	O
using	O
predicted	O
predicates	O
and	O
standard	O
word	Method
embeddings	Method
,	O
attaining	O
2.5	O
F1	Metric
absolute	O
higher	O
than	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
newswire	O
and	O
more	O
than	O
3.5	O
F1	Metric
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
,	O
nearly	O
10	O
%	O
reduction	O
in	O
error	Metric
.	O
On	O
ConLL	Material
-	Material
2012	Material
English	Material
SRL	Material
we	O
also	O
show	O
an	O
improvement	O
of	O
more	O
than	O
2.5	O
F1	Metric
.	O
LISA	Method
also	O
out	O
-	O
performs	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
with	O
contextually	Method
-	Method
encoded	Method
(	Method
ELMo	Method
)	Method
word	Method
representations	Method
,	O
by	O
nearly	O
1.0	O
F1	Metric
on	O
news	O
and	O
more	O
than	O
2.0	O
F1	Metric
on	O
out	O
-	O
of	O
-	O
domain	O
text	O
.	O
section	O
:	O
Introduction	O
Semantic	Task
role	Task
labeling	Task
(	O
SRL	Task
)	O
extracts	O
a	O
high	O
-	O
level	O
representation	O
of	O
meaning	O
from	O
a	O
sentence	O
,	O
labeling	O
e.g.	O
who	O
did	O
what	O
to	O
whom	O
.	O
Explicit	Method
representations	Method
of	O
such	O
semantic	O
information	O
have	O
been	O
shown	O
to	O
improve	O
results	O
in	O
challenging	O
downstream	Task
tasks	Task
such	O
as	O
dialog	Task
systems	Task
tur2005semi	O
,	O
chen2013unsupervised	O
,	O
machine	Task
reading	Task
berant2014modeling	O
,	O
wang2015machine	O
and	O
translation	Task
liu2010semantic	O
,	O
bazrafshan2013semantic	O
.	O
Though	O
syntax	O
was	O
long	O
considered	O
an	O
obvious	O
prerequisite	O
for	O
SRL	Task
systems	Task
levin1993english	O
,	O
punyakanok2008importance	O
,	O
recently	O
deep	Method
neural	Method
network	Method
architectures	Method
have	O
surpassed	O
syntactically	Method
-	Method
informed	Method
models	Method
zhou2015end	O
,	O
marcheggiani2017simple	O
,	O
he2017deep	O
,	O
tan2018deep	O
,	O
he2018jointly	O
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
art	O
SRL	Task
performance	O
with	O
no	O
explicit	Method
modeling	Method
of	Method
syntax	Method
.	O
An	O
additional	O
benefit	O
of	O
these	O
end	Method
-	Method
to	Method
-	Method
end	Method
models	Method
is	O
that	O
they	O
require	O
just	O
raw	O
tokens	O
and	O
(	O
usually	O
)	O
detected	O
predicates	O
as	O
input	O
,	O
whereas	O
richer	O
linguistic	O
features	O
typically	O
require	O
extraction	O
by	O
an	O
auxiliary	Method
pipeline	Method
of	Method
models	Method
.	O
Still	O
,	O
recent	O
work	O
roth2016neural	O
,	O
he2017deep	O
,	O
marcheggiani2017encoding	O
indicates	O
that	O
neural	Method
network	Method
models	Method
could	O
see	O
even	O
higher	O
accuracy	Metric
gains	O
by	O
leveraging	O
syntactic	O
information	O
rather	O
than	O
ignoring	O
it	O
.	O
he2017deep	O
indicate	O
that	O
many	O
of	O
the	O
errors	O
made	O
by	O
a	O
syntax	Method
-	Method
free	Method
neural	Method
network	Method
on	O
SRL	Task
are	O
tied	O
to	O
certain	O
syntactic	O
confusions	O
such	O
as	O
prepositional	O
phrase	O
attachment	O
,	O
and	O
show	O
that	O
while	O
constrained	Method
inference	Method
using	O
a	O
relatively	O
low	O
-	O
accuracy	Metric
predicted	Metric
parse	Metric
can	O
provide	O
small	O
improvements	O
in	O
SRL	Task
accuracy	O
,	O
providing	O
a	O
gold	Metric
-	Metric
quality	Metric
parse	Metric
leads	O
to	O
substantial	O
gains	O
.	O
marcheggiani2017encoding	O
incorporate	O
syntax	O
from	O
a	O
high	O
-	O
quality	O
parser	Method
kiperwasser2016simple	O
using	O
graph	Method
convolutional	Method
neural	Method
networks	Method
kipf2017semi	O
,	O
but	O
like	O
he2017deep	O
they	O
attain	O
only	O
small	O
increases	O
over	O
a	O
model	O
with	O
no	O
syntactic	O
parse	O
,	O
and	O
even	O
perform	O
worse	O
than	O
a	O
syntax	Method
-	Method
free	Method
model	Method
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
.	O
These	O
works	O
suggest	O
that	O
though	O
syntax	O
has	O
the	O
potential	O
to	O
improve	O
neural	Method
network	Method
SRL	Method
models	Method
,	O
we	O
have	O
not	O
yet	O
designed	O
an	O
architecture	O
which	O
maximizes	O
the	O
benefits	O
of	O
auxiliary	O
syntactic	O
information	O
.	O
In	O
response	O
,	O
we	O
propose	O
linguistically	Method
-	Method
informed	Method
self	Method
-	Method
attention	Method
(	O
LISA	Method
)	O
:	O
a	O
model	O
that	O
combines	O
multi	Method
-	Method
task	Method
learning	Method
caruana1993multitask	O
with	O
stacked	Method
layers	Method
of	Method
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
vaswani2017attention	O
;	O
the	O
model	O
is	O
trained	O
to	O
:	O
(	O
1	O
)	O
jointly	O
predict	O
parts	O
of	O
speech	O
and	O
predicates	O
;	O
(	O
2	O
)	O
perform	O
parsing	Task
;	O
and	O
(	O
3	O
)	O
attend	O
to	O
syntactic	O
parse	O
parents	O
,	O
while	O
(	O
4	O
)	O
assigning	O
semantic	O
role	O
labels	O
.	O
Whereas	O
prior	O
work	O
typically	O
requires	O
separate	O
models	O
to	O
provide	O
linguistic	Task
analysis	Task
,	O
including	O
most	O
syntax	Method
-	Method
free	Method
neural	Method
models	Method
which	O
still	O
rely	O
on	O
external	Method
predicate	Method
detection	Method
,	O
our	O
model	O
is	O
truly	O
end	O
-	O
to	O
-	O
end	O
:	O
earlier	O
layers	O
are	O
trained	O
to	O
predict	O
prerequisite	O
parts	O
-	O
of	O
-	O
speech	O
and	O
predicates	O
,	O
the	O
latter	O
of	O
which	O
are	O
supplied	O
to	O
later	O
layers	O
for	O
scoring	Task
.	O
Though	O
prior	O
work	O
re	O
-	O
encodes	O
each	O
sentence	O
to	O
predict	O
each	O
desired	O
task	O
and	O
again	O
with	O
respect	O
to	O
each	O
predicate	O
to	O
perform	O
SRL	Task
,	O
we	O
more	O
efficiently	O
encode	O
each	O
sentence	O
only	O
once	O
,	O
predict	O
its	O
predicates	O
,	O
part	O
-	O
of	O
-	O
speech	O
tags	O
and	O
labeled	O
syntactic	O
parse	O
,	O
then	O
predict	O
the	O
semantic	O
roles	O
for	O
all	O
predicates	O
in	O
the	O
sentence	O
in	O
parallel	O
.	O
The	O
model	O
is	O
trained	O
such	O
that	O
,	O
as	O
syntactic	Method
parsing	Method
models	Method
improve	O
,	O
providing	O
high	O
-	O
quality	O
parses	O
at	O
test	O
time	O
will	O
improve	O
its	O
performance	O
,	O
allowing	O
the	O
model	O
to	O
leverage	O
updated	O
parsing	Method
models	Method
without	O
requiring	O
re	Task
-	Task
training	Task
.	O
In	O
experiments	O
on	O
the	O
CoNLL	Material
-	Material
2005	Material
and	O
CoNLL	Material
-	Material
2012	Material
datasets	Material
we	O
show	O
that	O
our	O
linguistically	Method
-	Method
informed	Method
models	Method
out	O
-	O
perform	O
the	O
syntax	Method
-	Method
free	Method
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
On	O
CoNLL	Material
-	Material
2005	Material
with	O
predicted	O
predicates	O
and	O
standard	O
word	Method
embeddings	Method
,	O
our	O
single	O
model	O
out	O
-	O
performs	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
the	O
WSJ	O
test	O
set	O
by	O
2.5	O
F1	Metric
points	O
absolute	O
.	O
On	O
the	O
challenging	O
out	O
-	O
of	O
-	O
domain	O
Brown	O
test	O
set	O
,	O
our	O
model	O
improves	O
substantially	O
over	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
more	O
than	O
3.5	O
F1	Metric
,	O
a	O
nearly	O
10	O
%	O
reduction	O
in	O
error	Metric
.	O
On	O
CoNLL	Material
-	Material
2012	Material
,	O
our	O
model	O
gains	O
more	O
than	O
2.5	O
F1	Metric
absolute	O
over	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
Our	O
models	O
also	O
show	O
improvements	O
when	O
using	O
contextually	Method
-	Method
encoded	Method
word	Method
representations	Method
peters2018deep	O
,	O
obtaining	O
nearly	O
1.0	O
F1	Metric
higher	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
CoNLL	Material
-	Material
2005	Material
news	O
and	O
more	O
than	O
2.0	O
F1	Metric
improvement	O
on	O
out	O
-	O
of	O
-	O
domain	O
text	O
.	O
section	O
:	O
Model	O
[	O
scale=.8	O
]	O
no_words_simpler_compact	O
-	O
srl	O
-	O
model.pdf	O
[	O
scale=.24	O
]	O
attention	O
-	O
keynote	O
Our	O
goal	O
is	O
to	O
design	O
an	O
efficient	O
neural	Method
network	Method
model	Method
which	O
makes	O
use	O
of	O
linguistic	O
information	O
as	O
effectively	O
as	O
possible	O
in	O
order	O
to	O
perform	O
end	O
-	O
to	O
-	O
end	O
SRL	Task
.	O
LISA	Method
achieves	O
this	O
by	O
combining	O
:	O
(	O
1	O
)	O
A	O
new	O
technique	O
of	O
supervising	Method
neural	Method
attention	Method
to	O
predict	Task
syntactic	Task
dependencies	Task
with	O
(	O
2	O
)	O
multi	Method
-	Method
task	Method
learning	Method
across	O
four	O
related	O
tasks	O
.	O
Figure	O
[	O
reference	O
]	O
depicts	O
the	O
overall	O
architecture	O
of	O
our	O
model	O
.	O
The	O
basis	O
for	O
our	O
model	O
is	O
the	O
Transformer	Method
encoder	Method
introduced	O
by	O
vaswani2017attention	O
:	O
we	O
transform	O
word	Method
embeddings	Method
into	O
contextually	Method
-	Method
encoded	Method
token	Method
representations	Method
using	O
stacked	O
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
and	O
feed	Method
-	Method
forward	Method
layers	Method
(	O
§	O
[	O
reference	O
]	O
)	O
.	O
To	O
incorporate	O
syntax	O
,	O
one	O
self	O
-	O
attention	O
head	O
is	O
trained	O
to	O
attend	O
to	O
each	O
token	O
’s	O
syntactic	O
parent	O
,	O
allowing	O
the	O
model	O
to	O
use	O
this	O
attention	O
head	O
as	O
an	O
oracle	O
for	O
syntactic	O
dependencies	O
.	O
We	O
introduce	O
this	O
syntactically	O
-	O
informed	O
self	O
-	O
attention	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
in	O
more	O
detail	O
in	O
§	O
[	O
reference	O
]	O
.	O
Our	O
model	O
is	O
designed	O
for	O
the	O
more	O
realistic	O
setting	O
in	O
which	O
gold	O
predicates	O
are	O
not	O
provided	O
at	O
test	O
-	O
time	O
.	O
Our	O
model	O
predicts	O
predicates	O
and	O
integrates	O
part	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
information	O
into	O
earlier	O
layers	O
by	O
re	O
-	O
purposing	O
representations	O
closer	O
to	O
the	O
input	O
to	O
predict	O
predicate	O
and	O
POS	O
tags	O
using	O
hard	Method
parameter	Method
sharing	Method
(	O
§	O
[	O
reference	O
]	O
)	O
.	O
We	O
simplify	O
optimization	Task
and	O
benefit	O
from	O
shared	O
statistical	O
strength	O
derived	O
from	O
highly	O
correlated	O
POS	O
and	O
predicates	O
by	O
treating	O
tagging	Task
and	Task
predicate	Task
detection	Task
as	O
a	O
single	O
task	O
,	O
performing	O
multi	Task
-	Task
class	Task
classification	Task
into	O
the	O
joint	O
Cartesian	O
product	O
space	O
of	O
POS	O
and	O
predicate	O
labels	O
.	O
Though	O
typical	O
models	O
,	O
which	O
re	O
-	O
encode	O
the	O
sentence	O
for	O
each	O
predicate	O
,	O
can	O
simplify	O
SRL	Task
to	O
token	Task
-	Task
wise	Task
tagging	Task
,	O
our	O
joint	Method
model	Method
requires	O
a	O
different	O
approach	O
to	O
classify	O
roles	O
with	O
respect	O
to	O
each	O
predicate	O
.	O
Contextually	O
encoded	O
tokens	O
are	O
projected	O
to	O
distinct	O
predicate	O
and	O
role	O
embeddings	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
and	O
each	O
predicted	O
predicate	O
is	O
scored	O
with	O
the	O
sequence	Method
’s	Method
role	Method
representations	Method
using	O
a	O
bilinear	Method
model	Method
(	O
Eqn	O
.	O
[	O
reference	O
]	O
)	O
,	O
producing	O
per	O
-	O
label	O
scores	O
for	O
BIO	O
-	O
encoded	O
semantic	O
role	O
labels	O
for	O
each	O
token	O
and	O
each	O
semantic	O
frame	O
.	O
The	O
model	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
by	O
maximum	Method
likelihood	Method
using	O
stochastic	Method
gradient	Method
descent	Method
(	O
§	O
[	O
reference	O
]	O
)	O
.	O
subsection	O
:	O
Self	Method
-	Method
attention	Method
token	Method
encoder	Method
The	O
basis	O
for	O
our	O
model	O
is	O
a	O
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
token	Method
encoder	Method
,	O
recently	O
shown	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
SRL	Task
tan2018deep	O
,	O
and	O
which	O
provides	O
a	O
natural	O
mechanism	O
for	O
incorporating	O
syntax	O
,	O
as	O
described	O
in	O
§	O
[	O
reference	O
]	O
.	O
Our	O
implementation	O
replicates	O
vaswani2017attention	O
.	O
The	O
input	O
to	O
the	O
network	O
is	O
a	O
sequence	Method
of	Method
token	Method
representations	Method
.	O
In	O
the	O
standard	O
setting	O
these	O
token	Method
representations	Method
are	O
initialized	O
to	O
pre	O
-	O
trained	O
word	Method
embeddings	Method
,	O
but	O
we	O
also	O
experiment	O
with	O
supplying	O
pre	O
-	O
trained	O
ELMo	Method
representations	Method
combined	O
with	O
task	O
-	O
specific	O
learned	O
parameters	O
,	O
which	O
have	O
been	O
shown	O
to	O
substantially	O
improve	O
performance	O
of	O
other	O
SRL	Task
models	Task
peters2018deep	O
.	O
For	O
experiments	O
with	O
gold	O
predicates	O
,	O
we	O
concatenate	O
a	O
predicate	Method
indicator	Method
embedding	Method
following	O
previous	O
work	O
he2017deep	O
.	O
We	O
project	O
these	O
input	O
embeddings	O
to	O
a	O
representation	O
that	O
is	O
the	O
same	O
size	O
as	O
the	O
output	O
of	O
the	O
self	Method
-	Method
attention	Method
layers	Method
.	O
We	O
then	O
add	O
a	O
positional	O
encoding	O
vector	O
computed	O
as	O
a	O
deterministic	O
sinusoidal	O
function	O
of	O
,	O
since	O
the	O
self	Method
-	Method
attention	Method
has	O
no	O
innate	O
notion	O
of	O
token	O
position	O
.	O
We	O
feed	O
this	O
token	Method
representation	Method
as	O
input	O
to	O
a	O
series	O
of	O
residual	Method
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
layers	Method
with	O
feed	Method
-	Method
forward	Method
connections	Method
.	O
Denoting	O
the	O
th	O
self	O
-	O
attention	O
layer	O
as	O
,	O
the	O
output	O
of	O
that	O
layer	O
,	O
and	O
layer	Method
normalization	Method
,	O
the	O
following	O
recurrence	O
applied	O
to	O
initial	O
input	O
:	O
s_t^	O
(	O
j	O
)	O
=	O
LN	O
(	O
s_t^	O
(	O
j	O
-	O
1	O
)	O
+	O
T^	O
(	O
j	O
)(	O
s_t^	O
(	O
j	O
-	O
1	O
)	O
)	O
)	O
gives	O
our	O
final	O
token	Method
representations	Method
.	O
Each	O
consists	O
of	O
:	O
(	O
a	O
)	O
multi	Method
-	Method
head	Method
self	Method
-	Method
attention	Method
and	O
(	O
b	O
)	O
a	O
feed	Method
-	Method
forward	Method
projection	Method
.	O
The	O
multi	Task
-	Task
head	Task
self	Task
attention	Task
consists	O
of	O
attention	O
heads	O
,	O
each	O
of	O
which	O
learns	O
a	O
distinct	O
attention	O
function	O
to	O
attend	O
to	O
all	O
of	O
the	O
tokens	O
in	O
the	O
sequence	O
.	O
This	O
self	Method
-	Method
attention	Method
is	O
performed	O
for	O
each	O
token	O
for	O
each	O
head	O
,	O
and	O
the	O
results	O
of	O
the	O
self	O
-	O
attentions	O
are	O
concatenated	O
to	O
form	O
the	O
final	O
self	Method
-	Method
attended	Method
representation	Method
for	O
each	O
token	O
.	O
Specifically	O
,	O
consider	O
the	O
matrix	Method
of	Method
token	Method
representations	Method
at	O
layer	O
.	O
For	O
each	O
attention	O
head	O
,	O
we	O
project	O
this	O
matrix	O
into	O
distinct	O
key	O
,	O
value	Method
and	Method
query	Method
representations	Method
,	O
and	O
of	O
dimensions	O
,	O
,	O
and	O
,	O
respectively	O
.	O
We	O
can	O
then	O
multiply	O
by	O
to	O
obtain	O
a	O
matrix	O
of	O
attention	O
weights	O
between	O
each	O
pair	O
of	O
tokens	O
in	O
the	O
sentence	O
.	O
Following	O
vaswani2017attention	O
we	O
perform	O
scaled	Method
dot	Method
-	Method
product	Method
attention	Method
:	O
We	O
scale	O
the	O
weights	O
by	O
the	O
inverse	O
square	O
root	O
of	O
their	O
embedding	O
dimension	O
and	O
normalize	O
with	O
the	O
softmax	O
function	O
to	O
produce	O
a	O
distinct	O
distribution	O
for	O
each	O
token	O
over	O
all	O
the	O
tokens	O
in	O
the	O
sentence	O
:	O
A_h^	O
(	O
j	O
)	O
=	O
softmax	O
(	O
d_k^	O
-	O
0.5Q_h^	O
(	O
j	O
)	O
K_h^	O
(	O
j	O
)	O
^T	O
)	O
These	O
attention	O
weights	O
are	O
then	O
multiplied	O
by	O
for	O
each	O
token	O
to	O
obtain	O
the	O
self	Method
-	Method
attended	Method
token	Method
representations	Method
:	O
M_h^	O
(	O
j	O
)	O
=	O
A_h^	O
(	O
j	O
)	O
V_h^	O
(	O
j	O
)	O
Row	O
of	O
,	O
the	O
self	Method
-	Method
attended	Method
representation	Method
for	O
token	O
at	O
layer	O
,	O
is	O
thus	O
the	O
weighted	O
sum	O
with	O
respect	O
to	O
(	O
with	O
weights	O
given	O
by	O
)	O
over	O
the	O
token	Method
representations	Method
in	O
.	O
The	O
outputs	O
of	O
all	O
attention	O
heads	O
for	O
each	O
token	O
are	O
concatenated	O
,	O
and	O
this	O
representation	O
is	O
passed	O
to	O
the	O
feed	Method
-	Method
forward	Method
layer	Method
,	O
which	O
consists	O
of	O
two	O
linear	Method
projections	Method
each	O
followed	O
by	O
leaky	Method
ReLU	Method
activations	Method
maas2012rectifier	Method
.	O
We	O
add	O
the	O
output	O
of	O
the	O
feed	Method
-	Method
forward	Method
to	O
the	O
initial	O
representation	O
and	O
apply	O
layer	Method
normalization	Method
to	O
give	O
the	O
final	O
output	O
of	O
self	Method
-	Method
attention	Method
layer	Method
,	O
as	O
in	O
Eqn	O
.	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
Syntactically	O
-	O
informed	O
self	O
-	O
attention	O
Typically	O
,	O
neural	Method
attention	Method
mechanisms	Method
are	O
left	O
on	O
their	O
own	O
to	O
learn	O
to	O
attend	O
to	O
relevant	O
inputs	O
.	O
Instead	O
,	O
we	O
propose	O
training	O
the	O
self	O
-	O
attention	O
to	O
attend	O
to	O
specific	O
tokens	O
corresponding	O
to	O
the	O
syntactic	O
structure	O
of	O
the	O
sentence	O
as	O
a	O
mechanism	O
for	O
passing	O
linguistic	O
knowledge	O
to	O
later	O
layers	O
.	O
Specifically	O
,	O
we	O
replace	O
one	O
attention	O
head	O
with	O
the	O
deep	Method
bi	Method
-	Method
affine	Method
model	Method
of	O
dozat2017deep	O
,	O
trained	O
to	O
predict	O
syntactic	O
dependencies	O
.	O
Let	O
be	O
the	O
parse	O
attention	O
weights	O
,	O
at	O
layer	O
.	O
Its	O
input	O
is	O
the	O
matrix	Method
of	Method
token	Method
representations	Method
.	O
As	O
with	O
the	O
other	O
attention	O
heads	O
,	O
we	O
project	O
into	O
key	O
,	O
value	O
and	O
query	Method
representations	Method
,	O
denoted	O
,	O
,	O
.	O
Here	O
the	O
key	O
and	O
query	O
projections	O
correspond	O
to	O
and	O
representations	O
of	O
the	O
tokens	O
,	O
and	O
we	O
allow	O
their	O
dimensions	O
to	O
differ	O
from	O
the	O
rest	O
of	O
the	O
attention	O
heads	O
to	O
more	O
closely	O
follow	O
the	O
implementation	O
of	O
dozat2017deep	O
.	O
Unlike	O
the	O
other	O
attention	Method
heads	Method
which	O
use	O
a	O
dot	Method
product	Method
to	O
score	O
key	O
-	O
query	O
pairs	O
,	O
we	O
score	O
the	O
compatibility	O
between	O
and	O
using	O
a	O
bi	O
-	O
affine	O
operator	O
to	O
obtain	O
attention	O
weights	O
:	O
A_parse	O
=	O
softmax	O
(	O
Q_parse	O
U_heads	O
K_parse^T	O
)	O
These	O
attention	O
weights	O
are	O
used	O
to	O
compose	O
a	O
weighted	Method
average	Method
of	Method
the	Method
value	Method
representations	Method
as	O
in	O
the	O
other	O
attention	O
heads	O
.	O
We	O
apply	O
auxiliary	O
supervision	O
at	O
this	O
attention	O
head	O
to	O
encourage	O
it	O
to	O
attend	O
to	O
each	O
token	O
’s	O
parent	O
in	O
a	O
syntactic	O
dependency	O
tree	O
,	O
and	O
to	O
encode	O
information	O
about	O
the	O
token	O
’s	O
dependency	O
label	O
.	O
Denoting	O
the	O
attention	O
weight	O
from	O
token	O
to	O
a	O
candidate	O
head	O
as	O
,	O
we	O
model	O
the	O
probability	O
of	O
token	O
having	O
parent	O
as	O
:	O
P	O
(	O
q	O
=	O
head	O
(	O
t	O
)	O
X	O
)	O
=	O
A_parse	O
[	O
t	O
,	O
q	O
]	O
using	O
the	O
attention	O
weights	O
as	O
the	O
distribution	O
over	O
possible	O
heads	O
for	O
token	O
.	O
We	O
define	O
the	O
root	O
token	O
as	O
having	O
a	O
self	O
-	O
loop	O
.	O
This	O
attention	O
head	O
thus	O
emits	O
a	O
directed	O
graph	O
where	O
each	O
token	O
’s	O
parent	O
is	O
the	O
token	O
to	O
which	O
the	O
attention	O
assigns	O
the	O
highest	O
weight	O
.	O
We	O
also	O
predict	O
dependency	O
labels	O
using	O
per	O
-	O
class	O
bi	O
-	O
affine	O
operations	O
between	O
parent	Method
and	Method
dependent	Method
representations	Method
and	O
to	O
produce	O
per	O
-	O
label	O
scores	O
,	O
with	O
locally	O
normalized	O
probabilities	O
over	O
dependency	O
labels	O
given	O
by	O
the	O
softmax	Method
function	Method
.	O
We	O
refer	O
the	O
reader	O
to	O
dozat2017deep	O
for	O
more	O
details	O
.	O
This	O
attention	O
head	O
now	O
becomes	O
an	O
oracle	O
for	O
syntax	Task
,	O
denoted	O
,	O
providing	O
a	O
dependency	Method
parse	Method
to	O
downstream	O
layers	O
.	O
This	O
model	O
not	O
only	O
predicts	O
its	O
own	O
dependency	O
arcs	O
,	O
but	O
allows	O
for	O
the	O
injection	O
of	O
auxiliary	O
parse	O
information	O
at	O
test	O
time	O
by	O
simply	O
setting	O
to	O
the	O
parse	O
parents	O
produced	O
by	O
e.g.	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
parser	Method
.	O
In	O
this	O
way	O
,	O
our	O
model	O
can	O
benefit	O
from	O
improved	O
,	O
external	Method
parsing	Method
models	Method
without	O
re	O
-	O
training	O
.	O
Unlike	O
typical	O
multi	Method
-	Method
task	Method
models	Method
,	O
ours	O
maintains	O
the	O
ability	O
to	O
leverage	O
external	O
syntactic	O
information	O
.	O
subsection	O
:	O
Multi	Task
-	Task
task	Task
learning	Task
We	O
also	O
share	O
the	O
parameters	O
of	O
lower	O
layers	O
in	O
our	O
model	O
to	O
predict	O
POS	O
tags	O
and	O
predicates	O
.	O
Following	O
he2017deep	O
,	O
we	O
focus	O
on	O
the	O
end	Task
-	Task
to	Task
-	Task
end	Task
setting	Task
,	O
where	O
predicates	O
must	O
be	O
predicted	O
on	O
-	O
the	O
-	O
fly	O
.	O
Since	O
we	O
also	O
train	O
our	O
model	O
to	O
predict	O
syntactic	O
dependencies	O
,	O
it	O
is	O
beneficial	O
to	O
give	O
the	O
model	O
knowledge	O
of	O
POS	O
information	O
.	O
While	O
much	O
previous	O
work	O
employs	O
a	O
pipelined	Method
approach	Method
to	O
both	O
POS	Task
tagging	Task
for	O
dependency	Task
parsing	Task
and	O
predicate	Task
detection	Task
for	O
SRL	Task
,	O
we	O
take	O
a	O
multi	Method
-	Method
task	Method
learning	Method
(	O
MTL	Task
)	O
approach	O
caruana1993multitask	O
,	O
sharing	O
the	O
parameters	O
of	O
earlier	O
layers	O
in	O
our	O
SRL	Task
model	Task
with	O
a	O
joint	O
POS	O
and	O
predicate	Task
detection	Task
objective	Task
.	O
Since	O
POS	O
is	O
a	O
strong	O
predictor	O
of	O
predicates	O
and	O
the	O
complexity	Metric
of	O
training	O
a	O
multi	Method
-	Method
task	Method
model	Method
increases	O
with	O
the	O
number	O
of	O
tasks	O
,	O
we	O
combine	O
POS	Task
tagging	Task
and	O
predicate	Task
detection	Task
into	O
a	O
joint	O
label	O
space	O
:	O
For	O
each	O
POS	O
tag	O
tag	O
which	O
is	O
observed	O
co	O
-	O
occurring	O
with	O
a	O
predicate	O
,	O
we	O
add	O
a	O
label	O
of	O
the	O
form	O
tag	O
:	O
predicate	O
.	O
Specifically	O
,	O
we	O
feed	O
the	O
representation	O
from	O
a	O
layer	O
preceding	O
the	O
syntactically	Method
-	Method
informed	Method
layer	Method
to	O
a	O
linear	Method
classifier	Method
to	O
produce	O
per	Metric
-	Metric
class	Metric
scores	Metric
for	O
token	Task
.	O
We	O
compute	O
locally	O
-	O
normalized	O
probabilities	O
using	O
the	O
softmax	O
function	O
:	O
,	O
where	O
is	O
a	O
label	O
in	O
the	O
joint	O
space	O
.	O
subsection	O
:	O
Predicting	Task
semantic	Task
roles	Task
Our	O
final	O
goal	O
is	O
to	O
predict	O
semantic	O
roles	O
for	O
each	O
predicate	O
in	O
the	O
sequence	O
.	O
We	O
score	O
each	O
predicate	O
against	O
each	O
token	O
in	O
the	O
sequence	O
using	O
a	O
bilinear	Method
operation	Method
,	O
producing	O
per	O
-	O
label	O
scores	O
for	O
each	O
token	O
for	O
each	O
predicate	O
,	O
with	O
predicates	O
and	O
syntax	O
determined	O
by	O
oracles	O
and	O
.	O
First	O
,	O
we	O
project	O
each	O
token	Method
representation	Method
to	O
a	O
predicate	Method
-	Method
specific	Method
representation	Method
and	O
a	O
role	Method
-	Method
specific	Method
representation	Method
.	O
We	O
then	O
provide	O
these	O
representations	O
to	O
a	O
bilinear	Method
transformation	Method
for	O
scoring	Task
.	O
So	O
,	O
the	O
role	O
label	O
scores	O
for	O
the	O
token	O
at	O
index	O
with	O
respect	O
to	O
the	O
predicate	O
at	O
index	O
(	O
i.e.	O
token	O
and	O
frame	O
)	O
are	O
given	O
by	O
:	O
s_ft	O
=	O
(	O
s_f^pred	O
)	O
^T	O
U	O
s_t^role	O
which	O
can	O
be	O
computed	O
in	O
parallel	O
across	O
all	O
semantic	O
frames	O
in	O
an	O
entire	O
minibatch	O
.	O
We	O
calculate	O
a	O
locally	O
normalized	O
distribution	O
over	O
role	O
labels	O
for	O
token	O
in	O
frame	O
using	O
the	O
softmax	Method
function	Method
:	O
.	O
At	O
test	O
time	O
,	O
we	O
perform	O
constrained	Method
decoding	Method
using	O
the	O
Viterbi	Method
algorithm	Method
to	O
emit	O
valid	O
sequences	O
of	O
BIO	O
tags	O
,	O
using	O
unary	O
scores	O
and	O
the	O
transition	O
probabilities	O
given	O
by	O
the	O
training	O
data	O
.	O
subsection	O
:	O
Training	O
We	O
maximize	O
the	O
sum	O
of	O
the	O
likelihoods	O
of	O
the	O
individual	O
tasks	O
.	O
In	O
order	O
to	O
maximize	O
our	O
model	O
’s	O
ability	O
to	O
leverage	O
syntax	O
,	O
during	O
training	O
we	O
clamp	O
to	O
the	O
gold	O
parse	O
(	O
)	O
and	O
to	O
gold	O
predicates	O
when	O
passing	O
parse	Method
and	Method
predicate	Method
representations	Method
to	O
later	O
layers	O
,	O
whereas	O
syntactic	Method
head	Method
prediction	Method
and	O
joint	Method
predicate	Method
/	Method
POS	Method
prediction	Method
are	O
conditioned	O
only	O
on	O
the	O
input	O
sequence	O
.	O
The	O
overall	O
objective	O
is	O
thus	O
:	O
_	O
t=1^T	O
[	O
Align∑	O
_	O
f=1^F	O
P	O
(	O
y_ft^role	O
P	O
_	O
G	O
,	O
V	O
_	O
G	O
,	O
X	O
)	O
+	O
P	O
(	O
y_t^prp	O
X	O
)	O
+	O
_	O
1	O
P	O
(	O
head	O
(	O
t	O
)	O
X	O
)	O
+	O
_	O
2	O
P	O
(	O
y_t^dep	O
P	O
_	O
G	O
,	O
X	O
)	O
]	O
where	O
and	O
are	O
penalties	O
on	O
the	O
syntactic	O
attention	O
loss	O
.	O
We	O
train	O
the	O
model	O
using	O
Nadam	Method
dozat2016incorporating	O
SGD	O
combined	O
with	O
the	O
learning	O
rate	O
schedule	O
in	O
vaswani2017attention	O
.	O
In	O
addition	O
to	O
MTL	Task
,	O
we	O
regularize	O
our	O
model	O
using	O
dropout	Method
srivastava2014dropout	O
.	O
We	O
use	O
gradient	Method
clipping	Method
to	O
avoid	O
exploding	O
gradients	O
bengio1994learning	O
,	O
pascanu2013on	O
.	O
Additional	O
details	O
on	O
optimization	Task
and	O
hyperparameters	O
are	O
included	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
section	O
:	O
Related	O
work	O
Early	O
approaches	O
to	O
SRL	Task
pradhan2005semantic	O
,	O
surdeanu2007combination	O
,	O
johansson2008dependency	O
,	O
toutanova2008global	O
focused	O
on	O
developing	O
rich	O
sets	O
of	O
linguistic	O
features	O
as	O
input	O
to	O
a	O
linear	Method
model	Method
,	O
often	O
combined	O
with	O
complex	O
constrained	Method
inference	Method
e.g.	O
with	O
an	O
ILP	Task
punyakanok2008importance	O
.	O
tackstrom2015efficient	O
showed	O
that	O
constraints	O
could	O
be	O
enforced	O
more	O
efficiently	O
using	O
a	O
clever	Method
dynamic	Method
program	Method
for	O
exact	Task
inference	Task
.	O
sutton2005joint	Method
modeled	O
syntactic	Task
parsing	Task
and	O
SRL	Task
jointly	O
,	O
and	O
lewis2015joint	O
jointly	O
modeled	O
SRL	Task
and	O
CCG	Task
parsing	Task
.	O
collobert2011natural	O
were	O
among	O
the	O
first	O
to	O
use	O
a	O
neural	Method
network	Method
model	Method
for	O
SRL	Task
,	O
a	O
CNN	Method
over	O
word	Method
embeddings	Method
which	O
failed	O
to	O
out	O
-	O
perform	O
non	Method
-	Method
neural	Method
models	Method
.	O
fitzgerald2015semantic	O
successfully	O
employed	O
neural	Method
networks	Method
by	O
embedding	O
lexicalized	O
features	O
and	O
providing	O
them	O
as	O
factors	O
in	O
the	O
model	O
of	O
tackstrom2015efficient	O
.	O
More	O
recent	O
neural	Method
models	Method
are	O
syntax	O
-	O
free	O
.	O
zhou2015end	O
,	O
marcheggiani2017simple	O
and	O
he2017deep	O
all	O
use	O
variants	O
of	O
deep	Method
LSTMs	Method
with	O
constrained	Method
decoding	Method
,	O
while	O
tan2018deep	O
apply	O
self	Method
-	Method
attention	Method
to	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
SRL	Task
with	O
gold	O
predicates	O
.	O
Like	O
this	O
work	O
,	O
he2017deep	O
present	O
end	O
-	O
to	O
-	O
end	O
experiments	O
,	O
predicting	Task
predicates	Task
using	O
an	O
LSTM	Method
,	O
and	O
he2018jointly	O
jointly	O
predict	O
SRL	Task
spans	O
and	O
predicates	O
in	O
a	O
model	O
based	O
on	O
that	O
of	O
lee2017end	O
,	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
predicted	O
predicate	O
SRL	Task
.	O
Concurrent	O
to	O
this	O
work	O
,	O
peters2018deep	O
and	O
he2018jointly	O
report	O
significant	O
gains	O
on	O
PropBank	O
SRL	Task
by	O
training	O
a	O
wide	Method
LSTM	Method
language	Method
model	Method
and	O
using	O
a	O
task	O
-	O
specific	O
transformation	O
of	O
its	O
hidden	Method
representations	Method
(	O
ELMo	Method
)	O
as	O
a	O
deep	O
,	O
and	O
computationally	O
expensive	O
,	O
alternative	O
to	O
typical	O
word	Method
embeddings	Method
.	O
We	O
find	O
that	O
LISA	Method
obtains	O
further	O
accuracy	Metric
increases	O
when	O
provided	O
with	O
ELMo	Method
word	Method
representations	Method
,	O
especially	O
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
.	O
Some	O
work	O
has	O
incorporated	O
syntax	O
into	O
neural	Method
models	Method
for	O
SRL	Task
.	O
roth2016neural	O
incorporate	O
syntax	O
by	O
embedding	O
dependency	O
paths	O
,	O
and	O
similarly	O
marcheggiani2017encoding	O
encode	O
syntax	O
using	O
a	O
graph	Method
CNN	Method
over	O
a	O
predicted	O
syntax	O
tree	O
,	O
out	O
-	O
performing	O
models	O
without	O
syntax	O
on	O
CoNLL	O
-	O
2009	O
.	O
These	O
works	O
are	O
limited	O
to	O
incorporating	O
partial	O
dependency	O
paths	O
between	O
tokens	O
whereas	O
our	O
technique	O
incorporates	O
the	O
entire	O
parse	O
.	O
Additionally	O
,	O
marcheggiani2017encoding	O
report	O
that	O
their	O
model	O
does	O
not	O
out	O
-	O
perform	O
syntax	Method
-	Method
free	Method
models	Method
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
,	O
a	O
setting	O
in	O
which	O
our	O
technique	O
excels	O
.	O
MTL	Task
caruana1993multitask	O
is	O
popular	O
in	O
NLP	Task
,	O
and	O
others	O
have	O
proposed	O
MTL	Task
models	O
which	O
incorporate	O
subsets	O
of	O
the	O
tasks	O
we	O
do	O
collobert2011natural	O
,	O
zhang2016stack	O
,	O
hashimoto2017joint	O
,	O
peng2017deep	O
,	O
swayamdipta2017	O
,	O
and	O
we	O
build	O
off	O
work	O
that	O
investigates	O
where	O
and	O
when	O
to	O
combine	O
different	O
tasks	O
to	O
achieve	O
the	O
best	O
results	O
sogaard2016deep	O
,	O
bingel2017identifying	O
,	O
alonso2017when	O
.	O
Our	O
specific	O
method	O
of	O
incorporating	O
supervision	Task
into	O
self	Task
-	Task
attention	Task
is	O
most	O
similar	O
to	O
the	O
concurrent	O
work	O
of	O
liu2018learning	O
,	O
who	O
use	O
edge	O
marginals	O
produced	O
by	O
the	O
matrix	Method
-	Method
tree	Method
algorithm	Method
as	O
attention	O
weights	O
for	O
document	Task
classification	Task
and	O
natural	Task
language	Task
inference	Task
.	O
The	O
question	O
of	O
training	O
on	O
gold	O
versus	O
predicted	O
labels	O
is	O
closely	O
related	O
to	O
learning	O
to	O
search	Task
daume2009search	O
,	O
ross2011reduction	O
,	O
chang2015learning	O
and	O
scheduled	Task
sampling	Task
bengio2015scheduled	O
,	O
with	O
applications	O
in	O
NLP	Task
to	O
sequence	Task
labeling	Task
and	O
transition	Task
-	Task
based	Task
parsing	Task
choi2011getting	O
,	O
goldberg2012dynamic	O
,	O
ballesteros2016training	O
.	O
Our	O
approach	O
may	O
be	O
interpreted	O
as	O
an	O
extension	O
of	O
teacher	Method
forcing	Method
williams1989learning	O
to	O
MTL	Task
.	O
We	O
leave	O
exploration	O
of	O
more	O
advanced	O
scheduled	Method
sampling	Method
techniques	Method
to	O
future	O
work	O
.	O
section	O
:	O
Experimental	O
results	O
We	O
present	O
results	O
on	O
the	O
CoNLL	Material
-	Material
2005	Material
shared	O
task	O
carreras2005introduction	O
and	O
the	O
CoNLL	Material
-	Material
2012	Material
English	Material
subset	Material
of	O
OntoNotes	O
5.0	O
pradhan2013towards	O
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
a	O
single	O
model	O
with	O
predicted	O
predicates	O
on	O
both	O
corpora	O
.	O
We	O
experiment	O
with	O
both	O
standard	O
pre	Method
-	Method
trained	Method
GloVe	Method
word	Method
embeddings	Method
pennington2014glove	O
and	O
pre	O
-	O
trained	O
ELMo	Method
representations	Method
with	O
fine	O
-	O
tuned	O
task	O
-	O
specific	O
parameters	O
peters2018deep	O
in	O
order	O
to	O
best	O
compare	O
to	O
prior	O
work	O
.	O
Hyperparameters	Method
that	O
resulted	O
in	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set	O
were	O
selected	O
via	O
a	O
small	O
grid	Method
search	Method
,	O
and	O
models	O
were	O
trained	O
for	O
a	O
maximum	O
of	O
4	O
days	O
on	O
one	O
TitanX	O
GPU	O
using	O
early	O
stopping	O
on	O
the	O
validation	O
set	O
.	O
We	O
convert	O
constituencies	O
to	O
dependencies	O
using	O
the	O
Stanford	O
head	O
rules	O
v3.5	O
deMarneffe2008	O
.	O
A	O
detailed	O
description	O
of	O
hyperparameter	O
settings	O
and	O
data	Task
pre	Task
-	Task
processing	Task
can	O
be	O
found	O
in	O
Appendix	O
[	O
reference	O
]	O
.	O
We	O
compare	O
our	O
LISA	Method
models	O
to	O
four	O
strong	O
baselines	O
:	O
For	O
experiments	O
using	O
predicted	O
predicates	O
,	O
we	O
compare	O
to	O
he2018jointly	O
and	O
the	O
ensemble	Method
model	Method
(	O
PoE	Method
)	O
from	O
he2017deep	O
,	O
as	O
well	O
as	O
a	O
version	O
of	O
our	O
own	O
self	Method
-	Method
attention	Method
model	Method
which	O
does	O
not	O
incorporate	O
syntactic	O
information	O
(	O
SA	Method
)	O
.	O
To	O
compare	O
to	O
more	O
prior	O
work	O
,	O
we	O
present	O
additional	O
results	O
on	O
CoNLL	Material
-	Material
2005	Material
with	O
models	O
given	O
gold	O
predicates	O
at	O
test	O
time	O
.	O
In	O
these	O
experiments	O
we	O
also	O
compare	O
to	O
tan2018deep	O
,	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
art	O
SRL	Task
model	Task
using	O
gold	O
predicates	O
and	O
standard	O
embeddings	Method
.	O
We	O
demonstrate	O
that	O
our	O
models	O
benefit	O
from	O
injecting	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
predicted	O
parses	O
at	O
test	O
time	O
(	O
+	O
D	O
&	O
M	O
)	O
by	O
fixing	O
the	O
attention	O
to	O
parses	O
predicted	O
by	O
dozat2017deep	O
,	O
the	O
winner	O
of	O
the	O
2017	O
CoNLL	Task
shared	Task
task	Task
zeman2017conll	O
which	O
we	O
re	O
-	O
train	O
using	O
ELMo	Method
embeddings	Method
.	O
In	O
all	O
cases	O
,	O
using	O
these	O
parses	O
at	O
test	O
time	O
improves	O
performance	O
.	O
We	O
also	O
evaluate	O
our	O
model	O
using	O
the	O
gold	O
syntactic	O
parse	O
at	O
test	O
time	O
(	O
+	O
Gold	O
)	O
,	O
to	O
provide	O
an	O
upper	O
bound	O
for	O
the	O
benefit	O
that	O
syntax	O
could	O
have	O
for	O
SRL	Task
using	O
LISA	Method
.	O
These	O
experiments	O
show	O
that	O
despite	O
LISA	Method
’s	O
strong	O
performance	O
,	O
there	O
remains	O
substantial	O
room	O
for	O
improvement	O
.	O
In	O
§	O
[	O
reference	O
]	O
we	O
perform	O
further	O
analysis	O
comparing	O
SRL	Task
models	Task
using	O
gold	O
and	O
predicted	O
parses	O
.	O
subsection	O
:	O
Semantic	Task
role	Task
labeling	Task
Table	O
[	O
reference	O
]	O
lists	O
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
on	O
the	O
CoNLL	Material
-	Material
2005	Material
development	O
and	O
test	O
sets	O
using	O
predicted	O
predicates	O
.	O
For	O
models	O
using	O
GloVe	Method
embeddings	Method
,	O
our	O
syntax	O
-	O
free	O
SA	Method
model	O
already	O
achieves	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
jointly	O
predicting	Task
predicates	Task
,	O
POS	O
and	O
SRL	Task
.	O
LISA	Method
with	O
its	O
own	O
parses	Method
performs	O
comparably	O
to	O
SA	Method
,	O
but	O
when	O
supplied	O
with	O
D	O
&	O
M	O
parses	O
LISA	Method
out	O
-	O
performs	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
2.5	O
F1	Metric
points	O
.	O
On	O
the	O
out	Material
-	Material
of	Material
-	Material
domain	Material
Brown	Material
test	Material
set	Material
,	O
LISA	Method
also	O
performs	O
comparably	O
to	O
its	O
syntax	Method
-	Method
free	Method
counterpart	Method
with	O
its	O
own	O
parses	O
,	O
but	O
with	O
D	O
&	O
M	O
parses	O
LISA	Method
performs	O
exceptionally	O
well	O
,	O
more	O
than	O
3.5	O
F1	Metric
points	O
higher	O
than	O
he2018jointly	O
.	O
Incorporating	O
ELMo	Method
embeddings	Method
improves	O
all	O
scores	O
.	O
The	O
gap	O
in	O
SRL	Task
F1	Metric
between	O
models	O
using	O
LISA	Method
and	O
D	O
&	O
M	O
parses	O
is	O
smaller	O
due	O
to	O
LISA	Method
’s	O
improved	O
parsing	Metric
accuracy	Metric
(	O
see	O
§	O
[	O
reference	O
]	O
)	O
,	O
but	O
LISA	Method
with	O
D	O
&	O
M	O
parses	O
still	O
achieves	O
the	O
highest	O
F1	Metric
:	O
nearly	O
1.0	O
absolute	O
F1	Metric
higher	O
than	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
art	O
on	O
WSJ	Method
,	O
and	O
more	O
than	O
2.0	O
F1	Metric
higher	O
on	O
Brown	O
.	O
In	O
both	O
settings	O
LISA	Method
leverages	O
domain	O
-	O
agnostic	O
syntactic	O
information	O
rather	O
than	O
over	O
-	O
fitting	O
to	O
the	O
newswire	O
training	O
data	O
which	O
leads	O
to	O
high	O
performance	O
even	O
on	O
out	O
-	O
of	O
-	O
domain	O
text	O
.	O
To	O
compare	O
to	O
more	O
prior	O
work	O
we	O
also	O
evaluate	O
our	O
models	O
in	O
the	O
artificial	Task
setting	Task
where	O
gold	O
predicates	O
are	O
provided	O
at	O
test	O
time	O
.	O
For	O
fair	O
comparison	O
we	O
use	O
GloVe	Method
embeddings	Method
,	O
provide	O
predicate	Method
indicator	Method
embeddings	Method
on	O
the	O
input	O
and	O
re	O
-	O
encode	O
the	O
sequence	O
relative	O
to	O
each	O
gold	O
predicate	O
.	O
Here	O
LISA	Method
still	O
excels	O
:	O
with	O
D	Method
&	Method
M	Method
parses	Method
,	O
LISA	Method
out	O
-	O
performs	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
more	O
than	O
2	O
F1	Metric
on	O
both	O
WSJ	O
and	O
Brown	O
.	O
Table	O
[	O
reference	O
]	O
reports	O
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
on	O
the	O
CoNLL	Material
-	Material
2012	Material
test	Material
set	Material
.	O
We	O
observe	O
performance	O
similar	O
to	O
that	O
observed	O
on	O
ConLL	Material
-	Material
2005	Material
:	O
Using	O
GloVe	Method
embeddings	Method
our	O
SA	Method
baseline	O
already	O
out	O
-	O
performs	O
he2018jointly	O
by	O
nearly	O
1.5	O
F1	Metric
.	O
With	O
its	O
own	O
parses	O
,	O
LISA	Method
slightly	O
under	O
-	O
performs	O
our	O
syntax	Method
-	Method
free	Method
model	Method
,	O
but	O
when	O
provided	O
with	O
stronger	O
D	O
&	O
M	O
parses	O
LISA	Method
out	O
-	O
performs	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
more	O
than	O
2.5	O
F1	Metric
.	O
Like	O
CoNLL	Material
-	Material
2005	Material
,	O
ELMo	Method
representations	Method
improve	O
all	O
models	O
and	O
close	O
the	O
F1	Metric
gap	O
between	O
models	O
supplied	O
with	O
LISA	Method
and	O
D	O
&	O
M	O
parses	O
.	O
On	O
this	O
dataset	O
ELMo	Method
also	O
substantially	O
narrows	O
the	O
difference	O
between	O
models	O
with	O
-	O
and	O
without	O
syntactic	O
information	O
.	O
This	O
suggests	O
that	O
for	O
this	O
challenging	O
dataset	O
,	O
ELMo	Method
already	O
encodes	O
much	O
of	O
the	O
information	O
available	O
in	O
the	O
D	Method
&	Method
M	Method
parses	Method
.	O
Yet	O
,	O
higher	O
accuracy	Metric
parses	Metric
could	O
still	O
yield	O
improvements	O
since	O
providing	O
gold	O
parses	O
increases	O
F1	Metric
by	O
4	O
points	O
even	O
with	O
ELMo	Method
embeddings	Method
.	O
subsection	O
:	O
Parsing	Task
,	O
POS	Task
and	Task
predicate	Task
detection	Task
We	O
first	O
report	O
the	O
labeled	Metric
and	Metric
unlabeled	Metric
attachment	Metric
scores	Metric
(	O
LAS	Metric
,	O
UAS	O
)	O
of	O
our	O
parsing	Method
models	Method
on	O
the	O
CoNLL	Material
-	Material
2005	Material
and	O
2012	Material
test	Material
sets	Material
(	O
Table	O
[	O
reference	O
]	O
)	O
with	O
GloVe	Method
(	O
)	O
and	O
ELMo	Method
(	Method
)	Method
embeddings	Method
.	O
D	Method
&	Method
M	Method
achieves	O
the	O
best	O
scores	O
.	O
Still	O
,	O
LISA	Method
’s	O
GloVe	O
UAS	O
is	O
comparable	O
to	O
popular	O
off	Method
-	Method
the	Method
-	Method
shelf	Method
dependency	Method
parsers	Method
such	O
as	O
spaCy	Method
,	O
and	O
with	O
ELMo	Method
embeddings	Method
comparable	O
to	O
the	O
standalone	Method
D	Method
&	Method
M	Method
parser	Method
.	O
The	O
difference	O
in	O
parse	Metric
accuracy	Metric
between	O
LISA	Method
and	O
D	Method
&	Method
M	Method
likely	O
explains	O
the	O
large	O
increase	O
in	O
SRL	Task
performance	O
we	O
see	O
from	O
decoding	Task
with	O
D	Method
&	Method
M	Method
parses	Method
in	O
that	O
setting	O
.	O
In	O
Table	O
[	O
reference	O
]	O
we	O
present	O
predicate	Metric
detection	Metric
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
on	O
the	O
CoNLL	Material
-	Material
2005	Material
and	O
2012	Material
test	Material
sets	Material
.	O
SA	Method
and	O
LISA	Method
with	O
and	O
without	O
ELMo	Method
attain	O
comparable	O
scores	O
so	O
we	O
report	O
only	O
LISA	Method
+	O
GloVe	O
.	O
We	O
compare	O
to	O
he2017deep	O
on	O
CoNLL	Material
-	Material
2005	Material
,	O
the	O
only	O
cited	O
work	O
reporting	O
comparable	O
predicate	O
detection	O
F1	Metric
.	O
LISA	Method
attains	O
high	O
predicate	Metric
detection	Metric
scores	Metric
,	O
above	O
97	O
F1	Metric
,	O
on	O
both	O
in	O
-	O
domain	O
datasets	O
,	O
and	O
out	O
-	O
performs	O
he2017deep	O
by	O
1.5	O
-	O
2	O
F1	Metric
points	O
even	O
on	O
the	O
out	O
-	O
of	O
-	O
domain	O
Brown	O
test	O
set	O
,	O
suggesting	O
that	O
multi	Method
-	Method
task	Method
learning	Method
works	O
well	O
for	O
SRL	Task
predicate	O
detection	O
.	O
subsection	O
:	O
Analysis	O
First	O
we	O
assess	O
SRL	Task
F1	Metric
on	O
sentences	O
divided	O
by	O
parse	Metric
accuracy	Metric
.	O
Table	O
[	O
reference	O
]	O
lists	O
average	O
SRL	Task
F1	Metric
(	O
across	O
sentences	O
)	O
for	O
the	O
four	O
conditions	O
of	O
LISA	Method
and	O
D	O
&	O
M	O
parses	O
being	O
correct	O
or	O
not	O
(	O
L±	O
,	O
D±	O
)	O
.	O
Both	O
parsers	Method
are	O
correct	O
on	O
26	O
%	O
of	O
sentences	O
.	O
Here	O
there	O
is	O
little	O
difference	O
between	O
any	O
of	O
the	O
models	O
,	O
with	O
LISA	Method
models	O
tending	O
to	O
perform	O
slightly	O
better	O
than	O
SA	Method
.	O
Both	O
parsers	Method
make	O
mistakes	O
on	O
the	O
majority	O
of	O
sentences	O
(	O
57	O
%	O
)	O
,	O
difficult	O
sentences	O
where	O
SA	Method
also	O
performs	O
the	O
worst	O
.	O
These	O
examples	O
are	O
likely	O
where	O
gold	Method
and	Method
D	Method
&	Method
M	Method
parses	Method
improve	O
the	O
most	O
over	O
other	O
models	O
in	O
overall	O
F1	Metric
:	O
Though	O
both	O
parsers	Method
fail	O
to	O
correctly	O
parse	O
the	O
entire	O
sentence	O
,	O
the	O
D	Method
&	Method
M	Method
parser	Method
is	O
less	O
wrong	O
(	O
87.5	O
vs.	O
85.7	O
average	O
LAS	O
)	O
,	O
leading	O
to	O
higher	O
SRL	Task
F1	Metric
by	O
about	O
1.5	O
average	O
F1	Metric
.	O
Following	O
he2017deep	O
,	O
we	O
next	O
apply	O
a	O
series	O
of	O
corrections	O
to	O
model	Task
predictions	Task
in	O
order	O
to	O
understand	O
which	O
error	O
types	O
the	O
gold	O
parse	O
resolves	O
:	O
e.g.	O
Fix	O
Labels	O
fixes	O
labels	O
on	O
spans	O
matching	O
gold	O
boundaries	O
,	O
and	O
Merge	O
Spans	O
merges	O
adjacent	O
predicted	O
spans	O
into	O
a	O
gold	O
span	O
.	O
[	O
scale=0.52	O
]	O
errors.pdf	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
see	O
that	O
much	O
of	O
the	O
performance	O
gap	O
between	O
the	O
gold	O
and	O
predicted	O
parses	O
is	O
due	O
to	O
span	O
boundary	O
errors	O
(	O
Merge	O
Spans	O
,	O
Split	O
Spans	O
and	O
Fix	O
Span	O
Boundary	O
)	O
,	O
which	O
supports	O
the	O
hypothesis	O
proposed	O
by	O
he2017deep	O
that	O
incorporating	O
syntax	O
could	O
be	O
particularly	O
helpful	O
for	O
resolving	O
these	O
errors	O
.	O
he2017deep	O
also	O
point	O
out	O
that	O
these	O
errors	O
are	O
due	O
mainly	O
to	O
prepositional	O
phrase	O
(	O
PP	O
)	O
attachment	O
mistakes	O
.	O
We	O
also	O
find	O
this	O
to	O
be	O
the	O
case	O
:	O
Figure	O
[	O
reference	O
]	O
shows	O
a	O
breakdown	O
of	O
split	O
/	O
merge	O
corrections	O
by	O
phrase	O
type	O
.	O
Though	O
the	O
number	O
of	O
corrections	O
decreases	O
substantially	O
across	O
phrase	O
types	O
,	O
the	O
proportion	O
of	O
corrections	O
attributed	O
to	O
PPs	O
remains	O
the	O
same	O
(	O
approx	O
.	O
50	O
%	O
)	O
even	O
after	O
providing	O
the	O
correct	O
PP	O
attachment	O
to	O
the	O
model	O
,	O
indicating	O
that	O
PP	O
span	O
boundary	O
mistakes	O
are	O
a	O
fundamental	O
difficulty	O
for	O
SRL	Task
.	O
[	O
scale=0.55	O
]	O
phrase_bar_percent.pdf	O
section	O
:	O
Conclusion	O
We	O
present	O
linguistically	Method
-	Method
informed	Method
self	Method
-	Method
attention	Method
:	O
a	O
multi	O
-	O
task	O
neural	Method
network	Method
model	Method
that	O
effectively	O
incorporates	O
rich	O
linguistic	O
information	O
for	O
semantic	Task
role	Task
labeling	Task
.	O
LISA	Method
out	O
-	O
performs	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
two	O
benchmark	O
SRL	Task
datasets	O
,	O
including	O
out	O
-	O
of	O
-	O
domain	O
.	O
Future	O
work	O
will	O
explore	O
improving	O
LISA	Method
’s	O
parsing	O
accuracy	O
,	O
developing	O
better	O
training	Method
techniques	Method
and	O
adapting	O
to	O
more	O
tasks	O
.	O
section	O
:	O
Acknowledgments	O
We	O
are	O
grateful	O
to	O
Luheng	O
He	O
for	O
helpful	O
discussions	O
and	O
code	O
,	O
Timothy	O
Dozat	O
for	O
sharing	O
his	O
code	O
,	O
and	O
to	O
the	O
NLP	Task
reading	O
groups	O
at	O
Google	O
and	O
UMass	O
and	O
the	O
anonymous	O
reviewers	O
for	O
feedback	O
on	O
drafts	O
of	O
this	O
work	O
.	O
This	O
work	O
was	O
supported	O
in	O
part	O
by	O
an	O
IBM	O
PhD	O
Fellowship	O
Award	O
to	O
E.S.	O
,	O
in	O
part	O
by	O
the	O
Center	O
for	O
Intelligent	Task
Information	Task
Retrieval	Task
,	O
and	O
in	O
part	O
by	O
the	O
National	O
Science	O
Foundation	O
under	O
Grant	O
Nos	O
.	O
DMR	O
-	O
1534431	O
and	O
IIS	O
-	O
1514053	O
.	O
Any	O
opinions	O
,	O
findings	O
,	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
those	O
of	O
the	O
sponsor	O
.	O
bibliography	O
:	O
References	O
appendix	O
:	O
Supplemental	O
Material	O
subsection	O
:	O
Supplemental	O
analysis	O
Here	O
we	O
continue	O
the	O
analysis	O
from	O
§	O
[	O
reference	O
]	O
.	O
All	O
experiments	O
in	O
this	O
section	O
are	O
performed	O
on	O
CoNLL	Material
-	Material
2005	Material
development	O
data	O
unless	O
stated	O
otherwise	O
.	O
First	O
,	O
we	O
compare	O
the	O
impact	O
of	O
Viterbi	Method
decoding	Method
with	O
LISA	Method
,	O
D	Method
&	Method
M	Method
,	O
and	O
gold	O
syntax	O
trees	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
finding	O
the	O
same	O
trends	O
across	O
both	O
datasets	O
.	O
We	O
find	O
that	O
Viterbi	Method
has	O
nearly	O
the	O
same	O
impact	O
for	O
LISA	Method
,	O
D	O
&	O
M	O
and	O
gold	O
parses	O
:	O
Gold	O
parses	O
provide	O
little	O
improvement	O
over	O
predicted	O
parses	O
in	O
terms	O
of	O
BIO	Metric
label	Metric
consistency	Metric
.	O
[	O
scale=0.52	O
]	O
f1_by_sent_len.pdf	O
[	O
scale=0.52	O
]	O
f1_by_pred_dist.pdf	O
We	O
also	O
assess	O
SRL	Task
F1	Metric
as	O
a	O
function	O
of	O
sentence	O
length	O
and	O
distance	O
from	O
span	O
to	O
predicate	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
we	O
see	O
that	O
providing	O
LISA	Method
with	O
gold	O
parses	O
is	O
particularly	O
helpful	O
for	O
sentences	O
longer	O
than	O
10	O
tokens	O
.	O
This	O
likely	O
directly	O
follows	O
from	O
the	O
tendency	O
of	O
syntactic	Method
parsers	Method
to	O
perform	O
worse	O
on	O
longer	O
sentences	O
.	O
With	O
respect	O
to	O
distance	O
between	O
arguments	O
and	O
predicates	O
,	O
(	O
Figure	O
[	O
reference	O
]	O
)	O
,	O
we	O
do	O
not	O
observe	O
this	O
same	O
trend	O
,	O
with	O
all	O
distances	O
performing	O
better	O
with	O
better	O
parses	Method
,	O
and	O
especially	O
gold	O
.	O
subsection	O
:	O
Supplemental	O
results	O
Due	O
to	O
space	O
constraints	O
in	O
the	O
main	O
paper	O
we	O
list	O
additional	O
experimental	O
results	O
here	O
.	O
Table	O
[	O
reference	O
]	O
lists	O
development	Metric
scores	Metric
on	O
the	O
CoNLL	Material
-	Material
2005	Material
dataset	O
with	O
predicted	O
predicates	O
,	O
which	O
follow	O
the	O
same	O
trends	O
as	O
the	O
test	O
data	O
.	O
subsection	O
:	O
Data	O
and	O
pre	O
-	O
processing	O
details	O
We	O
initialize	O
word	Method
embeddings	Method
with	O
100d	O
pre	Method
-	Method
trained	Method
GloVe	Method
embeddings	Method
trained	O
on	O
6	O
billion	O
tokens	O
of	O
Wikipedia	O
and	O
Gigaword	O
pennington2014glove	O
.	O
We	O
evaluate	O
the	O
SRL	Task
performance	O
of	O
our	O
models	O
using	O
the	O
srl	Method
-	Method
eval.pl	Method
script	Method
provided	O
by	O
the	O
CoNLL	Material
-	Material
2005	Material
shared	O
task	O
,	O
which	O
computes	O
segment	Metric
-	Metric
level	Metric
precision	Metric
,	O
recall	Metric
and	O
F1	Metric
score	O
.	O
We	O
also	O
report	O
the	O
predicate	Metric
detection	Metric
scores	Metric
output	O
by	O
this	O
script	O
.	O
We	O
evaluate	O
parsing	Task
using	O
the	O
eval.pl	Method
CoNLL	Method
script	Method
,	O
which	O
excludes	O
punctuation	O
.	O
We	O
train	O
distinct	O
D	O
&	O
M	O
parsers	O
for	O
CoNLL	Material
-	Material
2005	Material
and	O
CoNLL	Material
-	Material
2012	Material
.	O
Our	O
D	O
&	O
M	O
parsers	O
are	O
trained	O
and	O
validated	O
using	O
the	O
same	O
SRL	Task
data	O
splits	O
,	O
except	O
that	O
for	O
CoNLL	Material
-	Material
2005	Material
section	O
22	O
is	O
used	O
for	O
development	O
(	O
rather	O
than	O
24	O
)	O
,	O
as	O
this	O
section	O
is	O
typically	O
used	O
for	O
validation	Task
in	O
PTB	Task
parsing	Task
.	O
We	O
use	O
Stanford	O
dependencies	O
v3.5	O
deMarneffe2008	O
and	O
POS	O
tags	O
from	O
the	O
Stanford	O
CoreNLP	O
left3words	O
model	O
toutanova2003feature	O
.	O
We	O
use	O
the	O
pre	O
-	O
trained	O
ELMo	Method
models	Method
and	O
learn	O
task	O
-	O
specific	O
combinations	O
of	O
the	O
ELMo	Method
representations	Method
which	O
are	O
provided	O
as	O
input	O
instead	O
of	O
GloVe	O
embeddings	O
to	O
the	O
D	Method
&	Method
M	Method
parser	Method
with	O
otherwise	O
default	O
settings	O
.	O
subsubsection	O
:	O
CoNLL	Material
-	Material
2012	Material
We	O
follow	O
the	O
CoNLL	O
-	O
2012	O
split	O
used	O
by	O
he2018jointly	O
to	O
evaluate	O
our	O
models	O
,	O
which	O
uses	O
the	O
annotations	O
from	O
here	O
but	O
the	O
subset	O
of	O
those	O
documents	O
from	O
the	O
CoNLL	Method
-	Method
2012	Method
co	Method
-	Method
reference	Method
split	Method
described	O
here	O
pradhan2013towards	O
.	O
This	O
dataset	O
is	O
drawn	O
from	O
seven	O
domains	O
:	O
newswire	O
,	O
web	O
,	O
broadcast	O
news	O
and	O
conversation	O
,	O
magazines	O
,	O
telephone	O
conversations	O
,	O
and	O
text	O
from	O
the	O
bible	O
.	O
The	O
text	O
is	O
annotated	O
with	O
gold	O
part	O
-	O
of	O
-	O
speech	O
,	O
syntactic	O
constituencies	O
,	O
named	O
entities	O
,	O
word	O
sense	O
,	O
speaker	O
,	O
co	O
-	O
reference	O
and	O
semantic	O
role	O
labels	O
based	O
on	O
the	O
PropBank	O
guidelines	O
palmer2005proposition	O
.	O
Propositions	O
may	O
be	O
verbal	O
or	O
nominal	O
,	O
and	O
there	O
are	O
41	O
distinct	O
semantic	O
role	O
labels	O
,	O
excluding	O
continuation	O
roles	O
and	O
including	O
the	O
predicate	O
.	O
We	O
convert	O
the	O
semantic	O
proposition	O
and	O
role	Method
segmentations	Method
to	O
BIO	O
boundary	O
-	O
encoded	O
tags	O
,	O
resulting	O
in	O
129	O
distinct	O
BIO	O
-	O
encoded	O
tags	O
(	O
including	O
continuation	O
roles	O
)	O
.	O
subsubsection	O
:	O
CoNLL	Material
-	Material
2005	Material
The	O
CoNLL	Material
-	Material
2005	Material
data	O
carreras2005introduction	O
is	O
based	O
on	O
the	O
original	O
PropBank	O
corpus	O
palmer2005proposition	O
,	O
which	O
labels	O
the	O
Wall	O
Street	O
Journal	O
portion	O
of	O
the	O
Penn	O
TreeBank	O
corpus	O
(	O
PTB	O
)	O
marcus1993building	O
with	O
predicate	O
-	O
argument	O
structures	O
,	O
plus	O
a	O
challenging	O
out	O
-	O
of	O
-	O
domain	O
test	O
set	O
derived	O
from	O
the	O
Brown	O
corpus	O
francis1964manual	O
.	O
This	O
dataset	O
contains	O
only	O
verbal	O
predicates	O
,	O
though	O
some	O
are	O
multi	O
-	O
word	O
verbs	O
,	O
and	O
28	O
distinct	O
role	O
label	O
types	O
.	O
We	O
obtain	O
105	O
SRL	Task
labels	O
including	O
continuations	O
after	O
encoding	O
predicate	O
argument	O
segment	O
boundaries	O
with	O
BIO	O
tags	O
.	O
subsection	O
:	O
Optimization	Task
and	O
hyperparameters	O
We	O
train	O
the	O
model	O
using	O
the	O
Nadam	Method
dozat2016incorporating	O
algorithm	O
for	O
adaptive	Method
stochastic	Method
gradient	Method
descent	Method
(	O
SGD	Method
)	Method
,	O
which	O
combines	O
Adam	Method
kingma2014adam	O
SGD	O
with	O
Nesterov	Method
momentum	O
nesterov1983method	O
.	O
We	O
additionally	O
vary	O
the	O
learning	Metric
rate	Metric
as	O
a	O
function	O
of	O
an	O
initial	O
learning	Metric
rate	Metric
and	O
the	O
current	O
training	O
step	O
as	O
described	O
in	O
vaswani2017attention	O
using	O
the	O
following	O
function	O
:	O
lr	O
=	O
lr_0	O
(	O
step^	O
-	O
0.5	O
,	O
step	O
warm^	O
-	O
1.5	O
)	O
which	O
increases	O
the	O
learning	Metric
rate	Metric
linearly	O
for	O
the	O
first	O
training	O
steps	O
,	O
then	O
decays	O
it	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number	O
.	O
We	O
found	O
this	O
learning	Metric
rate	Metric
schedule	Metric
essential	O
for	O
training	O
the	O
self	Method
-	Method
attention	Method
model	Method
.	O
We	O
only	O
update	O
optimization	Method
moving	Method
-	Method
average	Method
accumulators	Method
for	O
parameters	O
which	O
receive	O
gradient	O
updates	O
at	O
a	O
given	O
step	O
.	O
In	O
all	O
of	O
our	O
experiments	O
we	O
used	O
initial	O
learning	Metric
rate	Metric
0.04	O
,	O
,	O
,	O
and	O
dropout	Metric
rates	Metric
of	O
0.1	O
everywhere	O
.	O
We	O
use	O
10	O
or	O
12	O
self	Method
-	Method
attention	Method
layers	Method
made	O
up	O
of	O
8	O
attention	O
heads	O
each	O
with	O
embedding	O
dimension	O
25	O
,	O
with	O
800d	Method
feed	Method
-	Method
forward	Method
projections	Method
.	O
In	O
the	O
syntactically	O
-	O
informed	O
attention	O
head	O
,	O
has	O
dimension	O
500	O
and	O
has	O
dimension	O
100	O
.	O
The	O
size	O
of	O
and	Method
representations	Method
and	O
the	O
representation	O
used	O
for	O
joint	Task
part	Task
-	Task
of	Task
-	Task
speech	Task
/	Task
predicate	Task
classification	Task
is	O
200	O
.	O
We	O
train	O
with	O
warmup	O
steps	O
and	O
clip	O
gradient	O
norms	O
to	O
1	O
.	O
We	O
use	O
batches	O
of	O
approximately	O
5000	O
tokens	O
.	O
