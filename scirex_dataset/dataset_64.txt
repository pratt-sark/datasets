document O
: O
Direct Method
Output Method
Connection Method
for O
a O
High Method
- Method
Rank Method
Language Method
Model Method
This O
paper O
proposes O
a O
state O
- O
of O
- O
the O
- O
art O
recurrent Method
neural Method
network Method
( Method
RNN Method
) Method
language Method
model Method
that O
combines O
probability O
distributions O
computed O
not O
only O
from O
a O
final O
RNN Method
layer Method
but O
also O
from O
middle Method
layers Method
. O
Our O
proposed O
method O
raises O
the O
expressive O
power O
of O
a O
language Method
model Method
based O
on O
the O
matrix O
factorization O
interpretation O
of O
language Task
modeling Task
introduced O
by O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
. O
The O
proposed O
method O
improves O
the O
current O
state O
- O
of O
- O
the O
- O
art O
language Method
model Method
and O
achieves O
the O
best O
score O
on O
the O
Penn Material
Treebank Material
and O
WikiText Material
- Material
2 Material
, O
which O
are O
the O
standard O
benchmark O
datasets O
. O
Moreover O
, O
we O
indicate O
our O
proposed O
method O
contributes O
to O
two O
application O
tasks O
: O
machine Task
translation Task
and O
headline Task
generation Task
. O
Our O
code O
is O
publicly O
available O
at O
: O
https: O
// O
github.com O
/ O
nttcslab O
- O
nlp O
/ O
doc_lmhttps: O
// O
github.com O
/ O
nttcslab O
- O
nlp O
/ O
doc_lm O
. O
section O
: O
Introduction O
Neural Method
network Method
language Method
models Method
have O
played O
a O
central O
role O
in O
recent O
natural Task
language Task
processing Task
( O
NLP Task
) O
advances O
. O
For O
example O
, O
neural Method
encoder Method
- Method
decoder Method
models Method
, O
which O
were O
successfully O
applied O
to O
various O
natural Task
language Task
generation Task
tasks Task
including O
machine Task
translation Task
, O
summarization Task
, O
and O
dialogue Task
, O
can O
be O
interpreted O
as O
conditional Method
neural Method
language Method
models Method
. O
Neural Method
language Method
models Method
also O
positively O
influence O
syntactic Task
parsing Task
. O
Moreover O
, O
such O
word Method
embedding Method
methods Method
as O
Skip Method
- Method
gram Method
and O
vLBL Method
originated O
from O
neural Method
language Method
models Method
designed O
to O
handle O
much O
larger O
vocabulary O
and O
data O
sizes O
. O
Neural Method
language Method
models Method
can O
also O
be O
used O
as O
contextualized Method
word Method
representations Method
. O
Thus O
, O
language Task
modeling Task
is O
a O
good O
benchmark O
task O
for O
investigating O
the O
general O
frameworks O
of O
neural Method
methods Method
in O
NLP Task
field O
. O
In O
language Task
modeling Task
, O
we O
compute O
joint O
probability O
using O
the O
product O
of O
conditional O
probabilities O
. O
Let O
be O
a O
word O
sequence O
with O
length O
: O
. O
We O
obtain O
the O
joint O
probability O
of O
word O
sequence O
as O
follows O
: O
is O
generally O
assumed O
to O
be O
in O
this O
literature O
, O
that O
is O
, O
, O
and O
thus O
we O
can O
ignore O
its O
calculation O
. O
See O
the O
implementation O
of O
DBLP O
: O
journals O
/ O
corr O
/ O
ZarembaSV14 O
, O
for O
an O
example O
. O
RNN Method
language Method
models Method
obtain O
conditional O
probability O
from O
the O
probability O
distribution O
of O
each O
word O
. O
To O
compute O
the O
probability O
distribution O
, O
RNN Method
language Method
models Method
encode O
sequence O
into O
a O
fixed O
- O
length O
vector O
and O
apply O
a O
transformation O
matrix O
and O
the O
softmax Method
function Method
. O
Previous O
researches O
demonstrated O
that O
RNN Method
language Method
models Method
achieve O
high O
performance O
by O
using O
several O
regularizations Method
and O
selecting O
appropriate O
hyperparameters O
. O
However O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
proved O
that O
existing O
RNN Method
language Method
models Method
have O
low O
expressive Metric
power Metric
due O
to O
the O
Softmax O
bottleneck O
, O
which O
means O
the O
output O
matrix O
of O
RNN Method
language Method
models Method
is O
low O
rank O
when O
we O
interpret O
the O
training O
of O
RNN Method
language Method
models Method
as O
a O
matrix Task
factorization Task
problem Task
. O
To O
solve O
the O
Softmax Task
bottleneck Task
, O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
proposed O
Mixture Method
of Method
Softmaxes Method
( O
MoS Method
) O
, O
which O
increases O
the O
rank O
of O
the O
matrix O
by O
combining O
multiple O
probability O
distributions O
computed O
from O
the O
encoded O
fixed O
- O
length O
vector O
. O
In O
this O
study O
, O
we O
propose O
Direct Method
Output Method
Connection Method
( O
DOC Method
) O
as O
a O
generalization Method
of Method
MoS. Method
For O
stacked Method
RNNs Method
, O
DOC Method
computes O
the O
probability O
distributions O
from O
the O
middle O
layers O
including O
input O
embeddings O
. O
In O
addition O
to O
raising O
the O
rank O
, O
the O
proposed O
method O
helps O
weaken O
the O
vanishing Task
gradient Task
problem Task
in O
backpropagation Method
because O
DOC Method
provides O
a O
shortcut O
connection O
to O
the O
output O
. O
We O
conduct O
experiments O
on O
standard O
benchmark O
datasets O
for O
language Task
modeling Task
: O
the O
Penn Material
Treebank Material
and O
WikiText Material
- Material
2 Material
. O
Our O
experiments O
demonstrate O
that O
DOC Method
outperforms O
MoS Method
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
perplexities Metric
on O
each O
dataset O
. O
Moreover O
, O
we O
investigate O
the O
effect O
of O
DOC Method
on O
two O
applications O
: O
machine Task
translation Task
and O
headline Task
generation Task
. O
We O
indicate O
that O
DOC Method
can O
improve O
the O
performance O
of O
an O
encoder Method
- Method
decoder Method
with O
an O
attention Method
mechanism Method
, O
which O
is O
a O
strong O
baseline O
for O
such O
applications O
. O
In O
addition O
, O
we O
conduct O
an O
experiment O
on O
the O
Penn Material
Treebank Material
constituency O
parsing O
task O
to O
investigate O
the O
effectiveness O
of O
DOC Method
. O
section O
: O
RNN Method
Language Method
Model Method
In O
this O
section O
, O
we O
briefly O
overview O
RNN Method
language Method
models Method
. O
Let O
be O
the O
vocabulary O
size O
and O
let O
be O
the O
probability O
distribution O
of O
the O
vocabulary O
at O
timestep O
. O
Moreover O
, O
let O
be O
the O
dimension O
of O
the O
hidden O
state O
of O
the O
- O
th O
RNN Method
, O
and O
let O
be O
the O
dimensions O
of O
the O
embedding O
vectors O
. O
Then O
the O
RNN Method
language Method
models Method
predict O
probability Method
distribution Method
by O
the O
following O
equation O
: O
where O
is O
a O
weight O
matrix O
, O
is O
a O
word Method
embedding Method
matrix Method
, O
is O
a O
one O
- O
hot O
vector O
of O
input O
word O
at O
timestep O
, O
and O
is O
the O
hidden O
state O
of O
the O
- O
th O
RNN Method
at O
timestep O
. O
We O
define O
at O
timestep O
as O
a O
zero O
vector O
: O
. O
Let O
represent O
an O
abstract O
function O
of O
an O
RNN Method
, O
which O
might O
be O
the O
Elman Method
network Method
, O
the O
Long Method
Short Method
- Method
Term Method
Memory Method
( O
LSTM Method
) O
, O
the O
Recurrent Method
Highway Method
Network Method
( O
RHN Method
) Method
, O
or O
any O
other O
RNN Method
variant Method
. O
In O
this O
research O
, O
we O
stack O
three O
LSTM Method
layers O
based O
on O
merityRegOpt O
because O
they O
achieved O
high O
performance O
. O
section O
: O
Language Task
Modeling Task
as O
Matrix Task
Factorization Task
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
indicated O
that O
the O
training O
of O
language Method
models Method
can O
be O
interpreted O
as O
a O
matrix Task
factorization Task
problem Task
. O
In O
this O
section O
, O
we O
briefly O
introduce O
their O
description O
. O
Let O
word O
sequence O
be O
context O
. O
Then O
we O
can O
regard O
a O
natural O
language O
as O
a O
finite O
set O
of O
the O
pairs O
of O
a O
context O
and O
its O
conditional O
probability O
distribution O
: O
, O
where O
is O
the O
number O
of O
possible O
contexts O
and O
is O
a O
variable O
representing O
a O
one O
- O
hot O
vector O
of O
a O
word O
. O
Here O
, O
we O
consider O
matrix O
that O
represents O
the O
true O
log O
probability O
distributions O
and O
matrix O
that O
contains O
the O
hidden O
states O
of O
the O
final O
RNN Method
layer Method
for O
each O
context O
: O
Then O
we O
obtain O
set O
of O
matrices O
, O
where O
is O
an O
all O
- O
ones O
matrix O
, O
and O
is O
a O
diagonal O
matrix O
. O
contains O
matrices O
that O
shifted O
each O
row O
of O
by O
an O
arbitrary O
real O
number O
. O
In O
other O
words O
, O
if O
we O
take O
a O
matrix O
from O
and O
apply O
the O
softmax Method
function Method
to O
each O
of O
its O
rows O
, O
we O
obtain O
a O
matrix O
that O
consists O
of O
true O
probability O
distributions O
. O
Therefore O
, O
for O
some O
, O
training O
RNN Method
language Method
models Method
is O
to O
find O
the O
parameters O
satisfying O
the O
following O
equation O
: O
Equation O
[ O
reference O
] O
indicates O
that O
training O
RNN Method
language Method
models Method
can O
also O
be O
interpreted O
as O
a O
matrix Task
factorization Task
problem Task
. O
In O
most O
cases O
, O
the O
rank O
of O
matrix O
is O
because O
is O
smaller O
than O
and O
in O
common O
RNN Method
language Method
models Method
. O
Thus O
, O
an O
RNN Method
language Method
model Method
can O
not O
express O
true O
distributions O
if O
is O
much O
smaller O
than O
. O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
also O
argued O
that O
is O
as O
high O
as O
vocabulary Metric
size Metric
based O
on O
the O
following O
two O
assumptions O
: O
Natural O
language O
is O
highly O
context O
- O
dependent O
. O
In O
addition O
, O
since O
we O
can O
imagine O
many O
kinds O
of O
contexts O
, O
it O
is O
difficult O
to O
assume O
a O
basis O
that O
represents O
a O
conditional O
probability O
distribution O
for O
any O
contexts O
. O
In O
other O
words O
, O
compressing Task
is O
difficult O
. O
Since O
we O
also O
have O
many O
kinds O
of O
semantic O
meanings O
, O
it O
is O
difficult O
to O
assume O
basic O
meanings O
that O
can O
create O
all O
other O
semantic O
meanings O
by O
such O
simple O
operations O
as O
addition O
and O
subtraction O
; O
compressing Task
is O
difficult O
. O
In O
summary O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
indicated O
that O
is O
much O
smaller O
than O
because O
its O
scale O
is O
usually O
and O
vocabulary O
size O
is O
at O
least O
. O
section O
: O
Proposed O
Method O
: O
Direct Method
Output Method
Connection Method
To O
construct O
a O
high O
- O
rank O
matrix O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
proposed O
Mixture Method
of Method
Softmaxes Method
( O
MoS Method
) O
. O
MoS Method
computes O
multiple O
probability O
distributions O
from O
the O
hidden O
state O
of O
final O
RNN Method
layer Method
and O
regards O
the O
weighted O
average O
of O
the O
probability Method
distributions Method
as O
the O
final O
distribution O
. O
In O
this O
study O
, O
we O
propose O
Direct Method
Output Method
Connection Method
( O
DOC Method
) O
, O
which O
is O
a O
generalization Method
method Method
of O
MoS. O
DOC Method
computes O
probability O
distributions O
from O
the O
middle O
layers O
in O
addition O
to O
the O
final O
layer O
. O
In O
other O
words O
, O
DOC Method
directly O
connects O
the O
middle O
layers O
to O
the O
output O
. O
Figure O
[ O
reference O
] O
shows O
an O
overview O
of O
DOC Method
, O
that O
uses O
the O
middle O
layers O
( O
including O
word O
embeddings O
) O
to O
compute O
the O
probability O
distributions O
. O
Figure O
[ O
reference O
] O
computes O
three O
probability O
distributions O
from O
all O
the O
layers O
, O
but O
we O
can O
vary O
the O
number O
of O
probability O
distributions O
for O
each O
layer O
and O
select O
some O
layers O
to O
avoid O
. O
In O
our O
experiments O
, O
we O
search O
for O
the O
appropriate O
number O
of O
probability O
distributions O
for O
each O
layer O
. O
Formally O
, O
instead O
of O
Equation O
[ O
reference O
] O
, O
DOC Method
computes O
the O
output O
probability O
distribution O
at O
timestep O
by O
the O
following O
equation O
: O
where O
is O
a O
weight O
for O
each O
probability O
distribution O
, O
is O
a O
vector O
computed O
from O
each O
hidden O
state O
, O
and O
is O
a O
weight Method
matrix Method
. O
Thus O
, O
is O
the O
weighted Method
average Method
of Method
probability Method
distributions Method
. O
We O
define O
the O
diagonal O
matrix O
whose O
elements O
are O
weight O
for O
each O
context O
as O
. O
Then O
we O
obtain O
matrix O
: O
where O
is O
a O
matrix O
whose O
rows O
are O
vector O
. O
can O
be O
an O
arbitrary O
high O
rank O
because O
the O
righthand O
side O
of O
Equation O
[ O
reference O
] O
computes O
not O
only O
the O
matrix Method
multiplication Method
but O
also O
a O
nonlinear O
function O
. O
Therefore O
, O
an O
RNN Method
language Method
model Method
with O
DOC Method
can O
output O
a O
distribution O
matrix O
whose O
rank O
is O
identical O
to O
one O
of O
the O
true O
distributions O
. O
In O
other O
words O
, O
is O
a O
better O
approximation O
of O
than O
the O
output O
of O
a O
standard O
RNN Method
language Method
model Method
. O
Next O
we O
describe O
how O
to O
acquire O
weight O
and O
vector O
. O
Let O
be O
a O
vector O
whose O
elements O
are O
weight O
. O
Then O
we O
compute O
from O
the O
hidden O
state O
of O
the O
final O
RNN Method
layer Method
: O
where O
is O
a O
weight O
matrix O
. O
We O
next O
compute O
from O
the O
hidden O
state O
of O
the O
- Method
th Method
RNN Method
layer Method
: O
where O
is O
a O
weight O
matrix O
. O
In O
addition O
, O
let O
be O
the O
number O
of O
from O
. O
Then O
we O
define O
the O
sum O
of O
for O
all O
as O
; O
that O
is O
, O
. O
In O
short O
, O
DOC Method
computes O
probability O
distributions O
from O
all O
the O
layers O
, O
including O
the O
input O
embedding O
( O
) O
. O
For O
, O
DOC Method
becomes O
identical O
to O
MoS. O
In O
addition O
to O
increasing O
the O
rank O
, O
we O
expect O
that O
DOC Method
weakens O
the O
vanishing Task
gradient Task
problem Task
during O
backpropagation Method
because O
a O
middle Method
layer Method
is O
directly O
connected O
to O
the O
output O
, O
such O
as O
with O
the O
auxiliary Method
classifiers Method
described O
in O
43022 O
. O
For O
a O
network O
that O
computes O
the O
weights O
for O
several O
vectors O
, O
such O
as O
Equation O
[ O
reference O
] O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
ShazeerMMDLHD17 O
indicated O
that O
it O
often O
converges O
to O
a O
state O
where O
it O
always O
produces O
large O
weights O
for O
few O
vectors O
. O
In O
fact O
, O
we O
observed O
that O
DOC Method
tends O
to O
assign O
large O
weights O
to O
shallow O
layers O
. O
To O
prevent O
this O
phenomenon O
, O
we O
compute O
the O
coefficient O
of O
variation O
of O
Equation O
[ O
reference O
] O
in O
each O
mini O
- O
batch O
as O
a O
regularization Method
term Method
following O
DBLP Method
: O
journals O
/ O
corr O
/ O
ShazeerMMDLHD17 O
. O
In O
other O
words O
, O
we O
try O
to O
adjust O
the O
sum O
of O
the O
weights O
for O
each O
probability O
distribution O
with O
identical O
values O
in O
each O
mini O
- O
batch O
. O
Formally O
, O
we O
compute O
the O
following O
equation O
for O
a O
mini Task
- Task
batch Task
consisting O
of O
: O
where O
functions O
and O
are O
functions O
that O
respectively O
return O
an O
input O
’s O
standard O
deviation O
and O
its O
average O
. O
In O
the O
training O
step O
, O
we O
add O
multiplied O
by O
weight O
coefficient O
to O
the O
loss O
function O
. O
section O
: O
Experiments O
on O
Language Task
Modeling Task
We O
investigate O
the O
effect O
of O
DOC Method
on O
the O
language Task
modeling Task
task O
. O
In O
detail O
, O
we O
conduct O
word Task
- Task
level Task
prediction Task
experiments O
and O
show O
that O
DOC Method
improves O
the O
performance O
of O
MoS Method
, O
which O
only O
uses O
the O
final O
layer O
to O
compute O
the O
probability O
distributions O
. O
Moreover O
, O
we O
evaluate O
various O
combinations O
of O
layers O
to O
explore O
which O
combination O
achieves O
the O
best O
score O
. O
subsection O
: O
Datasets O
We O
used O
the O
Penn Material
Treebank Material
( O
PTB Material
) O
and O
WikiText Material
- Material
2 Material
datasets O
, O
which O
are O
the O
standard O
benchmark O
datasets O
for O
the O
word O
- O
level O
language Task
modeling Task
task O
. O
DBLP O
: O
conf O
/ O
interspeech O
/ O
MikolovKBCK10 O
and O
DBLP O
: O
journals O
/ O
corr O
/ O
MerityXBS16 O
respectively O
published O
preprocessed Material
PTB Material
and O
WikiText Material
- Material
2 Material
datasets O
. O
Table O
[ O
reference O
] O
describes O
their O
statistics O
. O
We O
used O
these O
preprocessed O
datasets O
for O
fair O
comparisons O
with O
previous O
studies O
. O
subsection O
: O
Hyperparameters O
Our O
implementation O
is O
based O
on O
the O
averaged Method
stochastic Method
gradient Method
descent Method
Weight Method
- Method
Dropped Method
LSTM Method
( O
AWD Method
- Method
LSTM Method
) O
proposed O
by O
merityRegOpt O
. O
AWD Method
- Method
LSTM Method
consists O
of O
three O
LSTMs Method
with O
various O
regularizations Method
. O
For O
the O
hyperparameters O
, O
we O
used O
the O
same O
values O
as O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
except O
for O
the O
dropout Metric
rate Metric
for O
vector O
and O
the O
non O
- O
monotone O
interval O
. O
Since O
we O
found O
that O
the O
dropout Metric
rate Metric
for O
vector O
greatly O
influences O
in O
Equation O
[ O
reference O
] O
, O
we O
varied O
it O
from O
to O
with O
intervals O
. O
We O
selected O
because O
this O
value O
achieved O
the O
best O
score O
on O
the O
PTB Material
validation Material
dataset Material
. O
For O
the O
non O
- O
monotone O
interval O
, O
we O
adopted O
the O
same O
value O
as O
fraternal O
. O
Table O
[ O
reference O
] O
summarizes O
the O
hyperparameters O
of O
our O
experiments O
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
shows O
the O
perplexities Metric
of O
AWD O
- O
LSTM Method
with O
DOC Method
on O
the O
PTB Material
dataset Material
. O
Each O
value O
of O
columns O
represents O
the O
number O
of O
probability O
distributions O
from O
hidden O
state O
. O
To O
find O
the O
best O
combination O
, O
we O
varied O
the O
number O
of O
probability O
distributions O
from O
each O
layer O
by O
fixing O
their O
total O
to O
20 O
: O
. O
Moreover O
, O
the O
top O
row O
of O
Table O
[ O
reference O
] O
shows O
the O
perplexity Metric
of O
AWD Method
- Method
LSTM Method
with Method
MoS Method
reported O
in O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
for O
comparison O
. O
Table O
[ O
reference O
] O
indicates O
that O
language Method
models Method
using O
middle Method
layers Method
outperformed O
one O
using O
only O
the O
final O
layer O
. O
In O
addition O
, O
Table O
[ O
reference O
] O
shows O
that O
increasing O
the O
distributions O
from O
the O
final O
layer O
( O
) O
degraded O
the O
score O
from O
the O
language Method
model Method
with O
( O
the O
top O
row O
of O
Table O
[ O
reference O
] O
) O
. O
Thus O
, O
to O
obtain O
a O
superior O
language Method
model Method
, O
we O
should O
not O
increase O
the O
number O
of O
distributions O
from O
the O
final O
layer O
; O
we O
should O
instead O
use O
the O
middle O
layers O
, O
as O
with O
our O
proposed O
DOC Method
. O
Table O
[ O
reference O
] O
shows O
that O
the O
setting O
achieved O
the O
best O
performance O
and O
the O
other O
settings O
with O
shallow O
layers O
have O
a O
little O
effect O
. O
This O
result O
implies O
that O
we O
need O
some O
layers O
to O
output O
accurate O
distributions O
. O
In O
fact O
, O
most O
previous O
studies O
adopted O
two O
LSTM Method
layers O
for O
language Task
modeling Task
. O
This O
suggests O
that O
we O
need O
at O
least O
two O
layers O
to O
obtain O
high O
- O
quality O
distributions O
. O
For O
the O
setting O
, O
we O
explored O
the O
effect O
of O
in O
. O
Although O
Table O
[ O
reference O
] O
shows O
that O
achieved O
the O
best O
perplexity Metric
, O
the O
effect O
is O
not O
consistent O
. O
Table O
[ O
reference O
] O
shows O
the O
coefficient O
of O
variation O
of O
Equation O
[ O
reference O
] O
, O
i.e. O
, O
in O
the O
PTB Material
dataset Material
. O
This O
table O
demonstrates O
that O
the O
coefficient O
of O
variation O
decreases O
with O
growth O
in O
. O
In O
other O
words O
, O
the O
model O
trained O
with O
a O
large O
assigns O
balanced O
weights O
to O
each O
probability O
distribution O
. O
These O
results O
indicate O
that O
it O
is O
not O
always O
necessary O
to O
equally O
use O
each O
probability O
distribution O
, O
but O
we O
can O
acquire O
a O
better O
model O
in O
some O
. O
Hereafter O
, O
we O
refer O
to O
the O
setting O
that O
achieved O
the O
best O
score O
( O
) O
as O
AWD Method
- Method
LSTM Method
- Method
DOC Method
. O
Table O
[ O
reference O
] O
shows O
the O
ranks O
of O
matrices O
containing O
log O
probability O
distributions O
from O
each O
method O
. O
In O
other O
words O
, O
Table O
[ O
reference O
] O
describes O
in O
Equation O
[ O
reference O
] O
for O
each O
method O
. O
As O
shown O
by O
this O
table O
, O
the O
output O
of O
AWD Method
- Method
LSTM Method
is O
restricted O
to O
. O
In O
contrast O
, O
AWD Method
- Method
LSTM Method
- Method
MoS Method
and O
AWD Method
- Method
LSTM Method
- Method
DOC Method
outputted O
matrices O
whose O
ranks O
equal O
the O
vocabulary O
size O
. O
This O
fact O
indicates O
that O
DOC Method
( O
including O
MoS Method
) O
can O
output O
the O
same O
matrix O
as O
the O
true O
distributions O
in O
view O
of O
a O
rank O
. O
Figure O
[ O
reference O
] O
illustrates O
the O
learning O
curves O
of O
each O
method O
on O
PTB Material
. O
This O
figure O
contains O
the O
validation Metric
scores Metric
of O
AWD Method
- Method
LSTM Method
, O
AWD Method
- Method
LSTM Method
- Method
MoS Method
, O
and O
AWD Method
- Method
LSTM Method
- Method
DOC Method
at O
each O
training O
epoch O
. O
We O
trained O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS Method
by O
setting O
the O
non O
- O
monotone O
interval O
to O
60 O
, O
as O
with O
AWD Method
- Method
LSTM Method
- Method
DOC Method
. O
In O
other O
words O
, O
we O
used O
hyperparameters O
identical O
to O
the O
original O
ones O
to O
train O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS Method
, O
except O
for O
the O
non O
- O
monotone O
interval O
. O
We O
note O
that O
the O
optimization Method
method Method
converts O
the O
ordinary Method
stochastic Method
gradient Method
descent Method
( O
SGD Method
) O
into O
the O
averaged O
SGD Method
at O
the O
point O
where O
convergence O
almost O
occurs O
. O
In O
Figure O
[ O
reference O
] O
, O
the O
turning O
point O
is O
the O
epoch O
when O
each O
method O
drastically O
decreases O
the O
perplexity Metric
. O
Figure O
[ O
reference O
] O
shows O
that O
each O
method O
similarly O
reduces O
the O
perplexity Metric
at O
the O
beginning O
. O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS Method
were O
slow O
to O
decrease O
the O
perplexity Metric
from O
50 O
epochs O
. O
In O
contrast O
, O
AWD Method
- Method
LSTM Method
- Method
DOC Method
constantly O
decreased O
the O
perplexity Metric
and O
achieved O
a O
lower O
value O
than O
the O
other O
methods O
with O
ordinary O
SGD Method
. O
Therefore O
, O
we O
conclude O
that O
DOC Method
positively O
affects O
the O
training O
of O
language Task
modeling Task
. O
Table O
[ O
reference O
] O
shows O
the O
AWD Method
- Method
LSTM Method
, O
AWD Method
- Method
LSTM Method
- Method
MoS Method
, O
and O
AWD Method
- Method
LSTM Method
- Method
DOC Method
results O
in O
our O
configurations O
. O
For O
AWD Method
- Method
LSTM Method
- Method
MoS Method
, O
we O
trained O
our O
implementation O
with O
the O
same O
dropout Metric
rates Metric
as O
AWD Method
- Method
LSTM Method
- Method
DOC Method
for O
a O
fair O
comparison O
. O
AWD Method
- Method
LSTM Method
- Method
DOC Method
outperformed O
both O
the O
original O
AWD Method
- Method
LSTM Method
- Method
MoS Method
and O
our O
implementation O
. O
In O
other O
words O
, O
DOC Method
outperformed O
MoS. Method
Since O
the O
averaged O
SGD Method
uses O
the O
averaged O
parameters O
from O
each O
update O
step O
, O
the O
parameters O
of O
the O
early O
steps O
are O
harmful O
to O
the O
final O
parameters O
. O
Therefore O
, O
when O
the O
model O
converges O
, O
recent O
studies O
and O
ours O
eliminate O
the O
history O
of O
and O
then O
retrains O
the O
model O
. O
merityRegOpt O
referred O
to O
this O
retraining Method
process Method
as O
fine Task
- Task
tuning Task
. O
Although O
most O
previous O
studies O
only O
conducted O
fine O
- O
tuning O
once O
, O
fraternal O
argued O
that O
two O
fine Method
- Method
tunings Method
provided O
additional O
improvement O
. O
Thus O
, O
we O
repeated O
fine O
- O
tuning O
until O
we O
achieved O
no O
more O
improvements O
in O
the O
validation O
data O
. O
We O
refer O
to O
the O
model O
as O
AWD O
- O
LSTM Method
- O
DOC Method
( O
fin O
) O
in O
Table O
[ O
reference O
] O
, O
which O
shows O
that O
repeated O
fine O
- O
tunings O
improved O
the O
perplexity Metric
by O
about O
0.5 O
. O
Tables O
[ O
reference O
] O
and O
[ O
reference O
] O
respectively O
show O
the O
perplexities Metric
of O
AWD Method
- Method
LSTM Method
- Method
DOC Method
and O
previous O
studies O
on O
PTB Material
and O
WikiText Material
- Material
2 Material
. O
These O
tables O
show O
that O
AWD Method
- Method
LSTM Method
- Method
DOC Method
achieved O
the O
best O
perplexity Metric
. O
AWD Method
- Method
LSTM Method
- Method
DOC Method
improved O
the O
perplexity Metric
by O
almost O
2.0 O
on O
PTB Material
and O
3.5 O
on O
WikiText Material
- Material
2 Material
from O
the O
state O
- O
of O
- O
the O
- O
art O
scores O
. O
The O
ensemble Method
technique Method
provided O
further O
improvement O
, O
as O
described O
in O
previous O
studies O
, O
and O
improved O
the O
perplexity Metric
by O
at O
least O
4 O
points O
on O
both O
datasets O
. O
Finally O
, O
the O
ensemble O
of O
the O
repeated Method
finetuning Method
models Method
achieved O
47.17 O
on O
the O
PTB Material
test O
and O
53.09 O
on O
the O
WikiText Material
- Material
2 Material
test O
. O
section O
: O
Experiments O
on O
Application Task
Tasks Task
As O
described O
in O
Section O
[ O
reference O
] O
, O
a O
neural Method
encoder Method
- Method
decoder Method
model Method
can O
be O
interpreted O
as O
a O
conditional Method
language Method
model Method
. O
To O
investigate O
the O
effect O
of O
DOC Method
on O
an O
encoder Method
- Method
decoder Method
model Method
, O
we O
incorporate O
DOC Method
into O
the O
decoder Method
and O
examine O
its O
performance O
. O
subsection O
: O
Dataset O
We O
conducted O
experiments O
on O
machine Task
translation Task
and Task
headline Task
generation Task
tasks Task
. O
For O
machine Task
translation Task
, O
we O
used O
two O
kinds O
of O
sentence O
pairs O
( O
English O
- O
German O
and O
English O
- O
French O
) O
in O
the O
IWSLT O
2016 O
dataset O
. O
The O
training O
set O
respectively O
contains O
about O
189 O
K O
and O
208 O
K O
sentence O
pairs O
of O
English O
- O
German O
and O
English O
- O
French O
. O
We O
experimented O
in O
four O
settings O
: O
from O
English O
to O
German O
( O
En O
- O
De O
) O
, O
its O
reverse O
( O
De O
- O
En O
) O
, O
from O
English O
to O
French O
( O
En O
- O
Fr O
) O
, O
and O
its O
reverse O
( O
Fr O
- O
En O
) O
. O
Headline Task
generation Task
is O
a O
task O
that O
creates O
a O
short Task
summarization Task
of O
an O
input O
sentence O
. O
rush O
- O
chopra O
- O
weston:2015:EMNLP O
constructed O
a O
headline Method
generation Method
dataset Method
by O
extracting O
pairs O
of O
first O
sentences O
of O
news O
articles O
and O
their O
headlines O
from O
the O
annotated O
English O
Gigaword O
corpus O
. O
They O
also O
divided O
the O
extracted O
sentence O
- O
headline O
pairs O
into O
three O
parts O
: O
training O
, O
validation O
, O
and O
test O
sets O
. O
The O
training O
set O
contains O
about O
3.8 O
M O
sentence O
- O
headline O
pairs O
. O
For O
our O
evaluation O
, O
we O
used O
the O
test O
set O
constructed O
by O
zhou O
- O
EtAl:2017:Long O
because O
the O
one O
constructed O
by O
rush O
- O
chopra O
- O
weston:2015:EMNLP O
contains O
some O
invalid O
instances O
, O
as O
reported O
in O
zhou O
- O
EtAl:2017:Long O
. O
subsection O
: O
Encoder Method
- Method
Decoder Method
Model Method
For O
the O
base O
model O
, O
we O
adopted O
an O
encoder Method
- Method
decoder Method
with O
an O
attention Method
mechanism Method
described O
in O
kiyono Method
. O
The O
encoder O
consists O
of O
a O
2 O
- O
layer O
bidirectional O
LSTM Method
, O
and O
the O
decoder Method
consists O
of O
a O
2 O
- O
layer O
LSTM Method
with O
attention Method
proposed O
by O
luong O
- O
pham O
- O
manning:2015:EMNLP O
. O
We O
interpreted O
the O
layer O
after O
computing O
the O
attention O
as O
the O
3rd O
layer O
of O
the O
decoder Method
. O
We O
refer O
to O
this O
encoder Method
- Method
decoder Method
as O
EncDec Method
. O
For O
the O
hyperparameters O
, O
we O
followed O
the O
setting O
of O
kiyono Method
except O
for O
the O
sizes O
of O
hidden O
states O
and O
embeddings O
. O
We O
used O
500 O
for O
machine Task
translation Task
and O
400 O
for O
headline Task
generation Task
. O
We O
constructed O
a O
vocabulary O
set O
by O
using O
Byte Method
- Method
Pair Method
- Method
Encoding Method
( O
BPE Method
) O
. O
We O
set O
the O
number O
of O
BPE Method
merge O
operations O
at O
16 O
K O
for O
the O
machine Task
translation Task
and O
5 O
K O
for O
the O
headline Task
generation Task
. O
In O
this O
experiment O
, O
we O
compare O
DOC Method
to O
the O
base O
EncDec Method
. O
We O
prepared O
two O
DOC Method
settings O
: O
using O
only O
the O
final O
layer O
, O
that O
is O
, O
a O
setting O
that O
is O
identical O
to O
MoS Method
, O
and O
using O
both O
the O
final O
and O
middle O
layers O
. O
We O
used O
the O
2nd O
and O
3rd O
layers O
in O
the O
latter O
setting O
because O
this O
case O
achieved O
the O
best O
performance O
on O
the O
language Task
modeling Task
task O
in O
Section O
[ O
reference O
] O
. O
We O
set O
and O
. O
For O
this O
experiment O
, O
we O
modified O
a O
publicly O
available O
encode Method
- Method
decoder Method
implementation Method
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
shows O
the O
BLEU Metric
scores Metric
of O
each O
method O
. O
Since O
an O
initial O
value O
often O
drastically O
varies O
the O
result O
of O
a O
neural Method
encoder Method
- Method
decoder Method
, O
we O
reported O
the O
average O
of O
three O
models O
trained O
from O
different O
initial O
values O
and O
random O
seeds O
. O
Table O
[ O
reference O
] O
indicates O
that O
EncDec O
+ O
DOC Method
outperformed O
EncDec Method
. O
Table O
[ O
reference O
] O
shows O
the O
ROUGE O
F1 Metric
scores Metric
of O
each O
method O
. O
In O
addition O
to O
the O
results O
of O
our O
implementations O
( O
the O
upper O
part O
) O
, O
the O
lower O
part O
represents O
the O
published O
scores O
reported O
in O
previous O
studies O
. O
For O
the O
upper O
part O
, O
we O
reported O
the O
average O
of O
three O
models O
( O
as O
in O
Table O
[ O
reference O
] O
) O
. O
EncDec O
+ O
DOC Method
outperformed O
EncDec Method
on O
all O
scores O
. O
Moreover O
, O
EncDec Method
outperformed O
the O
state O
- O
of O
- O
the O
- O
art O
method O
on O
the O
ROUGE Metric
- Metric
2 Metric
and O
ROUGE Metric
- Metric
L Metric
F1 Metric
scores Metric
. O
In O
other O
words O
, O
our O
baseline O
is O
already O
very O
strong O
. O
We O
believe O
that O
this O
is O
because O
we O
adopted O
a O
larger O
embedding O
size O
than O
zhou O
- O
EtAl:2017:Long O
. O
It O
is O
noteworthy O
that O
DOC Method
improved O
the O
performance O
of O
EncDec Method
even O
though O
EncDec Method
is O
very O
strong O
. O
These O
results O
indicate O
that O
DOC Method
positively O
influences O
a O
neural Method
encoder Method
- Method
decoder Method
model Method
. O
Using O
the O
middle O
layer O
also O
yields O
further O
improvement O
because O
EncDec O
+ O
DOC Method
( O
) O
outperformed O
EncDec O
+ O
DOC Method
( O
) O
. O
section O
: O
Experiments O
on O
Constituency Task
Parsing Task
choe O
- O
charniak:2016:EMNLP2016 O
achieved O
high O
F1 Metric
scores Metric
on O
the O
Penn Material
Treebank Material
constituency O
parsing O
task O
by O
transforming O
candidate O
trees O
into O
a O
symbol O
sequence O
( O
S O
- O
expression O
) O
and O
reranking O
them O
based O
on O
the O
perplexity Metric
obtained O
by O
a O
neural Method
language Method
model Method
. O
To O
investigate O
the O
effectiveness O
of O
DOC Method
, O
we O
evaluate O
our O
language Method
models Method
following O
their O
configurations O
. O
subsection O
: O
Dataset O
We O
used O
the O
Wall O
Street O
Journal O
of O
the O
Penn Material
Treebank Material
dataset O
. O
We O
used O
the O
section O
2 O
- O
21 O
for O
training O
, O
22 O
for O
validation Task
, O
and O
23 O
for O
testing O
. O
We O
applied O
the O
preprocessing Method
codes Method
of O
choe O
- O
charniak:2016:EMNLP2016 O
to O
the O
dataset O
and O
converted O
a O
token O
that O
appears O
fewer O
than O
ten O
times O
in O
the O
training O
dataset O
into O
a O
special O
token O
unk O
. O
For O
reranking Task
, O
we O
prepared O
500 O
candidates O
obtained O
by O
the O
Charniak Method
parser Method
. O
subsection O
: O
Models O
We O
compare O
AWD Method
- Method
LSTM Method
- Method
DOC Method
with O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS Method
. O
We O
trained O
each O
model O
with O
the O
same O
hyperparameters O
from O
our O
language Task
modeling Task
experiments O
( O
Section O
[ O
reference O
] O
) O
. O
We O
selected O
the O
model O
that O
achieved O
the O
best O
perplexity Metric
on O
the O
validation O
set O
during O
the O
training O
. O
subsection O
: O
Results O
Table O
[ O
reference O
] O
shows O
the O
bracketing O
F1 Metric
scores Metric
on O
the O
PTB Material
test Material
set Material
. O
This O
table O
is O
divided O
into O
three O
parts O
by O
horizontal O
lines O
; O
the O
upper O
part O
describes O
the O
scores O
by O
single O
language Task
modeling Task
based O
rerankers O
, O
the O
middle O
part O
shows O
the O
results O
by O
ensembling O
five O
rerankers O
, O
and O
the O
lower O
part O
represents O
the O
current O
state O
- O
of O
- O
the O
- O
art O
scores O
in O
the O
setting O
without O
external O
data O
. O
The O
upper O
part O
also O
contains O
the O
score O
reported O
in O
choe O
- O
charniak:2016:EMNLP2016 O
that O
reranked O
candidates O
by O
the O
simple O
LSTM Method
language Method
model Method
. O
This O
part O
indicates O
that O
our O
implemented O
rerankers Method
outperformed O
the O
simple O
LSTM Method
language Method
model Method
based Method
reranker Method
, O
which O
achieved O
92.6 O
F1 Metric
score Metric
. O
Moreover O
, O
AWD Method
- Method
LSTM Method
- Method
DOC Method
outperformed O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS. Method
These O
results O
correspond O
to O
the O
performance O
on O
the O
language Task
modeling Task
task O
( O
Section O
[ O
reference O
] O
) O
. O
The O
middle O
part O
shows O
that O
AWD Method
- Method
LSTM Method
- Method
DOC Method
also O
outperformed O
AWD Method
- Method
LSTM Method
and O
AWD Method
- Method
LSTM Method
- Method
MoS Method
in O
the O
ensemble Task
setting Task
. O
In O
addition O
, O
we O
can O
improve O
the O
performance O
by O
exchanging O
the O
base Method
parser Method
with O
a O
stronger O
one O
. O
In O
fact O
, O
we O
achieved O
94.29 O
F1 Metric
score Metric
by O
reranking O
the O
candidates O
from O
retrained Method
Recurrent Method
Neural Method
Network Method
Grammars Method
( O
RNNG Method
) O
, O
that O
achieved O
91.2 O
F1 Metric
score Metric
in O
our O
configuration O
. O
Moreover O
, O
the O
lowest O
row O
of O
the O
middle O
part O
indicates O
the O
result O
by O
reranking O
the O
candidates O
from O
the O
retrained Method
neural Method
encoder Method
- Method
decoder Method
based Method
parser Method
. O
Our O
base O
parser Method
has O
two O
different O
parts O
from O
P18 O
- O
2097 O
. O
First O
, O
we O
used O
the O
sum O
of O
the O
hidden O
states O
of O
the O
forward Method
and Method
backward Method
RNNs Method
as O
the O
hidden O
layer O
for O
each O
RNN Method
. O
Second O
, O
we O
tied O
the O
embedding O
matrix O
to O
the O
weight O
matrix O
to O
compute O
the O
probability O
distributions O
in O
the O
decoder Method
. O
The O
retrained Method
parser Method
achieved O
93.12 O
F1 Metric
score Metric
. O
Finally O
, O
we O
achieved O
94.47 O
F1 Metric
score Metric
by O
reranking O
its O
candidates O
with O
AWD Method
- Method
LSTM Method
- Method
DOC Method
. O
We O
expect O
that O
we O
can O
achieve O
even O
better O
score O
by O
replacing O
the O
base Method
parser Method
with O
the O
current O
state O
- O
of O
- O
the O
- O
art O
one O
. O
section O
: O
Related O
Work O
Bengio:2003:NPL:944919.944966 O
are O
pioneers O
of O
neural Method
language Method
models Method
. O
To O
address O
the O
curse O
of O
dimensionality Task
in O
language Task
modeling Task
, O
they O
proposed O
a O
method O
using O
word Method
embeddings Method
and O
a O
feed Method
- Method
forward Method
neural Method
network Method
( O
FFNN Method
) O
. O
They O
demonstrated O
that O
their O
approach O
outperformed O
n Method
- Method
gram Method
language Method
models Method
, O
but O
FFNN Method
can O
only O
handle O
fixed O
- O
length O
contexts O
. O
Instead O
of O
FFNN Method
, O
DBLP Method
: O
conf O
/ O
interspeech O
/ O
MikolovKBCK10 O
applied O
RNN Method
to O
language Task
modeling Task
to O
address O
the O
entire O
given O
sequence O
as O
a O
context O
. O
Their O
method O
outperformed O
the O
Kneser Method
- Method
Ney Method
smoothed Method
5 Method
- Method
gram Method
language Method
model Method
. O
Researchers O
continue O
to O
try O
to O
improve O
the O
performance O
of O
RNN Method
language Method
models Method
. O
DBLP O
: O
journals O
/ O
corr O
/ O
ZarembaSV14 O
used O
LSTM Method
instead O
of O
a O
simple O
RNN Method
for O
language Task
modeling Task
and O
significantly O
improved O
an O
RNN Method
language Method
model Method
by O
applying O
dropout Method
to O
all O
the O
connections O
except O
for O
the O
recurrent O
connections O
. O
To O
regularize O
the O
recurrent O
connections O
, O
Gal2016Theoretically O
proposed O
variational Method
inference Method
- Method
based Method
dropout Method
. O
Their O
method O
uses O
the O
same O
dropout O
mask O
at O
each O
timestep O
. O
fraternal Method
proposed O
fraternal Method
dropout Method
, O
which O
minimizes O
the O
differences O
between O
outputs O
from O
different O
dropout O
masks O
to O
be O
invariant O
to O
the O
dropout O
mask O
. O
DBLP O
: O
journals O
/ O
corr O
/ O
MelisDB17 O
used O
black Method
- Method
box Method
optimization Method
to O
find O
appropriate O
hyperparameters Method
for O
RNN Method
language Method
models Method
and O
demonstrated O
that O
the O
standard O
LSTM Method
with O
proper O
regularizations Method
can O
outperform O
other O
architectures O
. O
Apart O
from O
dropout Method
techniques Method
, O
DBLP O
: O
journals O
/ O
corr O
/ O
InanKS16 O
and O
press O
- O
wolf:2017:EACLshort O
proposed O
the O
word Method
tying Method
method Method
( O
WT Method
) O
, O
which O
unifies O
word O
embeddings O
( O
in O
Equation O
[ O
reference O
] O
) O
with O
the O
weight O
matrix O
to O
compute O
probability O
distributions O
( O
in O
Equation O
[ O
reference O
] O
) O
. O
In O
addition O
to O
quantitative Metric
evaluation Metric
, O
DBLP Method
: O
journals O
/ O
corr O
/ O
InanKS16 O
provided O
a O
theoretical O
justification O
for O
WT Method
and O
proposed O
the O
augmented Method
loss Method
technique Method
( O
AL Method
) O
, O
which O
computes O
an O
objective O
probability O
based O
on O
word O
embeddings O
. O
In O
addition O
to O
these O
regularization Method
techniques Method
, O
merityRegOpt Method
used O
DropConnect O
and O
averaged O
SGD Method
for O
an O
LSTM Method
language Method
model Method
. O
Their O
AWD Method
- Method
LSTM Method
achieved O
lower O
perplexity Metric
than O
DBLP O
: O
journals O
/ O
corr O
/ O
MelisDB17 O
on O
PTB Material
and O
WikiText Material
- Material
2 Material
. O
Previous O
studies O
also O
explored O
superior O
architecture O
for O
language Task
modeling Task
. O
zilly2016recurrent O
proposed O
recurrent Method
highway Method
networks Method
that O
use O
highway Method
layers Method
to O
deepen O
recurrent O
connections O
. O
45826 O
adopted O
reinforcement Method
learning Method
to O
construct O
the O
best O
RNN Method
structure Method
. O
However O
, O
as O
mentioned O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
MelisDB17 O
established O
that O
the O
standard O
LSTM Method
is O
superior O
to O
these O
architectures O
. O
Apart O
from O
RNN Method
architecture Method
, O
takase O
- O
suzuki O
- O
nagata:2017:I17 O
- O
2 O
proposed O
the O
input O
- O
to O
- O
output O
gate O
( O
IOG Method
) O
, O
which O
boosts O
the O
performance O
of O
trained O
language Method
models Method
. O
As O
described O
in O
Section O
[ O
reference O
] O
, O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
interpreted O
training O
language Task
modeling Task
as O
matrix Method
factorization Method
and O
improved O
performance O
by O
computing O
multiple O
probability O
distributions O
. O
In O
this O
study O
, O
we O
generalized O
their O
approach O
to O
use O
the O
middle O
layers O
of O
RNNs Method
. O
Finally O
, O
our O
proposed O
method O
, O
DOC Method
, O
achieved O
the O
state O
- O
of O
- O
the O
- O
art O
score O
on O
the O
standard O
benchmark O
datasets O
. O
Some O
studies O
provided O
methods O
that O
boost O
performance O
by O
using O
statistics O
obtained O
from O
test O
data O
. O
DBLP O
: O
journals O
/ O
corr O
/ O
GraveJU16 O
extended O
a O
cache Method
model Method
for O
RNN Method
language Method
models Method
. O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1709 O
- O
07432 O
proposed O
dynamic Method
evaluation Method
that O
updates O
parameters O
based O
on O
a O
recent O
sequence O
during O
testing O
. O
Although O
these O
methods O
might O
also O
improve O
the O
performance O
of O
DOC Method
, O
we O
omitted O
such O
investigation O
to O
focus O
on O
comparisons O
among O
methods O
trained O
only O
on O
the O
training O
set O
. O
section O
: O
Conclusion O
We O
proposed O
Direct Method
Output Method
Connection Method
( O
DOC Method
) O
, O
a O
generalization Method
method Method
of O
MoS Method
introduced O
by O
DBLP O
: O
journals O
/ O
corr O
/ O
abs O
- O
1711 O
- O
03953 O
. O
DOC Method
raises O
the O
expressive O
power O
of O
RNN Method
language Method
models Method
and O
improves O
quality O
of O
the O
model O
. O
DOC Method
outperformed O
MoS Method
and O
achieved O
the O
best O
perplexities Metric
on O
the O
standard O
benchmark O
datasets O
of O
language Task
modeling Task
: O
PTB Material
and O
WikiText Material
- Material
2 Material
. O
Moreover O
, O
we O
investigated O
its O
effectiveness O
on O
machine Task
translation Task
and O
headline Task
generation Task
. O
Our O
results O
show O
that O
DOC Method
also O
improved O
the O
performance O
of O
EncDec Method
and O
using O
a O
middle Method
layer Method
positively O
affected O
such O
application O
tasks O
. O
bibliography O
: O
References O
