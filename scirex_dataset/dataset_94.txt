document O
: O
Text Task
Understanding Task
with O
the O
Attention Method
Sum Method
Reader Method
Network Method
black O
Several O
large O
cloze O
- O
style O
context O
- O
question O
- O
answer O
datasets O
have O
been O
introduced O
recently O
: O
the O
CNN Material
and Material
Daily Material
Mail Material
news Material
data Material
and O
the O
Children Material
’s Material
Book Material
Test Material
. O
Thanks O
to O
the O
size O
of O
these O
datasets O
, O
the O
associated O
text Task
comprehension Task
task Task
is O
well O
suited O
for O
deep Method
- Method
learning Method
techniques Method
that O
currently O
seem O
to O
outperform O
all O
alternative O
approaches O
. O
We O
present O
a O
new O
, O
simple O
model O
that O
uses O
attention O
to O
directly O
pick O
the O
answer O
from O
the O
context O
as O
opposed O
to O
computing O
the O
answer O
using O
a O
blended Method
representation Method
of O
words O
in O
the O
document O
as O
is O
usual O
in O
similar O
models O
. O
This O
makes O
the O
model O
particularly O
suitable O
for O
question Task
- Task
answering Task
problems Task
where O
the O
answer O
is O
a O
single O
word O
from O
the O
document O
. O
blackEnsemble O
of O
our O
models O
sets O
new O
state O
of O
the O
art O
on O
all O
evaluated O
datasets O
. O
MemNNMemNNMemoryNetwork O
ptrnetPtr O
- O
NetPointerNetwork O
psrASReaderAttentionSumReader O
MenNNname O
= O
MemNN O
, O
description O
= O
isaprogrammablemachinethatreceivesinput O
, O
storesandmanipulatesdata O
, O
andprovidesoutputinausefulformat O
section O
: O
Introduction O
Most O
of O
the O
information O
humanity O
has O
gathered O
up O
to O
this O
point O
is O
stored O
in O
the O
form O
of O
plain O
text O
. O
Hence O
the O
task O
of O
teaching Task
machines Task
how O
to O
understand O
this O
data O
is O
of O
utmost O
importance O
in O
the O
field O
of O
Artificial Task
Intelligence Task
. O
One O
way O
of O
testing O
the O
level O
of O
text Task
understanding Task
is O
simply O
to O
ask O
the O
system O
questions O
for O
which O
the O
answer O
can O
be O
inferred O
from O
the O
text O
. O
A O
well O
- O
known O
example O
of O
a O
system O
that O
could O
make O
use O
of O
a O
huge O
collection O
of O
unstructured O
documents O
to O
answer O
questions O
is O
for O
instance O
IBM O
’s O
Watson Method
system Method
used O
for O
the O
Jeopardy O
challenge O
. O
Cloze O
- O
style O
questions O
, O
i.e. O
questions O
formed O
by O
removing O
a O
phrase O
from O
a O
sentence O
, O
are O
an O
appealing O
form O
of O
such O
questions O
( O
for O
example O
see O
Figure O
[ O
reference O
] O
) O
. O
While O
the O
task O
is O
easy O
to O
evaluate O
, O
one O
can O
vary O
the O
context O
, O
the O
question O
sentence O
or O
the O
specific O
phrase O
missing O
in O
the O
question O
to O
dramatically O
change O
the O
task O
structure O
and O
difficulty O
. O
One O
way O
of O
altering O
the O
task Metric
difficulty Metric
is O
to O
vary O
the O
word O
type O
being O
replaced O
, O
as O
in O
. O
The O
complexity O
of O
such O
variation O
comes O
from O
the O
fact O
that O
the O
level O
of O
context O
understanding O
needed O
in O
order O
to O
correctly O
predict O
different O
types O
of O
words O
varies O
greatly O
. O
While O
predicting Task
prepositions Task
can O
easily O
be O
done O
using O
relatively O
simple O
models O
with O
very O
little O
context O
knowledge O
, O
predicting O
named O
entities O
requires O
a O
deeper O
understanding O
of O
the O
context O
. O
Also O
, O
as O
opposed O
to O
selecting O
a O
random O
sentence O
from O
a O
text O
as O
in O
) O
, O
the O
question O
can O
be O
formed O
from O
a O
specific O
part O
of O
the O
document O
, O
such O
as O
a O
short O
summary O
or O
a O
list O
of O
tags O
. O
Since O
such O
sentences O
often O
paraphrase O
in O
a O
condensed O
form O
what O
was O
said O
in O
the O
text O
, O
they O
are O
particularly O
suitable O
for O
testing O
text Task
comprehension Task
. O
An O
important O
property O
of O
cloze O
- O
style O
questions O
is O
that O
a O
large O
amount O
of O
such O
questions O
can O
be O
automatically O
generated O
from O
real O
world O
documents O
. O
This O
opens O
the O
task O
to O
data Method
- Method
hungry Method
techniques Method
such O
as O
deep Method
learning Method
. O
This O
is O
an O
advantage O
compared O
to O
smaller O
machine O
understanding O
datasets O
like O
MCTest Method
that O
have O
only O
hundreds O
of O
training O
examples O
and O
therefore O
the O
best O
performing O
systems O
usually O
rely O
on O
hand O
- O
crafted O
features O
. O
In O
the O
first O
part O
of O
this O
article O
we O
introduce O
the O
task O
at O
hand O
and O
the O
main O
aspects O
of O
the O
relevant O
datasets O
. O
Then O
we O
present O
our O
own O
model O
to O
tackle O
the O
problem O
. O
Subsequently O
we O
compare O
the O
model O
to O
previously O
proposed O
architectures O
and O
finally O
describe O
the O
experimental O
results O
on O
the O
performance O
of O
our O
model O
. O
section O
: O
Task O
and O
datasets O
In O
this O
section O
we O
introduce O
the O
task O
that O
we O
are O
seeking O
to O
solve O
and O
relevant O
large O
- O
scale O
datasets O
that O
have O
recently O
been O
introduced O
for O
this O
task O
. O
subsection O
: O
Formal O
Task O
Description O
The O
task O
consists O
of O
answering O
a O
cloze Task
- Task
style Task
question Task
, O
the O
answer O
to O
which O
depends O
on O
the O
understanding O
of O
a O
context O
document O
provided O
with O
the O
question O
. O
The O
model O
is O
also O
provided O
with O
a O
set O
of O
possible O
answers O
from O
which O
the O
correct O
one O
is O
to O
be O
selected O
. O
This O
can O
be O
formalized O
as O
follows O
: O
The O
training O
data O
consist O
of O
tuples O
, O
where O
is O
a O
question O
, O
is O
a O
document O
that O
contains O
the O
answer O
to O
question O
, O
is O
a O
set O
of O
possible O
answers O
and O
is O
the O
ground O
truth O
answer O
. O
Both O
and O
are O
sequences O
of O
words O
from O
vocabulary O
. O
We O
also O
assume O
that O
all O
possible O
answers O
are O
words O
from O
the O
vocabulary O
, O
that O
is O
, O
and O
that O
the O
ground O
truth O
answer O
appears O
in O
the O
document O
, O
that O
is O
. O
subsection O
: O
Datasets O
We O
will O
now O
briefly O
summarize O
important O
features O
of O
the O
datasets O
. O
subsubsection O
: O
News O
Articles O
— O
CNN Material
and Material
Daily Material
Mail Material
The O
first O
two O
datasets O
were O
constructed O
from O
a O
large O
number O
of O
news O
articles O
from O
the O
CNN Material
and O
Daily O
Mail O
websites O
. O
The O
main O
body O
of O
each O
article O
forms O
a O
context O
, O
while O
the O
cloze O
- O
style O
question O
is O
formed O
from O
one O
of O
short O
highlight O
sentences O
, O
appearing O
at O
the O
top O
of O
each O
article O
page O
. O
Specifically O
, O
the O
question O
is O
created O
by O
replacing O
a O
named O
entity O
from O
the O
summary O
sentence O
( O
e.g. O
“ O
Producer O
X O
will O
not O
press O
charges O
against O
Jeremy O
Clarkson O
, O
his O
lawyer O
says O
. O
” O
) O
. O
Furthermore O
the O
named O
entities O
in O
the O
whole O
dataset O
were O
replaced O
by O
anonymous O
tokens O
which O
were O
further O
shuffled O
for O
each O
example O
so O
that O
the O
model O
can O
not O
build O
up O
any O
world O
knowledge O
about O
the O
entities O
and O
hence O
has O
to O
genuinely O
rely O
on O
the O
context O
document O
to O
search O
for O
an O
answer O
to O
the O
question O
. O
black O
Qualitative O
analysis O
of O
reasoning O
patterns O
needed O
to O
answer O
questions O
in O
the O
CNN Material
dataset Material
together O
with O
human O
performance O
on O
this O
task O
are O
provided O
in O
. O
subsubsection O
: O
Children Material
’s Material
Book Material
Test Material
The O
third O
dataset O
, O
the O
Children Material
’s Material
Book Material
Test Material
( O
CBT Material
) O
, O
is O
built O
from O
books O
that O
are O
freely O
available O
thanks O
to O
Project O
Gutenberg O
. O
Each O
context O
document O
is O
formed O
by O
consecutive O
sentences O
taken O
from O
a O
children O
’s O
book O
story O
. O
Due O
to O
the O
lack O
of O
summary O
, O
the O
cloze O
- O
style O
question O
is O
then O
constructed O
from O
the O
subsequent O
( O
st O
) O
sentence O
. O
One O
can O
also O
see O
how O
the O
task Metric
complexity Metric
varies O
with O
the O
type O
of O
the O
omitted O
word O
( O
named O
entity O
, O
common O
noun O
, O
verb O
, O
preposition O
) O
. O
have O
shown O
that O
while O
standard O
LSTM Method
language Method
models Method
have O
human O
level O
performance O
on O
predicting O
verbs O
and O
prepositions O
, O
they O
lack O
behind O
on O
named O
entities O
and O
common O
nouns O
. O
In O
this O
article O
we O
therefore O
focus O
only O
on O
predicting O
the O
first O
two O
word O
types O
. O
Basic O
statistics O
about O
the O
CNN Material
, Material
Daily Material
Mail Material
and O
CBT Material
datasets O
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
section O
: O
Our O
Model O
— O
Attention O
Sum O
Reader O
black O
Our O
model O
called O
the O
is O
tailor O
- O
made O
to O
leverage O
the O
fact O
that O
the O
answer O
is O
a O
word O
from O
the O
context O
document O
. O
This O
is O
a O
double O
- O
edged O
sword O
. O
While O
it O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
all O
of O
the O
mentioned O
datasets O
( O
where O
this O
assumption O
holds O
true O
) O
, O
it O
can O
not O
produce O
an O
answer O
which O
is O
not O
contained O
in O
the O
document O
. O
Intuitively O
, O
our O
model O
is O
structured O
as O
follows O
: O
We O
compute O
a O
vector Method
embedding Method
of Method
the Method
query Method
. O
We O
compute O
a O
vector Method
embedding Method
of O
each O
individual O
word O
in O
the O
context O
of O
the O
whole O
document O
( O
contextual Method
embedding Method
) O
. O
Using O
a O
dot O
product O
between O
the O
question Method
embedding Method
and O
the O
contextual Method
embedding Method
of O
each O
occurrence O
of O
a O
candidate O
answer O
in O
the O
document O
, O
we O
select O
the O
most O
likely O
answer O
. O
subsection O
: O
Formal O
Description O
Our O
model O
uses O
one O
word Method
embedding Method
function Method
and O
two O
encoder Method
functions Method
. O
The O
word Method
embedding Method
function Method
translates O
words O
into O
vector Method
representations Method
. O
The O
first O
encoder Method
function Method
is O
a O
document Method
encoder Method
that O
encodes O
every O
word O
from O
the O
document O
in O
the O
context O
of O
the O
whole O
document O
. O
We O
call O
this O
the O
contextual Method
embedding Method
. O
For O
convenience O
we O
will O
denote O
the O
contextual O
embedding O
of O
the O
- O
th O
word O
in O
as O
. O
The O
second O
encoder O
is O
used O
to O
translate O
the O
query O
into O
a O
fixed Method
length Method
representation Method
of O
the O
same O
dimensionality O
as O
each O
black O
. O
Both O
encoders O
use O
word O
embeddings O
computed O
by O
as O
their O
input O
. O
Then O
we O
compute O
a O
weight O
for O
every O
word O
in O
the O
document O
as O
the O
dot O
product O
of O
its O
contextual Method
embedding Method
and O
the O
query Method
embedding Method
. O
This O
weight O
might O
be O
viewed O
as O
an O
attention O
over O
the O
document O
. O
To O
form O
a O
proper O
probability O
distribution O
over O
the O
words O
in O
the O
document O
, O
we O
normalize O
the O
weights O
using O
the O
softmax Method
function Method
. O
This O
way O
we O
model O
probability O
that O
the O
answer O
to O
query O
appears O
at O
position O
in O
the O
document O
. O
In O
a O
functional O
form O
this O
is O
: O
Finally O
we O
compute O
the O
probability O
that O
word O
is O
a O
correct O
answer O
as O
: O
where O
is O
a O
set O
of O
positions O
where O
appears O
in O
the O
document O
. O
We O
call O
this O
mechanism O
pointer O
sum O
attention O
since O
we O
use O
attention O
as O
a O
pointer O
over O
discrete O
tokens O
in O
the O
context O
document O
and O
then O
we O
directly O
sum O
the O
word O
’s O
attention O
across O
all O
the O
occurrences O
. O
This O
differs O
from O
the O
usual O
use O
of O
attention Method
in O
sequence Method
- Method
to Method
- Method
sequence Method
models Method
where O
attention Method
is O
used O
to O
blend O
representations O
of O
words O
into O
a O
new O
embedding O
vector O
. O
Our O
use O
of O
attention O
was O
inspired O
by O
ptrnet Method
. O
A O
high O
level O
structure O
of O
our O
model O
is O
shown O
in O
Figure O
[ O
reference O
] O
. O
subsection O
: O
Model O
instance O
details O
In O
our O
model O
the O
document Method
encoder Method
is O
implemented O
as O
a O
bidirectional Method
Gated Method
Recurrent Method
Unit Method
( Method
GRU Method
) Method
network Method
whose O
hidden O
states O
form O
the O
contextual O
word O
embeddings O
, O
that O
is O
, O
where O
denotes O
vector O
concatenation O
and O
and O
denote O
forward O
and O
backward O
contextual O
embeddings O
from O
the O
respective O
recurrent Method
networks Method
. O
The O
query Method
encoder Method
is O
implemented O
by O
another O
bidirectional Method
GRU Method
network Method
. O
This O
time O
the O
last O
hidden O
state O
of O
the O
forward Method
network Method
is O
concatenated O
with O
the O
last O
hidden O
state O
of O
the O
backward Method
network Method
to O
form O
the O
query Task
embedding Task
, O
that O
is O
. O
The O
word Method
embedding Method
function Method
is O
implemented O
in O
a O
usual O
way O
as O
a O
look O
- O
up O
table O
. O
is O
a O
matrix O
whose O
rows O
can O
be O
indexed O
by O
words O
from O
the O
vocabulary O
, O
that O
is O
. O
Therefore O
, O
each O
row O
of O
contains O
embedding O
of O
one O
word O
from O
the O
vocabulary O
. O
During O
training O
we O
jointly O
optimize O
parameters O
of O
, O
and O
. O
section O
: O
Related O
Work O
black O
Several O
recent O
deep Method
neural Method
network Method
architectures Method
were O
applied O
to O
the O
task O
of O
text Task
comprehension Task
. O
The O
last O
two O
architectures O
were O
developed O
independently O
at O
the O
same O
time O
as O
our O
work O
. O
All O
of O
these O
architectures O
use O
an O
attention Method
mechanism Method
that O
allows O
them O
to O
highlight O
places O
in O
the O
document O
that O
might O
be O
relevant O
to O
answering O
the O
question O
. O
We O
will O
now O
briefly O
describe O
these O
architectures O
and O
compare O
them O
to O
our O
approach O
. O
subsection O
: O
Attentive O
and O
Impatient O
Readers O
Attentive O
and O
Impatient O
Readers O
were O
proposed O
in O
. O
The O
simpler O
Attentive Method
Reader Method
is O
very O
similar O
to O
our O
architecture O
. O
It O
also O
uses O
bidirectional Method
document Method
and O
query Method
encoders Method
to O
compute O
an O
attention O
in O
a O
similar O
way O
we O
do O
. O
The O
more O
complex O
Impatient O
Reader O
computes O
attention O
over O
the O
document O
after O
reading O
every O
word O
of O
the O
query O
. O
However O
, O
empirical O
evaluation O
has O
shown O
that O
both O
models O
perform O
almost O
identically O
on O
the O
CNN Material
and O
Daily O
Mail O
datasets O
. O
The O
key O
difference O
between O
the O
Attentive Method
Reader Method
and O
our O
model O
is O
that O
the O
Attentive Method
Reader Method
uses O
attention O
to O
compute O
a O
fixed Method
length Method
representation Method
of O
the O
document O
that O
is O
equal O
to O
a O
weighted O
sum O
of O
contextual O
embeddings O
of O
words O
in O
, O
that O
is O
. O
A O
joint Task
query Task
and Task
document Task
embedding Task
is O
then O
a O
non O
- O
linear O
function O
of O
and O
the O
query Method
embedding Method
. O
This O
joint Method
embedding Method
is O
in O
the O
end O
compared O
against O
all O
candidate O
answers O
using O
the O
dot O
product O
, O
in O
the O
end O
the O
scores O
are O
normalized O
by O
softmax Method
. O
That O
is O
: O
. O
black O
In O
contrast O
to O
the O
Attentive O
Reader O
, O
we O
select O
the O
answer O
from O
the O
context O
directly O
using O
the O
computed O
attention O
rather O
than O
using O
such O
attention O
for O
a O
weighted O
sum O
of O
the O
individual O
representations O
( O
see O
Eq O
. O
[ O
reference O
] O
) O
. O
The O
motivation O
for O
such O
simplification O
is O
the O
following O
. O
Consider O
a O
context O
“ O
A O
UFO O
was O
observed O
above O
our O
city O
in O
January O
and O
again O
in O
March O
. O
” O
and O
question O
“ O
An O
observer O
has O
spotted O
a O
UFO O
in O
_ O
_ O
_ O
. O
” O
Since O
both O
January O
and O
March O
are O
equally O
good O
candidates O
, O
the O
attention Method
mechanism Method
might O
put O
the O
same O
attention O
on O
both O
these O
candidates O
in O
the O
context O
. O
The O
blending Method
mechanism Method
described O
above O
would O
compute O
a O
vector O
between O
the O
representations O
of O
these O
two O
words O
and O
propose O
the O
closest O
word O
as O
the O
answer O
- O
this O
may O
well O
happen O
to O
be O
February O
( O
it O
is O
indeed O
the O
case O
for O
Word2Vec Method
trained O
on O
Google O
News O
) O
. O
By O
contrast O
, O
our O
model O
would O
correctly O
propose O
January O
or O
March O
. O
black O
subsection O
: O
Chen O
et O
al O
. O
2016 O
A O
model O
presented O
in O
is O
inspired O
by O
the O
Attentive O
Reader O
. O
One O
difference O
is O
that O
the O
attention O
weights O
are O
computed O
with O
a O
bilinear O
term O
instead O
of O
simple O
dot O
- O
product O
, O
that O
is O
. O
The O
document Task
embedding Task
is O
computed O
using O
a O
weighted Method
sum Method
as O
in O
the O
Attentive O
Reader O
, O
. O
In O
the O
end O
, O
where O
is O
a O
new O
embedding O
function O
. O
Even O
though O
it O
is O
a O
simplification O
of O
the O
Attentive Method
Reader Method
this O
model O
performs O
significantly O
better O
than O
the O
original O
. O
black O
subsection O
: O
Memory Method
Networks Method
MenNN Method
were O
applied O
to O
the O
task O
of O
text Task
comprehension Task
in O
. O
The O
best O
performing O
memory Method
networks Method
model Method
setup Method
- Method
window Method
memory Method
- O
uses O
windows O
of O
fixed O
length O
( O
8 O
) O
centered O
around O
the O
candidate O
words O
as O
memory O
cells O
. O
Due O
to O
this O
limited O
context O
window O
, O
the O
model O
is O
unable O
to O
capture O
dependencies O
out O
of O
scope O
of O
this O
window O
. O
Furthermore O
, O
the O
representation O
within O
such O
window O
is O
computed O
simply O
as O
the O
sum O
of O
embeddings O
of O
words O
in O
that O
window O
. O
By O
contrast O
, O
in O
our O
model O
the O
representation O
of O
each O
individual O
word O
is O
computed O
using O
a O
recurrent Method
network Method
, O
which O
not O
only O
allows O
it O
to O
capture O
context O
from O
the O
entire O
document O
but O
also O
the O
embedding Task
computation Task
is O
much O
more O
flexible O
than O
a O
simple O
sum O
. O
To O
improve O
on O
the O
initial O
accuracy Metric
, O
a O
heuristic Method
approach Method
called O
self Method
supervision Method
is O
used O
in O
to O
help O
the O
network O
to O
select O
the O
right O
supporting O
“ O
memories O
” O
using O
an O
attention Method
mechanism Method
showing O
similarities O
to O
the O
ours O
. O
Plain O
MenNN Method
without O
this O
heuristic O
are O
not O
competitive O
on O
these O
machine Task
reading Task
tasks Task
. O
Our O
model O
does O
not O
need O
any O
similar O
heuristics O
. O
black O
subsection O
: O
Dynamic Method
Entity Method
Representation Method
The O
Dynamic Method
Entity Method
Representation Method
model Method
has O
a O
complex O
architecture O
also O
based O
on O
the O
weighted Method
attention Method
mechanism Method
and O
max Method
- Method
pooling Method
over O
contextual O
embeddings O
of O
vectors O
for O
each O
named O
entity O
. O
subsection O
: O
Pointer Method
Networks Method
Our O
model O
architecture O
was O
inspired O
by O
ptrnet Method
in O
using O
an O
attention Method
mechanism Method
to O
select O
the O
answer O
in O
the O
context O
rather O
than O
to O
blend O
words O
from O
the O
context O
into O
an O
answer Method
representation Method
. O
While O
a O
ptrnet Method
consists O
of O
an O
encoder Method
as O
well O
as O
a O
decoder Method
, O
which O
uses O
the O
attention O
to O
select O
the O
output O
at O
each O
step O
, O
our O
model O
outputs O
the O
answer O
in O
a O
single O
step O
. O
Furthermore O
, O
the O
pointer Method
networks Method
assume O
that O
no O
input O
in O
the O
sequence O
appears O
more O
than O
once O
, O
which O
is O
not O
the O
case O
in O
our O
settings O
. O
subsection O
: O
Summary O
Our O
model O
combines O
the O
best O
features O
of O
the O
architectures O
mentioned O
above O
. O
We O
use O
recurrent Method
networks Method
to O
“ O
read O
” O
the O
document O
and O
the O
query O
as O
black O
done O
in O
and O
we O
use O
attention O
in O
a O
way O
similar O
to O
ptrnet Method
. O
We O
also O
use O
summation O
of O
attention O
weights O
in O
a O
way O
similar O
to O
MenNN Method
. O
black O
From O
a O
high O
level O
perspective O
we O
simplify O
all O
the O
discussed O
text Method
comprehension Method
models Method
by O
removing O
all O
transformations O
past O
the O
attention O
step O
. O
Instead O
we O
use O
the O
attention O
directly O
to O
compute O
the O
answer O
probability O
. O
section O
: O
Evaluation O
In O
this O
section O
we O
evaluate O
our O
model O
on O
the O
CNN Material
, Material
Daily Material
Mail Material
and O
CBT Material
datasets Material
. O
We O
show O
that O
despite O
the O
model O
’s O
simplicity O
its O
ensembles O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
each O
of O
these O
datasets O
. O
subsection O
: O
Training O
Details O
black O
To O
train O
the O
model O
we O
used O
stochastic Method
gradient Method
descent Method
with O
the O
ADAM Method
update Method
rule Method
and O
learning Method
rate Method
of Method
or Method
. O
During O
training O
we O
minimized O
the O
following O
negative O
log O
- O
likelihood O
with O
respect O
to O
: O
where O
is O
the O
correct O
answer O
for O
query O
and O
document O
, O
and O
represents O
parameters O
of O
the O
encoder Method
functions Method
and O
and O
of O
the O
word Method
embedding Method
function Method
. O
The O
optimized O
probability O
distribution O
is O
defined O
in O
Eq O
. O
[ O
reference O
] O
. O
The O
initial O
weights O
in O
the O
word O
embedding O
matrix O
were O
drawn O
randomly O
uniformly O
from O
the O
interval O
. O
Weights O
in O
the O
GRU Method
networks Method
were O
initialized O
by O
random O
orthogonal O
matrices O
and O
biases O
were O
initialized O
to O
zero O
. O
We O
also O
used O
a O
gradient O
clipping O
threshold O
of O
10 O
and O
batches O
of O
size O
32 O
. O
During O
training O
we O
randomly O
shuffled O
all O
examples O
in O
each O
epoch O
. O
To O
speedup O
training Task
, O
we O
always O
pre O
- O
fetched O
batches O
worth O
of O
examples O
and O
sorted O
them O
according O
to O
document O
length O
. O
Hence O
each O
batch O
contained O
documents O
of O
roughly O
the O
same O
length O
. O
For O
each O
batch O
of O
the O
CNN Material
and O
Daily O
Mail O
datasets O
we O
randomly O
reshuffled O
the O
assignment O
of O
named O
entities O
to O
the O
corresponding O
word O
embedding O
vectors O
to O
match O
the O
procedure O
proposed O
in O
. O
This O
guaranteed O
that O
word O
embeddings O
of O
named O
entities O
were O
used O
only O
as O
semantically O
meaningless O
labels O
not O
encoding O
any O
intrinsic O
features O
of O
the O
represented O
entities O
. O
This O
forced O
the O
model O
to O
truly O
deduce O
the O
answer O
from O
the O
single O
context O
document O
associated O
with O
the O
question O
. O
black O
We O
also O
do O
not O
use O
pre O
- O
trained O
word O
embeddings O
to O
make O
our O
training O
procedure O
comparable O
to O
. O
We O
did O
not O
perform O
any O
text Task
pre Task
- Task
processing Task
since O
the O
original O
datasets O
were O
already O
tokenized O
. O
black O
We O
do O
not O
use O
any O
regularization Method
since O
in O
our O
experience O
it O
leads O
to O
longer O
training Metric
times Metric
of O
single Method
models Method
, O
however O
, O
performance O
of O
a O
model Method
ensemble Method
is O
usually O
the O
same O
. O
This O
way O
we O
can O
train O
the O
whole O
ensemble O
faster O
when O
using O
multiple O
GPUs Method
for O
parallel Task
training Task
. O
black O
For O
Additional O
details O
about O
the O
training O
procedure O
see O
Appendix O
[ O
reference O
] O
. O
black O
subsection O
: O
Evaluation O
Method O
We O
evaluated O
the O
proposed O
model O
both O
as O
a O
single O
model O
and O
using O
ensemble Method
averaging Method
. O
blackAlthough O
the O
model O
computes O
attention O
for O
every O
word O
in O
the O
document O
we O
restrict O
the O
model O
to O
select O
an O
answer O
from O
a O
list O
of O
candidate O
answers O
associated O
with O
each O
question O
- O
document O
pair O
. O
For O
single Method
models Method
we O
are O
reporting O
results O
for O
the O
best O
model O
as O
well O
as O
the O
average Metric
of Metric
accuracies Metric
for O
the O
best O
20 O
% O
of O
models O
with O
best O
performance O
on O
validation O
data O
since O
single Method
models Method
display O
considerable O
variation O
of O
results O
due O
to O
random Method
weight Method
initialization Method
even O
for O
identical O
hyperparameter O
values O
. O
Single Method
model Method
performance O
may O
consequently O
prove O
difficult O
to O
reproduce O
. O
What O
concerns O
ensembles O
, O
we O
used O
simple O
averaging O
of O
the O
answer O
probabilities O
predicted O
by O
ensemble Method
members Method
. O
For O
ensembling Task
we O
used O
14 O
, O
16 O
, O
84 O
and O
53 O
models O
for O
CNN Material
, O
Daily Material
Mail Material
and O
CBT Material
CN O
and O
NE O
respectively O
. O
The O
ensemble Method
models Method
were O
chosen O
either O
as O
the O
top O
70 O
% O
of O
all O
trained O
models O
, O
we O
call O
this O
avg Method
ensemble Method
. O
Alternatively O
we O
use O
the O
following O
algorithm O
: O
We O
started O
with O
the O
best O
performing O
model O
according O
to O
validation Metric
performance Metric
. O
Then O
in O
each O
step O
we O
tried O
adding O
the O
best O
performing O
model O
that O
had O
not O
been O
previously O
tried O
. O
We O
kept O
it O
in O
the O
ensemble O
if O
it O
did O
improve O
its O
validation O
performance O
and O
discarded O
it O
otherwise O
. O
This O
way O
we O
gradually O
tried O
each O
model O
once O
. O
We O
call O
the O
resulting O
model O
a O
greedy Method
ensemble Method
. O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
subsection O
: O
Results O
black O
Performance O
of O
our O
models O
on O
the O
CNN Material
and O
Daily O
Mail O
datasets O
is O
summarized O
in O
Table O
[ O
reference O
] O
, O
Table O
[ O
reference O
] O
shows O
results O
on O
the O
CBT Material
dataset Material
. O
The O
tables O
also O
list O
performance O
of O
other O
published O
models O
that O
were O
evaluated O
on O
these O
datasets O
. O
Ensembles O
of O
our O
models O
set O
new O
state O
- O
of O
- O
the O
- O
art O
results O
on O
all O
evaluated O
datasets O
. O
Table O
[ O
reference O
] O
then O
measures O
accuracy Metric
as O
the O
proportion O
of O
test O
cases O
where O
the O
ground O
truth O
was O
among O
the O
top O
answers O
proposed O
by O
the O
greedy Method
ensemble Method
model O
for O
. O
CNN Material
and O
Daily O
Mail O
. O
blackThe O
CNN Material
dataset Material
is O
the O
most O
widely O
used O
dataset O
for O
evaluation Task
of Task
text Task
comprehension Task
systems Task
published O
so O
far O
. O
Performance O
of O
our O
single O
model O
is O
a O
little O
bit O
worse O
than O
performance O
of O
simultaneously O
published O
models O
. O
Compared O
to O
our O
work O
these O
models O
were O
trained O
with O
Dropout Method
regularization Method
which O
might O
improve O
single O
model O
performance O
. O
However O
, O
ensemble O
of O
our O
models O
outperforms O
these O
models O
even O
though O
they O
use O
pre O
- O
trained O
word Method
embeddings Method
. O
On O
the O
CNN Material
dataset Material
our O
single O
model O
with O
best O
validation Metric
accuracy Metric
achieves O
a O
test Metric
accuracy Metric
of O
69.5 O
% O
. O
The O
average O
performance O
of O
the O
top O
20 O
% O
models O
according O
to O
validation Metric
accuracy Metric
is O
69.9 O
% O
which O
is O
even O
0.5 O
% O
better O
than O
the O
single O
best O
- O
validation Method
model Method
. O
This O
shows O
that O
there O
were O
many O
models O
that O
performed O
better O
on O
test O
set O
than O
the O
best O
- O
validation Method
model Method
. O
Fusing O
multiple Method
models Method
then O
gives O
a O
significant O
further O
increase O
in O
accuracy Metric
on O
both O
CNN Material
and O
Daily O
Mail O
datasets O
.. O
CBT Material
. O
In O
named Task
entity Task
prediction Task
our O
best O
single O
model O
with O
accuracy Metric
of O
68.6 O
% O
performs O
2 O
% O
absolute O
better O
than O
the O
MenNN Method
with O
self Method
supervision Method
, O
the O
averaging Method
ensemble Method
performs O
4 O
% O
absolute O
better O
than O
the O
best O
previous O
result O
. O
In O
common Task
noun Task
prediction Task
our O
single Method
models Method
is O
0.4 O
% O
absolute O
better O
than O
MenNN Method
however O
the O
ensemble O
improves O
the O
performance O
to O
69 O
% O
which O
is O
6 O
% O
absolute O
better O
than O
MenNN Method
. O
black O
section O
: O
Analysis O
To O
further O
analyze O
the O
properties O
of O
our O
model O
, O
we O
examined O
the O
dependence O
of O
accuracy Metric
on O
the O
length O
of O
the O
context O
document O
( O
Figure O
[ O
reference O
] O
) O
, O
the O
number O
of O
candidate O
answers O
( O
Figure O
[ O
reference O
] O
) O
and O
the O
frequency O
of O
the O
correct O
answer O
in O
the O
context O
( O
Figure O
[ O
reference O
] O
) O
. O
On O
the O
CNN Material
and O
Daily O
Mail O
datasets O
, O
the O
accuracy Metric
decreases O
with O
increasing O
document O
length O
( O
Figure O
[ O
reference O
] O
) O
. O
We O
hypothesize O
this O
may O
be O
due O
to O
multiple O
factors O
. O
Firstly O
long O
documents O
may O
make O
the O
task O
more O
complex O
. O
Secondly O
such O
cases O
are O
quite O
rare O
in O
the O
training O
data O
( O
Figure O
[ O
reference O
] O
) O
which O
motivates O
the O
model O
to O
specialize O
on O
shorter O
contexts O
. O
Finally O
the O
context O
length O
is O
correlated O
with O
the O
number O
of O
named O
entities O
, O
i.e. O
the O
number O
of O
possible O
answers O
which O
is O
itself O
negatively O
correlated O
with O
accuracy Metric
( O
see O
Figure O
[ O
reference O
] O
) O
. O
On O
the O
CBT Material
dataset Material
this O
negative O
trend O
seems O
to O
disappear O
( O
Fig O
. O
[ O
reference O
] O
) O
. O
This O
supports O
the O
later O
two O
explanations O
since O
the O
distribution O
of O
document O
lengths O
is O
somewhat O
more O
uniform O
( O
Figure O
[ O
reference O
] O
) O
and O
the O
number O
of O
candidate O
answers O
is O
constant O
( O
) O
for O
all O
examples O
in O
this O
dataset O
. O
The O
effect O
of O
increasing O
number O
of O
candidate O
answers O
on O
the O
model O
’s O
accuracy Metric
can O
be O
seen O
in O
Figure O
[ O
reference O
] O
. O
We O
can O
clearly O
see O
that O
as O
the O
number O
of O
candidate O
answers O
increases O
, O
the O
accuracy Metric
drops O
. O
On O
the O
other O
hand O
, O
the O
amount O
of O
examples O
with O
large O
number O
of O
candidate O
answers O
is O
quite O
small O
( O
Figure O
[ O
reference O
] O
) O
. O
Finally O
, O
since O
the O
summation O
of O
attention O
in O
our O
model O
inherently O
favours O
frequently O
occurring O
tokens O
, O
we O
also O
visualize O
how O
the O
accuracy Metric
depends O
on O
the O
frequency O
of O
the O
correct O
answer O
in O
the O
document O
. O
Figure O
[ O
reference O
] O
shows O
that O
the O
accuracy Metric
significantly O
drops O
as O
the O
correct O
answer O
gets O
less O
and O
less O
frequent O
in O
the O
document O
compared O
to O
other O
candidate O
answers O
. O
On O
the O
other O
hand O
, O
the O
correct O
answer O
is O
likely O
to O
occur O
frequently O
( O
Fig O
. O
[ O
reference O
] O
) O
. O
black O
section O
: O
Conclusion O
In O
this O
article O
we O
presented O
a O
new O
neural Method
network Method
architecture Method
for O
natural Task
language Task
text Task
comprehension Task
. O
While O
our O
model O
is O
simpler O
than O
previously O
published O
models O
, O
it O
gives O
a O
new O
state O
- O
of O
- O
the O
- O
art O
accuracy Metric
on O
all O
evaluated O
datasets O
. O
An O
analysis O
by O
suggests O
that O
on O
CNN Material
and O
Daily O
Mail O
datasets O
a O
significant O
proportion O
of O
questions O
is O
ambiguous O
or O
too O
difficult O
to O
answer O
even O
for O
humans O
( O
partly O
due O
to O
entity O
anonymization O
) O
so O
the O
ensemble O
of O
our O
models O
may O
be O
very O
near O
to O
the O
maximal O
accuracy Metric
achievable O
on O
these O
datasets O
. O
section O
: O
Acknowledgments O
We O
would O
like O
to O
thank O
Tim O
Klinger O
for O
providing O
us O
with O
masked O
softmax O
code O
that O
we O
used O
in O
our O
implementation O
. O
bibliography O
: O
References O
section O
: O
Training O
Details O
During O
training O
we O
evaluated O
the O
model O
performance O
after O
each O
epoch O
and O
stopped O
the O
training O
when O
the O
error O
on O
the O
validation O
set O
started O
increasing O
. O
The O
models O
usually O
converged O
after O
two O
epochs O
of O
training O
. O
Time O
needed O
to O
complete O
a O
single O
epoch O
of O
training O
on O
each O
dataset O
on O
an O
Nvidia O
K40 O
GPU O
is O
shown O
in O
Table O
[ O
reference O
] O
. O
black O
The O
hyperparameters O
, O
namely O
the O
recurrent O
hidden O
layer O
dimension O
and O
the O
source O
embedding O
dimension O
, O
were O
chosen O
by O
grid Method
search Method
. O
We O
started O
with O
a O
range O
of O
128 O
to O
384 O
for O
both O
parameters O
and O
subsequently O
kept O
increasing O
the O
upper O
bound O
by O
128 O
until O
we O
started O
observing O
a O
consistent O
decrease O
in O
validation Metric
accuracy Metric
. O
The O
region O
of O
the O
parameter O
space O
that O
we O
explored O
together O
with O
the O
parameters O
of O
the O
model O
with O
best O
validation Metric
accuracy Metric
are O
summarized O
in O
Table O
[ O
reference O
] O
. O
black O
Our O
model O
was O
implemented O
using O
Theano O
and O
Blocks O
. O
section O
: O
Dependence O
of O
accuracy Metric
on O
the O
frequency O
of O
the O
correct O
answer O
In O
Section O
[ O
reference O
] O
we O
analysed O
how O
the O
test Metric
accuracy Metric
depends O
on O
how O
frequent O
the O
correct O
answer O
is O
compared O
to O
other O
answer O
candidates O
for O
the O
news O
datasets O
. O
The O
plots O
for O
the O
Children Material
’s Material
Book Material
Test Material
looks O
very O
similar O
, O
however O
we O
are O
adding O
it O
here O
for O
completeness O
. O
[ O
b O
] O
0.475 O
[ O
b O
] O
0.475 O
