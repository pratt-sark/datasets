document O
: O
Very O
Deep Method
Multilingual Method
Convolutional Method
Neural Method
Networks Method
for O
LVCSR Task
Convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
are O
a O
standard O
component O
of O
many O
current O
state O
- O
of O
- O
the O
- O
art O
Large Task
Vocabulary Task
Continuous Task
Speech Task
Recognition Task
( O
LVCSR Task
) O
systems O
. O
However O
, O
CNNs Method
in O
LVCSR Task
have O
not O
kept O
pace O
with O
recent O
advances O
in O
other O
domains O
where O
deeper Method
neural Method
networks Method
provide O
superior O
performance O
. O
In O
this O
paper O
we O
propose O
a O
number O
of O
architectural Method
advances Method
in O
CNNs Method
for O
LVCSR Task
. O
First O
, O
we O
introduce O
a O
very O
deep Method
convolutional Method
network Method
architecture Method
with O
up O
to O
14 O
weight O
layers O
. O
There O
are O
multiple O
convolutional Method
layers Method
before O
each O
pooling Method
layer Method
, O
with O
small O
3 O
3 O
kernels O
, O
inspired O
by O
the O
VGG Method
Imagenet Method
2014 Method
architecture Method
. O
Then O
, O
we O
introduce O
multilingual O
CNNs Method
with O
multiple O
untied O
layers O
. O
Finally O
, O
we O
introduce O
multi O
- O
scale O
input O
features O
aimed O
at O
exploiting O
more O
context O
at O
negligible O
computational Metric
cost Metric
. O
We O
evaluate O
the O
improvements O
first O
on O
a O
Babel Task
task Task
for O
low Task
resource Task
speech Task
recognition Task
, O
obtaining O
an O
absolute O
5.77 O
% O
WER Metric
improvement O
over O
the O
baseline O
PLP Method
DNN Method
by O
training O
our O
CNN Method
on O
the O
combined O
data O
of O
six O
different O
languages O
. O
We O
then O
evaluate O
the O
very O
deep O
CNNs Method
on O
the O
Hub5’00 Material
benchmark Material
( O
using O
the O
262 O
hours O
of O
SWB Material
- Material
1 Material
training Material
data Material
) O
achieving O
a O
word Metric
error Metric
rate Metric
of O
11.8 O
% O
after O
cross Method
- Method
entropy Method
training Method
, O
a O
1.4 O
% O
WER Metric
improvement O
( O
10.6 O
% O
relative O
) O
over O
the O
best O
published O
CNN Method
result O
so O
far O
. O
TomSercu O
ChristianPuhrsch O
BrianKingsbury O
YannLeCun O
CenterforDataScience O
, O
CourantInstituteofMathematicalSciences O
, O
NewYorkUniversity O
IBMT.J.WatsonResearchCenter O
, O
YorktownHeights O
, O
NY O
, O
10598 O
, O
U.S.A. O
{ O
tsercu O
, O
bedk O
} O
@us.ibm.com O
, O
1cpuhrsch@nyu.edu O
, O
yann@cs.nyu.edu O
Convolutional Method
Networks Method
, O
Multilingual Task
, Task
Acoustic Task
Modeling Task
, O
Speech Task
Recognition Task
, O
Neural Task
Networks Task
section O
: O
INTRODUCTION O
Convolutional Method
Neural Method
Networks Method
( O
CNNs Method
) O
have O
recently O
pushed O
the O
state O
of O
the O
art O
on O
large Task
- Task
scale Task
tasks Task
in O
many O
domains O
dealing O
with O
natural O
data O
, O
most O
notably O
in O
computer Task
vision Task
tasks Task
like O
image Task
classification Task
, O
object Task
detection Task
, O
object Task
localization Task
and O
segmentation Task
. O
Early O
applications O
of O
neural Method
nets Method
to O
speech Task
recognition Task
used O
Time Method
- Method
Delay Method
Neural Method
Nets Method
which O
can O
be O
seen O
as O
simple O
forms O
of O
CNNs Method
without O
pooling O
or O
subsampling Method
. O
Full O
- O
fledged O
CNNs Method
with O
pooling Method
and O
subsampling Method
were O
soon O
applied O
to O
speech Task
recognition Task
and O
combined O
with O
dynamic Method
time Method
warping Method
. O
While O
the O
globally Method
- Method
trained Method
combination Method
of Method
neural Method
nets Method
and O
HMMs Method
for O
speech O
and O
handwriting O
goes O
back O
to O
the O
1990s O
, O
only O
due O
to O
recent O
developments O
HMM Method
/ Method
DNN Method
hybrid Method
modeling Method
became O
dominant O
in O
ASR Method
. O
In O
the O
context O
of O
these O
hybrid Method
models Method
, O
the O
use O
of O
CNNs Method
is O
relatively O
recent O
. O
CNNs Method
were O
shown O
to O
achieve O
state O
of O
the O
art O
performance O
on O
the O
benchmark O
datasets O
Broadcast O
News O
and O
Switchboard Material
300 Material
. O
However O
, O
in O
contrast O
to O
the O
trend O
in O
other O
domains O
where O
deeper Method
architectures Method
are O
often O
shown O
to O
gain O
performance O
, O
the O
classical O
CNN Method
architecture O
in O
LVCSR Task
has O
only O
two O
convolutional Method
layers Method
. O
Our O
network Method
architecture Method
( O
Section O
[ O
reference O
] O
) O
is O
strongly O
inspired O
by O
the O
work O
of O
Simonyan O
et O
al O
. O
( O
subsequently O
referred O
to O
as O
“ O
VGG Method
Net Method
” O
) O
which O
obtained O
second O
place O
in O
the O
classification Task
section O
of O
the O
Imagenet Task
2014 Task
competition Task
. O
The O
central O
idea O
of O
VGG Method
Net Method
is O
to O
replace O
large O
convolutional Method
kernels Method
by O
a O
stack Method
of Method
3 Method
3 Method
kernels Method
with O
ReLU Method
nonlinearities Method
without O
pooling O
between O
these O
layers O
; O
The O
authors O
argue O
the O
advantage O
of O
this O
is O
twofold O
: O
( O
1 O
) O
additional O
nonlinearity O
hence O
more O
expressive O
power O
, O
and O
( O
2 O
) O
a O
reduced O
number O
of O
parameters O
. O
Using O
these O
principles O
, O
very O
deep Method
networks Method
are O
trained O
with O
up O
to O
19 O
weight O
layers O
( O
of O
which O
16 O
are O
convolutional Method
and O
3 O
fully O
connected O
) O
. O
By O
contrast O
, O
the O
classical O
CNNs Method
deployed O
in O
LVCSR Task
have O
typically O
only O
two O
convolutional Method
layers Method
, O
use O
large O
( O
9 O
9 O
) O
kernels O
in O
the O
first O
layer O
, O
and O
use O
sigmoid Method
activation Method
functions Method
. O
The O
first O
goal O
of O
this O
work O
is O
to O
adapt O
the O
VGG Method
Net Method
architecture Method
to O
LVCSR Task
. O
Most O
closely O
related O
to O
this O
is O
, O
which O
also O
uses O
VGG O
Net O
- O
inspired O
CNNs Method
for O
LVCSR Task
. O
In O
contrast O
to O
our O
work O
, O
the O
architectures O
investigated O
in O
are O
quite O
different O
and O
the O
paper O
only O
provides O
results O
from O
training O
on O
a O
non O
- O
standard O
Switchboard Material
- Material
51h Material
dataset Material
, O
with O
WER Metric
not O
close O
to O
state O
of O
the O
art O
performance O
on O
Hub5’00 Material
. O
In O
the O
context O
of O
low Task
- Task
resource Task
language Task
tasks Task
, O
it O
can O
be O
crucial O
to O
leverage O
training O
data O
in O
languages O
other O
than O
the O
target O
language O
. O
Therefore O
we O
trained O
multilingual O
deep O
CNNs Method
, O
which O
we O
describe O
in O
Section O
[ O
reference O
] O
. O
This O
is O
related O
to O
multilingual Method
neural Method
networks Method
in O
hybrid Method
NN Method
- Method
HMM Method
systems Method
which O
have O
been O
extended O
to O
multilingual Method
bottleneck Method
architectures Method
for O
tandem Method
models Method
and O
have O
proven O
valuable O
for O
spoken Task
term Task
detection Task
. O
To O
our O
knowledge O
, O
no O
work O
has O
been O
published O
that O
extends O
the O
multilingual Task
setup Task
to O
CNNs Method
. O
The O
multi O
- O
scale O
features O
described O
in O
Section O
[ O
reference O
] O
aim O
at O
exploiting O
more O
context O
at O
very O
low O
computational Metric
cost Metric
. O
They O
are O
inspired O
by O
the O
recent O
success O
of O
combining O
information O
at O
multiple O
scales O
in O
tasks O
like O
traffic Task
sign Task
recognition Task
, O
semantic Task
segmentation Task
and O
depth Task
map Task
prediction Task
. O
In O
LVCSR Task
the O
multi Method
- Method
scale Method
idea Method
has O
been O
explored O
in O
tandem Method
systems Method
and O
the O
CLDNN Method
architecture Method
. O
As O
training Task
becomes O
more O
challenging O
with O
increasing O
depth O
, O
we O
used O
two O
recently O
proposed O
optimization Method
algorithms Method
, O
Adadelta Method
and O
Adam Method
( O
Section O
[ O
reference O
] O
) O
. O
Both O
algorithms O
are O
first Method
order Method
gradient Method
- Method
based Method
optimization Method
methods Method
, O
which O
keep O
track O
of O
an O
estimate O
of O
the O
first O
and O
second O
order O
moment O
of O
the O
gradient O
to O
tune O
the O
step O
size O
of O
each O
weight O
separately O
. O
The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
In O
Section O
[ O
reference O
] O
we O
introduce O
the O
novel O
aspects O
of O
our O
work O
: O
very O
deep O
CNN Method
architectures O
in O
[ O
reference O
] O
, O
multilingual O
CNN Method
training O
in O
[ O
reference O
] O
, O
multi O
- O
scale O
features O
in O
[ O
reference O
] O
, O
and O
training O
details O
in O
[ O
reference O
] O
. O
We O
then O
show O
experimental O
results O
on O
Babel O
in O
[ O
reference O
] O
and O
on O
Switchboard O
in O
[ O
reference O
] O
. O
section O
: O
ARCHITECTURAL Task
AND Task
TRAINING Task
NOVELTIES Task
subsection O
: O
Very O
Deep O
Convolutional Method
Networks Method
The O
very O
deep Method
convolutional Method
networks Method
we O
describe O
here O
are O
adaptations O
of O
the O
VGG Method
Net Method
architecture Method
to O
the O
LVCSR Task
domain O
, O
where O
until O
now O
networks O
with O
two O
convolutional Method
layers Method
dominated O
. O
Table O
[ O
reference O
] O
shows O
the O
configurations O
of O
the O
deep O
CNNs Method
. O
The O
deepest Method
configuration Method
, O
WDX Method
, O
has O
14 O
weight Method
layers Method
: O
10 O
convolutional Method
and O
4 O
fully O
connected O
. O
As O
in O
, O
we O
omit O
the O
Rectified Method
Linear Method
Unit Method
( Method
ReLU Method
) Method
layers Method
following O
every O
convolutional Method
and Method
fully Method
connected Method
layer Method
. O
The O
convolutional Method
layers Method
are O
written O
as O
conv O
( O
{input O
feature O
maps}–{output O
feature O
maps O
} O
) O
where O
each O
kernel O
is O
understood O
to O
be O
size O
3 O
3 O
. O
The O
pooling Method
layers Method
are O
written O
as O
( O
time O
x O
frequency O
) O
with O
stride O
equal O
to O
the O
pool O
size O
. O
For O
architectures O
VDX Method
and O
WDX Method
, O
we O
apply O
zero O
padding O
of O
size O
1 O
at O
every O
side O
before O
every O
convolution O
, O
while O
for O
architecture O
VC O
( O
X O
) O
and O
VB O
( O
X O
) O
we O
use O
the O
convolutions Method
to O
reduce O
the O
size O
of O
the O
feature O
maps O
, O
hence O
only O
in O
the O
higher O
layers O
of O
VC Method
( Method
X Method
) Method
padding Method
is O
applied O
. O
In O
contrast O
to O
, O
we O
do O
not O
reinitialize O
the O
deeper Method
models Method
with O
the O
shallower Method
models Method
. O
Each O
model O
is O
trained O
from O
scratch O
with O
random Method
initialization Method
from O
a O
uniform O
distribution O
in O
the O
range O
. O
This O
follows O
the O
argument O
of O
to O
initialize O
the O
weights O
such O
that O
the O
variance O
of O
the O
activations O
on O
each O
layer O
does O
not O
explode O
or O
vanish O
during O
the O
forward O
pass O
. O
subsection O
: O
Multilingual O
Convolutional Method
Networks Method
Figure O
[ O
reference O
] O
shows O
a O
multilingual Method
VBX Method
network Method
, O
which O
we O
used O
for O
most O
of O
our O
Babel Task
experiments Task
. O
It O
is O
similar O
to O
previous O
multilingual Method
deep Method
neural Method
networks Method
, O
with O
the O
main O
difference O
that O
the O
shared O
lower O
layers O
of O
the O
network O
are O
convolutional Method
. O
A O
second O
difference O
is O
that O
we O
untie O
more O
than O
only O
the O
last O
layer O
, O
meaning O
that O
the O
weights O
and O
biases O
of O
multiple O
fully O
connected O
layers O
are O
different O
for O
each O
language O
. O
Since O
the O
output O
dimension O
of O
the O
convolutional Method
stages Method
is O
typically O
large O
when O
using O
large O
context O
windows O
, O
most O
of O
the O
weights O
are O
in O
the O
first O
fully Method
connected Method
layer Method
, O
which O
acts O
on O
the O
flattened O
output O
of O
the O
convolutional Method
stages Method
. O
This O
is O
an O
argument O
to O
share O
this O
large O
, O
first O
fully Method
connected Method
layer Method
across O
languages O
. O
We O
experimentally O
confirmed O
that O
for O
all O
architectures O
, O
untying O
all O
fully O
connected O
layers O
except O
the O
lowest O
one O
gives O
optimal O
performance O
, O
with O
strong O
degradation O
if O
the O
first O
fully O
connected O
layer O
is O
also O
untied O
. O
This O
untying O
corresponds O
to O
a O
view O
of O
the O
shared O
layers O
and O
the O
first O
fully Method
connected Method
layer Method
as O
a O
shared Method
multilingual Method
feature Method
extractor Method
, O
while O
the O
fully O
connected O
layers O
higher O
up O
form O
the O
classifier Method
. O
The O
multilingual O
CNN Method
is O
trained O
in O
a O
round O
- O
robin O
fashion O
: O
we O
process O
a O
mini O
- O
batch O
for O
each O
language O
before O
making O
an O
update O
to O
the O
weights O
. O
In O
the O
shared O
part O
of O
the O
network O
the O
gradients O
of O
all O
mini O
- O
batches O
are O
accumulated O
between O
weight Method
updates Method
. O
subsection O
: O
Multi Task
- Task
scale Task
feature Task
maps Task
The O
main O
goal O
of O
constructing O
multi Task
- Task
scale Task
feature Task
maps Task
is O
to O
add O
more O
context O
without O
increasing O
the O
computational Metric
cost Metric
. O
Figure O
[ O
reference O
] O
illustrates O
the O
concept O
of O
multi Task
- Task
scale Task
feature Task
maps Task
, O
where O
additional O
input O
feature O
maps O
contain O
a O
larger O
view O
of O
the O
context O
of O
the O
frame O
by O
downsampling O
larger O
context O
windows O
with O
different O
strides O
. O
Kernels Method
on O
the O
first O
convolutional Method
layer Method
are O
able O
to O
combine O
information O
from O
multiple O
scales O
, O
i.e. O
different O
distances O
from O
the O
central O
frame O
. O
Because O
the O
only O
difference O
for O
the O
convnet Method
configuration Method
is O
the O
first O
convolutional Method
layer Method
having O
more O
feature O
maps O
, O
the O
additional O
computational Metric
cost Metric
and O
number O
of O
parameters O
is O
small O
. O
We O
found O
this O
style O
of O
multi Method
- Method
scale Method
training Method
to O
give O
small O
gains O
. O
Increasing O
the O
context O
size O
had O
a O
stronger O
positive O
impact O
, O
though O
at O
the O
expense O
of O
increased O
computational Metric
cost Metric
. O
subsection O
: O
Training O
We O
use O
Adadelta Method
and O
Adam Method
to O
do O
initial O
training O
of O
the O
deep O
CNNs Method
. O
Using O
Adadelta Method
has O
two O
main O
advantages O
. O
Firstly O
, O
in O
our O
experience O
the O
optimization Task
problem Task
converges O
much O
faster O
than O
with O
SGD Method
; O
for O
the O
Babel Task
experiments Task
we O
typically O
see O
convergence O
after O
about O
40 O
million O
frames O
using O
the O
18 O
hours O
of O
Babel Material
training Material
data Material
( O
after O
silence O
removal O
about O
5.8 O
million O
frames O
) O
. O
Secondly O
, O
the O
optimal O
working O
point O
of O
Adadelta Method
’s Method
hyperparameters Method
and O
was O
stable O
across O
architectures O
, O
always O
giving O
optimal O
performance O
. O
This O
was O
crucial O
in O
order O
to O
explore O
architectural O
variations O
. O
After O
initial O
training O
with O
Adadelta Method
, O
we O
fine O
tune O
using O
SGD Method
with O
a O
small O
learning Metric
rate Metric
. O
Another O
aspect O
of O
training O
that O
improved O
our O
results O
is O
data Task
balancing Task
( O
something O
similar O
was O
done O
in O
) O
. O
We O
construct O
batches O
on O
the O
fly O
by O
sampling O
target O
with O
probability O
, O
where O
is O
related O
to O
the O
frequency O
of O
context O
dependent O
state O
as O
. O
After O
sampling O
, O
we O
sample O
uniformly O
across O
all O
frames O
with O
that O
target O
. O
The O
exponent O
takes O
values O
between O
balanced O
training O
( O
) O
and O
unbalanced Method
training Method
using O
the O
natural O
frequencies O
( O
) O
. O
In O
our O
experiments O
on O
Babel O
it O
proved O
optimal O
to O
start O
with O
and O
raise O
it O
during O
training O
to O
its O
final O
value O
of O
. O
In O
our O
experiments O
on O
switchboard O
we O
varied O
typically O
from O
0.4 O
to O
0.8 O
and O
decoded O
with O
HMM O
priors O
adjusted O
to O
match O
the O
final O
distribution O
. O
section O
: O
EXPERIMENTAL O
RESULTS O
subsection O
: O
Babel O
Our O
first O
set O
of O
experiments O
on O
Babel Material
focuses O
on O
the O
multilingual Task
and Task
multi Task
- Task
scale Task
aspects Task
of O
this O
work O
. O
The O
IARPA O
Babel Task
program Task
is O
aimed O
at O
developing O
robust O
keyword Method
search Method
technology Method
for O
low O
resource O
languages O
. O
Though O
the O
word Metric
error Metric
rates Metric
reported O
here O
are O
too O
high O
to O
be O
useful O
for O
simple O
speech Task
to Task
text Task
applications Task
, O
useful O
keyword Task
search Task
( O
KWS Method
) Method
systems Method
can O
still O
be O
built O
based O
on O
these O
ASR Method
models Method
. O
As O
training O
data O
we O
use O
a O
combination O
of O
6 O
languages O
, O
with O
3 O
hours O
of O
training O
data O
per O
language O
. O
The O
languages O
used O
for O
training O
are O
languages O
from O
the O
second O
Option O
Period O
of O
the O
Babel Task
program Task
, O
i.e. O
Kurmanji O
( O
KUR O
) O
, O
Tok O
Pisin O
( O
TOK O
) O
, O
Cebuano O
( O
CEB Material
) O
, O
Kazakh Material
( O
KAZ O
) O
, O
Telugu Material
( O
TEL O
) O
, O
and O
Lithuanian O
( O
LIT O
) O
. O
The O
features O
used O
in O
these O
experiments O
are O
standard O
log O
- O
Mel O
features O
, O
standardized O
with O
a O
global O
mean O
and O
variance O
shared O
across O
the O
speakers O
and O
langauges O
. O
Unless O
explicitly O
mentioned O
, O
we O
use O
multi O
- O
scale O
features O
with O
context O
20 O
in O
the O
Babel Task
experiments Task
. O
We O
report O
results O
after O
cross Method
- Method
entropy Method
training Method
with O
adadelta Method
( Method
, O
) O
, O
and O
varying O
from O
0 O
to O
1 O
. O
We O
trained O
the O
multilingual O
deep O
CNN Method
architecture O
on O
6 O
Babel Material
languages Material
using O
alignments O
from O
6 O
baseline O
speaker Method
independent Method
HMM Method
/ Method
DNN Method
systems Method
using O
PLP O
features O
, O
with O
1000 O
context O
dependent O
states O
. O
The O
context O
dependent O
states O
are O
specific O
to O
each O
language O
. O
Each O
baseline O
system O
is O
cross Method
- Method
entropy Method
trained O
on O
a O
single O
language O
with O
3 O
hours O
of O
data O
. O
We O
will O
report O
the O
WER Metric
of O
the O
CNNs Method
compared O
to O
the O
baseline O
DNN O
, O
and O
summarize O
this O
in O
the O
average Metric
absolute Metric
WER Metric
improvement Metric
over O
the O
baseline O
DNN O
, O
which O
gives O
one O
number O
to O
compare O
different O
models O
. O
The O
WER Metric
improvements Metric
over O
the O
baseline O
DNN O
are O
fairly O
consistent O
across O
languages O
. O
Tables O
[ O
reference O
] O
through O
[ O
reference O
] O
show O
the O
results O
outlining O
the O
performance O
gains O
from O
the O
different O
architectural O
improvements O
discussed O
in O
Section O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
and O
[ O
reference O
] O
respectively O
. O
From O
table O
[ O
reference O
] O
note O
that O
even O
in O
the O
monolingual O
case O
( O
3 O
hours O
of O
data O
) O
the O
VBX O
CNN Method
architecture O
outperforms O
both O
the O
classical O
CNN Method
and O
the O
baseline Method
DNN Method
. O
subsection O
: O
Switchboard O
300 O
We O
evaluate O
our O
deep O
CNN Method
architecture O
by O
training O
on O
the O
262 Material
- Material
hour Material
SWB Material
- Material
1 Material
training Material
data Material
, O
and O
report O
the O
Word Metric
Error Metric
Rates Metric
on O
Hub5’00 Material
SWB O
( O
table O
[ O
reference O
] O
) O
. O
The O
Switchboard Task
experiments Task
focus O
on O
the O
very O
deep O
aspect O
of O
our O
work O
. O
Apart O
from O
not O
involving O
multilingual Task
training Task
, O
we O
did O
not O
use O
multi O
- O
scale O
features O
in O
the O
Switchboard Task
experiments Task
, O
but O
did O
use O
speaker O
- O
dependent O
VTLN O
and O
deltas O
and O
double O
deltas O
as O
this O
is O
shown O
to O
help O
performance O
for O
classical O
CNNs Method
. O
In O
the O
switchboard O
experiments O
, O
using O
a O
large O
context O
only O
gave O
marginal O
gains O
which O
were O
not O
worth O
the O
computational Metric
cost Metric
, O
so O
we O
worked O
with O
context O
windows O
of O
8 O
. O
We O
use O
a O
data O
balancing O
value O
of O
, O
chosen O
from O
. O
After O
training O
with O
multiple O
combinations O
of O
Adam Method
, O
Adadelta Method
and O
SGD Method
, O
we O
settled O
on O
two O
possible O
strategies O
for O
optimization Task
: O
the O
first O
strategy O
is O
to O
use O
Adadelta Method
or O
Adam Method
for O
initial O
training Task
, O
followed O
by O
SGD Method
finetuning Method
. O
This O
way O
one O
can O
typically O
achieve O
good O
performance O
in O
minimal Metric
time Metric
. O
The O
second O
strategy O
, O
training O
from O
scratch O
using O
only O
SGD Method
, O
requires O
more O
training O
, O
however O
the O
performance O
is O
slightly O
superior O
. O
Classical Method
momentum Method
yielded O
no O
gains O
and O
sometimes O
slight O
degradation O
over O
plain O
SGD Method
. O
We O
provide O
the O
results O
and O
total O
number O
of O
frames O
until O
convergence O
. O
Note O
that O
with O
the O
first O
strategy O
, O
we O
achieve O
12.2 O
% O
WER Metric
after O
140 O
M O
frames O
, O
i.e. O
only O
1.5 O
passes O
through O
the O
dataset O
( O
which O
has O
92.1 O
M O
frames O
) O
. O
Using O
just O
SGD Method
we O
achieve O
11.8 O
% O
WER Metric
in O
3.5 O
passes O
through O
the O
data O
. O
We O
only O
present O
results O
after O
cross Method
- Method
entropy Method
training Method
, O
so O
we O
compare O
against O
the O
best O
published O
cross O
- O
entropy O
trained O
CNNs Method
. O
The O
baseline O
is O
the O
work O
of O
Soltau O
et O
al O
. O
using O
classical O
CNNs Method
with O
512 O
feature O
maps O
on O
both O
convolutional Method
layers Method
. O
A O
second O
baseline O
is O
the O
work O
of O
Saon O
et O
al O
. O
which O
introduces O
annealed O
dropout O
maxout O
CNN Method
’s O
with O
a O
large O
number O
of O
HMM O
states O
, O
achieving O
12.6 O
% O
WER Metric
after O
cross Method
- Method
entropy Method
training Method
( O
not O
in O
the O
paper O
, O
from O
personal O
communication O
) O
. O
Note O
that O
these O
improvements O
could O
readily O
be O
integrated O
with O
our O
very O
deep O
CNN Method
architectures O
. O
section O
: O
DISCUSSION O
In O
this O
paper O
we O
proposed O
a O
number O
of O
architectural Method
advances Method
in O
CNNs Method
for O
LVCSR Task
. O
We O
introduced O
a O
very O
deep Method
convolutional Method
network Method
architecture Method
with O
small O
3 O
3 O
kernels O
and O
multiple O
convolutional Method
layers Method
before O
each O
pooling Method
layer Method
, O
inspired O
by O
the O
VGG Method
Imagenet Method
2014 Method
architecture Method
. O
Our O
best O
performing O
model O
has O
14 O
weight O
layers O
. O
We O
also O
introduced O
multilingual O
CNNs Method
which O
proved O
valuable O
in O
the O
context O
of O
low Task
resource Task
speech Task
recognition Task
. O
We O
introduced O
multi O
- O
scale O
input O
features O
aimed O
at O
exploiting O
more O
acoustic O
context O
with O
minimal O
computational Metric
increase Metric
. O
We O
showed O
an O
improvement O
of O
2.50 O
% O
WER Metric
over O
a O
standard O
DNN Method
PLP Method
baseline Method
using O
3 O
hours O
of O
data O
, O
and O
an O
improvement O
of O
5.77 O
% O
WER Metric
by O
combining O
six O
languages O
to O
train O
on O
18 O
hours O
of O
data O
. O
We O
then O
showed O
results O
on O
Hub5’00 Material
after O
training O
on O
262 O
hours O
of O
SWB Material
- Material
1 Material
data Material
where O
we O
get O
11.8 O
% O
WER Metric
, O
which O
is O
an O
improvement O
of O
2.0 O
% O
WER Metric
( O
14.5 O
% O
relative O
) O
over O
our O
own O
baseline O
, O
and O
a O
1.4 O
% O
WER Metric
( O
10.6 O
% O
relative O
) O
improvement O
over O
the O
best O
result O
published O
on O
classical O
CNNs Method
after O
cross Method
- Method
entropy Method
training Method
. O
We O
expect O
additional O
gains O
from O
sequence Task
training Task
, O
joint Method
training Method
with O
DNNs Method
, O
and O
integrating O
improvements O
like O
annealed Method
dropout Method
and O
maxout Method
nonlinearities Method
. O
section O
: O
ACKNOWLEDGEMENT O
This O
effort O
uses O
the O
very O
limited O
language O
packs O
from O
IARPA Method
Babel Method
Program Method
language Method
collections Method
IARPA O
- O
babel205b O
- O
v1.0a O
, O
IARPA O
- O
babel207b O
- O
v1.0e O
, O
IARPA O
- O
babel301b O
- O
v2.0b O
, O
IARPA Method
- O
babel302b O
- O
v1.0a O
, O
IARPA Method
- O
babel303b O
- O
v1.0a O
, O
and O
IARPA Method
- O
babel304b O
- O
v1.0b O
. O
This O
work O
is O
supported O
by O
the O
Intelligence O
Advanced O
Research O
Projects O
Activity O
( O
IARPA O
) O
via O
Department O
of O
Defense O
U.S. O
Army O
Research O
Laboratory O
( O
DoD O
/ O
ARL O
) O
contract O
number O
W911NF O
- O
12 O
- O
C O
- O
0012 O
. O
The O
U.S. O
Government O
is O
authorized O
to O
reproduce O
and O
distribute O
reprints O
for O
Governmental O
purposes O
notwithstanding O
any O
copyright O
annotation O
thereon O
. O
Disclaimer O
: O
The O
views O
and O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
official O
policies O
or O
endorsements O
, O
either O
expressed O
or O
implied O
, O
of O
IARPA O
, O
DoD O
/ O
ARL O
, O
or O
the O
U.S. O
Government O
. O
We O
gratefully O
acknowledge O
the O
support O
of O
NVIDIA O
Corporation O
. O
The O
authors O
would O
like O
to O
thank O
Pierre O
Sermanet O
for O
the O
initial O
code O
base O
, O
George O
Saon O
, O
Vaibhava O
Goel O
, O
Etienne O
Marcheret O
and O
Xiaodong O
Cui O
for O
valuable O
discussions O
and O
comments O
. O
bibliography O
: O
References O
