Virtual Method
Adversarial Method
Training Method
: O
A O
Regularization Method
Method Method
for O
Supervised Task
and O
Semi Task
- Task
Supervised Task
Learning Task
section O
: O
Abstract O
- O
We O
propose O
a O
new O
regularization Method
method Method
based O
on O
virtual Method
adversarial Method
loss Method
: O
a O
new O
measure O
of O
local O
smoothness O
of O
the O
conditional O
label O
distribution O
given O
input O
. O
Virtual O
adversarial O
loss O
is O
defined O
as O
the O
robustness Metric
of O
the O
conditional O
label O
distribution O
around O
each O
input O
data O
point O
against O
local O
perturbation O
. O
Unlike O
adversarial Method
training Method
, O
our O
method O
defines O
the O
adversarial O
direction O
without O
label O
information O
and O
is O
hence O
applicable O
to O
semi Task
- Task
supervised Task
learning Task
. O
Because O
the O
directions O
in O
which O
we O
smooth O
the O
model O
are O
only O
" O
virtually O
" O
adversarial O
, O
we O
call O
our O
method O
virtual O
adversarial Method
training Method
( O
VAT Method
) O
. O
The O
computational Metric
cost Metric
of O
VAT Method
is O
relatively O
low O
. O
For O
neural Method
networks Method
, O
the O
approximated O
gradient O
of O
virtual O
adversarial O
loss O
can O
be O
computed O
with O
no O
more O
than O
two O
pairs O
of O
forward Method
- Method
and Method
back Method
- Method
propagations Method
. O
In O
our O
experiments O
, O
we O
applied O
VAT Method
to O
supervised Task
and O
semi Task
- Task
supervised Task
learning Task
tasks Task
on O
multiple O
benchmark O
datasets O
. O
With O
a O
simple O
enhancement O
of O
the O
algorithm O
based O
on O
the O
entropy Method
minimization Method
principle Method
, O
our O
VAT Method
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
semi Task
- Task
supervised Task
learning Task
tasks Task
on O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
section O
: O
INTRODUCTION O
I O
N O
practical Task
regression Task
and Task
classification Task
problems Task
, O
one O
must O
face O
two O
problems O
on O
opposite O
ends O
; O
underfitting O
and O
overfitting O
. O
On O
one O
end O
, O
poor O
design O
of O
model O
and O
optimization Method
process Method
can O
result O
in O
large O
error O
for O
both O
training O
and O
testing O
dataset O
( O
underfitting O
) O
. O
On O
the O
other O
end O
, O
the O
size O
of O
the O
sample O
that O
can O
be O
used O
to O
tune O
the O
parameters O
of O
model O
is O
always O
finite O
, O
and O
the O
evaluation O
of O
the O
objective Metric
function Metric
in O
practice O
will O
always O
be O
a O
mere O
empirical O
approximation O
of O
the O
true O
expectation O
of O
the O
target O
value O
over O
the O
sample O
space O
. O
Therefore O
, O
even O
with O
successful O
optimization Method
and O
low Metric
error Metric
rate Metric
on O
the O
training O
dataset O
( O
training Metric
error Metric
) O
, O
the O
true Metric
expected Metric
error Metric
( O
test Metric
error Metric
) O
can O
be O
large O
[ O
reference O
] O
, O
[ O
reference O
] O
( O
overfitting O
) O
. O
The O
subject O
of O
our O
study O
is O
the O
latter O
. O
Regularization Method
is O
a O
process O
of O
introducing O
additional O
information O
in O
order O
to O
manage O
this O
inevitable O
gap O
between O
the O
training Metric
error Metric
and O
the O
test Metric
error Metric
. O
In O
this O
study O
, O
we O
introduce O
a O
novel O
regularization Method
method Method
applicable O
to O
semisupervised Task
learning Task
that O
identifies O
the O
direction O
in O
which O
the O
classifier O
's O
behavior O
is O
most O
sensitive O
. O
Regularization Method
is O
often O
carried O
out O
by O
augmenting O
the O
loss O
function O
with O
a O
so O
- O
called O
regularization O
term O
, O
which O
prevents O
the O
model O
from O
overfitting O
to O
the O
loss O
function O
evaluated O
on O
a O
finite O
set O
of O
sample O
points O
. O
From O
Bayesian O
standpoint O
, O
regularization O
term O
can O
be O
interpreted O
as O
a O
prior O
distribution O
that O
reflects O
our O
educated O
a O
priori O
knowledge O
or O
belief O
regarding O
the O
model O
[ O
reference O
] O
. O
A O
popular O
a O
priori O
belief O
based O
on O
widely O
observed O
facts O
is O
that O
the O
outputs O
of O
most O
naturally O
occurring O
systems O
are O
smooth O
with O
respect O
to O
spatial O
and O
/ O
or O
temporal O
inputs O
[ O
reference O
] O
. O
What O
often O
underlie O
this O
belief O
are O
the O
laws O
of O
physics O
governing O
the O
system O
• O
Contact O
: O
takeru.miyato@gmail.com O
• O
* O
Preferred Method
Networks Method
, O
Inc. O
, O
Tokyo O
, O
Japan O
, O
† O
Graduate O
school O
of O
Informatics O
, O
Kyoto O
University O
, O
Kyoto O
, O
Japan O
, O
‡ O
ATR O
Cognitive O
Mechanisms O
Laboratories O
, O
Kyoto O
, O
Japan O
, O
§ O
Department O
of O
Mathematical O
Science O
, O
Ritsumeikan O
University O
, O
Kyoto O
, O
Japan O
of O
interest O
, O
which O
in O
many O
cases O
are O
described O
by O
smooth Method
models Method
based O
on O
differential Method
equations Method
[ O
reference O
] O
. O
When O
we O
are O
constructing O
the O
probability Method
model Method
, O
this O
belief O
prompts O
us O
to O
prefer O
conditional O
output O
distribution O
p O
( O
y|x O
) O
( O
or O
just O
output O
distribution O
for O
short O
) O
that O
are O
smooth O
with O
respect O
to O
conditional O
input O
x. O
In O
fact O
, O
smoothing O
the O
output O
distribution O
often O
works O
to O
our O
advantage O
in O
actual O
practice O
. O
For O
example O
, O
label Task
propagation Task
[ O
reference O
] O
is O
an O
algorithm O
that O
improves O
the O
performance O
of O
classifier Method
by O
assigning O
class O
labels O
to O
unlabeled O
training O
samples O
based O
on O
the O
belief O
that O
close O
input O
data O
points O
tend O
to O
have O
similar O
class O
labels O
. O
Also O
, O
it O
is O
known O
that O
, O
for O
neural Method
networks Method
( O
NNs Method
) O
, O
one O
can O
improve O
the O
generalization Task
performance O
by O
applying O
random O
perturbations O
to O
each O
input O
in O
order O
to O
generate O
artificial O
input O
points O
and O
encouraging O
the O
model O
to O
assign O
similar O
outputs O
to O
the O
set O
of O
artificial O
inputs O
derived O
from O
the O
same O
point O
[ O
reference O
] O
. O
Several O
studies O
have O
also O
confirmed O
that O
this O
philosophy O
of O
making O
the O
predictor Method
robust O
against O
random O
and O
local O
perturbation O
is O
effective O
in O
semi Task
- Task
supervised Task
learning Task
. O
For O
selected O
examples O
, O
see O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
However O
, O
[ O
reference O
] O
and O
[ O
reference O
] O
found O
a O
weakness O
in O
naive O
application O
of O
this O
philosophy O
. O
They O
found O
that O
standard O
isotropic Method
smoothing Method
via O
random Method
noise Method
and O
random Method
data Method
augmentation Method
often O
leaves O
the O
predictor O
particularly O
vulnerable O
to O
a O
small O
perturbation O
in O
a O
specific O
direction O
, O
that O
is O
, O
the O
adversarial O
direction O
, O
which O
is O
the O
direction O
in O
the O
input O
space O
in O
which O
the O
label O
probability O
p O
( O
y O
= O
k|x O
) O
of O
the O
model O
is O
most O
sensitive O
. O
[ O
reference O
] O
and O
[ O
reference O
] O
experimentally O
verified O
that O
the O
predictors Method
trained O
with O
the O
standard O
regularization Method
technique Method
such O
as O
L Method
1 Method
and O
L Method
2 Method
regularization Method
are O
likely O
to O
make O
mistakes O
when O
the O
signal O
is O
perturbed O
in O
the O
adversarial O
direction O
, O
even O
when O
the O
norm O
of O
the O
perturbation O
is O
so O
small O
that O
it O
can O
not O
be O
perceived O
by O
human O
eyes O
. O
Inspired O
by O
this O
finding O
, O
Goodfellow O
et O
al O
. O
[ O
reference O
] O
developed O
adversarial Method
training Method
that O
trains O
the O
model O
to O
assign O
to O
each O
in O
- O
put O
data O
a O
label O
that O
is O
similar O
to O
the O
labels O
to O
be O
assigned O
to O
its O
neighbors O
in O
the O
adversarial O
direction O
. O
This O
attempt O
succeeded O
in O
improving O
generalization Task
performance O
and O
made O
the O
model O
robust O
against O
adversarial Method
perturbation Method
. O
Goodfellow O
et O
al O
. O
[ O
reference O
] O
s O
work O
suggests O
that O
the O
locally O
isotropic O
output O
distribution O
can O
not O
be O
achieved O
by O
making O
the O
model O
robust O
against O
isotropic O
noise O
. O
In O
retrospect O
, O
this O
observation O
is O
in O
fact O
quite O
intuitive O
. O
If O
the O
distribution O
around O
a O
given O
input O
is O
anisotropic O
and O
the O
goal O
is O
to O
resolve O
this O
anisotropy O
, O
it O
does O
not O
make O
much O
sense O
to O
exert O
equal O
smoothing O
" O
pressure O
" O
into O
all O
directions O
. O
Our O
proposed O
regularization Method
technique Method
is O
a O
method O
that O
trains O
the O
output O
distribution O
to O
be O
isotropically O
smooth O
around O
each O
input O
data O
point O
by O
selectively O
smoothing O
the O
model O
in O
its O
most O
anisotropic O
direction O
. O
In O
order O
to O
quantify O
this O
idea O
, O
we O
introduce O
a O
notion O
of O
virtual O
adversarial O
direction O
, O
which O
is O
a O
direction O
of O
the O
perturbation O
that O
can O
most O
greatly O
alter O
the O
output O
distribution O
in O
the O
sense O
of O
distributional O
divergence O
. O
Virtual O
adversarial O
direction O
is O
our O
very O
interpretation O
of O
the O
' O
most O
' O
anisotropic O
direction O
. O
In O
contrast O
, O
adversarial O
direction O
introduced O
by O
Goodfellow O
et O
al O
. O
[ O
reference O
] O
at O
an O
input O
data O
point O
is O
a O
direction O
of O
the O
perturbation O
that O
can O
most O
reduce O
the O
model O
's O
probability O
of O
correct Metric
classification Metric
, O
or O
the O
direction O
that O
can O
most O
greatly O
" O
deviate O
" O
the O
prediction O
of O
the O
model O
from O
the O
correct O
label O
. O
Unlike O
adversarial O
direction O
, O
virtual O
adversarial O
direction O
can O
be O
defined O
on O
unlabeled O
data O
point O
, O
because O
it O
is O
the O
direction O
that O
can O
most O
greatly O
deviate O
the O
current O
inferred O
output O
distribution O
from O
the O
status O
quo O
. O
In O
other O
words O
, O
even O
in O
the O
absence O
of O
label O
information O
, O
virtual O
adversarial O
direction O
can O
be O
defined O
on O
an O
unlabeled O
data O
point O
as O
if O
there O
is O
a O
" O
virtual O
" O
label O
; O
hence O
the O
name O
" O
virtual O
" O
adversarial O
direction O
. O
With O
the O
definition O
of O
virtual O
adversarial O
direction O
, O
we O
can O
quantify O
the O
local O
anisotropy O
of O
the O
model O
at O
each O
input O
point O
without O
using O
the O
supervisory O
signal O
. O
We O
define O
the O
local Metric
distributional Metric
smoothness Metric
( O
LDS Method
) O
to O
be O
the O
divergencebased Metric
distributional Metric
robustness Metric
of O
the O
model O
against O
virtual O
adversarial O
direction O
. O
We O
propose O
a O
novel O
training Method
method Method
that O
uses O
an O
efficient O
approximation O
in O
order O
to O
maximize O
the O
likelihood O
of O
the O
model O
while O
promoting O
the O
model O
's O
LDS Method
on O
each O
training O
input O
data O
point O
. O
For O
brevity O
, O
we O
call O
this O
method O
virtual O
adversarial Method
training Method
( O
VAT Method
) O
. O
The O
following O
list O
summarizes O
the O
advantages O
of O
this O
new O
method O
: O
• O
applicability O
to O
semi Task
- Task
supervised Task
learning Task
tasks Task
• O
applicability O
to O
any O
parametric Method
models Method
for O
which O
we O
can O
evaluate O
the O
gradient O
with O
respect O
to O
input O
and O
parameter O
• O
small O
number O
of O
hyperparameters O
• O
parametrization Method
invariant Method
regularization Method
The O
second O
advantage O
is O
worth O
emphasizing O
. O
At O
first O
glance O
, O
our O
algorithm O
may O
appear O
as O
if O
it O
needs O
to O
solve O
an O
internal Task
optimization Task
problem Task
in O
order O
to O
determine O
the O
virtual O
adversarial O
direction O
. O
For O
models O
such O
as O
NNs Method
for O
which O
we O
can O
evaluate O
the O
gradient O
of O
the O
output O
with O
respect O
to O
the O
input O
, O
however O
, O
virtual Method
adversarial Method
perturbation Method
admits O
an O
approximation O
that O
can O
be O
computed O
efficiently O
with O
the O
power Method
method Method
[ O
reference O
] O
. O
This O
property O
enables O
us O
to O
implement O
VAT Method
for O
NNs Method
with O
no O
more O
than O
three O
times O
the O
computational Metric
cost Metric
of O
the O
standard O
, Method
regularization Method
- Method
free Method
training Method
. O
This O
approximation O
step O
is O
an O
important O
part O
of O
the O
VAT Method
algorithm O
that O
makes O
it O
readily O
applicable O
for O
various O
settings O
and O
model O
architectures O
. O
Finally O
, O
the O
fourth O
advantage O
is O
not O
to O
be O
overlooked O
, O
because O
this O
is O
the O
most O
essential O
point O
at O
which O
our O
VAT Method
is O
fundamentally O
different O
from O
popular O
regularization Method
methods Method
like O
L Method
p Method
regularization Method
. O
For O
linear Method
models Method
, O
L Method
p Method
regularization Method
has O
an O
effect O
of O
mitigating O
the O
oversensitivity O
of O
the O
output O
with O
respect O
to O
input O
, O
and O
one O
can O
control O
the O
strength O
of O
its O
effect O
via O
the O
hyperparameters O
. O
When O
the O
model O
in O
concern O
is O
highly O
nonlinear O
, O
as O
in O
the O
case O
of O
neural Method
networks Method
, O
however O
, O
the O
user O
has O
little O
control O
over O
the O
effect O
of O
L Method
p Method
regularization Method
. O
Manipulation O
of O
the O
parameters O
in O
the O
first O
layer O
would O
have O
different O
effect O
on O
the O
final O
output O
depending O
on O
the O
choice O
of O
the O
parameters O
in O
the O
middle O
layers O
, O
and O
the O
same O
argument O
applies O
to O
the O
effect O
of O
regularization O
. O
In O
the O
language O
of O
Bayesian Task
statistics Task
with O
which O
we O
interpret O
the O
regularization O
term O
as O
prior O
distribution O
, O
this O
is O
to O
say O
that O
the O
nature O
of O
the O
prior O
distributions O
favored O
by O
the O
L Method
p Method
regularization Method
depends O
on O
the O
current O
parameter O
- O
setting O
and O
is O
hence O
ambiguous O
and O
difficult O
to O
assess O
. O
Parameterization Method
invariant Method
regularization Method
, O
on O
the O
other O
hand O
, O
does O
not O
suffer O
from O
such O
a O
problem O
. O
In O
more O
precise O
terms O
, O
by O
parametrization Method
invariant Method
regularization Method
we O
mean O
the O
regularization Method
based O
on O
an O
objective Method
function Method
L O
( O
θ O
) O
with O
the O
property O
that O
the O
corresponding O
optimal O
distribution O
p O
( O
X O
; O
θ O
* O
) O
is O
invariant O
under O
the O
oneto O
- O
one O
transformation O
ω O
= O
T O
( O
θ O
) O
, O
. O
VAT Method
is O
a O
parameterization Method
invariant Method
regularization Method
, O
because O
it O
directly O
regularizes O
the O
output O
distribution O
by O
its O
local O
sensitivity O
of O
the O
output O
with O
respect O
to O
input O
, O
which O
is O
, O
by O
definition O
, O
independent O
from O
the O
way O
to O
parametrize O
the O
model O
. O
When O
we O
applied O
VAT Method
to O
the O
supervised Task
and O
semisupervised O
learning O
for O
the O
permutation Task
invariant Task
task Task
on O
the O
MNIST O
dataset O
, O
our O
method O
outperformed O
all O
contemporary O
methods O
other O
than O
some O
cutting Method
- Method
edge Method
methods Method
that O
use O
sophisticated O
network Method
architecture Method
. O
We O
also O
applied O
our O
method O
to O
semi Task
- Task
supervised Task
learning Task
on O
CIFAR Material
- Material
10 Material
and O
Street Material
View Material
House Material
Numbers Material
( O
SVHN Material
) O
datasets O
, O
and O
confirmed O
that O
our O
method O
achieves O
superior O
or O
comparable O
performance O
in O
comparison O
to O
those O
of O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
This O
article O
also O
extends O
our O
earlier O
work O
[ O
reference O
] O
in O
five O
aspects O
: O
• O
clarification O
of O
the O
objective Metric
function Metric
• O
comparison O
between O
VAT Method
and O
random Method
perturbation Method
training Method
( O
RPT Method
) O
1 O
• O
additional O
set O
of O
extensive O
experiments O
• O
evaluation O
of O
the O
virtual O
adversarial O
examples O
• O
enhancement O
of O
the O
algorithm O
with O
entropy Method
minimization Method
section O
: O
RELATED O
WORKS O
Many O
classic O
regularization Method
methods Method
for O
NNs Method
regularize O
the O
models O
by O
applying O
random O
perturbations O
to O
input O
and O
hidden O
layers O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
An O
early O
work O
by O
[ O
reference O
] O
. O
a O
downgraded O
version O
of O
VAT Method
introduced O
in O
this O
paper O
that O
smooths O
the O
label O
distribution O
at O
each O
point O
with O
same O
force O
in O
all O
directions O
. O
Please O
see O
the O
detail O
definition O
of O
RPT Method
in O
Section O
3.4 O
. O
Bishop O
[ O
reference O
] O
showed O
that O
adding O
Gaussian O
perturbation O
to O
inputs O
during O
the O
training Task
process Task
is O
equivalent O
to O
adding O
an O
extra O
regularization O
term O
to O
the O
objective O
function O
. O
For O
small O
perturbations O
, O
the O
regularization O
term O
induced O
by O
such O
perturbation O
behaves O
similarly O
to O
a O
class O
of O
Tikhonov Method
regularizers Method
[ O
reference O
] O
. O
The O
application O
of O
random O
perturbations O
to O
inputs O
has O
an O
effect O
of O
smoothing O
the O
input O
- O
output O
relation O
of O
the O
NNs Method
. O
Another O
way O
to O
smooth O
the O
input O
- O
output O
relation O
is O
to O
impose O
constraints O
on O
the O
derivatives O
. O
For O
example O
, O
constraints O
may O
be O
imposed O
on O
the O
Frobenius O
norm O
of O
the O
Jacobian O
matrix O
of O
the O
output O
with O
respect O
to O
the O
input O
. O
This O
approach O
was O
taken O
by O
Gu O
and O
Rigazio O
[ O
reference O
] O
in O
their O
deep Method
contractive Method
network Method
. O
Instead O
of O
computing O
the O
computationally O
expensive O
full O
Jacobian O
, O
however O
, O
they O
approximated O
the O
Jacobian O
by O
the O
sum O
of O
the O
Frobenius O
norms O
of O
the O
layer O
- O
wise O
Jacobians O
computed O
for O
all O
adjacent O
pairs O
of O
hidden O
layers O
. O
Possibly O
because O
of O
their O
layer Method
- Method
wise Method
approximation Method
, O
however O
, O
deep Method
contractive Method
network Method
was O
not O
successful O
in O
significantly O
decreasing O
the O
test Metric
error Metric
. O
Dropout Method
[ O
reference O
] O
is O
another O
popular O
method O
for O
regularizing Task
NNs Task
with O
random O
noise O
. O
Dropout Method
is O
a O
method O
that O
assigns O
random O
masks O
on O
inputs O
/ O
hidden O
layers O
in O
the O
network O
during O
its O
training O
. O
From O
a O
Bayesian Method
perspective Method
, O
dropout Method
is O
a O
method O
to O
introduce O
prior O
distribution O
for O
the O
parameters O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
In O
this O
interpretation O
, O
dropout Method
is O
a O
method O
that O
regularizes O
the O
model O
via O
Bayesian Method
model Method
ensembles Method
, O
and O
is O
complementary O
to O
our O
approach O
, O
which O
directly O
augments O
the O
function O
with O
a O
regularization O
term O
. O
Adversarial Method
training Method
was O
originally O
proposed O
by O
[ O
reference O
] O
. O
They O
discovered O
that O
some O
architectures O
of O
NNs Method
, O
including O
state O
- O
of O
- O
the O
- O
art O
NN Method
models O
, O
are O
particularly O
vulnerable O
to O
a O
perturbation O
applied O
to O
an O
input O
in O
the O
direction O
to O
which O
the O
models O
' O
label O
assignment O
to O
the O
input O
is O
most O
sensitive O
( O
adversarial O
) O
, O
even O
when O
the O
perturbation O
is O
so O
small O
that O
human O
eyes O
can O
not O
discern O
the O
difference O
. O
They O
also O
showed O
that O
training O
the O
models O
to O
be O
robust O
against O
adversarial Method
perturbation Method
is O
effective O
in O
reducing O
the O
test Metric
error Metric
. O
However O
, O
the O
definition O
of O
adversarial Method
perturbation Method
in O
[ O
reference O
] O
required O
a O
computationally O
expensive O
inner O
loop O
in O
order O
to O
evaluate O
the O
adversarial O
direction O
. O
To O
overcome O
this O
problem O
, O
Goodfellow O
et O
al O
. O
[ O
reference O
] O
proposed O
another O
definition O
of O
adversarial Method
perturbation Method
that O
admits O
a O
form O
of O
approximation Method
that O
is O
free O
of O
the O
expensive O
inner O
loop O
( O
see O
the O
next O
section O
for O
details O
) O
. O
Bachman O
et O
al O
. O
[ O
reference O
] O
studied O
the O
effect O
of O
random O
perturbation O
in O
the O
setting O
of O
semi Task
- Task
supervised Task
learning Task
. O
Pseudo Metric
Ensemble Metric
Agreement Metric
introduced O
in O
[ O
reference O
] O
trains O
the O
model O
in O
a O
way O
that O
the O
the O
output O
from O
each O
layer O
in O
the O
NN Method
does O
not O
vary O
too O
much O
by O
the O
introduction O
of O
random O
perturbations O
. O
On O
the O
other O
hand O
, O
ladder Method
networks Method
[ O
reference O
] O
achieved O
high O
performance O
for O
semi Task
- Task
supervised Task
learning Task
tasks Task
by O
making O
an O
effort O
so O
that O
one O
can O
reconstruct O
the O
original O
signal O
of O
the O
lower O
layer O
from O
the O
signal O
of O
the O
uppermost O
layer O
and O
the O
noise O
- O
perturbed O
outputs O
from O
the O
hidden O
layers O
. O
Our O
method O
is O
similar O
in O
philosophy O
to O
[ O
reference O
] O
, O
and O
we O
use O
the O
virtual Method
adversarial Method
perturbation Method
for O
their O
noise Method
process Method
. O
As O
we O
show O
later O
, O
in O
our O
experiments O
, O
this O
choice O
of O
perturbation O
was O
able O
to O
improve O
the O
generalization Metric
performance Metric
. O
Random Task
image Task
augmentation Task
is O
a O
variant O
of O
random Method
perturbation Method
that O
simply O
augments O
the O
dataset O
with O
images O
perturbed O
by O
regular O
deformation O
. O
For O
more O
theoretical O
overview O
of O
related O
methods O
of O
noise Method
regularizations Method
, O
Jacobian Method
regularizations Method
and O
their O
extensions O
like O
PEA Method
[ O
reference O
] O
, O
ladder Method
networks Method
[ O
reference O
] O
and O
VAT Method
, O
we O
refer O
the O
readers O
to O
[ O
reference O
] O
. O
Several O
works O
[ O
reference O
] O
, O
[ O
reference O
] O
succeeded O
in O
using O
the O
random Method
image Method
augmentation Method
to O
improve O
generalization Task
performance O
for O
semi Task
- Task
supervised Task
tasks Task
of Task
image Task
classification Task
. O
These O
methods O
can O
also O
be O
interpreted O
as O
types O
of O
techniques O
that O
smooth O
the O
model O
around O
input O
data O
points O
and O
extrapolate O
the O
labels O
of O
unlabeled O
examples O
. O
In O
the O
area O
of O
nonparametric Task
studies Task
, O
this O
type O
of O
method O
is O
referred O
to O
as O
label Method
propagation Method
[ O
reference O
] O
. O
Another O
family O
of O
methods O
for O
the O
semi O
- O
supervised Task
learning O
of O
NNs O
that O
are O
worth O
mentioning O
is O
the O
family O
based O
on O
sophisticated O
generative Method
models Method
. O
The O
methods O
belonging O
to O
this O
family O
are O
different O
from O
those O
that O
we O
introduced O
above O
because O
they O
do O
not O
require O
an O
explicit O
definition O
of O
smoothness O
. O
Kingma O
et O
al O
. O
[ O
reference O
] O
applied O
a O
variational Method
- Method
autoencoder Method
- Method
based Method
generative Method
model Method
to O
semisupervised Task
learning Task
. O
This O
work O
was O
followed O
by O
several O
variants O
[ O
reference O
] O
. O
Generative Method
adversarial Method
networks Method
( O
GANs Method
) O
proposed O
by O
[ O
reference O
] O
are O
a O
recently O
popular O
high O
- O
performance O
framework O
that O
can O
also O
be O
applied O
to O
semi Task
- Task
supervised Task
learning Task
[ O
reference O
] O
, O
[ O
reference O
] O
. O
In O
practice O
, O
these O
methods O
often O
require O
careful O
tuning O
of O
many O
hyperparameters O
in O
the O
generative Method
model Method
, O
and O
are O
usually O
not O
easy O
to O
implement O
without O
high O
expertise O
in O
its O
optimization Task
process Task
. O
section O
: O
METHODS O
We O
begin O
this O
section O
with O
a O
set O
of O
notations O
. O
Let O
x O
∈ O
R O
I O
and O
y O
∈ O
Q O
respectively O
denote O
an O
input O
vector O
and O
an O
output O
label O
, O
where O
I O
is O
the O
input O
dimension O
and O
Q O
is O
the O
space O
of O
all O
labels O
. O
Additionally O
, O
we O
denote O
the O
output O
distribution O
parameterized O
by O
θ O
as O
p O
( O
y|x O
, O
θ O
) O
. O
We O
useθ O
to O
denote O
the O
vector O
of O
the O
model O
parameters O
at O
a O
specific O
iteration O
step O
of O
the O
training Method
process Method
. O
We O
use O
D O
l O
= O
{ O
x O
ul O
|m O
= O
1 O
, O
. O
. O
. O
, O
N O
ul O
} O
to O
denote O
an O
unlabeled O
dataset O
. O
We O
train O
the O
model O
p O
( O
y|x O
, O
θ O
) O
using O
D O
l O
and O
D O
ul O
. O
section O
: O
Adversarial Method
Training Method
Our O
method O
is O
closely O
related O
to O
the O
adversarial Method
training Method
proposed O
by O
Goodfellow O
et O
al O
. O
[ O
reference O
] O
. O
We O
therefore O
formulate O
adversarial Method
training Method
before O
introducing O
our O
method O
. O
The O
loss O
function O
of O
adversarial Method
training Method
in O
[ O
reference O
] O
can O
be O
written O
as O
( O
1 O
) O
where O
r O
adv O
: O
= O
arg O
max O
where O
D O
[ O
p O
, O
p O
] O
is O
a O
non O
- O
negative O
function O
that O
measures O
the O
divergence O
between O
two O
distributions O
p O
and O
p O
. O
For O
example O
, O
D O
can O
be O
the O
cross O
entropy O
D O
[ O
p O
, O
p O
] O
= O
− O
i O
p O
i O
log O
p O
i O
, O
where O
p O
and O
p O
are O
vectors O
whose O
i O
- O
th O
coordinate O
represents O
the O
probability O
for O
the O
i O
- O
th O
class O
. O
The O
function O
q O
( O
y|x O
l O
) O
is O
the O
true O
distribution O
of O
the O
output O
label O
, O
which O
is O
unknown O
. O
The O
goal O
with O
this O
loss Method
function Method
is O
to O
approximate O
the O
true O
distribution O
q O
( O
y|x O
l O
) O
by O
a O
parametric Method
model Method
p O
( O
y|x O
l O
, O
θ O
) O
that O
is O
robust O
against O
adversarial O
attack O
to O
x. O
In O
[ O
reference O
] O
, O
the O
function O
q O
( O
y|x O
l O
) O
was O
approximated O
by O
one O
hot O
vector O
h O
( O
y O
; O
y O
l O
) O
, O
whose O
entries O
are O
all O
zero O
except O
for O
the O
index O
corresponding O
to O
the O
true O
label O
( O
output O
) O
y O
l O
. O
Likewise O
, O
for O
regression Task
tasks Task
, O
we O
can O
use O
the O
normal Method
distribution Method
centered O
at O
y O
l O
with O
constant O
variance O
, O
or O
the O
delta O
function O
with O
the O
atom O
at O
y O
= O
y O
l O
. O
Generally O
, O
we O
can O
not O
obtain O
a O
closed O
form O
for O
the O
exact O
adversarial Method
perturbation Method
r O
adv O
. O
However O
, O
we O
can O
approximate O
r O
adv O
with O
a O
linear Method
approximation Method
of Method
D Method
with O
respect O
to O
r O
in O
Eq O
. O
[ O
reference O
] O
. O
When O
the O
norm O
is O
L O
2 O
, O
adversarial Method
perturbation Method
can O
be O
approximated O
by O
When O
the O
norm O
is O
L O
∞ O
, O
adversarial Method
perturbation Method
can O
be O
approximated O
by O
where O
g O
is O
the O
same O
function O
that O
appeared O
in O
Eq. O
( O
3 O
) O
. O
[ O
reference O
] O
originally O
used O
( O
4 O
) O
for O
their O
adversarial Method
training Method
. O
Note O
that O
, O
for O
NNs Method
, O
the O
gradient O
can O
be O
efficiently O
computed O
by O
backpropagation Method
. O
By O
optimizing O
the O
loss O
function O
of O
the O
adversarial Method
training Method
in O
Eq O
. O
( O
1 O
) O
based O
on O
the O
adversarial Method
perturbation Method
defined O
by O
Eq. O
( O
3 O
) O
( O
or O
( O
4 O
) O
) O
, O
[ O
reference O
] O
and O
[ O
reference O
] O
were O
able O
to O
train O
a O
model O
with O
better O
generalization Task
performance O
than O
the O
model O
trained O
with O
random O
perturbations O
. O
section O
: O
Virtual Method
Adversarial Method
Training Method
Adversarial Method
training Method
is O
a O
successful O
method O
that O
works O
for O
many O
supervised Task
problems O
. O
However O
, O
full O
label O
information O
is O
not O
available O
at O
all O
times O
. O
Let O
x O
* O
represent O
either O
x O
l O
or O
x O
ul O
. O
Our O
objective Metric
function Metric
is O
now O
given O
by O
where O
r O
qadv O
: O
= O
arg O
max O
Indeed O
, O
we O
have O
no O
direct O
information O
about O
q O
( O
y|x O
ul O
) O
. O
We O
therefore O
take O
the O
strategy O
to O
replace O
q O
( O
y|x O
) O
with O
its O
current O
approximation O
, O
p O
( O
y|x O
, O
θ O
) O
. O
This O
approximation O
is O
not O
necessarily O
naive O
, O
because O
p O
( O
y|x O
, O
θ O
) O
shall O
be O
close O
to O
q O
( O
y|x O
) O
when O
the O
number O
of O
labeled O
training O
samples O
is O
large O
. O
This O
is O
also O
the O
motivation O
behind O
our O
inclusion O
of O
the O
term O
" O
virtual O
" O
in O
our O
work O
. O
Literally O
, O
we O
use O
" O
virtual O
" O
labels O
that O
are O
probabilistically O
generated O
from O
p O
( O
y|x O
, O
θ O
) O
in O
place O
of O
labels O
that O
are O
unknown O
to O
the O
user O
, O
and O
compute O
adversarial O
direction O
based O
on O
the O
virtual O
labels O
. O
Therefore O
, O
in O
this O
study O
, O
we O
use O
the O
current O
estimate O
p O
( O
y|x O
, O
θ O
) O
in O
place O
of O
q O
( O
y|x O
) O
. O
With O
this O
compromise O
, O
we O
arrive O
at O
our O
rendition O
of O
Eq O
. O
( O
2 O
) O
given O
by O
which O
defines O
our O
virtual Method
adversarial Method
perturbation Method
. O
The O
loss O
LDS O
( O
x O
, O
θ O
) O
can O
be O
considered O
as O
a O
negative Metric
measure Metric
of Metric
the Metric
local Metric
smoothness Metric
of O
the O
current Method
model Method
at O
each O
input O
data O
point O
x O
, O
and O
its O
reduction O
would O
make O
the O
model O
smooth O
at O
each O
data O
point O
. O
The O
regularization Method
term Method
we O
propose O
in O
this O
study O
is O
the O
average O
of O
LDS O
( O
x O
* O
, O
θ O
) O
over O
all O
input O
data O
points O
: O
The O
full O
objective Metric
function Metric
is O
thus O
given O
by O
where O
( O
D O
l O
, O
θ O
) O
is O
the O
negative O
log O
- O
likelihood O
for O
the O
labeled O
dataset O
. O
VAT Method
is O
a O
training Method
method Method
with O
the O
regularizer Method
R Method
vadv Method
. O
One O
notable O
advantage O
of O
VAT Method
is O
that O
there O
are O
just O
two O
scalar O
- O
valued O
hyperparameters O
: O
( O
1 O
) O
the O
norm O
constraint O
> O
0 O
for O
the O
adversarial O
direction O
and O
( O
2 O
) O
the O
regularization O
coefficient O
α O
> O
0 O
that O
controls O
the O
relative O
balance O
between O
the O
negative O
log O
- O
likelihood O
and O
the O
regularizer Method
R Method
vadv Method
. O
In O
fact O
, O
for O
all O
our O
experiments O
, O
our O
VAT Method
achieved O
superior O
performance O
by O
tuning O
only O
the O
hyperparameter O
, O
while O
fixing O
α O
= O
1 O
. O
Theoretically O
, O
these O
two O
hyperparameters O
play O
similar O
roles O
, O
as O
discussed O
later O
in O
Section O
4.2 O
. O
One O
advantage O
of O
VAT Method
is O
the O
number O
of O
hyperparameters O
. O
For O
many O
generative Method
model Method
- Method
based Method
supervised Method
and O
semi Method
- Method
supervised Method
learning Method
methods Method
aimed O
at O
learning O
p O
( O
y O
, O
x O
) O
, O
a O
bottleneck O
of O
training Task
is O
the O
difficulty O
of O
the O
optimization O
of O
hyperparameters O
for O
the O
generative Method
model Method
( O
i.e. O
p O
( O
x O
) O
or O
p O
( O
x|y O
) O
) O
. O
Also O
, O
as O
opposed O
to O
adversarial Method
training Method
[ O
reference O
] O
, O
the O
definition O
of O
virtual Method
adversarial Method
perturbation Method
only O
requires O
input O
x O
and O
does O
not O
require O
label O
y. O
This O
is O
the O
property O
that O
allows O
us O
to O
apply O
VAT Method
to O
semi Task
- Task
supervised Task
learning Task
. O
Fig O
. O
1 O
shows O
how O
VAT Method
works O
on O
semi Task
- Task
supervised Task
learning Task
on O
a O
two O
- O
dimensional O
synthetic O
dataset O
. O
We O
used O
an O
NN Method
classifier O
with O
one O
hidden Method
layer Method
with O
50 O
hidden O
units O
. O
At O
the O
beginning O
of O
the O
training O
, O
the O
classifier Method
predicted O
different O
labels O
for O
input O
data O
points O
in O
the O
same O
cluster O
, O
and O
LDS O
on O
the O
boundaries O
were O
very O
high O
( O
see O
panel O
[ O
reference O
] O
in O
Fig O
. O
1 O
) O
. O
The O
algorithm O
exerts O
high O
pressure O
for O
the O
model O
to O
be O
smooth O
around O
the O
points O
with O
large O
LDS O
values O
. O
As O
the O
training O
progressed O
, O
the O
model O
evolved O
so O
that O
the O
label Task
prediction Task
on O
the O
points O
with O
large O
LDS O
values O
are O
strongly O
influenced O
by O
the O
labeled O
inputs O
in O
the O
vicinity O
. O
This O
encouraged O
the O
model O
to O
predict O
the O
same O
label O
for O
the O
set O
of O
points O
that O
belong O
to O
the O
same O
cluster O
, O
which O
is O
what O
we O
often O
desire O
in O
semi Task
- Task
supervised Task
learning Task
. O
As O
we O
can O
see O
in O
Fig O
. O
1 O
, O
VAT Method
on O
semi Task
- Task
supervised Task
learning Task
can O
be O
given O
a O
similar O
interpretation O
as O
label Task
propagation Task
[ O
reference O
] O
, O
which O
is O
another O
branch O
of O
method O
for O
semi Task
- Task
supervised Task
learning Task
. O
section O
: O
Fast Method
Approximation Method
Method Method
for O
r Task
vadv Task
and O
the O
Derivative O
of O
the O
Objective O
Function O
Once O
virtual Method
adversarial Method
perturbation Method
r O
vadv O
is O
computed O
, O
the O
evaluation Task
of Task
LDS Task
( Task
x Task
* Task
, O
θ O
) O
simply O
becomes O
the O
computation O
of O
the O
divergence O
D O
between O
the O
output O
distributions O
p O
( O
y|x O
* O
, O
θ O
) O
and O
p O
( O
y|x O
* O
+ O
r O
vadv O
, O
θ O
) O
. O
However O
, O
the O
evaluation O
of O
r Task
vadv Task
can O
not O
be O
performed O
with O
the O
linear Method
approximation Method
as O
in O
the O
original O
adversarial Method
training Method
( O
Eq. O
( O
3 O
) O
) O
because O
the O
gradient O
of O
D O
[ O
p O
( O
y|x O
* O
, O
θ O
) O
, O
p O
( O
y|x O
* O
+ O
r O
, O
θ O
) O
] O
with O
respect O
to O
r O
is O
always O
0 O
at O
r O
= O
0 O
. O
In O
the O
following O
, O
we O
propose O
an O
efficient O
computation Task
of Task
r Task
vadv Task
, O
for O
which O
there O
is O
no O
evident O
closed O
form O
. O
For O
simplicity O
, O
we O
denote O
D O
[ O
p O
( O
y|x O
* O
, O
θ O
) O
, O
p O
( O
y|x O
* O
+ O
r O
, O
θ O
) O
] O
by O
D O
( O
r O
, O
x O
* O
, O
θ O
) O
. O
We O
assume O
that O
p O
( O
y|x O
* O
, O
θ O
) O
is O
twice O
differentiable O
with O
respect O
to O
θ O
and O
x O
almost O
everywhere O
. O
Because O
D O
( O
r O
, O
x O
* O
, O
θ O
) O
takes O
the O
minimal O
value O
at O
r O
= O
0 O
, O
the O
differentiability O
assumption O
dictates O
that O
its O
first O
derivative O
Fig O
. O
1 O
: O
Demonstration O
of O
how O
our O
VAT Method
works O
on O
semi Task
- Task
supervised Task
learning Task
. O
We O
generated O
8 O
labeled O
data O
points O
( O
y O
= O
1 O
and O
y O
= O
0 O
are O
green O
and O
purple O
, O
respectively O
) O
, O
and O
1 O
, O
000 O
unlabeled O
data O
points O
in O
2 O
- O
D O
space O
. O
The O
panels O
in O
the O
first O
row O
( O
I O
) O
show O
the O
prediction O
p O
( O
y O
= O
1|x O
, O
θ O
) O
on O
the O
unlabeled O
input O
points O
at O
different O
stages O
of O
the O
algorithm O
. O
We O
used O
a O
continuous O
colormap O
to O
designate O
the O
predicted O
values O
of O
p O
( O
y O
= O
1|x O
, O
θ O
) O
, O
with O
Green O
, O
gray O
, O
and O
purple O
respectively O
corresponding O
to O
the O
values O
1.0 O
, O
0.5 O
, O
and O
0.0 O
. O
The O
panels O
in O
the O
second O
row O
( O
II O
) O
are O
heat O
maps O
of O
the O
regularization O
term O
LDS O
( O
x O
, O
θ O
) O
on O
the O
input O
points O
. O
The O
values O
of O
LDS O
on O
blue O
- O
colored O
points O
are O
relatively O
high O
in O
comparison O
to O
the O
gray O
- O
colored O
points O
. O
We O
used O
KL O
divergence O
for O
the O
choice O
of O
D O
in O
Eq O
. O
[ O
reference O
] O
. O
Note O
that O
, O
at O
the O
onset O
of O
training O
, O
all O
the O
data O
points O
have O
similar O
influence O
on O
the O
classifier Method
. O
After O
10 O
updates O
, O
the O
model O
boundary O
was O
still O
appearing O
over O
the O
inputs O
. O
As O
the O
training O
progressed O
, O
VAT Method
pushed O
the O
boundary O
away O
from O
the O
labeled O
input O
data O
points O
. O
∇ O
r O
D O
( O
r O
, O
x O
, O
θ O
) O
| O
r=0 O
is O
zero O
. O
Therefore O
, O
the O
second Method
- Method
order Method
Taylor Method
approximation Method
of Method
D Method
is O
where O
H O
( O
x O
, O
θ O
) O
is O
the O
Hessian O
matrix O
given O
by O
H O
( O
x O
, O
θ O
) O
: O
= O
∇∇ Method
r O
D O
( O
r O
, O
x O
, O
θ O
) O
| O
r=0 O
. O
Under O
this O
approximation O
, O
r O
vadv O
emerges O
as O
the O
first O
dominant O
eigenvector O
u O
( O
x O
, O
θ O
) O
of O
H O
( O
x O
, O
θ O
) O
with O
magnitude O
: O
wherev O
denotes O
the O
unit O
vector O
whose O
direction O
is O
the O
same O
as O
its O
argument O
vector O
v O
; O
that O
is O
, O
v O
≡ O
v O
v O
2 O
. O
Hereafter O
, O
we O
denote O
H O
( O
x O
, O
θ O
) O
by O
H O
for O
simplicity O
. O
Next O
, O
we O
need O
to O
address O
the O
O O
( O
I O
3 O
) O
runtime Metric
required O
for O
the O
computation O
of O
the O
eigenvectors O
of O
the O
Hessian O
H. O
We O
resolve O
this O
issue O
with O
the O
approximation Method
via O
the O
power Method
iteration Method
method Method
[ O
reference O
] O
and O
the O
finite Method
difference Method
method Method
. O
Let O
d O
be O
a O
randomly O
sampled O
unit O
vector O
. O
Provided O
that O
d O
is O
not O
perpendicular O
to O
the O
dominant O
eigenvector O
u O
, O
the O
iterative Method
calculation Method
of O
makes O
d O
converge O
to O
u. O
To O
reduce O
the O
computational Metric
time Metric
, O
we O
perform O
this O
operation O
without O
the O
direct O
computation O
of O
H. O
Note O
that O
Hd Method
can O
be O
approximated O
using O
the O
finite Method
difference Method
method Method
: O
with O
ξ O
= O
0 O
. O
In O
the O
computation O
above O
, O
we O
use O
the O
fact O
that O
∇ O
r O
D O
( O
r O
, O
x O
, O
θ O
) O
| O
r=0 O
= O
0 O
again O
. O
To O
summarize O
, O
we O
can O
approximate O
r Method
vadv Method
with O
the O
repeated O
application O
of O
the O
following O
update O
: O
The O
computation Task
of Task
∇ Task
r Method
D Method
can O
be O
performed O
in O
a O
straightforward O
manner O
. O
For O
NNs Method
, O
this O
can O
be O
performed O
with O
one O
set O
of O
backpropagation Method
. O
The O
approximation O
introduced O
here O
can O
be O
improved O
monotonically O
by O
increasing O
the O
number O
of O
the O
power O
iterations O
K. O
Thus O
, O
for O
NNs Method
, O
the O
computation Task
of Task
r Task
vadv Task
can O
be O
performed O
with O
K O
sets O
of O
backpropagations Method
. O
Surprisingly O
, O
only O
one O
power Method
iteration Method
was O
sufficient O
for O
high O
performance O
on O
various O
benchmark O
datasets O
. O
This O
approximation O
of O
r Method
vadv Method
with O
K O
= O
1 O
results O
in O
an O
approximation O
that O
is O
similar O
in O
form O
to O
Eq. O
( O
3 O
) O
: O
where O
We O
further O
discuss O
the O
effects O
of O
the O
number O
of O
the O
power O
iterations O
in O
Section O
4 O
. O
After O
computing O
r O
vadv O
, O
the O
derivative O
of O
R O
vadv O
can O
be O
easily O
computed O
with O
one O
set O
of O
forwardand Method
back Method
- Method
propagation Method
on O
NNs Method
. O
Meanwhile O
, O
the O
derivative O
of O
r O
vadv O
with O
respect O
to O
θ O
is O
not O
only O
convoluted O
and O
computationally O
costly O
, O
but O
also O
introduces O
another O
source O
of O
variance O
to O
the O
gradient O
and O
negatively O
affect O
the O
performance O
of O
the O
algorithm O
. O
Our O
VAT Method
therefore O
ignores O
the O
dependency O
of O
r O
vadv O
on O
θ O
. O
In O
total O
, O
the O
derivative O
of O
the O
full O
objective O
function O
including O
the O
log O
- O
likelihood O
term O
( O
8 O
) O
can O
be O
computed O
with O
K O
+ O
2 O
sets O
of O
backpropagation Method
. O
Algorithm O
1 O
summarizes O
the O
procedure O
for O
mini Task
- Task
batch Task
SGD Task
with O
the O
approximation Task
of Task
∇ Task
θ O
R O
vadv O
carried O
out O
with O
one O
power Method
iteration Method
. O
VAT Method
is O
an O
algorithm O
that O
updates O
the O
model O
by O
the O
weighted O
sum O
of O
the O
gradient O
of O
the O
likelihood O
and O
the O
gradient O
∇ O
θ O
R O
vadv O
computed O
with O
Algorithm O
1 O
. O
Algorithm O
1 O
Mini Method
- Method
batch Method
SGD Method
for O
∇ Task
θ O
R O
vadv O
( O
θ O
) O
| O
θ O
= O
θ O
, O
with O
a O
one Method
- Method
time Method
power Method
iteration Method
method Method
. O
3 O
) O
Calculate O
r O
vadv O
via O
taking O
the O
gradient O
of O
D O
with O
respect O
to O
r O
on O
r O
= O
ξd O
( O
i O
) O
on O
each O
input O
data O
point O
x O
( O
i O
) O
: O
section O
: O
Virtual Method
Adversarial Method
Training Method
vs. O
Random Method
Perturbation Method
Training Method
The O
regularization O
function O
we O
use O
for O
VAT Method
can O
be O
generally O
written O
as O
where O
r O
K O
is O
obtained O
by O
applying O
the O
power Method
iteration Method
Ktimes Method
on O
a O
sample O
from O
the O
uniform Method
distribution Method
on O
the O
sphere O
U O
( O
r| O
) O
with O
radius O
. O
In O
practice O
, O
for O
the O
computation O
of O
( O
16 O
) O
we O
use O
an O
empirical O
expectation O
about O
the O
random O
perturbation O
r O
K O
. O
For O
the O
implementation O
of O
VAT Method
, O
we O
use O
this O
regularizer Method
with O
K O
≥ O
1 O
. O
Meanwhile O
, O
We O
refer O
to O
the O
training O
with O
R O
( O
0 O
) O
as O
Random Method
Perturbation Method
Training Method
( O
RPT Method
) O
. O
RPT Method
is O
a O
downgraded Method
version Method
of O
VAT Method
that O
does O
not O
perform O
the O
power Method
iteration Method
. O
By O
definition O
, O
RPT Method
only O
smooths O
the O
function O
isotropically O
around O
each O
input O
data O
point O
. O
As O
we O
discuss O
further O
in O
Section O
4 O
, O
RPT Method
falls O
behind O
VAT Method
in O
its O
sheer O
ability O
to O
reduce O
the O
generalization Metric
error Metric
. O
There O
could O
be O
two O
reasons O
for O
the O
superiority O
of O
VAT Method
. O
First O
, O
the O
learning Method
process Method
of O
VAT Method
is O
inherently O
more O
stable O
than O
that O
of O
RPT Method
. O
At O
each O
step O
of O
the O
algorithm O
, O
the O
power Method
iteration Method
generates O
a O
vector O
that O
has O
a O
large O
projection O
to O
the O
virtual O
adversarial O
direction O
with O
high O
probability O
. O
Note O
that O
, O
as O
K O
→ O
∞ O
, O
under O
the O
sufficient O
regularity O
of O
the O
model O
, O
the O
gradient O
of O
D O
in O
the O
expression O
( O
16 O
) O
approaches O
the O
deterministic O
vector O
1 O
/ O
2 O
2 O
∇ O
θ O
λ O
1 O
( O
x O
, O
θ O
) O
, O
where O
λ O
1 O
is O
the O
dominant O
eigenvalue O
of O
H O
( O
x O
, O
θ O
) O
. O
Thus O
, O
the O
direction O
to O
which O
VAT Method
smooths O
the O
model O
is O
more O
deterministic O
than O
the O
direction O
to O
which O
RPT Method
smooths O
the O
model O
, O
which O
is O
uniformly O
distributed O
over O
the O
sphere O
of O
radius O
; O
the O
stability O
of O
the O
learning Task
of O
RPT Method
always O
suffers O
from O
the O
variance O
of O
[ O
reference O
] O
. O
Second O
, O
the O
regularization Method
function Method
of O
RPT Method
has O
an O
essentially O
effect O
on O
the O
model O
. O
For O
each O
observed O
input O
point O
, O
VAT Method
trains O
the O
model O
to O
assign O
similar O
label O
distribution O
only O
to O
the O
set O
of O
proximal O
points O
aligned O
in O
the O
virtual O
adversarial O
direction O
. O
In O
contrast O
, O
RPT Method
encourages O
the O
model O
to O
assign O
the O
same O
label O
distribution O
to O
all O
input O
points O
in O
the O
isotropic O
neighborhood O
of O
each O
observed O
input O
. O
From O
spectral O
perspective O
, O
the O
difference O
between O
VAT Method
and O
RPT Method
is O
that O
VAT Method
penalizes O
the O
spectral O
norm O
( O
largest O
singular O
value O
) O
of O
the O
Hessian O
matrix O
H O
( O
9 O
) O
, O
while O
RPT Method
penalizes O
the O
sum O
of O
the O
eigenvalues O
[ O
reference O
] O
. O
Therefore O
, O
so O
long O
that O
the O
true O
output O
distribution O
is O
isotropically O
smooth O
around O
the O
input O
data O
point O
, O
VAT Method
tends O
to O
be O
more O
effective O
in O
improving O
the O
generalization Task
performance O
. O
In O
Section O
4.5 O
, O
we O
investigate O
the O
variance O
of O
the O
gradients O
in O
more O
detail O
and O
compare O
RPT Method
and O
VAT Method
from O
this O
perspective O
. O
section O
: O
EXPERIMENTS O
We O
conducted O
a O
set O
of O
numerical O
experiments O
to O
assess O
the O
following O
aspects O
of O
VAT Method
: O
• O
the O
sheer O
efficacy O
of O
VAT Method
in O
comparison O
to O
RPT Method
and O
to O
a O
collection O
of O
recent O
competitive Method
algorithms Method
for O
supervised Task
and O
semi Task
- Task
supervised Task
learning Task
, O
• O
the O
effect O
of O
hyperparameters O
( O
the O
perturbation O
size O
, O
the O
regularization O
coefficient O
α O
, O
and O
the O
number O
of O
the O
power O
iterations O
K O
) O
on O
the O
performance O
of O
VAT Method
, O
• O
VAT Method
's O
effect O
on O
the O
robustness Metric
of O
the O
trained O
NNs Method
against O
virtual O
adversarial O
perturbations O
, O
and O
• O
the O
mechanism O
behind O
the O
advantage O
of O
using O
virtual Method
adversarial Method
perturbation Method
as O
opposed O
to O
random Method
perturbation Method
. O
We O
would O
like O
to O
remind O
the O
readers O
that O
, O
by O
the O
term O
VAT Method
here O
, O
we O
mean O
the O
algorithm O
that O
uses O
the O
approximation Method
step Method
we O
introduced O
in O
Section O
3.3 O
and O
Algorithm O
1 O
. O
In O
the O
following O
, O
we O
describe O
the O
experimental O
settings O
and O
outcomes O
of O
the O
experiments O
. O
For O
the O
performance O
evaluation O
of O
our O
method O
, O
we O
used O
standard O
benchmarks O
like O
MNIST O
, O
CIFAR Material
- Material
10 Material
and O
SVHN Material
. O
For O
the O
methods O
to O
compare O
, O
we O
used O
the O
methods O
that O
were O
state O
- O
of O
- O
the O
- O
art O
at O
the O
time O
of O
this O
research O
. O
For O
more O
details O
on O
the O
dataset O
and O
model O
architectures O
, O
please O
see O
the O
appendix O
sections O
. O
For O
all O
our O
experiments O
, O
we O
used O
fully Method
connected Method
NNs Method
or O
convolutional Method
neural Method
networks Method
( O
CNNs Method
) O
as O
the O
architectures O
of O
the O
classifiers Method
, O
and O
used O
Theano O
[ O
reference O
] O
and O
TensorFlow Method
[ O
reference O
] O
to O
train O
the O
models O
[ O
reference O
] O
. O
We O
use O
p O
( O
y|x O
, O
θ O
) O
to O
denote O
the O
label O
distribution O
of O
2 O
. O
TensorFlow Method
implementation Method
for O
the O
experiments O
is O
available O
at O
https: O
// O
github.com O
/ O
takerum O
/ O
vat O
tf O
. O
Chainer O
[ O
reference O
] O
implementation O
is O
also O
available O
at O
https: O
// O
github.com O
/ O
takerum O
/ O
vat O
chainer O
. O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
We O
also O
used O
Batch Method
Normalization Method
[ O
reference O
] O
. O
For O
the O
divergence O
D O
in O
Eq O
. O
( O
1 O
) O
, O
we O
chose O
the O
KL O
divergence O
where O
Q O
is O
the O
domain O
of O
y. O
For O
classification Task
problems Task
, O
Q O
is O
the O
set O
of O
all O
possible O
labels O
. O
We O
set O
ξ O
in O
Eq O
. O
( O
12 O
) O
to O
be O
1e O
- O
6 O
in O
all O
of O
our O
experiments O
. O
For O
each O
set O
of O
experiments O
, O
we O
repeated O
the O
same O
procedure O
multiple O
times O
with O
different O
random O
seeds O
for O
the O
weight Method
initialization Method
and O
for O
the O
selection Task
of Task
labeled Task
samples Task
( O
for O
semi Task
- Task
supervised Task
learning Task
) O
. O
The O
values O
reported O
on O
the O
tables O
are O
the O
means O
and O
the O
standard O
deviations O
of O
the O
results O
. O
Please O
see O
the O
appendix O
sections O
for O
more O
details O
. O
section O
: O
Testing O
the O
Efficacy O
of O
VAT Method
on O
Benchmark Task
Tasks Task
section O
: O
Supervised Task
Learning O
on O
MNIST O
and O
CIFAR Material
- Material
10 Material
We O
first O
applied O
our O
algorithm O
to O
supervised Task
learning O
on O
MNIST O
dataset O
. O
We O
used O
NNs Method
with O
four O
hidden O
layers O
, O
whose O
numbers O
of O
units O
were O
( O
1200 O
, O
600 O
, O
300 O
, O
150 O
) O
. O
We O
provide O
more O
details O
of O
the O
experiments O
in O
Appendix O
A. O
For O
each O
regularization Method
method Method
, O
we O
used O
the O
set O
of O
hyperparameters O
that O
achieved O
the O
best O
performance O
on O
the O
validation O
dataset O
of O
size O
10 O
, O
000 O
, O
which O
was O
selected O
from O
the O
pool O
of O
training O
samples O
of O
size O
60 O
, O
000 O
. O
Test O
datasets O
were O
produced O
so O
that O
they O
have O
no O
intersection O
with O
the O
training O
dataset O
. O
We O
fed O
the O
test O
dataset O
into O
the O
trained O
NNs Method
and O
recorded O
their O
test O
errors O
. O
Fig O
. O
2 O
shows O
the O
transition O
of O
R O
vadv O
and O
the O
learning Metric
curves Metric
for O
the O
baseline O
NN Method
trained O
without O
VAT Method
( O
denoted O
by O
' O
wo O
/ O
VAT Method
' O
) O
and O
the O
NN Method
trained O
with O
VAT Method
( O
denoted O
by O
' O
w O
/ O
VAT Method
' O
) O
. O
As O
the O
training O
progressed O
, O
R O
vadv O
of O
the O
NN Method
trained O
with O
VAT Method
exceeded O
that O
of O
the O
baseline O
; O
that O
is O
, O
the O
model O
trained O
with O
VAT Method
grew O
smoother O
than O
the O
baseline O
in O
terms O
of O
LDS Metric
. O
for O
supervised Task
learning O
on O
MNIST Task
. O
We O
set O
= O
2.0 O
for O
the O
evaluation O
of O
R Task
vadv Task
for O
both O
the O
baseline O
and O
VAT Method
. O
This O
is O
the O
value O
of O
with O
which O
the O
VAT Method
- O
trained O
model O
achieved O
the O
best O
performance O
on O
the O
validation O
dataset O
. O
Table O
1 O
summarizes O
the O
performance O
of O
our O
regularization Method
method Method
( O
VAT Method
) O
and O
the O
other O
regularization Method
methods Method
for O
supervised Task
learning O
on O
MNIST O
. O
VAT Method
performed O
better O
than O
all O
the O
contemporary O
methods O
except O
ladder Method
networks Method
, O
which O
is O
a O
highly O
advanced O
method O
based O
on O
special O
network O
structure O
. O
We O
also O
tested O
VAT Method
with O
K O
> O
1 O
in O
order O
to O
study O
the O
dependence O
of O
the O
number O
of O
the O
power O
iterations O
K O
on O
the O
performance O
of O
VAT Method
. O
As O
we O
will O
discuss O
in O
Section O
4.3 O
, O
however O
, O
we O
were O
not O
able O
to O
achieve O
substantial O
improvement O
by O
increasing O
the O
value O
of O
K. O
We O
also O
applied O
our O
algorithm O
to O
supervised Task
learning O
on O
CIFAR Material
- Material
10 Material
[ O
reference O
] O
. O
For O
the O
baseline O
for O
this O
set O
of O
experiments O
, O
we O
used O
a O
' O
Conv Method
- Method
Large Method
' O
with O
dropout Method
( O
Table O
7 O
, O
Appendix O
D O
) O
. O
Table O
2 O
summarizes O
the O
test O
performance O
of O
supervised Method
learning Method
methods Method
implemented O
with O
CNN Method
on O
CIFAR10 Material
. O
We O
also O
compared O
the O
performance O
of O
VAT Method
to O
advanced O
architectures O
like O
ResNet Method
[ O
reference O
] O
and O
DenseNet Method
[ O
reference O
] O
in O
order O
to O
confirm O
that O
the O
baseline O
model O
of O
our O
algorithm O
is O
" O
mediocre O
enough O
" O
so O
that O
we O
can O
rightly O
attribute O
the O
effectiveness O
of O
our O
algorithm O
to O
its O
way O
of O
the O
regularization Method
itself O
, O
not O
to O
the O
network O
structure O
we O
used O
in O
the O
experiments O
. O
For O
CIFAR Material
- Material
10 Material
, O
our O
VAT Method
achieved O
satisfactory O
performance O
relative O
to O
the O
standards O
. O
section O
: O
Semi Task
- Task
Supervised Task
Learning Task
on O
MNIST O
, O
SVHN Material
, O
and O
CIFAR Material
- Material
10 Material
Recall O
that O
our O
definition O
of O
LDS O
( O
x O
, O
θ O
) O
at O
any O
point O
x O
does O
not O
require O
the O
supervisory O
signal O
for O
x. O
In O
particular O
, O
this O
means O
that O
we O
can O
apply O
VAT Method
to O
semi Task
- Task
supervised Task
learning Task
tasks Task
. O
We O
emphasize O
that O
this O
is O
a O
property O
not O
shared O
by O
adversarial Method
training Method
. O
We O
applied O
VAT Method
to O
semi O
- O
supervised Task
image O
classification O
tasks O
on O
three O
datasets O
: O
MNIST O
, O
SVHN Material
[ O
reference O
] O
, O
and O
CIFAR Material
- Material
10 Material
[ O
reference O
] O
. O
We O
provide O
the O
details O
of O
the O
experimental O
settings O
in O
Appendix O
D. O
For O
MNIST O
, O
we O
used O
the O
same O
architecture O
of O
NN Method
as O
in O
the O
previous O
section O
. O
We O
also O
used O
batch Method
normalization Method
in O
our O
implementation O
. O
We O
used O
a O
mini O
- O
batch O
of O
size O
64 O
for O
the O
calculation O
of O
the O
negative O
log O
- O
likelihood O
term O
, O
and O
a O
minibatch O
of O
size O
256 O
for O
the O
calculation O
of O
R O
vadv O
in O
Eq O
. O
[ O
reference O
] O
. O
As O
mentioned O
in O
Section O
3.2 O
, O
we O
used O
both O
labeled O
and O
unlabeled O
sets O
for O
the O
calculation O
of O
R Task
vadv Task
. O
Table O
3 O
summarizes O
the O
results O
for O
the O
permutation Task
invariant Task
MNIST Task
task Task
. O
All O
the O
methods O
listed O
in O
the O
table O
belong O
to O
the O
family O
of O
semisupervised Method
learning Method
methods Method
. O
For O
the O
MNIST O
dataset O
, O
VAT Method
outperformed O
all O
the O
contemporary O
methods O
other O
than O
the O
methods O
based O
on O
generative Method
models Method
, O
such O
as O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
which O
are O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
For O
the O
experiments O
on O
SVHN Material
and O
CIFAR Material
- Material
10 Material
, O
we O
used O
two O
types O
of O
CNNs Method
( O
Conv Method
- Method
Small Method
and O
Conv Method
- Method
Large Method
) O
used O
in O
recent O
state O
- O
of O
- O
the O
- O
art O
semi Method
- Method
supervised Method
learning Method
methods Method
( O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
) O
. O
' O
Conv Method
- Method
Small Method
CNN O
' O
has O
practically O
the O
same O
structure O
as O
the O
CNN Method
used O
in O
[ O
reference O
] O
, O
and O
' O
Conv Method
- Method
Large Method
CNN O
' O
has O
practically O
the O
same O
structure O
as O
the O
CNN Method
used O
in O
[ O
reference O
] O
. O
We O
provide O
more O
details O
of O
the O
architectures O
in O
Table O
7 O
of O
Appendix O
D. O
Also O
, O
in O
SVHN Material
and O
CIFAR Material
- Material
10 Material
experiments O
, O
we O
adopted O
conditional O
entropy O
of O
p O
( O
y|x O
, O
θ O
) O
as O
an O
additional O
cost O
: O
This O
cost O
was O
introduced O
by O
[ O
reference O
] O
, O
and O
similar O
idea O
has O
been O
used O
in O
[ O
reference O
] O
. O
The O
conditional Method
entropy Method
minimization Method
has O
an O
effect O
of O
exaggerating O
the O
prediction O
of O
the O
model O
p O
( O
y|x O
, O
θ O
) O
on O
each O
data O
point O
. O
For O
semi O
- O
supervised Task
image O
classification O
tasks O
, O
this O
additional O
cost O
is O
especially O
helpful O
. O
In O
what O
follows O
, O
' O
VAT Method
+ O
EntMin Method
' O
indicates O
the O
training O
with O
R O
vadv O
+ O
R O
cent O
. O
Table O
4 O
summarizes O
the O
results O
of O
semi Task
- Task
supervised Task
learning Task
tasks Task
on O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
Our O
method O
achieved O
the O
test Metric
error Metric
rate Metric
of O
14.82 O
( O
% O
) O
with O
VAT Method
, O
which O
outperformed O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
for O
semi Task
- Task
supervised Task
learning Task
on O
CIFAR Material
- Material
10 Material
. O
' O
VAT Method
+ O
EntMin Method
' O
outperformed O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
for O
semi Task
- Task
supervised Task
learning Task
on O
both O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
Table O
5 O
shows O
the O
performance O
of O
VAT Method
and O
contemporary O
semi Method
- Method
supervised Method
learning Method
methods Method
implemented O
with O
moderate O
image O
data O
augmentation O
( O
translation O
and O
horizontal O
flip O
) O
. O
All O
methods O
other O
than O
VAT Method
were O
implemented O
with O
the O
strong O
standing O
assumption O
on O
the O
unlabeled O
training O
samples O
that O
the O
label O
of O
the O
image O
does O
not O
change O
by O
the O
deformation O
. O
On O
CIFAR Material
- Material
10 Material
, O
VAT Method
still O
outperformed O
the O
listed O
methods O
with O
this O
handicap O
. O
' O
VAT Method
+ O
EntMin Method
' O
with O
moderate O
data Method
augmentation Method
also O
outperformed O
the O
current O
state O
- O
of O
- O
the O
- O
art O
methods O
for O
semi Task
- Task
supervised Task
learning Task
on O
both O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
This O
result O
tells O
us O
that O
the O
effect O
of O
VAT Method
does O
not O
overlap O
so O
much O
with O
that O
of O
the O
data Method
augmentation Method
methods Method
, O
so O
that O
they O
can O
be O
used O
in O
combination O
to O
boost O
the O
performance O
. O
section O
: O
Effects O
of O
Perturbation Metric
Size Metric
and O
Regularization O
Coefficient O
α O
Generally O
, O
there O
is O
a O
high O
computational Metric
cost Metric
for O
optimizing O
the O
hyperparameters O
of O
large Method
NNs Method
on O
a O
large O
dataset O
. O
One O
advantage O
of O
VAT Method
is O
that O
the O
algorithm O
involves O
only O
two O
hyperparameters O
: O
α O
and O
. O
Even O
better O
, O
our O
experiments O
suggest O
that O
, O
in O
an O
appropriate O
setting O
, O
VAT Method
can O
be O
made O
effective O
with O
the O
optimization Task
of Task
alone Task
. O
In O
all O
our O
experiments O
in O
Section O
4.1 O
, O
VAT Method
achieved O
competitive O
results O
while O
fixing O
α O
= O
1 O
. O
This O
result O
is O
not O
too O
counter O
- O
intuitive O
. O
For O
small O
, O
the O
hyperparameter O
α O
plays O
a O
similar O
role O
as O
. O
To O
see O
this O
, O
consider O
the O
Taylor Method
expansion Method
of Method
Eq Method
. O
( O
5 O
) O
for O
small O
, O
given O
where O
H O
( O
x O
, O
θ O
) O
and O
λ O
1 O
( O
x O
, O
θ O
) O
are O
respectively O
the O
Hessian O
matrix O
of O
D O
in O
Eq O
. O
( O
9 O
) O
and O
its O
dominant O
eigenvalue O
. O
Substituting O
this O
into O
the O
objective O
function O
( O
Eq O
. O
( O
8 O
) O
) O
, O
we O
obtain O
Thus O
, O
at O
least O
for O
small O
, O
the O
strength O
of O
the O
regularization O
in O
VAT Method
is O
proportional O
to O
the O
product O
of O
the O
two O
hyperparameters O
, O
α O
and O
2 O
; O
that O
is O
, O
in O
the O
region O
of O
small O
, O
the O
hyperparameter Method
search Method
for O
only O
either O
or O
α O
suffices O
. O
However O
, O
when O
we O
consider O
a O
relatively O
large O
value O
of O
, O
the O
hyperparameters O
α O
and O
can O
not O
be O
brought O
together O
. O
In O
such O
a O
case O
, O
we O
shall O
strive O
to O
search O
for O
the O
best O
pair O
of O
hyperparameters O
that O
attains O
optimal O
performance O
. O
In O
our O
experiments O
on O
MNIST O
, O
tuning O
of O
alone O
sufficed O
for O
achieving O
satisfactory O
performance O
. O
We O
therefore O
recommend O
on O
empirical O
basis O
that O
the O
user O
prioritizes O
the O
parameter O
search O
for O
over O
the O
search O
for O
α O
. O
Fig O
. O
3 O
shows O
the O
effects O
of O
and O
α O
on O
the O
validation Metric
performance Metric
of O
supervised Task
learning O
on O
MNIST O
. O
In O
Fig O
. O
3a O
, O
we O
show O
the O
effect O
of O
with O
fixed O
α O
= O
1 O
, O
and O
in O
Fig O
. O
3b O
we O
show O
the O
effects O
of O
α O
with O
different O
fixed O
values O
of O
in O
the O
range O
{ O
1.0 O
, O
2.0 O
, O
3.0}. O
During O
the O
parameter Task
search Task
for O
supervised Task
learning O
on O
MNIST O
, O
the O
algorithm O
performed O
optimally O
when O
α O
= O
1 O
. O
Based O
on O
this O
result O
, O
we O
fixed O
α O
= O
1 O
while O
searching O
for O
optimal O
in O
all O
benchmark O
experiments O
, O
including O
both O
supervised Task
learning O
on O
MNIST O
and O
CIFAR Material
- Material
10 Material
, O
as O
well O
as O
unsupervised Task
learning Task
on O
MNIST O
, O
CIFAR Material
- Material
10 Material
, O
and O
SVHN Material
. O
As O
we O
showed O
in O
Section O
4.1 O
, O
this O
simple O
tuning O
of O
was O
sufficient O
for O
good O
performance O
of O
VAT Method
, O
and O
it O
even O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
several O
tasks O
. O
Fig O
. O
4 O
shows O
the O
R Metric
vadv Metric
values Metric
of O
the O
models O
trained O
with O
different O
K O
( O
the O
number O
of O
the O
power O
iterations O
) O
for O
supervised Task
learning O
on O
MNIST O
and O
semi Task
- Task
supervised Task
learning Task
on O
CIFAR Material
- Material
10 Material
. O
We O
can O
observe O
a O
significant O
increase O
in O
the O
value O
of O
R O
vadv O
over O
the O
transition O
from O
K O
= O
0 O
( O
random O
perturbations O
) O
to O
K O
= O
1 O
( O
virtual O
adversarial O
perturbations O
) O
. O
We O
can O
also O
observe O
that O
the O
value O
saturates O
at O
K O
= O
1 O
. O
The O
fact O
that O
one O
power Method
iteration Method
sufficed O
for O
good O
performance O
tells O
that O
the O
ratio O
λ O
1 O
/ O
λ O
2 O
for O
our O
experimental O
settings O
was O
very O
large O
. O
section O
: O
Effect O
of O
the O
Number O
of O
the O
Power O
Iterations O
K O
In O
fact O
, O
we O
could O
not O
achieve O
notable O
improvement O
in O
performance O
by O
increasing O
the O
value O
of O
K. O
Table O
6 O
shows O
the O
test Metric
accuracies Metric
for O
the O
semi O
- O
supervised Task
learning O
task O
on O
CIFAR10 Material
with O
different O
values O
of O
K. O
We O
however O
shall O
note O
that O
there O
is O
no O
guarantee O
that O
the O
spectrum O
of O
Hessian O
is O
always O
skew O
. O
Depending O
on O
the O
dataset O
and O
the O
model O
, O
K O
= O
1 O
might O
not O
be O
sufficient O
. O
section O
: O
Visualization Task
of Task
Virtual Task
Adversarial Task
Examples Task
section O
: O
Virtual O
Adversarial O
Examples O
Produced O
by O
the O
Model O
Trained O
with O
Different O
Choices O
of O
One O
might O
be O
interested O
in O
the O
actual O
visual O
appearance O
of O
the O
virtual O
adversarial O
examples O
with O
an O
appropriate O
size O
of O
that O
improves O
the O
regularization Metric
performance Metric
. O
In O
Fig O
. O
5 O
, O
we O
aligned O
( O
I O
) O
the O
transition O
of O
the O
performance O
of O
VAT Method
on O
SVHN Material
and O
CIFAR Material
- Material
10 Material
with O
respect O
to O
along O
( O
II O
) O
the O
actual O
virtual O
adversarial O
examples O
that O
the O
models O
trained O
with O
corresponding O
generated O
at O
the O
end O
of O
its O
training O
. O
For O
small O
( O
designated O
as O
( O
1 O
) O
in O
the O
figure O
) O
, O
it O
is O
difficult O
for O
human O
eyes O
to O
distinguish O
the O
virtual O
adversarial O
examples O
from O
the O
clean O
images O
. O
The O
size O
of O
that O
achieved O
the O
best O
validation Task
performance O
is O
designated O
by O
[ O
reference O
] O
. O
Especially O
for O
CIFAR Material
- Material
10 Material
, O
the O
virtual O
adversarial O
examples O
with O
of O
size O
( O
2 O
) O
are O
on O
the O
verge O
of O
total O
corruption O
. O
For O
a O
larger O
value O
of O
( O
designated O
by O
( O
3 O
) O
in O
the O
figure O
) O
, O
we O
can O
clearly O
observe O
the O
effect O
of O
over Method
- Method
regularization Method
. O
In O
fact O
, O
the O
virtual O
adversarial O
examples O
generated O
by O
the O
models O
trained O
with O
this O
range O
of O
are O
very O
far O
from O
the O
clean O
image O
, O
and O
we O
observe O
that O
the O
algorithm O
implemented O
with O
this O
large O
an O
did O
the O
unnecessary O
work O
of O
smoothing O
the O
output O
distribution O
over O
the O
set O
of O
images O
that O
are O
" O
unnatural O
. O
" O
section O
: O
Robustness Metric
against O
Virtual O
Adversarial O
Examples O
after O
Training O
We O
studied O
the O
nature O
of O
the O
robustness Metric
that O
can O
be O
attained O
by O
VAT Method
. O
We O
trained O
CNNs Method
on O
CIFAR Material
- Material
10 Material
with O
and O
without O
VAT Method
and O
prepared O
a O
set O
of O
pairs O
of O
virtual O
adversarial O
examples O
generated O
from O
the O
same O
picture O
, O
each O
consisting O
of O
( O
1 O
) O
the O
virtual O
adversarial O
example O
generated O
by O
the O
model O
trained O
with O
VAT Method
( O
w O
/ O
VAT Method
) O
and O
( O
2 O
) O
the O
virtual O
adversarial O
example O
generated O
by O
the O
model O
trained O
without O
VAT Method
( O
wo O
/ O
VAT Method
) O
. O
We O
studied O
the O
rates O
of O
misidentification Metric
by O
the O
classifiers Method
( O
w O
/ O
VAT Method
and O
wo O
/ O
VAT Method
) O
on O
these O
pairs O
of O
adversarial O
examples O
. O
Fig O
. O
6 O
shows O
the O
rates O
at O
which O
the O
two O
models O
( O
w O
/ O
VAT Method
and O
wo O
/ O
VAT Method
) O
misidentified O
the O
images O
corrupted O
by O
virtual O
adversarial O
perturbations O
of O
different O
magnitudes O
. O
The O
figure O
( O
A O
) O
in O
the O
middle O
panel O
shows O
the O
rates O
of O
misidentification O
made O
on O
the O
virtual O
adversarial O
examples O
generated O
by O
the O
model O
trained O
with O
VAT Method
. O
The O
figure O
( O
B O
) O
shows O
the O
rates O
on O
the O
virtual O
adversarial O
examples O
generated O
by O
the O
model O
trained O
without O
VAT Method
. O
The O
example O
pictures O
shown O
beneath O
the O
figures O
( O
A O
) O
and O
( O
B O
) O
are O
the O
adversarial O
examples O
generated O
from O
the O
set O
of O
images O
that O
were O
correctly O
identified O
by O
both O
the O
model O
trained O
with O
VAT Method
and O
the O
model O
trained O
without O
VAT Method
when O
fed O
without O
perturbation O
. O
As O
expected O
, O
the O
error Metric
rates Metric
increased O
monotonically O
with O
the O
intensity O
of O
corruption O
for O
both O
models O
. O
Overall O
, O
we O
recognize O
that O
the O
adversarial O
examples O
generated O
by O
both O
models O
are O
almost O
identical O
to O
the O
original O
image O
for O
human O
eyes O
when O
∼ O
10 O
−1 O
. O
The O
adversarial O
examples O
around O
∼ O
10 O
0 O
are O
almost O
identifiable O
, O
but O
are O
so O
corrupted O
that O
any O
further O
corruption O
would O
make O
the O
image O
unidentifiable O
by O
human O
eyes O
. O
The O
virtual O
adversarial O
examples O
with O
this O
range O
of O
are O
therefore O
the O
examples O
on O
which O
we O
wish O
the O
classifier Method
to O
make O
no O
mistakes O
. O
We O
can O
clearly O
observe O
that O
the O
rate O
of O
misidentification Metric
made O
by O
the O
VAT Method
- O
trained O
model O
for O
this O
range O
of O
is O
much O
lower O
than O
that O
of O
the O
model O
trained O
without O
VAT Method
. O
Also O
note O
in O
the O
bottom O
panel O
that O
the O
model O
trained O
with O
VAT Method
correctly O
identifies O
both O
the O
adversarial O
examples O
generated O
by O
itself O
and O
the O
adversarial O
examples O
generated O
by O
the O
model O
trained O
without O
VAT Method
for O
this O
range O
of O
. O
Simultaneously O
, O
note O
that O
the O
model O
trained O
with O
VAT Method
alters O
its O
decision O
on O
the O
images O
when O
the O
perturbation O
is O
too O
large O
. O
In O
contrast O
, O
the O
model O
trained O
without O
VAT Method
is O
assigning O
the O
original O
labels O
to O
the O
over O
- O
perturbed O
images O
at O
much O
higher O
rate O
than O
the O
VAT Method
- O
trained O
model O
, O
which O
is O
completely O
unnecessary O
, O
and O
is O
possibly O
even O
harmful O
in O
practice O
. O
Thus O
, O
we O
observe O
that O
the O
VAT Method
- O
trained O
model O
behaves O
much O
more O
' O
naturally O
' O
than O
the O
model O
trained O
without O
VAT Method
. O
section O
: O
Experimental O
Assessment O
of O
the O
Difference O
between O
VAT Method
and O
RPT Method
As O
we O
showed O
in O
the O
previous O
section O
, O
VAT Method
outperforms O
RPT Method
in O
many O
aspects O
. O
There O
are O
two O
possible O
reasons O
for O
the O
difference O
in O
performance O
. O
First O
, O
as O
mentioned O
in O
Section O
3.4 O
, O
the O
power Method
iteration Method
in O
VAT Method
promotes O
a O
faster O
learning Method
process Method
by O
decreasing O
the O
variance O
of O
the O
derivative O
of O
the O
objective O
function O
. O
Second O
, O
smoothing O
the O
function O
in O
the O
direction O
in O
which O
the O
model O
is O
most O
sensitive O
seems O
to O
be O
much O
more O
effective O
in O
improving O
the O
generalization Task
performance O
than O
smoothing O
the O
output O
distribution O
isotropically O
around O
the O
input O
. O
We O
discuss O
the O
extent O
to O
which O
these O
claims O
might O
be O
true O
. O
Let O
D O
M O
be O
the O
mini O
- O
batch O
of O
size O
M O
randomly O
extracted O
from O
D O
, O
and O
letR O
( O
K O
) O
( O
θ O
; O
D O
M O
, O
r O
K O
) O
be O
the O
approximation O
of O
R O
( O
K O
) O
( O
θ O
, O
D O
) O
computed O
with O
the O
mini O
- O
batch O
D O
M O
and O
random O
perturbations O
r O
K O
generated O
by O
K O
- O
times O
power O
iterations O
. O
To O
quantify O
the O
magnitude O
of O
the O
variance O
of O
the O
stochastic O
gradient O
, O
we O
define O
the O
normalized Metric
standard Metric
deviation Metric
( O
SD Metric
) O
norm O
for O
the O
gradient O
∇ O
θR O
, O
which O
is O
given O
as O
the O
square O
root O
of O
the O
trace O
of O
its O
variance O
normalized O
by O
the O
L O
2 O
norm O
of O
its O
expectation O
: O
where O
respectively O
represent O
the O
variance O
and O
expectation O
with O
respect O
to O
the O
randomly O
selected O
mini O
- O
batch O
D O
M O
and O
perturbation O
r O
K O
. O
Fig O
. O
7 O
shows O
the O
transition O
of O
the O
normalized O
SD Metric
norm O
during O
the O
VAT Method
process O
of O
NNs Method
for O
the O
supervised Task
learning O
task O
on O
MNIST O
( O
Fig O
. O
7a O
) O
and O
the O
semi O
- O
supervised Task
learning O
task O
on O
CIFAR Material
- Material
10 Material
( O
Fig O
. O
7b O
) O
with O
K O
= O
0 O
and O
K O
= O
1 O
( O
i.e. O
, O
RPT O
and O
VAT Method
) O
. O
We O
set O
M O
to O
be O
100 O
and O
128 O
on O
MNIST O
and O
CIFAR Material
- Material
10 Material
respectively O
. O
From O
the O
figure O
, O
we O
can O
observe O
that O
the O
normalized O
SD Metric
norm O
for O
K O
= O
1 O
is O
lower O
than O
that O
for O
K O
= O
0 O
in O
most O
of O
the O
early O
stages O
of O
the O
training O
for O
both O
MNIST O
and O
CIFAR Material
- Material
10 Material
. O
Meanwhile O
, O
for O
the O
instances O
on O
which O
the O
normalized O
SD Metric
norm O
of O
the O
gradient O
of O
RPT Method
falls O
below O
that O
of O
VAT Method
, O
the O
difference O
is O
subtle O
. O
( O
1 O
) O
ε=0.1 O
( O
2 O
) O
ε=3.0 O
For O
MNIST O
, O
the O
normalized O
SD Metric
norm O
of O
RPT Method
becomes O
as O
large O
as O
3 O
times O
that O
of O
VAT Method
. O
To O
understand O
how O
much O
the O
normalized O
SD Metric
norm O
affects O
the O
performance O
, O
we O
compared O
( O
1 O
) O
VAT Method
with O
α Method
= O
1 O
against O
( O
2 O
) O
RPT Method
implemented O
with O
optimal Method
α Method
and O
an O
additional O
procedure O
of O
sampling O
9 O
= O
3 O
2 O
random O
perturbations O
and O
using O
the O
average O
of O
the O
9 O
gradients O
for O
each O
update O
. O
Note O
that O
the O
normalized O
SD Metric
norm O
of O
the O
gradient O
does O
not O
depend O
on O
α O
. O
With O
this O
setting O
, O
the O
normalized O
SD Metric
norm O
of O
RPT Method
is O
not O
greater O
than O
that O
of O
VAT Method
at O
the O
beginning O
of O
the O
training O
( O
Fig O
. O
8 O
) O
. O
Remarkably O
, O
even O
with O
optimal O
α O
and O
increased O
S O
, O
the O
performance O
of O
RPT Method
still O
falls O
behind O
that O
of O
VAT Method
with O
α O
= O
1 O
. O
Even O
with O
similar O
SD Metric
norm O
, O
the O
model O
trained O
with O
VAT Method
is O
more O
robust O
against O
virtual Method
adversarial Method
perturbation Method
than O
the O
model O
trained O
with O
RPT Method
. O
This O
, O
in O
particular O
, O
means O
that O
we O
can O
not O
explain O
the O
superiority O
of O
VAT Method
over O
RPT Method
by O
the O
reduction O
in O
the O
variance Metric
alone O
. O
All O
these O
results O
suggest O
that O
the O
superior O
performance O
of O
VAT Method
owes O
much O
to O
the O
unique O
nature O
of O
its O
objective O
function O
. O
Intuition O
tells O
us O
that O
the O
" O
max Method
" Method
component Method
in O
the O
objective O
function O
as O
opposed O
to O
" O
expectation O
" O
is O
working O
in O
favor O
of O
the O
performance O
. O
At O
any O
phase O
in O
the O
training O
, O
the O
model O
might O
lack O
isotropic O
smoothness O
around O
some O
sample O
input O
points O
; O
it O
is O
natural O
that O
, O
to O
fix O
this O
, O
we O
must O
endeavor O
to O
smooth O
the O
model O
in O
the O
direction O
in O
which O
the O
model O
is O
most O
sensitive O
; O
that O
is O
, O
we O
need O
to O
take O
the O
maximum O
. O
section O
: O
CONCLUSIONS O
The O
results O
of O
our O
experiments O
on O
the O
three O
benchmark O
datasets O
, O
MNIST O
, O
SVHN Material
, O
and O
CIFAR Material
- Material
10 Material
indicate O
that O
VAT Method
is O
an O
effective O
method O
for O
both O
supervised Task
and O
semisupervised O
learning O
. O
For O
the O
MNIST O
dataset O
, O
VAT Method
outperformed O
recent O
popular O
methods O
other O
than O
ladder Method
networks Method
, O
which O
are O
the O
current O
state O
- O
of O
- O
the O
- O
art O
method O
that O
uses O
special O
network O
structure O
. O
VAT Method
also O
greatly O
outperformed O
the O
current O
state O
- O
of O
- O
the O
- O
art O
semi Method
- Method
supervised Method
learning Method
methods Method
for O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
The O
simplicity O
of O
our O
method O
is O
also O
worth O
reemphasizing O
. O
With O
our O
approximation Method
of Method
R Method
vadv Method
, O
VAT Method
can O
avoid O
the O
internal Task
optimization Task
when O
choosing O
the O
adversarial O
direction O
and O
can O
be O
implemented O
with O
small O
computational Metric
cost Metric
. O
Additionally O
, O
VAT Method
has O
only O
two O
hyperparameters O
( O
and O
α O
) O
and O
works O
sufficiently O
well O
on O
the O
benchmark O
dataset O
with O
the O
optimization Task
of Task
alone Task
. O
To O
add O
even O
more O
, O
VAT Method
is O
applicable O
to O
wide O
variety O
of O
models O
regardless O
of O
its O
architecture O
. O
Also O
, O
unlike O
generative Method
model Method
- Method
based Method
methods Method
, O
VAT Method
does O
not O
require O
the O
training O
of O
additional O
models O
other O
than O
the O
discriminative Method
model Method
( O
output O
distribution O
) O
itself O
. O
At O
the O
same O
time O
, O
in O
our O
comparative O
studies O
, O
we O
reconfirmed O
the O
effectiveness O
of O
generative Method
model Method
- Method
based Method
semi Method
- Method
supervised Method
learning Method
methods Method
on O
several O
experimental O
settings O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
Essentially O
, O
most O
generative Method
model Method
- Method
based Method
methods Method
promote O
generalization Task
performance O
by O
making O
the O
model O
robust O
against O
perturbations O
in O
the O
region O
of O
high O
p O
( O
x O
) O
values O
, O
the O
region O
over O
which O
we O
are O
likely O
to O
receive O
new O
input O
data O
points O
in O
the O
future O
. O
In O
principle O
, O
this O
is O
complementary O
to O
our O
method O
, O
which O
aims O
to O
isotropically O
smooth O
the O
output O
distribution O
p O
( O
y|x O
) O
over O
each O
observed O
input O
point O
without O
any O
explicit O
assumption O
on O
the O
input O
distribution O
p O
( O
x O
) O
. O
Combination O
of O
these O
two O
ideas O
is O
a O
future O
work O
. O
section O
: O
APPENDIX O
A O
SUPERVISED Task
CLASSIFICATION Task
FOR O
THE O
MNIST O
DATASET O
The O
MNIST O
dataset O
consists O
of O
28 O
× O
28 O
pixel O
images O
of O
handwritten O
digits O
and O
their O
corresponding O
labels O
. O
The O
input O
dimension O
is O
therefore O
28 O
× O
28 O
= O
784 O
, O
and O
each O
label O
is O
one O
of O
the O
numerals O
from O
0 O
to O
9 O
. O
The O
following O
list O
summarizes O
the O
ranges O
our O
hyper Method
parameter Method
search Method
: O
All O
experiments O
were O
conducted O
with O
α O
= O
1 O
except O
when O
checking O
the O
effects O
of O
α O
in O
Section O
4.2 O
. O
Training Method
was O
conducted O
using O
mini Method
- Method
batch Method
SGD Method
based O
on O
ADAM Method
[ O
reference O
] O
. O
We O
chose O
the O
mini O
- O
batch O
size O
of O
100 O
and O
used O
the O
default O
values O
of O
[ O
reference O
] O
for O
the O
tunable O
parameters O
of O
ADAM Method
. O
We O
trained O
the O
NNs Method
with O
60 O
, O
000 O
parameter Method
updates Method
. O
For O
the O
base O
learning Metric
rate Metric
in O
validation Task
, O
we O
selected O
the O
initial O
value O
of O
0.002 O
, O
and O
adopted O
the O
schedule O
of O
exponential Method
decay Method
with O
rate O
0.9 O
per O
600 O
updates O
. O
We O
repeated O
the O
experiments O
10 O
times O
with O
different O
random O
seeds O
for O
the O
weight Method
initialization Method
and O
reported O
the O
mean O
and O
standard O
deviation O
of O
the O
results O
. O
section O
: O
APPENDIX O
B O
SUPERVISED Task
CLASSIFICATION Task
FOR O
CIFAR Material
- Material
10 Material
DATASET O
The O
CIFAR Material
- Material
10 Material
dataset O
consists O
of O
32 O
× O
32 O
× O
3 O
pixel O
RGB O
images O
of O
categorized O
objects O
( O
cars O
, O
trucks O
, O
planes O
, O
animals O
, O
and O
humans O
) O
. O
The O
number O
of O
training O
examples O
and O
test O
examples O
in O
the O
dataset O
are O
50 O
, O
000 O
and O
10 O
, O
000 O
, O
respectively O
. O
We O
used O
10 O
, O
000 O
out O
of O
50 O
, O
000 O
training O
examples O
for O
validation O
and O
we O
applied O
ZCA Method
whitening Method
prior O
to O
the O
experiment O
. O
We O
also O
augmented O
the O
training O
dataset O
by O
applying O
random O
2 O
× O
2 O
translation O
and O
random O
horizontal O
flip O
. O
We O
trained O
the O
ConvLarge Method
model Method
( O
See O
Table7 O
) O
over O
300 O
epochs O
with O
batch O
size O
100 O
. O
For O
training Task
, O
we O
used O
ADAM Method
with O
essentially O
the O
same O
learning Metric
rate Metric
schedule Metric
as O
the O
one O
used O
in O
[ O
reference O
] O
. O
In O
particular O
, O
we O
set O
the O
initial O
learning Metric
rate Metric
of O
ADAM Method
to O
be O
0.003 O
and O
linearly O
decayed O
the O
rate O
over O
the O
last O
half O
of O
training O
. O
We O
repeated O
the O
experiments O
3 O
times O
with O
different O
random O
seeds O
for O
the O
weight Method
initialization Method
and O
reported O
the O
mean O
and O
standard O
deviation O
of O
the O
results O
. O
section O
: O
APPENDIX O
C O
SEMI O
- O
SUPERVISED Task
CLASSIFICATION Task
FOR O
THE O
MNIST O
DATASET O
For O
semi Task
- Task
supervised Task
learning Task
of O
MNIST O
, O
we O
used O
the O
same O
network O
as O
the O
network O
used O
for O
supervised Task
learning O
; O
however O
, O
we O
added O
zero O
- O
mean O
Gaussian O
random O
noise O
with O
0.5 O
standard O
deviation O
to O
the O
hidden O
variables O
during O
the O
training O
. O
This O
modification O
stabilized O
the O
training O
on O
semi Task
- Task
supervised Task
learning Task
with O
VAT Method
. O
We O
experimented O
with O
two O
sizes O
of O
labeled O
training O
samples O
, O
N O
l O
∈ O
{ O
100 O
, O
1000 O
} O
, O
and O
observed O
the O
effect O
of O
N O
l O
on O
the O
test Metric
error Metric
. O
We O
used O
the O
validation O
set O
of O
fixed O
size O
( O
1 O
, O
000 O
) O
, O
and O
used O
all O
the O
training O
samples O
, O
excluding O
the O
validation O
set O
and O
labeled O
training O
samples O
, O
to O
train O
the O
NNs Method
; O
that O
is O
, O
when O
N O
l O
= O
100 O
, O
the O
unlabeled O
training O
set O
N O
ul O
had O
the O
size O
of O
60 O
, O
000 O
− O
100 O
− O
1 O
, O
000 O
= O
58 O
, O
900 O
. O
We O
searched O
for O
the O
best O
hyperparameter O
from O
[ O
0.05 O
, O
10.0 O
] O
. O
All O
experiments O
were O
conducted O
with O
α O
= O
1 O
and O
K O
= O
1 O
. O
For O
the O
optimization Method
method Method
, O
we O
again O
used O
ADAM Method
- Method
based Method
mini Method
- Method
batch Method
SGD Method
with O
the O
same O
hyperparameter O
values O
that O
we O
used O
in O
supervised Task
setting O
. O
We O
note O
that O
the O
likelihood O
term O
can O
be O
computed O
from O
labeled O
data O
only O
. O
We O
used O
two O
separate O
mini O
- O
batches O
at O
each O
step O
: O
one O
mini O
- O
batch O
of O
size O
64 O
from O
labeled O
samples O
to O
compute O
the O
likelihood Method
term Method
, O
and O
another O
mini O
- O
batch O
of O
size O
256 O
from O
both O
labeled O
and O
unlabeled O
samples O
to O
compute O
the O
regularization Method
term Method
. O
We O
trained O
the O
NNs Method
over O
100 O
, O
000 O
parameter O
updates O
, O
and O
started O
to O
decay O
the O
learning Metric
rate Metric
of O
ADAM Method
linearly O
after O
we O
50 O
, O
000 O
- O
th O
update O
. O
We O
repeated O
the O
experiments O
3 O
times O
with O
different O
random O
seeds O
for O
the O
weight Method
initialization Method
and O
for O
the O
selection Task
of Task
labeled Task
samples Task
. O
We O
reported O
the O
mean O
and O
standard O
deviation O
of O
the O
results O
. O
section O
: O
APPENDIX O
D O
SEMI O
- O
SUPERVISED Task
CLASSIFICATION Task
FOR O
THE O
SVHN Material
AND O
CIFAR Material
- Material
10 Material
DATASETS O
The O
SVHN Material
dataset O
consists O
of O
32 O
× O
32 O
× O
3 O
pixel O
RGB O
images O
of O
house O
numbers O
and O
their O
corresponding O
labels O
( O
0 O
- O
9 O
) O
. O
The O
number O
of O
training O
samples O
and O
test O
samples O
within O
the O
dataset O
are O
73 O
, O
257 O
and O
26 O
, O
032 O
, O
respectively O
. O
We O
reserved O
a O
sample O
dataset O
of O
size O
1 O
, O
000 O
for O
validation O
. O
From O
the O
remainder O
, O
we O
selected O
sample O
dataset O
of O
size O
1 O
, O
000 O
as O
a O
labeled O
dataset O
in O
semisupervised Task
training Task
. O
Likewise O
in O
the O
supervised Task
learning O
, O
we O
conducted O
ZCA Method
preprocessing Method
prior O
to O
the O
semi Task
- Task
supervised Task
learning Task
of O
CIFAR Material
- Material
10 Material
. O
We O
also O
augmented O
the O
training O
datasets O
with O
a O
random O
2 O
× O
2 O
translation O
. O
For O
CIFAR Material
- Material
10 Material
exclusively O
, O
we O
also O
applied O
random O
horizontal O
flip O
as O
well O
. O
For O
the O
labeled O
dataset O
, O
we O
used O
4 O
, O
000 O
samples O
randomly O
selected O
from O
the O
training O
dataset O
, O
from O
which O
we O
selected O
1 O
, O
000 O
samples O
for O
validation O
. O
We O
repeated O
the O
experiment O
three O
times O
with O
different O
choices O
of O
labeled O
and O
unlabeled O
datasets O
on O
both O
SVHN Material
and O
CIFAR Material
- Material
10 Material
. O
For O
each O
benchmark O
dataset O
, O
we O
decided O
on O
the O
value O
of O
the O
hyperparameter O
based O
on O
the O
validation O
set O
. O
We O
also O
used O
a O
mini O
- O
batch O
of O
size O
32 O
for O
the O
calculation O
of O
the O
negative O
log O
- O
likelihood O
term O
and O
used O
a O
mini O
- O
batch O
of O
size O
128 O
for O
the O
calculation O
of O
R O
vadv O
in O
Eq O
. O
[ O
reference O
] O
. O
We O
trained O
each O
model O
with O
48 O
, O
000 O
updates O
. O
This O
corresponds O
to O
84 O
epochs O
for O
SVHN Material
TABLE O
7 O
: O
CNN Method
models Method
used O
in O
our O
experiments O
on O
CIFAR Material
- Material
10 Material
and O
SVHN Material
, O
based O
on O
[ O
reference O
] O
, O
[ O
reference O
] O
, O
[ O
reference O
] O
. O
All O
the O
convolutional Method
layers Method
and O
fully Method
connected Method
layers Method
are O
followed O
by O
batch Method
normalization Method
[ O
reference O
] O
except O
the O
fully Method
connected Method
layer Method
on O
CIFAR Material
- Material
10 Material
. O
The O
slopes O
of O
all O
lReLU O
[ O
reference O
] O
functions O
in O
the O
networks O
are O
set O
to O
0.1 O
. O
and O
123 O
epochs O
for O
CIFAR Material
- Material
10 Material
. O
We O
used O
ADAM Method
for O
the O
training O
. O
We O
set O
the O
initial O
learning Metric
rate Metric
of O
ADAM Method
to O
0.001 O
and O
linearly O
decayed O
the O
rate O
over O
the O
last O
16 O
, O
000 O
updates O
. O
The O
performance O
of O
CNN Method
- Method
Small Method
and Method
CNN Method
- Method
Large Method
that O
we O
reported O
in O
Section O
4.1.2 O
are O
all O
based O
on O
the O
trainings O
with O
data Task
augmentation Task
and O
the O
choices O
of O
that O
we O
described O
above O
. O
On O
SVHN Material
, O
we O
tested O
the O
performance O
of O
the O
algorithm O
with O
and O
without O
data Task
augmentation Task
, O
and O
used O
the O
same O
setting O
that O
we O
used O
in O
the O
validation O
experiments O
for O
both O
Conv Method
- Method
Small Method
and O
Conv Method
- Method
Large Method
. O
For O
CIFAR Material
- Material
10 Material
, O
however O
, O
the O
models O
did O
not O
seem O
to O
converge O
with O
48 O
, O
000 O
updates O
; O
so O
we O
reported O
the O
results O
with O
200 O
, O
000 O
updates O
. O
We O
repeated O
the O
experiments O
3 O
times O
with O
different O
random O
seeds O
for O
the O
weight Method
initialization Method
and O
for O
the O
selection Task
of Task
labeled Task
samples Task
. O
We O
reported O
the O
mean O
and O
standard O
deviation O
of O
the O
results O
. O
section O
: O
section O
: O
ACKNOWLEDGMENTS O
This O
study O
was O
supported O
by O
the O
New O
Energy O
and O
Industrial O
Technology O
Development O
Organization O
( O
NEDO O
) O
, O
Japan O
. O
section O
: O
