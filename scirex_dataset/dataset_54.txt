document	O
:	O
Meta	Method
-	Method
Transfer	Method
Learning	Method
for	O
Few	Task
-	Task
Shot	Task
Learning	Task
Meta	Method
-	Method
learning	Method
has	O
been	O
proposed	O
as	O
a	O
framework	O
to	O
address	O
the	O
challenging	O
few	O
-	Material
shot	Material
learning	O
setting	O
.	O
The	O
key	O
idea	O
is	O
to	O
leverage	O
a	O
large	O
number	O
of	O
similar	O
few	O
-	Material
shot	Material
tasks	O
in	O
order	O
to	O
learn	O
how	O
to	O
adapt	O
a	O
base	Method
-	Method
learner	Method
to	O
a	O
new	O
task	O
for	O
which	O
only	O
a	O
few	O
labeled	O
samples	O
are	O
available	O
.	O
As	O
deep	Method
neural	Method
networks	Method
(	O
DNNs	Method
)	O
tend	O
to	O
overfit	O
using	O
a	O
few	O
samples	O
only	O
,	O
meta	Method
-	Method
learning	Method
typically	O
uses	O
shallow	Method
neural	Method
networks	Method
(	O
SNNs	Method
)	O
,	O
thus	O
limiting	O
its	O
effectiveness	O
.	O
In	O
this	O
paper	O
we	O
propose	O
a	O
novel	O
few	O
-	Material
shot	Material
learning	O
method	O
called	O
meta	Method
-	Method
transfer	Method
learning	Method
(	O
MTL	Method
)	O
which	O
learns	O
to	O
adapt	O
a	O
deep	Method
NN	Method
for	O
few	Task
shot	Task
learning	Task
tasks	Task
.	O
Specifically	O
,	O
meta	Method
refers	O
to	O
training	O
multiple	O
tasks	O
,	O
and	O
transfer	Task
is	O
achieved	O
by	O
learning	O
scaling	Method
and	Method
shifting	Method
functions	Method
of	Method
DNN	Method
weights	Method
for	O
each	O
task	O
.	O
In	O
addition	O
,	O
we	O
introduce	O
the	O
hard	Method
task	Method
(	O
HT	Method
)	O
meta	Method
-	Method
batch	Method
scheme	Method
as	O
an	O
effective	O
learning	Method
curriculum	Method
for	O
MTL	Method
.	O
We	O
conduct	O
experiments	O
using	O
(	O
5	Task
-	Task
class	Task
,	O
1	O
-	Material
shot	Material
)	O
and	O
(	O
5	Task
-	Task
class	Task
,	O
5	O
-	Material
shot	Material
)	O
recognition	O
tasks	O
on	O
two	O
challenging	O
few	O
-	Material
shot	Material
learning	O
benchmarks	O
:	O
miniImageNet	Material
and	O
Fewshot	O
-	O
CIFAR100	O
.	O
Extensive	O
comparisons	O
to	O
related	O
works	O
validate	O
that	O
our	O
meta	Method
-	Method
transfer	Method
learning	Method
approach	O
trained	O
with	O
the	O
proposed	O
HT	Method
meta	Method
-	Method
batch	Method
scheme	Method
achieves	O
top	O
performance	O
.	O
An	O
ablation	O
study	O
also	O
shows	O
that	O
both	O
components	O
contribute	O
to	O
fast	O
convergence	Metric
and	O
high	O
accuracy	Metric
.	O
section	O
:	O
Introduction	O
While	O
deep	Method
learning	Method
systems	Method
have	O
achieved	O
great	O
performance	O
when	O
sufficient	O
amounts	O
of	O
labeled	O
data	O
are	O
available	O
,	O
there	O
has	O
been	O
growing	O
interest	O
in	O
reducing	O
the	O
required	O
amount	O
of	O
data	O
.	O
Few	O
-	Material
shot	Material
learning	O
tasks	O
have	O
been	O
defined	O
for	O
this	O
purpose	O
.	O
The	O
aim	O
is	O
to	O
learn	O
new	O
concepts	O
from	O
few	O
labeled	O
examples	O
,	O
e.g.	O
1	O
-	Material
shot	Material
learning	O
.	O
While	O
humans	O
tend	O
to	O
be	O
highly	O
effective	O
in	O
this	O
context	O
,	O
often	O
grasping	O
the	O
essential	O
connection	O
between	O
new	O
concepts	O
and	O
their	O
own	O
knowledge	O
and	O
experience	O
,	O
it	O
remains	O
challenging	O
for	O
machine	Method
learning	Method
approaches	Method
.	O
E.g.	O
,	O
on	O
the	O
CIFAR	O
-	O
100	O
dataset	O
,	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
achieves	O
only	O
accuracy	Metric
for	O
1	O
-	Material
shot	Material
learning	O
,	O
compared	O
to	O
for	O
the	O
all	Task
-	Task
class	Task
fully	Task
supervised	Task
case	Task
.	O
Few	O
-	Material
shot	Material
learning	O
methods	O
can	O
be	O
roughly	O
categorized	O
into	O
two	O
classes	O
:	O
data	Task
augmentation	Task
and	O
task	Method
-	Method
based	Method
meta	Method
-	Method
learning	Method
.	O
Data	Task
augmentation	Task
is	O
a	O
classic	O
technique	O
to	O
increase	O
the	O
amount	O
of	O
available	O
data	O
and	O
thus	O
also	O
useful	O
for	O
few	O
-	Material
shot	Material
learning	O
.	O
Several	O
methods	O
propose	O
to	O
learn	O
a	O
data	Method
generator	Method
e.g.	O
conditioned	O
on	O
Gaussian	O
noise	O
.	O
However	O
,	O
the	O
generation	Method
models	Method
often	O
underperform	O
when	O
trained	O
on	O
few	O
-	Material
shot	Material
data	O
.	O
An	O
alternative	O
is	O
to	O
merge	O
data	O
from	O
multiple	O
tasks	O
which	O
,	O
however	O
,	O
is	O
not	O
effective	O
due	O
to	O
variances	O
of	O
the	O
data	O
across	O
tasks	O
.	O
In	O
contrast	O
to	O
data	Method
-	Method
augmentation	Method
methods	Method
,	O
meta	Method
-	Method
learning	Method
is	O
a	O
task	Method
-	Method
level	Method
learning	Method
method	Method
.	O
Meta	Method
-	Method
learning	Method
aims	O
to	O
accumulate	O
experience	O
from	O
learning	O
multiple	O
tasks	O
,	O
while	O
base	Task
-	Task
learning	Task
focuses	O
on	O
modeling	O
the	O
data	Task
distribution	Task
of	O
a	O
single	O
task	O
.	O
A	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
representative	O
of	O
this	O
,	O
namely	O
Model	Method
-	Method
Agnostic	Method
Meta	Method
-	Method
Learning	Method
(	O
MAML	Method
)	Method
,	O
learns	O
to	O
search	O
for	O
the	O
optimal	O
initialization	O
state	O
to	O
fast	O
adapt	O
a	O
base	Method
-	Method
learner	Method
to	O
a	O
new	O
task	O
.	O
Its	O
task	O
-	O
agnostic	O
property	O
makes	O
it	O
possible	O
to	O
generalize	O
to	O
few	O
-	Material
shot	Material
supervised	O
learning	O
as	O
well	O
as	O
unsupervised	Method
reinforcement	Method
learning	Method
.	O
However	O
,	O
in	O
our	O
view	O
,	O
there	O
are	O
two	O
main	O
limitations	O
of	O
this	O
type	O
of	O
approaches	O
limiting	O
their	O
effectiveness	O
:	O
i	O
)	O
these	O
methods	O
usually	O
require	O
a	O
large	O
number	O
of	O
similar	O
tasks	O
for	O
meta	Method
-	Method
training	Method
which	O
is	O
costly	O
;	O
and	O
ii	O
)	O
each	O
task	O
is	O
typically	O
modeled	O
by	O
a	O
low	Method
-	Method
complexity	Method
base	Method
learner	Method
(	O
such	O
as	O
a	O
shallow	Method
neural	Method
network	Method
)	O
to	O
avoid	O
model	Task
overfitting	Task
,	O
thus	O
being	O
unable	O
to	O
use	O
deeper	O
and	O
more	O
powerful	O
architectures	O
.	O
For	O
example	O
,	O
for	O
the	O
miniImageNet	Material
dataset	Material
,	O
MAML	Method
uses	O
a	O
shallow	Method
CNN	Method
with	O
only	O
CONV	Method
layers	Method
and	O
its	O
optimal	O
performance	O
was	O
obtained	O
learning	O
on	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
meta	Method
-	Method
learning	Method
method	Method
called	O
meta	Method
-	Method
transfer	Method
learning	Method
(	O
MTL	Method
)	O
leveraging	O
the	O
advantages	O
of	O
both	O
transfer	Method
and	O
meta	Method
learning	Method
(	O
see	O
conceptual	O
comparison	O
of	O
related	O
methods	O
in	O
Figure	O
[	O
reference	O
]	O
)	O
.	O
In	O
a	O
nutshell	O
,	O
MTL	Method
is	O
a	O
novel	O
learning	Method
method	Method
that	O
helps	O
deep	Method
neural	Method
nets	Method
converge	O
faster	O
while	O
reducing	O
the	O
probability	O
to	O
overfit	O
when	O
using	O
few	O
labeled	O
training	O
data	O
only	O
.	O
In	O
particular	O
,	O
“	O
transfer	O
”	O
means	O
that	O
DNN	Method
weights	Method
trained	O
on	O
large	O
-	O
scale	O
data	O
can	O
be	O
used	O
in	O
other	O
tasks	O
by	O
two	O
light	Method
-	Method
weight	Method
neuron	Method
operations	Method
:	O
Scaling	O
and	O
Shifting	O
(	O
SS	Task
)	O
,	O
i.e.	O
.	O
“	O
Meta	O
”	O
means	O
that	O
the	O
parameters	O
of	O
these	O
operations	O
can	O
be	O
viewed	O
as	O
hyper	Method
-	Method
parameters	Method
trained	O
on	O
few	O
-	Material
shot	Material
learning	O
tasks	O
.	O
Large	O
-	O
scale	O
trained	O
DNN	Method
weights	Method
offer	O
a	O
good	O
initialization	O
,	O
enabling	O
fast	O
convergence	Metric
of	O
meta	Method
-	Method
transfer	Method
learning	Method
with	O
fewer	O
tasks	O
,	O
e.g.	O
only	O
tasks	O
for	O
miniImageNet	Material
,	O
times	O
fewer	O
than	O
MAML	Method
.	O
Light	Method
-	Method
weight	Method
operations	Method
on	O
DNN	O
neurons	O
have	O
less	O
parameters	O
to	O
learn	O
,	O
e.g.	O
less	O
than	O
if	O
considering	O
neurons	O
of	O
size	O
(	O
for	O
and	O
for	O
)	O
,	O
reducing	O
the	O
chance	O
of	O
overfitting	O
.	O
In	O
addition	O
,	O
these	O
operations	O
keep	O
those	O
trained	O
DNN	O
weights	O
unchanged	O
,	O
and	O
thus	O
avoid	O
the	O
problem	O
of	O
“	O
catastrophic	O
forgetting	O
”	O
which	O
means	O
forgetting	O
general	O
patterns	O
when	O
adapting	O
to	O
a	O
specific	O
task	O
.	O
The	O
second	O
main	O
contribution	O
of	O
this	O
paper	O
is	O
an	O
effective	O
meta	Method
-	Method
training	Method
curriculum	O
.	O
Curriculum	Task
learning	Task
and	O
hard	Task
negative	Task
mining	Task
both	O
suggest	O
that	O
faster	O
convergence	Metric
and	O
stronger	O
performance	O
can	O
be	O
achieved	O
by	O
a	O
better	O
arrangement	O
of	O
training	O
data	O
.	O
Inspired	O
by	O
these	O
ideas	O
,	O
we	O
design	O
our	O
hard	Method
task	Method
(	O
HT	Method
)	O
meta	Method
-	Method
batch	Method
strategy	Method
to	O
offer	O
a	O
challenging	O
but	O
effective	O
learning	Task
curriculum	Task
.	O
As	O
shown	O
in	O
the	O
bottom	O
rows	O
of	O
Figure	O
[	O
reference	O
]	O
,	O
a	O
conventional	O
meta	O
-	O
batch	O
contains	O
a	O
number	O
of	O
random	O
tasks	O
,	O
but	O
our	O
HT	Method
meta	Method
-	Method
batch	Method
online	O
re	O
-	O
samples	O
harder	O
ones	O
according	O
to	O
past	O
failure	O
tasks	O
with	O
lowest	O
validation	O
accuracy	Metric
.	O
Our	O
overall	O
contribution	O
is	O
thus	O
three	O
-	O
fold	O
:	O
i	O
)	O
we	O
propose	O
a	O
novel	O
MTL	Method
method	O
that	O
learns	O
to	O
transfer	O
large	O
-	O
scale	O
pre	Method
-	O
trained	O
DNN	O
weights	O
for	O
solving	O
few	O
-	Material
shot	Material
learning	O
tasks	O
;	O
ii	O
)	O
we	O
propose	O
a	O
novel	O
HT	Method
meta	Method
-	Method
batch	Method
learning	Method
strategy	Method
that	O
forces	O
meta	Method
-	Method
transfer	Method
to	O
“	O
grow	O
faster	O
and	O
stronger	O
through	O
hardship	O
”	O
;	O
and	O
iii	O
)	O
we	O
conduct	O
extensive	O
experiments	O
on	O
two	O
few	O
-	Material
shot	Material
learning	O
benchmarks	O
,	O
namely	O
miniImageNet	Material
and	O
Fewshot	O
-	O
CIFAR100	O
(	O
FC100	O
)	O
,	O
and	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
section	O
:	O
Related	O
work	O
Few	O
-	Material
shot	Material
learning	O
Research	O
literature	O
on	O
few	O
-	Material
shot	Material
learning	O
exhibits	O
great	O
diversity	O
.	O
In	O
this	O
section	O
,	O
we	O
focus	O
on	O
methods	O
using	O
the	O
supervised	O
meta	Method
-	Method
learning	Method
paradigm	O
most	O
relevant	O
to	O
ours	O
and	O
compared	O
to	O
in	O
the	O
experiments	O
.	O
We	O
can	O
divide	O
these	O
methods	O
into	O
three	O
categories	O
.	O
1	O
)	O
Metric	Method
learning	Method
methods	Method
learn	O
a	O
similarity	O
space	O
in	O
which	O
learning	Task
is	O
efficient	O
for	O
few	O
-	Material
shot	Material
examples	O
.	O
2	O
)	O
Memory	Method
network	Method
methods	Method
learn	O
to	O
store	O
“	O
experience	O
”	O
when	O
learning	O
seen	O
tasks	O
and	O
then	O
generalize	O
that	O
to	O
unseen	O
tasks	O
.	O
3	O
)	O
Gradient	Method
descent	Method
based	Method
methods	Method
have	O
a	O
specific	O
meta	Method
-	Method
learner	Method
that	O
learns	O
to	O
adapt	O
a	O
specific	O
base	Method
-	Method
learner	Method
(	O
to	O
few	O
-	Material
shot	Material
examples	O
)	O
through	O
different	O
tasks	O
.	O
E.g.	Method
MAML	Method
uses	O
a	O
meta	Method
-	Method
learner	Method
that	O
learns	O
to	O
effectively	O
initialize	O
a	O
base	Method
-	Method
learner	Method
for	O
a	O
new	O
learning	Task
task	Task
.	O
Meta	Method
-	Method
learner	Method
optimization	Method
is	O
done	O
by	O
gradient	Method
descent	Method
using	O
the	O
validation	Metric
loss	Metric
of	O
the	O
base	Method
-	Method
learner	Method
.	O
Our	O
method	O
is	O
closely	O
related	O
.	O
An	O
important	O
difference	O
is	O
that	O
our	O
MTL	Method
approach	O
leverages	O
transfer	Method
learning	Method
and	O
benefits	O
from	O
referencing	O
neuron	O
knowledge	O
in	O
pre	Method
-	O
trained	O
deep	Method
nets	Method
.	O
Although	O
MAML	Method
can	O
start	O
from	O
a	O
pre	Method
-	O
trained	O
network	O
,	O
its	O
element	Method
-	Method
wise	Method
fine	Method
-	Method
tuning	Method
makes	O
it	O
hard	O
to	O
learn	O
deep	Method
nets	Method
without	O
overfitting	O
(	O
validated	O
in	O
our	O
experiments	O
)	O
.	O
Transfer	Task
learning	Task
What	O
and	O
how	O
to	O
transfer	O
are	O
key	O
issues	O
to	O
be	O
addressed	O
in	O
transfer	Task
learning	Task
,	O
as	O
different	O
methods	O
are	O
applied	O
to	O
different	O
source	O
-	O
target	O
domains	O
and	O
bridge	O
different	O
transfer	O
knowledge	O
.	O
For	O
deep	Method
models	Method
,	O
a	O
powerful	O
transfer	Method
method	Method
is	O
adapting	O
a	O
pre	Method
-	O
trained	O
model	O
for	O
a	O
new	O
task	O
,	O
often	O
called	O
fine	Task
-	Task
tuning	Task
(	O
FT	Task
)	O
.	O
Models	O
pre	Method
-	O
trained	O
on	O
large	O
-	O
scale	O
datasets	O
have	O
proven	O
to	O
generalize	O
better	O
than	O
randomly	O
initialized	O
ones	O
.	O
Another	O
popular	O
transfer	Method
method	Method
is	O
taking	O
pre	Method
-	O
trained	O
networks	O
as	O
backbone	O
and	O
adding	O
high	O
-	O
level	O
functions	O
,	O
e.g.	O
for	O
object	Task
detection	Task
and	Task
recognition	Task
and	O
image	Task
segmentation	Task
.	O
Our	O
meta	Method
-	Method
transfer	Method
learning	Method
leverages	O
the	O
idea	O
of	O
transferring	O
pre	Method
-	O
trained	O
weights	O
and	O
aims	O
to	O
meta	O
-	O
learn	O
how	O
to	O
effectively	O
transfer	O
.	O
In	O
this	O
paper	O
,	O
large	O
-	O
scale	O
trained	O
DNN	O
weights	O
are	O
what	O
to	O
transfer	O
,	O
and	O
the	O
operations	O
of	O
Scaling	O
and	O
Shifting	Task
indicate	O
how	O
to	O
transfer	O
.	O
Similar	O
operations	O
have	O
been	O
used	O
to	O
modulating	O
the	O
per	O
-	O
feature	O
-	O
map	O
distribution	O
of	O
activations	O
for	O
visual	Task
reasoning	Task
.	O
Some	O
few	O
-	Material
shot	Material
learning	O
methods	O
have	O
been	O
proposed	O
to	O
use	O
pre	Method
-	O
trained	O
weights	O
as	O
initialization	O
.	O
Typically	O
,	O
weights	O
are	O
fine	O
-	O
tuned	O
for	O
each	O
task	O
,	O
while	O
we	O
learn	O
a	O
meta	Method
-	Method
transfer	Method
learner	Method
through	O
all	O
tasks	O
,	O
which	O
is	O
different	O
in	O
terms	O
of	O
the	O
underlying	O
learning	Method
paradigm	Method
.	O
Curriculum	Task
learning	Task
&	O
Hard	Task
sample	Task
mining	Task
Curriculum	Task
learning	Task
was	O
proposed	O
by	O
Bengio	O
et	O
al	O
.	O
and	O
is	O
popular	O
for	O
multi	Task
-	Task
task	Task
learning	Task
.	O
They	O
showed	O
that	O
instead	O
of	O
observing	O
samples	O
at	O
random	O
it	O
is	O
better	O
to	O
organize	O
samples	O
in	O
a	O
meaningful	O
way	O
so	O
that	O
fast	O
convergence	Metric
,	O
effective	O
learning	Task
and	O
better	O
generalization	Task
can	O
be	O
achieved	O
.	O
Pentina	O
et	O
al	O
.	O
use	O
adaptive	O
SVM	Method
classifiers	Method
to	O
evaluate	O
task	O
difficulty	O
for	O
later	Task
organization	Task
.	O
Differently	O
,	O
our	O
MTL	Method
method	O
does	O
task	Task
evaluation	Task
online	O
at	O
the	O
phase	O
of	O
episode	O
test	Task
,	O
without	O
needing	O
any	O
auxiliary	Method
model	Method
.	O
Hard	Method
sample	Method
mining	Method
was	O
proposed	O
by	O
Shrivastava	O
et	O
al	O
.	O
for	O
object	Task
detection	Task
.	O
It	O
treats	O
image	O
proposals	O
overlapped	O
with	O
ground	O
truth	O
as	O
hard	O
negative	O
samples	O
.	O
Training	O
on	O
more	O
confusing	O
data	O
enables	O
the	O
model	O
to	O
achieve	O
higher	O
robustness	Metric
and	O
better	O
performance	O
.	O
Inspired	O
by	O
this	O
,	O
we	O
sample	O
harder	O
tasks	O
online	O
and	O
make	O
our	O
MTL	Method
learner	O
“	O
grow	O
faster	O
and	O
stronger	O
through	O
more	O
hardness	O
”	O
.	O
In	O
our	O
experiments	O
,	O
we	O
show	O
that	O
this	O
can	O
be	O
generalized	O
to	O
enhance	O
other	O
meta	Method
-	Method
learning	Method
methods	Method
,	O
e.g.	O
MAML	Method
.	O
section	O
:	O
Preliminary	O
We	O
introduce	O
the	O
problem	O
setup	O
and	O
notations	O
of	O
meta	Method
-	Method
learning	Method
,	O
following	O
related	O
work	O
.	O
Meta	Method
-	Method
learning	Method
consists	O
of	O
two	O
phases	O
:	O
meta	Method
-	Method
train	Method
and	O
meta	O
-	O
test	Task
.	O
A	O
meta	Method
-	Method
training	Method
example	O
is	O
a	O
classification	Task
task	Task
sampled	O
from	O
a	O
distribution	O
.	O
is	O
called	O
episode	Method
,	O
including	O
a	O
training	O
split	O
to	O
optimize	O
the	O
base	Method
-	Method
learner	Method
,	O
and	O
a	O
test	Task
split	O
to	O
optimize	O
the	O
meta	Method
-	Method
learner	Method
.	O
In	O
particular	O
,	O
meta	Method
-	Method
training	Method
aims	O
to	O
learn	O
from	O
a	O
number	O
of	O
episodes	O
sampled	O
from	O
.	O
An	O
unseen	O
task	O
in	O
meta	O
-	O
test	Task
will	O
start	O
from	O
that	O
experience	O
of	O
the	O
meta	Method
-	Method
learner	Method
and	O
adapt	O
the	O
base	Method
-	Method
learner	Method
.	O
The	O
final	O
evaluation	O
is	O
done	O
by	O
testing	O
a	O
set	O
of	O
unseen	O
datapoints	O
.	O
Meta	Method
-	Method
training	Method
phase	Method
.	O
This	O
phase	O
aims	O
to	O
learn	O
a	O
meta	Method
-	Method
learner	Method
from	O
multiple	O
episodes	O
.	O
In	O
each	O
episode	O
,	O
meta	Method
-	Method
training	Method
has	O
a	O
two	O
-	O
stage	Method
optimization	Method
.	O
Stage	Method
-	Method
1	Method
is	O
called	O
base	Task
-	Task
learning	Task
,	O
where	O
the	O
cross	Method
-	Method
entropy	Method
loss	Method
is	O
used	O
to	O
optimize	O
the	O
parameters	O
of	O
the	O
base	Method
-	Method
learner	Method
.	O
Stage	Method
-	Method
2	Method
contains	O
a	O
feed	O
-	O
forward	O
test	Task
on	O
episode	O
test	Task
datapoints	O
.	O
The	O
test	Task
loss	O
is	O
used	O
to	O
optimize	O
the	O
parameters	O
of	O
the	O
meta	Method
-	Method
learner	Method
.	O
Specifically	O
,	O
given	O
an	O
episode	O
,	O
the	O
base	Method
-	Method
learner	Method
is	O
learned	O
from	O
episode	O
training	O
data	O
and	O
its	O
corresponding	O
loss	Metric
.	O
After	O
optimizing	O
this	O
loss	O
,	O
the	O
base	Method
-	Method
learner	Method
has	O
parameters	O
.	O
Then	O
,	O
the	O
meta	Method
-	Method
learner	Method
is	O
updated	O
using	O
test	Task
loss	O
.	O
After	O
meta	Method
-	Method
training	Method
on	O
all	O
episodes	O
,	O
the	O
meta	Method
-	Method
learner	Method
is	O
optimized	O
by	O
test	Task
losses	O
.	O
Therefore	O
,	O
the	O
number	O
of	O
meta	Method
-	Method
learner	Method
updates	Method
equals	O
to	O
the	O
number	O
of	O
episodes	O
.	O
Meta	O
-	O
test	Task
phase	O
.	O
This	O
phase	O
aims	O
to	O
test	Task
the	O
performance	O
of	O
the	O
trained	O
meta	Method
-	Method
learner	Method
for	O
fast	Task
adaptation	Task
to	O
unseen	Task
task	Task
.	O
Given	O
,	O
the	O
meta	Method
-	Method
learner	Method
teaches	O
the	O
base	Method
-	Method
learner	Method
to	O
adapt	O
to	O
the	O
objective	O
of	O
by	O
some	O
means	O
,	O
e.g.	O
through	O
initialization	Task
.	O
Then	O
,	O
the	O
test	Task
result	O
on	O
is	O
used	O
to	O
evaluate	O
the	O
meta	Method
-	Method
learning	Method
approach	Method
.	O
If	O
there	O
are	O
multiple	O
unseen	O
tasks	O
,	O
the	O
average	O
result	O
on	O
will	O
be	O
the	O
final	O
evaluation	O
.	O
section	O
:	O
Methodology	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
,	O
our	O
method	O
consists	O
of	O
three	O
phases	O
.	O
First	O
,	O
we	O
train	O
a	O
DNN	Method
on	O
large	O
-	O
scale	O
data	O
,	O
e.g.	O
on	O
miniImageNet	Material
(	O
-	O
class	O
,	O
-	Material
shot	Material
)	O
,	O
and	O
then	O
fix	O
the	O
low	Method
-	Method
level	Method
layers	Method
as	O
Feature	Method
Extractor	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Second	O
,	O
in	O
the	O
meta	Method
-	Method
transfer	Method
learning	Method
phase	O
,	O
MTL	Method
learns	O
the	O
Scaling	Method
and	Method
Shifting	Method
(	O
SS	Task
)	O
parameters	O
for	O
the	O
Feature	Method
Extractor	Method
neurons	Method
,	O
enabling	O
fast	O
adaptation	Task
to	O
few	O
-	Material
shot	Material
tasks	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
For	O
improved	O
overall	Task
learning	Task
,	O
we	O
use	O
our	O
HT	Method
meta	Method
-	Method
batch	Method
strategy	Method
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
The	O
training	O
steps	O
are	O
detailed	O
in	O
Algorithm	O
[	O
reference	O
]	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Finally	O
,	O
the	O
typical	O
meta	O
-	O
test	Task
phase	O
is	O
performed	O
,	O
as	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
.	O
subsection	O
:	O
DNN	Method
training	Method
on	O
large	O
-	O
scale	O
data	O
This	O
phase	O
is	O
similar	O
to	O
the	O
classic	O
pre	Method
-	O
training	O
stage	O
as	O
,	O
e.g.	O
,	O
pre	Method
-	O
training	O
on	O
Imagenet	Task
for	O
object	Task
recognition	Task
.	O
Here	O
,	O
we	O
do	O
not	O
consider	O
data	Task
/	Task
domain	Task
adaptation	Task
from	O
other	O
datasets	O
,	O
and	O
pre	Method
-	O
train	O
on	O
readily	O
available	O
data	O
of	O
few	O
-	Material
shot	Material
learning	O
benchmarks	O
,	O
allowing	O
for	O
fair	O
comparison	O
with	O
other	O
few	O
-	Material
shot	Material
learning	O
methods	O
.	O
Specifically	O
,	O
for	O
a	O
particular	O
few	O
-	Material
shot	Material
dataset	O
,	O
we	O
merge	O
all	O
-	O
class	O
data	O
for	O
pre	Method
-	O
training	O
.	O
For	O
instance	O
,	O
for	O
miniImageNet	Material
,	O
there	O
are	O
totally	O
classes	O
in	O
the	O
training	O
split	O
of	O
and	O
each	O
class	O
contains	O
samples	O
used	O
to	O
pre	Method
-	O
train	O
a	O
-	Method
class	Method
classifier	Method
.	O
We	O
first	O
randomly	O
initialize	O
a	O
feature	Method
extractor	Method
(	O
e.g.	O
CONV	Method
layers	Method
in	O
ResNets	Method
)	O
and	O
a	O
classifier	Method
(	O
e.g.	O
the	O
last	O
FC	O
layer	O
in	O
ResNets	O
)	O
,	O
and	O
then	O
optimize	O
them	O
by	O
gradient	Method
descent	Method
as	O
follows	O
,	O
where	O
denotes	O
the	O
following	O
empirical	Metric
loss	Metric
,	O
e.g.	O
cross	Metric
-	Metric
entropy	Metric
loss	Metric
,	O
and	O
denotes	O
the	O
learning	Metric
rate	Metric
.	O
In	O
this	O
phase	O
,	O
the	O
feature	Method
extractor	Method
is	O
learned	O
.	O
It	O
will	O
be	O
frozen	O
in	O
the	O
following	O
meta	Method
-	Method
training	Method
and	O
meta	Method
-	Method
test	Method
phases	Method
,	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
.	O
The	O
learned	O
classifier	Method
will	O
be	O
discarded	O
,	O
because	O
subsequent	O
few	O
-	Material
shot	Material
tasks	O
contain	O
different	O
classification	O
objectives	O
,	O
e.g.	O
-	O
class	O
instead	O
of	O
-	Method
class	Method
classification	Method
for	O
miniImageNet	Material
.	O
subsection	O
:	O
Meta	Method
-	Method
transfer	Method
learning	Method
(	O
MTL	Method
)	O
As	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
,	O
our	O
proposed	O
meta	Method
-	Method
transfer	Method
learning	Method
(	O
MTL	Method
)	O
method	O
optimizes	O
the	O
meta	Task
operations	Task
Scaling	Task
and	Task
Shifting	Task
(	O
SS	Task
)	O
through	O
HT	Method
meta	O
-	O
batch	O
training	O
(	O
Section	O
[	O
reference	O
]	O
)	O
.	O
Figure	O
[	O
reference	O
]	O
visualizes	O
the	O
difference	O
of	O
updating	O
through	O
SS	Task
and	O
FT	Task
.	O
SS	Task
operations	O
,	O
denoted	O
as	O
and	O
,	O
do	O
not	O
change	O
the	O
frozen	O
neuron	O
weights	O
of	O
during	O
learning	O
,	O
while	O
FT	Task
updates	O
the	O
complete	O
.	O
In	O
the	O
following	O
,	O
we	O
detail	O
the	O
SS	Task
operations	O
.	O
Given	O
a	O
task	O
,	O
the	O
loss	O
of	O
is	O
used	O
to	O
optimize	O
the	O
current	O
base	Method
-	Method
learner	Method
(	O
classifier	O
)	O
by	O
gradient	Method
descent	Method
:	O
which	O
is	O
different	O
to	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
as	O
we	O
do	O
not	O
update	O
.	O
Note	O
that	O
here	O
is	O
different	O
to	O
the	O
one	O
from	O
the	O
previous	O
phase	O
,	O
the	O
large	Method
-	Method
scale	Method
classifier	Method
in	O
Eq	O
.	O
[	O
reference	O
]	O
.	O
This	O
concerns	O
only	O
a	O
few	O
of	O
classes	O
,	O
e.g.	O
5	O
classes	O
,	O
to	O
classify	O
each	O
time	O
in	O
a	O
novel	O
few	O
-	Material
shot	Material
setting	O
.	O
corresponds	O
to	O
a	O
temporal	Method
classifier	Method
only	O
working	O
in	O
the	O
current	O
task	O
,	O
initialized	O
by	O
the	O
optimized	O
for	O
the	O
previous	O
task	O
(	O
see	O
Eq	O
.	O
[	O
reference	O
]	O
)	O
.	O
is	O
initialized	O
by	O
ones	O
and	O
by	O
zeros	O
.	O
Then	O
,	O
they	O
are	O
optimized	O
by	O
the	O
test	Task
loss	O
of	O
as	O
follows	O
,	O
In	O
this	O
step	O
,	O
is	O
updated	O
with	O
the	O
same	O
learning	Metric
rate	Metric
as	O
in	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
Re	O
-	O
linking	O
to	O
Eq	O
.	O
[	O
reference	O
]	O
,	O
we	O
note	O
that	O
the	O
above	O
comes	O
from	O
the	O
last	O
epoch	O
of	O
base	Task
-	Task
learning	Task
on	O
.	O
Next	O
,	O
we	O
describe	O
how	O
we	O
apply	O
to	O
the	O
frozen	O
neurons	O
as	O
shown	O
in	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
.	O
Given	O
the	O
trained	O
,	O
for	O
its	O
-	O
th	O
layer	O
containing	O
neurons	O
,	O
we	O
have	O
pairs	O
of	O
parameters	O
,	O
respectively	O
as	O
weight	O
and	O
bias	O
,	O
denoted	O
as	O
.	O
Note	O
that	O
the	O
neuron	O
location	O
will	O
be	O
omitted	O
for	O
readability	O
.	O
Based	O
on	O
MTL	Method
,	O
we	O
learn	O
pairs	O
of	O
scalars	O
.	O
Assuming	O
is	O
input	O
,	O
we	O
apply	O
to	O
as	O
where	O
denotes	O
the	O
element	Method
-	Method
wise	Method
multiplication	Method
.	O
Taking	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
as	O
an	O
example	O
of	O
a	O
single	O
filter	Method
,	O
after	O
SS	Task
operations	O
,	O
this	O
filter	O
is	O
scaled	O
by	O
then	O
the	O
feature	O
maps	O
after	O
convolutions	O
are	O
shifted	O
by	O
in	O
addition	O
to	O
the	O
original	O
bias	O
.	O
Detailed	O
steps	O
of	O
SS	Task
are	O
given	O
in	O
Algorithm	O
[	O
reference	O
]	O
in	O
Section	O
[	O
reference	O
]	O
.	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
a	O
typical	O
parameter	O
-	O
level	O
Fine	Method
-	Method
Tuning	Method
(	O
FT	Task
)	O
operation	O
,	O
which	O
is	O
in	O
the	O
meta	Task
optimization	Task
phase	Task
of	O
our	O
related	O
work	O
MAML	Method
.	O
It	O
is	O
obvious	O
that	O
FT	Task
updates	O
the	O
complete	O
values	O
of	O
and	O
,	O
and	O
has	O
a	O
large	O
number	O
of	O
parameters	O
,	O
and	O
our	O
SS	Task
reduces	O
this	O
number	O
to	O
below	O
in	O
the	O
example	O
of	O
the	O
figure	O
.	O
In	O
summary	O
,	O
SS	Task
can	O
benefit	O
MTL	Method
in	O
three	O
aspects	O
.	O
1	O
)	O
It	O
starts	O
from	O
a	O
strong	O
initialization	Method
based	O
on	O
a	O
large	Method
-	Method
scale	Method
trained	Method
DNN	Method
,	O
yielding	O
fast	Metric
convergence	Metric
for	O
MTL	Method
.	O
2	O
)	O
It	O
does	O
not	O
change	O
DNN	O
weights	O
,	O
thereby	O
avoiding	O
the	O
problem	O
of	O
“	O
catastrophic	O
forgetting	O
”	O
when	O
learning	Task
specific	Task
tasks	Task
in	O
MTL	Method
.	O
3	O
)	O
It	O
is	O
light	O
-	O
weight	O
,	O
reducing	O
the	O
chance	O
of	O
overfitting	O
of	O
MTL	Method
in	O
few	O
-	Material
shot	Material
scenarios	O
.	O
subsection	O
:	O
Hard	Task
task	Task
(	O
HT	Method
)	O
meta	O
-	O
batch	O
In	O
this	O
section	O
,	O
we	O
introduce	O
a	O
method	O
to	O
schedule	O
hard	Task
tasks	Task
in	O
meta	Method
-	Method
training	Method
batches	O
.	O
The	O
conventional	O
meta	Method
-	Method
batch	Method
is	O
composed	O
of	O
randomly	O
sampled	O
tasks	O
,	O
where	O
the	O
randomness	O
implies	O
random	O
difficulties	O
.	O
In	O
our	O
meta	Method
-	Method
training	Method
pipeline	Method
,	O
we	O
intentionally	O
pick	O
up	O
failure	O
cases	O
in	O
each	O
task	O
and	O
re	O
-	O
compose	O
their	O
data	O
to	O
be	O
harder	O
tasks	O
for	O
adverse	O
re	Task
-	Task
training	Task
.	O
We	O
aim	O
to	O
force	O
our	O
meta	Method
-	Method
learner	Method
to	O
“	O
grow	O
up	O
through	O
hardness	O
”	O
.	O
Pipeline	Method
.	O
Each	O
task	O
has	O
two	O
splits	O
,	O
and	O
,	O
for	O
base	Task
-	Task
learning	Task
and	O
test	Task
,	O
respectively	O
.	O
As	O
shown	O
in	O
Algorithm	O
[	O
reference	O
]	O
line	O
2	O
-	O
5	O
,	O
base	Method
-	Method
learner	Method
is	O
optimized	O
by	O
the	O
loss	O
of	O
(	O
in	O
multiple	O
epochs	O
)	O
.	O
SS	Task
parameters	O
are	O
then	O
optimized	O
by	O
the	O
loss	O
of	O
once	O
.	O
We	O
can	O
also	O
get	O
the	O
recognition	O
accuracy	Metric
of	O
for	O
classes	O
.	O
Then	O
,	O
we	O
choose	O
the	O
lowest	O
accuracy	Metric
to	O
determine	O
the	O
most	O
difficult	O
class	O
-	O
(	O
also	O
called	O
failure	O
class	O
)	O
in	O
the	O
current	O
task	O
.	O
After	O
obtaining	O
all	O
failure	O
classes	O
(	O
indexed	O
by	O
)	O
from	O
tasks	O
in	O
current	O
meta	O
-	O
batch	O
,	O
we	O
re	O
-	O
sample	O
tasks	O
from	O
their	O
data	O
.	O
Specifically	O
,	O
we	O
assume	O
is	O
the	O
task	O
distribution	O
,	O
we	O
sample	O
a	O
“	O
harder	O
”	O
task	O
.	O
Two	O
important	O
details	O
are	O
given	O
below	O
.	O
Choosing	O
hard	O
class	O
-	O
m	O
.	O
We	O
choose	O
the	O
failure	O
class	O
-	O
from	O
each	O
task	O
by	O
ranking	O
the	O
class	Metric
-	Metric
level	Metric
accuracies	Metric
instead	O
of	O
fixing	O
a	O
threshold	O
.	O
In	O
a	O
dynamic	Task
online	Task
setting	Task
as	O
ours	O
,	O
it	O
is	O
more	O
sensible	O
to	O
choose	O
the	O
hardest	O
cases	O
based	O
on	O
ranking	Task
rather	O
than	O
fixing	O
a	O
threshold	O
ahead	O
of	O
time	O
.	O
Two	O
methods	O
of	O
hard	Task
tasking	Task
using	O
{	O
m}.	O
Chosen	O
,	O
we	O
can	O
re	O
-	O
sample	O
tasks	O
by	O
(	O
1	O
)	O
directly	O
using	O
the	O
samples	O
of	O
class	O
-	O
in	O
the	O
current	O
task	O
,	O
or	O
(	O
2	O
)	O
indirectly	O
using	O
the	O
label	O
of	O
class	O
-	O
to	O
sample	O
new	O
samples	O
of	O
that	O
class	O
.	O
In	O
fact	O
,	O
setting	O
(	O
2	O
)	O
considers	O
to	O
include	O
more	O
data	O
variance	O
of	O
class	O
-	O
and	O
it	O
works	O
better	O
than	O
setting	O
(	O
1	O
)	O
in	O
general	O
.	O
subsection	O
:	O
Algorithm	O
Algorithm	O
[	O
reference	O
]	O
summarizes	O
the	O
training	O
process	O
of	O
two	O
main	O
stages	O
:	O
large	Task
-	Task
scale	Task
DNN	Task
training	Task
(	O
line	O
1	O
-	O
5	O
)	O
and	O
meta	Method
-	Method
transfer	Method
learning	Method
(	O
line	O
6	O
-	O
22	O
)	O
.	O
HT	Method
meta	Method
-	Method
batch	Method
re	Method
-	Method
sampling	Method
and	O
continuous	Method
training	Method
phases	Method
are	O
shown	O
in	O
lines	O
16	O
-	O
20	O
,	O
for	O
which	O
the	O
failure	O
classes	O
are	O
returned	O
by	O
Algorithm	O
[	O
reference	O
]	O
,	O
see	O
line	O
14	O
.	O
Algorithm	O
[	O
reference	O
]	O
presents	O
the	O
learning	Method
process	Method
on	O
a	O
single	O
task	O
that	O
includes	O
episode	Task
training	Task
(	O
lines	O
2	O
-	O
5	O
)	O
and	O
episode	O
test	Task
,	O
i.e.	O
meta	O
-	O
level	O
update	O
(	O
lines	O
6	O
)	O
.	O
In	O
lines	O
7	O
-	O
11	O
,	O
the	O
recognition	Metric
rates	Metric
of	O
all	O
test	Task
classes	O
are	O
computed	O
and	O
returned	O
to	O
Algorithm	O
[	O
reference	O
]	O
(	O
line	O
14	O
)	O
for	O
hard	Method
task	Method
sampling	O
.	O
Meta	Method
-	Method
transfer	Method
learning	Method
(	O
MTL	Method
)	O
KwDataInput	O
KwResultOutput	O
Task	O
distribution	O
and	O
corresponding	O
dataset	O
,	O
learning	Metric
rates	Metric
,	O
and	O
Feature	Method
extractor	Method
,	O
base	Method
learner	Method
,	O
SS	Task
parameters	O
Randomly	O
initialize	O
and	O
samples	O
in	O
Evaluate	O
by	O
Eq	O
.	O
[	O
reference	O
]	O
Optimize	O
and	O
by	O
Eq	O
.	O
[	O
reference	O
]	O
Initialize	O
by	O
ones	O
,	O
initialize	O
by	O
zeros	O
Reset	O
and	O
re	O
-	O
initialize	O
for	O
few	O
-	Material
shot	Material
tasks	O
meta	O
-	O
batches	O
Randomly	O
sample	O
tasks	O
from	O
not	O
done	O
Sample	O
task	O
}	O
Optimize	O
and	O
with	O
by	O
Algorithm	O
[	O
reference	O
]	O
Get	O
the	O
returned	O
class	O
-	O
then	O
add	O
it	O
to	O
Sample	O
hard	O
tasks	O
from	O
not	O
done	O
Sample	O
task	O
}	O
Optimize	O
and	O
with	O
by	O
Algorithm	O
[	O
reference	O
]	O
Empty	O
.	O
Detail	O
learning	O
steps	O
within	O
a	O
task	O
T	O
KwDataInput	O
KwResultOutput	O
,	O
learning	Metric
rates	Metric
and	O
,	O
feature	Method
extractor	Method
,	O
base	Method
learner	Method
,	O
SS	Task
parameters	O
Updated	O
and	O
,	O
the	O
worst	O
classified	O
class	O
-	O
in	O
Sample	O
and	O
from	O
samples	O
in	O
Evaluate	O
Optimize	O
by	O
Eq	O
.	O
[	O
reference	O
]	O
Optimize	O
and	O
by	O
Eq	O
.	O
[	O
reference	O
]	O
and	O
Eq	O
.	O
[	O
reference	O
]	O
not	O
done	O
Sample	O
class	O
-	O
in	O
Compute	O
for	O
Return	O
class	O
-	O
with	O
the	O
lowest	O
accuracy	Metric
.	O
section	O
:	O
Experiments	O
We	O
evaluate	O
the	O
proposed	O
MTL	Method
and	O
HT	Method
meta	Method
-	Method
batch	Method
in	O
terms	O
of	O
few	O
-	Material
shot	Material
recognition	O
accuracy	Metric
and	O
model	Metric
convergence	Metric
speed	Metric
.	O
Below	O
we	O
describe	O
the	O
datasets	O
and	O
detailed	O
settings	O
,	O
followed	O
by	O
an	O
ablation	O
study	O
and	O
a	O
comparison	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O
subsection	O
:	O
Datasets	O
and	O
implementation	O
details	O
We	O
conduct	O
few	O
-	Material
shot	Material
learning	O
experiments	O
on	O
two	O
benchmarks	O
,	O
miniImageNet	Material
and	O
Fewshot	O
-	O
CIFAR100	O
(	O
FC100	O
)	O
.	O
miniImageNet	Material
is	O
widely	O
used	O
in	O
related	O
works	O
.	O
FC100	O
is	O
newly	O
proposed	O
in	O
and	O
is	O
more	O
challenging	O
in	O
terms	O
of	O
lower	O
image	Metric
resolution	Metric
and	O
stricter	O
training	O
-	O
test	Task
splits	O
than	O
miniImageNet	Material
.	O
miniImageNet	Material
was	O
proposed	O
by	O
Vinyals	O
et	O
al	O
.	O
for	O
few	O
-	Material
shot	Material
learning	O
evaluation	O
.	O
Its	O
complexity	Metric
is	O
high	O
due	O
to	O
the	O
use	O
of	O
ImageNet	O
images	O
,	O
but	O
requires	O
less	O
resource	O
and	O
infrastructure	O
than	O
running	O
on	O
the	O
full	O
ImageNet	O
dataset	O
.	O
In	O
total	O
,	O
there	O
are	O
classes	O
with	O
samples	O
of	O
color	O
images	O
per	O
class	O
.	O
These	O
classes	O
are	O
divided	O
into	O
,	O
,	O
and	O
classes	O
respectively	O
for	O
sampling	Task
tasks	Task
for	O
meta	Method
-	Method
training	Method
,	O
meta	Task
-	Task
validation	Task
and	O
meta	O
-	O
test	Task
,	O
following	O
related	O
works	O
.	O
Fewshot	O
-	O
CIFAR100	O
(	O
FC100	O
)	O
is	O
based	O
on	O
the	O
popular	O
object	O
classification	O
dataset	O
CIFAR100	O
.	O
The	O
splits	O
were	O
proposed	O
by	O
(	O
Please	O
check	O
details	O
in	O
the	O
supplementary	O
)	O
.	O
It	O
offers	O
a	O
more	O
challenging	O
scenario	O
with	O
lower	O
image	O
resolution	O
and	O
more	O
challenging	O
meta	Method
-	Method
training	Method
/	O
test	Task
splits	O
that	O
are	O
separated	O
according	O
to	O
object	O
super	O
-	O
classes	O
.	O
It	O
contains	O
object	O
classes	O
and	O
each	O
class	O
has	O
samples	O
of	O
color	O
images	O
.	O
The	O
classes	O
belong	O
to	O
super	O
-	O
classes	O
.	O
Meta	O
-	O
training	O
data	O
are	O
from	O
classes	O
belonging	O
to	O
super	O
-	O
classes	O
.	O
Meta	Method
-	Method
validation	Method
and	O
meta	O
-	O
test	Task
sets	O
contain	O
classes	O
belonging	O
to	O
super	O
-	O
classes	O
,	O
respectively	O
.	O
These	O
splits	O
accord	O
to	O
super	O
-	O
classes	O
,	O
thus	O
minimize	O
the	O
information	O
overlap	O
between	O
training	O
and	O
val	O
/	O
test	Task
tasks	O
.	O
The	O
following	O
settings	O
are	O
used	O
on	O
both	O
datasets	O
.	O
We	O
train	O
a	O
large	Method
-	Method
scale	Method
DNN	Method
with	O
all	O
training	O
datapoints	O
(	O
Section	O
[	O
reference	O
]	O
)	O
and	O
stop	O
this	O
training	O
after	O
iterations	O
.	O
We	O
use	O
the	O
same	O
task	Method
sampling	Method
method	Method
as	O
related	O
works	O
.	O
Specifically	O
,	O
1	O
)	O
we	O
consider	O
the	O
5	Task
-	Task
class	Task
classification	O
and	O
2	O
)	O
we	O
sample	O
5	Task
-	Task
class	Task
,	O
1	O
-	Material
shot	Material
(	O
5	O
-	Material
shot	Material
or	O
10	O
-	Material
shot	Material
)	O
episodes	O
to	O
contain	O
1	O
(	O
5	O
or	O
10	O
)	O
samples	O
for	O
train	O
episode	O
,	O
and	O
(	O
uniform	O
)	O
samples	O
for	O
episode	O
test	Task
.	O
Note	O
that	O
in	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
work	O
,	O
and	O
samples	O
are	O
respectively	O
used	O
in	O
5	Material
-	Material
shot	Material
and	O
10	Task
-	Task
shot	Task
settings	Task
for	O
episode	O
test	Task
.	O
In	O
total	O
,	O
we	O
sample	O
tasks	O
for	O
meta	Method
-	Method
training	Method
(	O
same	O
for	O
w	O
/	O
and	O
w	O
/	O
o	O
HT	Method
meta	O
-	O
batch	O
)	O
,	O
and	O
respectively	O
sample	O
random	Method
tasks	Method
for	O
meta	Task
-	Task
validation	Task
and	O
meta	O
-	O
test	Task
.	O
Please	O
check	O
the	O
supplementary	O
document	O
(	O
or	O
)	O
for	O
other	O
implementation	O
details	O
,	O
e.g.	O
learning	Metric
rate	Metric
and	O
dropout	Metric
rate	Metric
.	O
Network	Method
architecture	Method
.	O
We	O
present	O
the	O
details	O
for	O
the	O
Feature	Method
Extractor	Method
,	O
MTL	Method
meta	Method
-	Method
learner	Method
with	O
Scaling	Method
and	Method
Shifting	Method
,	O
and	O
MTL	Method
base	Method
-	Method
learner	Method
(	O
classifier	Method
)	O
.	O
The	O
architecture	O
of	O
Θ	Method
have	O
two	O
options	O
,	O
ResNet	Method
-	Method
12	Method
and	O
4CONV	Method
,	O
commonly	O
used	O
in	O
related	O
works	O
.	O
4CONV	Method
consists	O
of	O
layers	O
with	O
convolutions	Method
and	Method
filters	Method
,	O
followed	O
by	O
batch	Method
normalization	Method
(	O
BN	Method
)	Method
,	O
a	O
ReLU	Method
nonlinearity	Method
,	O
and	O
max	Method
-	Method
pooling	Method
.	O
ResNet	Method
-	Method
12	Method
is	O
more	O
popular	O
in	O
recent	O
works	O
.	O
It	O
contains	O
residual	O
blocks	O
and	O
each	O
block	O
has	O
CONV	Method
layers	Method
with	O
kernels	O
.	O
At	O
the	O
end	O
of	O
each	O
residual	O
block	O
,	O
a	O
max	Method
-	Method
pooling	Method
layer	Method
is	O
applied	O
.	O
The	O
number	O
of	O
filters	O
starts	O
from	O
and	O
is	O
doubled	O
every	O
next	O
block	O
.	O
Following	O
blocks	O
,	O
there	O
is	O
a	O
mean	Method
-	Method
pooling	Method
layer	Method
to	O
compress	O
the	O
output	O
feature	O
maps	O
to	O
a	O
feature	Method
embedding	Method
.	O
The	O
difference	O
between	O
using	O
4CONV	Method
and	O
using	O
ResNet	Method
-	Method
12	Method
in	O
our	O
methods	O
is	O
that	O
ResNet	Method
-	Method
12	Method
MTL	Method
sees	O
the	O
large	Task
-	Task
scale	Task
data	Task
training	Task
,	O
but	O
4CONV	Method
MTL	Method
is	O
learned	O
from	O
scratch	O
because	O
of	O
its	O
poor	O
performance	O
for	O
large	Task
-	Task
scale	Task
data	Task
training	Task
(	O
see	O
results	O
in	O
the	O
supplementary	O
)	O
.	O
Therefore	O
,	O
we	O
emphasize	O
the	O
experiments	O
of	O
using	O
ResNet	Method
-	Method
12	Method
MTL	Method
for	O
its	O
superior	O
performance	O
.	O
The	O
architectures	O
of	O
ΦS1	O
and	O
ΦS2	Method
are	O
generated	O
according	O
to	O
the	O
architecture	O
of	O
,	O
as	O
introduced	O
in	O
Section	O
[	O
reference	O
]	O
.	O
That	O
is	O
when	O
using	O
ResNet	Method
-	Method
12	Method
in	O
MTL	Method
,	O
and	O
also	O
have	O
12	O
layers	O
,	O
respectively	O
.	O
The	O
architecture	O
of	O
θ	Method
is	O
an	O
FC	Method
layer	Method
.	O
We	O
empirically	O
find	O
that	O
a	O
single	O
FC	Method
layer	Method
is	O
faster	O
to	O
train	O
and	O
more	O
effective	O
for	O
classification	Task
than	O
multiple	O
layers	O
.	O
(	O
see	O
comparisons	O
in	O
the	O
supplementary	O
)	O
.	O
subsection	O
:	O
Ablation	O
study	O
setting	O
In	O
order	O
to	O
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
we	O
design	O
some	O
ablative	Method
settings	Method
:	O
two	O
baselines	O
without	O
meta	Method
-	Method
learning	Method
but	O
more	O
classic	Method
learning	Method
,	O
three	O
baselines	O
of	O
Fine	Method
-	Method
Tuning	Method
(	O
FT	Task
)	O
on	O
smaller	O
number	O
of	O
parameters	O
(	O
Table	O
[	O
reference	O
]	O
)	O
,	O
and	O
two	O
MAML	Method
variants	Method
using	O
our	O
deeper	O
pre	Method
-	O
trained	O
model	O
and	O
HT	Method
meta	Method
-	Method
batch	Method
(	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
)	O
.	O
Note	O
that	O
the	O
alternative	O
meta	Method
-	Method
learning	Method
operation	Method
to	O
SS	Task
is	O
the	O
FT	Task
used	O
in	O
MAML	Method
.	O
Some	O
bullet	O
names	O
are	O
explained	O
as	O
follows	O
.	O
update	O
[	O
Θ;θ	O
]	O
(	O
or	O
θ	O
)	O
.	O
There	O
is	O
no	O
meta	Method
-	Method
training	Method
phase	O
.	O
During	O
test	Task
phase	O
,	O
each	O
task	O
has	O
its	O
whole	O
model	O
(	O
or	O
the	O
classifier	Method
)	O
updated	O
on	O
,	O
and	O
then	O
tested	O
on	O
.	O
FT	Task
[	O
⁢Θ4;θ	O
]	O
(	O
or	O
θ	O
)	O
.	O
These	O
are	O
straight	O
-	O
forward	O
ways	O
to	O
define	O
a	O
smaller	O
set	O
of	O
meta	Method
-	Method
learner	Method
parameters	O
than	O
MAML	Method
.	O
We	O
can	O
freeze	O
low	O
-	O
level	O
pre	Method
-	O
trained	O
layers	O
and	O
meta	O
-	O
learn	O
the	O
classifier	Method
layer	Method
with	O
(	O
or	O
without	O
)	O
high	Method
-	Method
level	Method
CONV	Method
layer	Method
that	O
is	O
the	O
4th	O
residual	O
block	O
of	O
ResNet	Method
-	Method
12	Method
.	O
subsection	O
:	O
Results	O
and	O
analysis	O
Table	O
[	O
reference	O
]	O
,	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
present	O
the	O
overall	O
results	O
on	O
miniImageNet	Material
and	O
FC100	O
datasets	O
.	O
Extensive	O
comparisons	O
are	O
done	O
with	O
ablative	Method
methods	Method
and	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
.	O
Note	O
that	O
tables	O
present	O
the	O
highest	O
accuracies	Metric
for	O
which	O
the	O
iterations	O
were	O
chosen	O
by	O
validation	O
.	O
For	O
the	O
miniImageNet	Material
,	O
iterations	O
for	O
1	Material
-	Material
shot	Material
and	O
5	Material
-	Material
shot	Material
are	O
at	O
and	O
,	O
respectively	O
.	O
For	O
the	O
FC100	O
,	O
iterations	O
are	O
all	O
at	O
.	O
Figure	O
[	O
reference	O
]	O
shows	O
the	O
performance	O
gap	O
between	O
with	O
and	O
without	O
HT	Method
meta	Method
-	Method
batch	Method
in	O
terms	O
of	O
accuracy	Metric
and	O
converging	Metric
speed	Metric
.	O
Result	O
overview	O
on	O
miniImageNet	Material
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
can	O
see	O
that	O
the	O
proposed	O
MTL	Method
with	O
SS	Task
,	O
HT	Method
meta	Method
-	Method
batch	Method
and	O
ResNet	Method
-	Method
12	Method
(	O
pre	Method
)	O
achieves	O
the	O
best	O
few	Metric
-	Metric
shot	Metric
classification	Metric
performance	Metric
with	O
for	O
(	O
5	Task
-	Task
class	Task
,	O
1	O
-	Material
shot	Material
)	O
.	O
Besides	O
,	O
it	O
tackles	O
the	O
(	O
5	Task
-	Task
class	Task
,	O
5	O
-	Material
shot	Material
)	O
tasks	O
with	O
an	O
accuracy	Metric
of	O
that	O
is	O
comparable	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
i.e.	O
,	O
reported	O
by	O
TADAM	O
whose	O
model	O
used	O
additional	O
FC	Method
layers	Method
in	O
the	O
ResNet	Method
-	Method
12	Method
arch	O
.	O
In	O
terms	O
of	O
the	O
network	Task
arch	Task
,	O
it	O
is	O
obvious	O
that	O
models	O
using	O
ResNet	Method
-	Method
12	Method
(	O
pre	Method
)	O
outperforms	O
those	O
using	O
4CONV	Method
by	O
large	O
margins	O
,	O
e.g.	O
4CONV	Method
models	O
have	O
the	O
best	O
1	O
-	Material
shot	Material
result	O
with	O
which	O
is	O
lower	O
than	O
our	O
best	O
.	O
Result	O
overview	O
on	O
FC100	O
.	O
In	O
Table	O
[	O
reference	O
]	O
,	O
we	O
give	O
the	O
results	O
of	O
TADAM	O
using	O
their	O
reported	O
numbers	O
in	O
the	O
paper	O
.	O
We	O
used	O
the	O
public	O
code	O
of	O
MAML	Method
to	O
get	O
its	O
results	O
for	O
this	O
new	O
dataset	O
.	O
Comparing	O
these	O
methods	O
,	O
we	O
can	O
see	O
that	O
MTL	Method
consistently	O
outperforms	O
MAML	Method
by	O
large	O
margins	O
,	O
i.e.	O
around	O
in	O
all	O
tasks	O
;	O
and	O
surpasses	O
TADAM	O
by	O
a	O
relatively	O
larger	O
number	O
of	O
for	O
1	Material
-	Material
shot	Material
,	O
and	O
with	O
and	O
respectively	O
for	O
5	Material
-	Material
shot	Material
and	O
10	O
-	Material
shot	Material
tasks	O
.	O
MTL	Method
vs.	O
No	O
meta	Method
-	Method
learning	Method
.	O
Table	O
[	O
reference	O
]	O
shows	O
the	O
results	O
of	O
No	O
meta	Method
-	Method
learning	Method
on	O
the	O
top	O
block	O
.	O
Compared	O
to	O
these	O
,	O
our	O
approach	O
achieves	O
significantly	O
better	O
performance	O
even	O
without	O
HT	Method
meta	Method
-	Method
batch	Method
,	O
e.g.	O
the	O
largest	O
margins	O
are	O
for	O
1	Material
-	Material
shot	Material
and	O
for	O
5	Material
-	Material
shot	Material
on	O
miniImageNet	Material
.	O
This	O
validates	O
the	O
effectiveness	O
of	O
our	O
meta	Method
-	Method
learning	Method
method	Method
for	O
tackling	O
few	O
-	Material
shot	Material
learning	O
problems	O
.	O
Between	O
two	O
No	Method
meta	Method
-	Method
learning	Method
methods	Method
,	O
we	O
can	O
see	O
that	O
updating	O
both	O
feature	Method
extractor	Method
and	O
classifier	Method
is	O
inferior	O
to	O
updating	O
only	O
,	O
e.g.	O
around	O
reduction	Task
on	O
miniImageNet	Material
1	Material
-	Material
shot	Material
.	O
One	O
reason	O
is	O
that	O
in	O
few	O
-	Material
shot	Material
settings	O
,	O
there	O
are	O
too	O
many	O
parameters	O
to	O
optimize	O
with	O
little	O
data	O
.	O
This	O
supports	O
our	O
motivation	O
to	O
learn	O
only	O
during	O
base	Task
-	Task
learning	Task
.	O
Performance	O
effects	O
of	O
MTL	Method
components	O
.	O
MTL	Method
with	O
full	Method
components	Method
,	O
SS	Task
,	O
HT	Method
meta	Method
-	Method
batch	Method
and	O
ResNet	Method
-	Method
12	Method
(	O
pre	Method
)	O
,	O
achieves	O
the	O
best	O
performances	O
for	O
all	O
few	O
-	Material
shot	Material
settings	O
on	O
both	O
datasets	O
,	O
see	O
Table	O
[	O
reference	O
]	O
and	O
Table	O
[	O
reference	O
]	O
.	O
We	O
can	O
conclude	O
that	O
our	O
large	Method
-	Method
scale	Method
network	Method
training	Method
on	O
deep	Method
CNN	Method
significantly	O
boost	O
the	O
few	O
-	Material
shot	Material
learning	O
performance	O
.	O
This	O
is	O
an	O
important	O
gain	O
brought	O
by	O
the	O
transfer	Method
learning	Method
idea	Method
in	O
our	O
MTL	Method
approach	O
.	O
It	O
is	O
interesting	O
to	O
note	O
that	O
this	O
gain	O
on	O
FC100	O
is	O
not	O
as	O
large	O
as	O
for	O
miniImageNet	Material
:	O
only	O
,	O
and	O
.	O
The	O
possible	O
reason	O
is	O
that	O
FC100	Task
tasks	Task
for	O
meta	Task
-	Task
train	Task
and	O
meta	O
-	O
test	Task
are	O
clearly	O
split	O
according	O
to	O
super	O
-	O
classes	O
.	O
The	O
data	O
domain	O
gap	O
is	O
larger	O
than	O
that	O
for	O
miniImageNet	Material
,	O
which	O
makes	O
transfer	Task
more	O
difficult	O
.	O
HT	Method
meta	Method
-	Method
batch	Method
and	O
ResNet	Method
-	Method
12	Method
(	O
pre	Method
)	O
in	O
our	O
approach	O
can	O
be	O
generalized	O
to	O
other	O
meta	Method
-	Method
learning	Method
models	Method
.	O
MAML	O
4CONV	Method
with	O
HT	Method
meta	Method
-	Method
batch	Method
gains	O
averagely	O
on	O
two	O
datasets	O
.	O
When	O
changing	O
4CONV	Method
by	O
deep	O
ResNet	Method
-	Method
12	Method
(	O
pre	Method
)	O
it	O
achieves	O
significant	O
improvements	O
,	O
e.g.	O
and	O
on	O
miniImageNet	Material
.	O
Compared	O
to	O
MAML	Method
variants	Method
,	O
our	O
MTL	Method
results	O
are	O
consistently	O
higher	O
,	O
e.g.	O
on	O
FC100	O
.	O
People	O
may	O
argue	O
that	O
MAML	Method
fine	Method
-	Method
tuning	Method
(	O
FT	Task
)	O
all	O
network	Method
parameters	Method
is	O
likely	O
to	O
overfit	O
to	O
few	O
-	Material
shot	Material
data	O
.	O
In	O
the	O
middle	O
block	O
of	O
Table	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
ablation	O
study	O
of	O
freezing	O
low	O
-	O
level	O
pre	Method
-	O
trained	O
layers	O
and	O
meta	O
-	O
learn	O
only	O
the	O
high	O
-	O
level	O
layers	O
(	O
e.g.	O
the	O
-	O
th	O
residual	O
block	O
of	O
ResNet	Method
-	Method
12	Method
)	O
by	O
the	O
FT	Task
operations	O
of	O
MAML	Method
.	O
These	O
all	O
yield	O
inferior	O
performances	O
than	O
using	O
our	O
SS	Task
.	O
An	O
additional	O
observation	O
is	O
that	O
SS	Task
*	O
performs	O
consistently	O
better	O
than	O
FT	Task
*	O
.	O
Speed	Metric
of	Metric
convergence	Metric
of	O
MTL	Method
.	O
MAML	Method
used	O
tasks	O
to	O
achieve	O
the	O
best	O
performance	O
on	O
miniImageNet	Material
.	O
Impressively	O
,	O
our	O
MTL	Method
methods	O
used	O
only	O
tasks	O
,	O
see	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)(	O
b	O
)	O
(	O
note	O
that	O
each	O
iteration	O
contains	O
2	O
tasks	O
)	O
.	O
This	O
advantage	O
is	O
more	O
obvious	O
for	O
FC100	O
on	O
which	O
MTL	Method
methods	O
need	O
at	O
most	O
tasks	O
,	O
Figure	O
[	O
reference	O
]	O
(	O
c	O
)(	O
d	O
)(	O
e	O
)	O
.	O
We	O
attest	O
this	O
to	O
two	O
reasons	O
.	O
First	O
,	O
MTL	Method
starts	O
from	O
the	O
pre	Method
-	O
trained	O
ResNet	Method
-	Method
12	Method
.	O
And	O
second	O
,	O
SS	Task
(	O
in	O
MTL	Method
)	O
needs	O
to	O
learn	O
only	O
parameters	O
of	O
the	O
number	O
of	O
FT	Task
(	O
in	O
MAML	Method
)	O
when	O
using	O
ResNet	Method
-	Method
12	Method
.	O
Speed	Metric
of	Metric
convergence	Metric
of	O
HT	Method
meta	Method
-	Method
batch	Method
.	O
Figure	O
[	O
reference	O
]	O
shows	O
1	O
)	O
MTL	Method
with	O
HT	Method
meta	Method
-	Method
batch	Method
consistently	O
achieves	O
higher	O
performances	O
than	O
MTL	Method
with	O
the	O
conventional	O
meta	Method
-	Method
batch	Method
,	O
in	O
terms	O
of	O
the	O
recognition	O
accuracy	Metric
in	O
all	O
settings	O
;	O
and	O
2	O
)	O
it	O
is	O
impressive	O
that	O
MTL	Method
with	O
HT	Method
meta	Method
-	Method
batch	Method
achieves	O
top	O
performances	O
early	O
,	O
after	O
about	O
iterations	O
for	O
1	Material
-	Material
shot	Material
,	O
for	O
5	Material
-	Material
shot	Material
and	O
for	O
10	O
-	Material
shot	Material
,	O
on	O
the	O
more	O
challenging	O
dataset	O
–	O
FC100	O
.	O
section	O
:	O
Conclusions	O
In	O
this	O
paper	O
,	O
we	O
show	O
that	O
our	O
novel	O
MTL	Method
trained	O
with	O
HT	Method
meta	Method
-	Method
batch	Method
learning	Method
curriculum	Method
achieves	O
the	O
top	O
performance	O
for	O
tackling	O
few	O
-	Material
shot	Material
learning	O
problems	O
.	O
The	O
key	O
operations	O
of	O
MTL	Method
on	O
pre	Method
-	O
trained	O
DNN	Method
neurons	Method
proved	O
highly	O
efficient	O
for	O
adapting	O
learning	O
experience	O
to	O
the	O
unseen	Task
task	Task
.	O
The	O
superiority	O
was	O
particularly	O
achieved	O
in	O
the	O
extreme	O
1	O
-	Material
shot	Material
cases	O
on	O
two	O
challenging	O
benchmarks	O
–	O
miniImageNet	Material
and	O
FC100	O
.	O
In	O
terms	O
of	O
learning	Method
scheme	Method
,	O
HT	Method
meta	Method
-	Method
batch	Method
showed	O
consistently	O
good	O
performance	O
for	O
all	O
baselines	O
and	O
ablative	Method
models	Method
.	O
On	O
the	O
more	O
challenging	O
FC100	O
benchmark	O
,	O
it	O
showed	O
to	O
be	O
particularly	O
helpful	O
for	O
boosting	Metric
convergence	Metric
speed	Metric
.	O
This	O
design	O
is	O
independent	O
from	O
any	O
specific	O
model	O
and	O
could	O
be	O
generalized	O
well	O
whenever	O
the	O
hardness	O
of	O
task	O
is	O
easy	O
to	O
evaluate	O
in	O
online	O
iterations	O
.	O
section	O
:	O
Acknowledgments	O
This	O
research	O
is	O
part	O
of	O
NExT	O
research	O
which	O
is	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
,	O
Prime	O
Minister	O
’s	O
Office	O
,	O
Singapore	O
under	O
its	O
IRC@SG	O
Funding	O
Initiative	O
.	O
It	O
is	O
also	O
partially	O
supported	O
by	O
German	O
Research	O
Foundation	O
(	O
DFG	O
CRC	O
1223	O
)	O
,	O
and	O
National	O
Natural	O
Science	O
Foundation	O
of	O
China	O
(	O
61772359	O
)	O
.	O
bibliography	O
:	O
References	O
Supplementary	O
materials	O
These	O
materials	O
include	O
the	O
details	O
of	O
network	Method
architecture	Method
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
implementation	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
FC100	O
dataset	O
splits	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
standard	O
variance	Method
analysis	Method
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
additional	O
ablation	O
results	O
(	O
§	O
[	O
reference	O
]	O
)	O
,	O
and	O
some	O
interpretation	O
of	O
our	O
meta	Method
-	Method
learned	Method
model	Method
(	O
§	O
[	O
reference	O
]	O
)	O
.	O
In	O
addition	O
,	O
our	O
open	O
-	O
source	O
code	O
is	O
on	O
GitHub	O
.	O
section	O
:	O
Network	Method
architectures	Method
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
present	O
the	O
4CONV	Method
architecture	O
for	O
feature	Task
extractor	Task
,	O
as	O
illustrated	O
in	O
Section	O
5.1	O
“	O
Network	Method
architecture	Method
”	O
of	O
the	O
main	O
paper	O
.	O
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
present	O
the	O
other	O
architecture	O
–	O
ResNet	Method
-	Method
12	Method
.	O
Figure	O
[	O
reference	O
]	O
(	O
a	O
)	O
shows	O
the	O
details	O
of	O
a	O
single	O
residual	O
block	O
and	O
Figure	O
[	O
reference	O
]	O
(	O
b	O
)	O
shows	O
the	O
whole	O
network	O
consisting	O
of	O
four	O
residual	O
blocks	O
and	O
a	O
mean	Method
-	Method
pooling	Method
layer	Method
.	O
The	O
input	O
of	O
is	O
the	O
-	O
channel	O
RGB	O
image	O
,	O
and	O
the	O
output	O
is	O
the	O
-	O
dimensional	O
feature	O
vector	O
.	O
is	O
set	O
for	O
all	O
leakyReLU	Method
activation	Method
functions	Method
in	O
ResNet	Method
-	Method
12	Method
.	O
section	O
:	O
Implementation	O
details	O
For	O
the	O
phase	O
of	O
DNN	Task
training	Task
on	O
large	O
-	O
scale	O
data	O
,	O
the	O
model	O
is	O
trained	O
by	O
Adam	Method
optimizer	Method
.	O
Its	O
learning	Metric
rate	Metric
is	O
initialized	O
as	O
,	O
and	O
decays	O
to	O
its	O
half	O
every	O
iterations	O
until	O
it	O
is	O
lower	O
that	O
.	O
We	O
set	O
the	O
keep	O
probability	O
of	O
the	O
dropout	O
as	O
and	O
batch	O
-	O
size	O
as	O
.	O
The	O
pre	Method
-	O
training	O
stops	O
after	O
iterations	O
.	O
Note	O
that	O
for	O
the	O
hyperparameter	Task
selection	Task
,	O
we	O
randomly	O
choose	O
samples	O
each	O
class	O
as	O
the	O
training	O
set	O
,	O
and	O
the	O
rest	O
as	O
validation	O
.	O
After	O
the	O
grid	O
search	O
of	O
hyperparameters	O
,	O
we	O
fix	O
them	O
and	O
mix	O
up	O
all	O
samples	O
(	O
classes	O
,	O
samples	O
each	O
class	O
)	O
,	O
in	O
order	O
to	O
do	O
the	O
final	O
pre	Method
-	O
training	O
.	O
Besides	O
,	O
these	O
pre	Method
-	O
training	O
samples	O
are	O
augmented	O
with	O
horizontal	O
flip	O
.	O
For	O
the	O
meta	Task
-	Task
train	Task
phase	Task
,	O
we	O
sample	O
5	Task
-	Task
class	Task
,	O
1	Material
-	Material
shot	Material
(	O
5	Material
-	Material
shot	Material
or	O
10	O
-	Material
shot	Material
)	O
episodes	O
to	O
contain	O
(	O
or	O
)	O
sample	O
(	O
s	O
)	O
for	O
episode	Task
training	Task
,	O
and	O
samples	O
for	O
episode	O
test	Task
uniformly	O
,	O
following	O
the	O
setting	O
of	O
MAML	Method
.	O
The	O
base	Method
-	Method
learner	Method
is	O
optimized	O
by	O
batch	Method
gradient	Method
descent	Method
with	O
the	O
learning	Metric
rate	Metric
of	O
.	O
It	O
gets	O
updated	O
with	O
and	O
epochs	O
respectively	O
for	O
1	Material
-	Material
shot	Material
and	O
5	O
-	Material
shot	Material
tasks	O
on	O
the	O
miniImageNet	Material
dataset	Material
,	O
and	O
epochs	O
for	O
all	O
tasks	O
on	O
the	O
FC100	O
dataset	O
.	O
The	O
meta	Method
-	Method
learner	Method
,	O
,	O
the	O
parameters	O
of	O
the	O
SS	Task
operations	O
,	O
is	O
optimized	O
by	O
Adam	Method
optimizer	Method
.	O
Its	O
learning	Metric
rate	Metric
is	O
initialized	O
as	O
,	O
and	O
decays	O
to	O
the	O
half	O
every	O
iterations	O
until	O
.	O
The	O
size	O
of	O
meta	O
-	O
batch	O
is	O
set	O
to	O
(	O
tasks	O
)	O
due	O
to	O
the	O
memory	O
limit	O
.	O
Using	O
our	O
HT	Method
meta	Method
-	Method
batch	Method
strategy	Method
,	O
hard	O
tasks	O
are	O
sampled	O
every	O
time	O
after	O
running	O
meta	O
-	O
batches	O
,	O
,	O
the	O
failure	O
classes	O
used	O
for	O
sampling	O
hard	O
tasks	O
are	O
from	O
tasks	O
.	O
The	O
number	O
of	O
hard	Method
task	Method
is	O
selected	O
for	O
different	O
settings	O
by	O
validation	Task
:	O
and	O
hard	O
tasks	O
respectively	O
for	O
the	O
1	Material
-	Material
shot	Material
and	O
5	Material
-	Material
shot	Material
experiments	O
on	O
the	O
miniImageNet	Material
dataset	Material
;	O
and	O
respectively	O
,	O
and	O
hard	O
tasks	O
for	O
the	O
1	Material
-	Material
shot	Material
,	O
5	Material
-	Material
shot	Material
and	O
10	O
-	Material
shot	Material
experiments	O
on	O
the	O
FC100	O
dataset	O
.	O
For	O
the	O
meta	O
-	O
test	Task
phase	O
,	O
we	O
sample	O
5	Task
-	Task
class	Task
,	O
1	Material
-	Material
shot	Material
(	O
5	Material
-	Material
shot	Material
or	O
10	O
-	Material
shot	Material
)	O
episodes	O
and	O
each	O
episode	O
contains	O
(	O
or	O
)	O
sample	O
(	O
s	O
)	O
for	O
both	O
episode	Task
train	Task
and	O
episode	O
test	Task
.	O
On	O
each	O
dataset	O
,	O
we	O
sample	O
meta	O
-	O
test	Task
tasks	O
.	O
All	O
these	O
settings	O
are	O
exactly	O
the	O
same	O
as	O
MAML	Method
.	O
section	O
:	O
Super	O
-	O
class	O
splits	O
on	O
FC100	Method
In	O
this	O
section	O
,	O
we	O
show	O
the	O
details	O
of	O
the	O
FC100	O
splits	O
according	O
to	O
the	O
super	O
-	O
class	O
labels	O
,	O
same	O
with	O
TADAM	O
.	O
Training	O
split	O
super	O
-	O
class	O
indexes	O
:	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
9	O
,	O
10	O
,	O
15	O
,	O
17	O
,	O
18	O
,	O
19	O
;	O
and	O
corresponding	O
labels	O
:	O
fish	O
,	O
flowers	O
,	O
food_containers	O
,	O
fruit_and_vegetables	O
,	O
household_electrical_devices	O
,	O
household_furniture	O
,	O
large_man	O
-	O
made_outdoor_things	O
,	O
large_natural_outdoor_scenes	O
,	O
reptiles	O
,	O
trees	O
,	O
vehicles_1	O
,	O
vehicles_2	O
.	O
Validation	O
split	O
super	O
-	O
class	O
indexes	O
:	O
8	O
,	O
11	O
,	O
13	O
,	O
16	O
;	O
and	O
corresponding	O
labels	O
:	O
large_carnivores	O
,	O
large_omnivores_and_herbivores	O
,	O
non	O
-	O
insect_invertebrates	O
,	O
small_mammals	O
.	O
Test	O
split	O
super	O
-	O
class	O
indexes	O
:	O
0	O
,	O
7	O
,	O
12	O
,	O
14	O
;	O
and	O
corresponding	O
labels	O
:	O
aquatic_mammals	O
,	O
insects	O
,	O
medium_mammals	O
,	O
people	O
.	O
An	O
episode	O
(	O
task	O
)	O
is	O
independently	O
sampled	O
from	O
a	O
corresponding	O
split	O
,	O
a	O
meta	O
-	O
train	O
episode	O
contains	O
classes	O
that	O
can	O
only	O
be	O
belonging	O
to	O
the	O
super	O
-	O
classes	O
in	O
the	O
training	O
split	O
.	O
Therefore	O
,	O
there	O
is	O
no	O
fine	O
-	O
grained	O
information	O
overlap	O
between	O
meta	O
-	O
train	O
and	O
meta	O
-	O
test	Task
tasks	O
.	O
section	O
:	O
Standard	O
variance	Method
analysis	Method
The	O
final	O
accuracy	Metric
results	O
reported	O
in	O
our	O
main	O
paper	O
are	O
the	O
mean	O
values	O
and	O
standard	O
variances	O
of	O
the	O
results	O
of	O
meta	O
-	O
test	Task
tasks	O
.	O
The	O
standard	Metric
variance	Metric
is	O
affected	O
by	O
the	O
number	O
of	O
episode	O
test	Task
samples	O
.	O
As	O
introduced	O
in	O
§	O
[	O
reference	O
]	O
,	O
we	O
use	O
the	O
same	O
setting	O
as	O
MAML	Method
which	O
used	O
a	O
smaller	O
number	O
of	O
samples	O
for	O
episode	O
test	Task
(	O
sample	O
for	O
1	O
-	Material
shot	Material
episode	O
test	Task
and	O
samples	O
for	O
5	Material
-	Material
shot	Material
)	O
,	O
making	O
the	O
result	O
variance	O
higher	O
.	O
Other	O
works	O
that	O
used	O
more	O
samples	O
for	O
episode	O
test	Task
got	O
lower	O
variances	O
,	O
,	O
TADAM	O
used	O
samples	O
and	O
its	O
variances	O
are	O
about	O
and	O
of	O
MAML	Method
’s	Method
respectively	O
for	O
miniImageNet	Material
1	Material
-	Material
shot	Material
and	O
5	Material
-	Material
shot	Material
.	O
In	O
order	O
to	O
have	O
a	O
fair	O
comparison	O
with	O
TADAM	O
in	O
terms	O
of	O
this	O
issue	O
,	O
we	O
supplement	O
the	O
experiments	O
using	O
episode	O
test	Task
samples	O
at	O
the	O
meta	O
-	O
test	Task
.	O
We	O
get	O
the	O
new	O
confidence	O
intervals	O
(	O
using	O
our	O
method	O
:	O
MTL	Method
w	O
/	O
o	O
HT	Method
meta	Method
-	Method
batch	Method
)	O
as	O
(	O
for	O
TADAM	O
)	O
and	O
(	O
for	O
TADAM	O
)	O
respectively	O
for	O
1	Material
-	Material
shot	Material
and	O
5	Material
-	Material
shot	Material
on	O
the	O
miniImageNet	Material
dataset	Material
,	O
and	O
(	O
for	O
TADAM	O
)	O
,	O
(	O
for	O
TADAM	O
)	O
and	O
(	O
for	O
TADAM	O
)	O
respectively	O
for	O
1	Material
-	Material
shot	Material
,	O
5	Material
-	Material
shot	Material
and	O
10	O
-	Material
shot	Material
on	O
the	O
FC100	O
dataset	O
.	O
section	O
:	O
Additional	O
ablation	O
study	O
We	O
supplement	O
the	O
results	O
in	O
Table	O
[	O
reference	O
]	O
,	O
for	O
the	O
comparisons	O
mentioned	O
in	O
Section	O
5.1	O
of	O
main	O
paper	O
.	O
Red	O
numbers	O
on	O
the	O
bottom	O
row	O
are	O
copied	O
from	O
the	O
main	O
paper	O
(	O
corresponding	O
to	O
the	O
MTL	Method
setting	O
:	O
SS	Task
,	O
meta	O
-	O
batch	O
)	O
and	O
shown	O
here	O
for	O
the	O
convenience	O
of	O
comparison	O
.	O
To	O
get	O
the	O
first	O
row	O
,	O
we	O
train	O
4CONV	Method
net	O
by	O
large	O
-	O
scale	O
data	O
(	O
same	O
to	O
the	O
pre	Method
-	O
training	O
of	O
ResNet	Method
-	Method
12	Method
)	O
and	O
get	O
inferior	O
results	O
,	O
as	O
we	O
declared	O
in	O
the	O
main	O
paper	O
.	O
Results	O
on	O
the	O
second	O
and	O
third	O
rows	O
show	O
the	O
performance	O
drop	O
when	O
changing	O
the	O
single	O
FC	O
layer	O
to	O
multiple	O
layers	O
,	O
FC	O
layers	O
and	O
FC	Method
layers	Method
.	O
Results	O
on	O
the	O
fourth	O
row	O
show	O
the	O
performance	O
drop	O
when	O
updating	O
both	O
and	O
for	O
the	O
base	Task
-	Task
learning	Task
.	O
The	O
reason	O
is	O
that	O
has	O
too	O
many	O
parameters	O
to	O
update	O
with	O
too	O
little	O
data	O
.	O
section	O
:	O
Interpretation	O
of	O
meta	O
-	O
learned	O
SS	Task
In	O
Figure	O
[	O
reference	O
]	O
,	O
we	O
show	O
the	O
statistic	O
histograms	O
of	O
learned	O
SS	Task
parameters	O
,	O
taking	O
miniImageNet	Material
1	Material
-	Material
shot	Material
as	O
an	O
example	O
setting	O
.	O
Scaling	O
parameters	O
are	O
initialized	O
as	O
1	O
and	O
shifting	O
parameters	O
as	O
0	O
.	O
After	O
meta	Task
-	Task
train	Task
,	O
we	O
observe	O
that	O
these	O
statistics	O
are	O
close	O
to	O
Gaussian	O
distributions	O
respectively	O
with	O
(	O
,	O
)	O
and	O
(	O
,	O
)	O
as	O
(	O
mean	O
,	O
variance	O
)	O
values	O
,	O
which	O
shows	O
that	O
the	O
uniform	Method
initialization	Method
has	O
been	O
changed	O
to	O
Gaussian	Method
distribution	Method
through	O
few	O
-	Material
shot	Material
learning	O
.	O
Possible	O
interpretations	O
are	O
in	O
three	O
-	O
fold	O
:	O
1	O
)	O
majority	O
patterns	O
trained	O
by	O
a	O
large	O
number	O
of	O
few	O
-	Material
shot	Material
tasks	O
are	O
close	O
to	O
the	O
ones	O
trained	O
by	O
large	O
-	O
scale	O
data	O
;	O
2	O
)	O
tail	O
patterns	O
with	O
clear	O
scale	O
and	O
shift	O
values	O
are	O
the	O
ones	O
really	O
contributing	O
to	O
adapting	O
the	O
model	O
to	O
few	O
-	Material
shot	Material
tasks	O
;	O
3	O
)	O
tail	O
patterns	O
are	O
of	O
small	O
quantity	O
,	O
enabling	O
the	O
fast	O
learning	Task
convergence	Task
.	O
