Failure modes and effects analysis (FMEA) is a methodology for prioritizing actions to mitigate the effects of failures in products and processes.
Although originally used by product designers, FMEA is currently more widely used in industry in Six Sigma quality improvement efforts.
Two prominent criticisms of the traditional application of FMEA are that the risk priority number (RPN) used to rank failure modes is an invalid measure according to measurement theory, and that the RPN does not weight the three decision criteria used in FMEA.
Various methods have been proposed to mitigate these concerns, including many using fuzzy logic.
We develop a new ranking method in this article using a data-elicitation technique.
Furthermore, we develop an efficient means of eliciting data to reduce the effort associated with the new method.
Subsequently, we conduct an experimental study to evaluate that proposed method against the traditional method using RPN and against an approach using fuzzy logic.

A new class of G-η-monotone mappings is introduced and studied in Hilbert space.
By using the properties of G-η-monotone mappings, the solvability of a class of nonlinear variational inclusions using the generalized resolvent operators technique is given.

The clearing of interbank payments is a process which usually involves an immense amount of money and a large number of participants.
It can be modeled as a discrete optimization problem, the Bank Clearing Problem (BCP), where the clearing volume is the objective function and the deposits of the participants are the limiting resources.
In this paper we discuss the BCP as it occurs in Germany's largest interbank payment system.
We present several simple but surprisingly efficient heuristic algorithms that are analyzed by a simulation approach.

Nowadays, research in text mining has become one of the widespread fields in analyzing natural language documents.
The present study demonstrates a comprehensive overview about text mining and its current research status.
As indicated in the literature, there is a limitation in addressing Information Extraction from research articles using Data Mining techniques.
The synergy between them helps to discover different interesting text patterns in the retrieved articles.
In our study, we collected, and textually analyzed through various text mining techniques, three hundred refereed journal articles in the field of mobile learning from six scientific databases, namely: Springer, Wiley, Science Direct, SAGE, IEEE, and Cambridge.
The selection of the collected articles was based on the criteria that all these articles should incorporate mobile learning as the main component in the higher educational context.
Experimental results indicated that Springer database represents the main source for research articles in the field of mobile education for the medical domain.
Moreover, results where the similarity among topics could not be detected were due to either their interrelations or ambiguity in their meaning.
Furthermore, findings showed that there was a booming increase in the number of published articles during the years 2015 through 2016.
In addition, other implications and future perspectives are presented in the study.

We examine decision analysis' central "decomposition principle" in the context of work-time estimates of software writers.
Two experiments examined the abilities of advanced programming students to estimate how long they would take to complete specific software projects.
They estimated their own work times both for entire projects and for their constituent subtasks.
Estimates showed varying degrees of overoptimism and overpessimism but all were much too tight, with almost half of actual outcomes falling in the 1% tails of estimated distributions.
This overtightness was unaffected by task decomposition, question wording, question order, or training in estimation.
It was, however, significantly reduced by a procedure aimed at inducing generous upper and lower plausible limits.
An underlying model of incomplete search is used to connect these findings to existing themes in cognition and judgment research, as well as to practical application.
The findings suggest that the best level of decomposition at which to elicit work-time estimates may depend on task, judge, and elicitation method.

In standard event-related potential (ERP) recordings, activity at the reference site affects measurements at all the “active” electrode sites.
Analyses of ERP data would be improved if reference site activity could be separated out.
Apart from current source density methods, which can miss deep or distributed events, the major alternative to an “indifferent” electrode reference is one that combines all active electrodes, subtracting out the activity common to all of them.
The average reference has merits (such as an insensitivity to scalp currents near any single electrode) and limitations (such as a dependence on the number and locations of all the electrodes in the average).
This review compares the effects of different references on the scalp topography of the auditory N1 recorded with an array of 128 electrodes.
Furthermore, it shows how assumptions associated with each reference affect inferential methods such as analysis of variance and correlation.
Finally, it seeks to evaluate the efficacy of the average reference in estimating the true zero potential line.

This paper details the concept of using an exergy-based method as a thermal design methodology tool for integrated aircraft thermal systems.
An exergy-based approach was applied to the design of an environmental control system (ECS) of an advanced aircraft.
Concurrently, a traditional energybased approach was applied to the same system.
Simplified analytical models of the ECS were developed for each method and compared to determine the validity of using the exergy approach to facilitate the design process in optimizing the overall system for a minimum gross takeoff weight (GTW).
The study identified some roadblocks to assessing the value of using an exergy-based approach.
Energy and exergy methods seek answers to somewhat different questions making direct comparisons awkward.
Also, high entropy generating devices can dominate the design objective of the exergy approach.
Nonetheless, exergy methods do provide information to aid design providing a ready estimate for efficiency on a component and system basis.
The results from the two analyses did provide similar while not exact solutions.
While the paper will illustrate the methodology and its implementation, further progress is necessary to validate the hypothesis that exergybased methods are advantageous for the design of integrated systems.
INTRODUCTION In thermal systems, decisions are based in part on the thermodynamic behavior of the component or system.
Traditional design procedures use an energy-based approach for decisionmaking.
Essentially this is a thermodynamic first law analysis.
Exergy-based methods deal with the simultaneous application of the thermodynamic principles of the first law and second law to component or system design.
Exergy analysis yields an optimal solution based on an objective of entropy generation minimization.
Energy-based methods are built on the fundamental concept that energy flows into and out of a system through heat transfer, work, and mass flow.
That energy must be conserved is the basic premise of the first law.
On the other hand, exergy represents the ability to do work or, put in a different way, the ability to bring about a desired change.
Exergy is not conserved and, in fact, is partially or totally destroyed.
The amount of exergy destroyed is proportional to the amount of entropy generated.
It is the destroyed exergy that brings about the component or system inefficiency.
Hence, a design process based on minimizing entropy generation reduces exergy destruction to improve efficiency.
Exergy-based methods applied to the design of aircraft integrated systems have been discussed as being advantageous to traditional methods.
The approach has been applied to components and to land-based power plant design [e.g.
1-3].
Tipton et al.
[4] first applied exergy methods to the environmental control system of an aircraft and this paper follows up on that study.
By looking at the irreversibilities associated with the entropy generation of each component, an attempt is made to meet design objectives that make best use of available energy.
An attractive feature of the method comes from entropy as a property.
Irreversibility within a system is related to the sum of the entropy generated by each component in the system.
Constraints, such as size or weight, are readily imposed.
However, the demonstration of a complete optimized design of an aircraft system using exergy methods has not been documented.
The original motivation for this work was prompted by a study to evaluate the aircraft-level impact of using spray cooling technology in an avionics chassis, which was part of the environmental control system (ECS) of an advanced aircraft [4].
In particular, the aircraft level impacts of this new cooling technology were to be evaluated.
While evaluation of this technology is not the focus of this paper, the avionics chassis remains within the model and its impact will be studied.
More recently, participants from an Air Force sponsored workshop [5] identified several unresolved questions of importance in moving towards an innovative, fully integrated (c)2000 American Institute of Aeronautics & Astronautics or Published with Permission of Author(s) and/or Author(s)' Sponsoring Organization.
design methodology for ah vehicles.
Of these, an unambiguous demonstration of exergy-based methodology advantages at the system and aircraft levels was a priority.
This paper details the concept of using an exergy-based method as a thermal design methodology tool for integrated aircraft thermal systems.

It is common in technological development to identify and explore several approaches to a particular objective so that the best approach may be chosen.
The outcome of any approach is uncertain; hence, it is difficult to choose the best one at an early date.
To deal with this uncertainty, two or more approaches to the objective may be continued in parallel until a clear choice between them can be made, i.e., a parallel strategy.
Such a strategy can provide better information for a decision, maintain options, or hedge against the occurrence of an unsatisfactory outcome.
This paper addresses the manager's problem of deciding when to use or continue a parallel strategy.
The principal focus is based on studies of 14 projects and illustrates the application, in one setting, of a general model appropriate to the structure of the decision as it is widely faced in practice.
It discusses the information requirements for a sound choice between parallel and sequential strategies and the consequences of choosing a strategy on the basis of incomplete information.

AbstractInformation about the design and construction of buildings can be structured in a particular way.
This is especially correct given the increasing complexity of building product models and the emergence of building information models with project documents linked to them.
In addition, engineers usually have distinct information needs.
Research shows that engineers working with building information models place particular importance on the understanding of retrieved content before using it or applying it and that exploration of context is essential for this understanding.
Both these factors (the nature of engineering content and the information needs of engineers) make general information retrieval techniques for computing relevance and visualizing search results less applicable in civil engineering information retrieval systems.
This paper argues that granularity is a fundamental concept that needs to be considered when measuring relevance and visualizing search results in information retrieval sys...

Web Service composition is becoming the most promising way for business-to-business systems integration.
However, current mechanisms for service composition entail a trade-off on multiple and complex factors.
Thereby, existing solutions based on business Web Services, semantic Web Services, or the recent RESTful services, lack of a standardized adoption.
This paper gives an overview of current approaches according to a set of features.
Moreover, related core problems and future directions of service composition mechanisms are pointed out.

This paper reports the extension of the key words method for the comparison of corpora.
Using automatic tagging software that assigns part-of-speech and semantic field (domain) tags, a method is described which permits the extraction of key domains by applying the keyness calculation to tag frequency lists.
The combination of the key words and key domains methods is shown to allow macroscopic analysis (the study of the characteristics of whole texts or varieties of language) to inform the microscopic level (focussing on the use of a particular linguistic feature) and thereby suggesting those linguistic features which should be investigated further.
The resulting 'data-driven' approach presented here combines elements of both the 'corpus-based' and 'corpus-driven' paradigms in corpus linguistics.
A web-based tool, Wmatrix, implementing the proposed method is applied in a case study: the comparison of UK 2001 general election manifestos of the Labour and Liberal Democratic parties.

A few representations have been used for capturing design rationale.
To understand their scope and adequacy, we need to know how to evaluate them.
In this article, we develop a framework for evaluating the expressive adequacy of design rationale representations.
This framework is built by progressively differentiating the elements of design rationale that, when made explicit, support an increasing number of the design tasks.
Using this framework, we present and assess DRL (Decision Representation Language), a language for representing rationales that we believe is the most expressive of the existing representations.
We also use the framework to assess the expressiveness of other design rationale representations and compare them to DRL.
We conclude by pointing out the need for articulating other dimensions along which to evaluate design rationale representations.

ABSTRACT In this paper, recent research efforts at the University of Illinois at Urbana-Champaign (UIUC) on the development of machining process models are summarized under a unified mechanistic modeling framework.
The fundamental elements of the basic cutting force model are described first, which include the computation of chip load and chip flow, force transformation relations, workpiece-cutter intersection algorithm, and the model calibration procedure.
Some advanced model enhancements are then presented, such as the modeling of process faults, modeling of chip-control tools, and the incorporation of ploughing and tool wear effects.
The integration of the process model with the machining system dynamics is discussed next.
Finally, representative results are shown to validate the mechanistic modeling approach.

The authors have solved the all pairs shortest distances (APSD) problem for graphs with integer edge lengths.
Our algorithm is subcubic for edge lengths of small (?M) absolute value.
In this paper we show how to transform these algorithms to solve the all pairs shortest paths (APSP), in the same time complexity, up to a polylogarithmic factor.
Forn=|V| the number of vertices,Mthe bound on edge length, and?the exponent of matrix multiplication, we get the following results: 1.
A directed nonnegative APSP(n, M) algorithm which runs inO(T(n, M)) time, where T(n, m)=\big\{\begin{align}M^{\omega -1)/2} n^{3+\omega )/2}, & 1\le M\le n^{3-\omega )/(\omega +1)}\\ Mn^{5\omega -3)/(\omega +1)}, & n^{(3-\omega )/(\omega +1)}\le M\le n^2(3-\omega )/(\omega +1)}.\end{align} 2.
An undirected APSP(n, M) algorithm which runs inO(M(?+1)/2n?log(Mn)) time.
3.
A general APSP(n, M) algorithm which runs inO((Mn)(3+?
)/2).

This paper proposes an enhanced interactive satisfying optimization method based on goal programming for the multiple objective optimization problem with preemptive priorities.
Based on the previous method, the approach presented makes the higher priority achieve the higher satisfying degree.
For three fuzzy relations of the objective functions, the corresponding optimization models are proposed.
Not only can satisfying results for all the objectives be acquired, but the preemptive priority requirement can also be simultaneously actualized.
The balance between optimization and priorities is realized.
We demonstrate the power of this proposed method by illustrative examples.

A flexible Wireless Sensor Network platform has been developed and deployed at the Instituto Superior Tecnico/Technical University of Lisbon (IST-TUL).
This test-bed integrates multiple projects into a single network, creating an expandable platform that facilitates future developments.
To achieve this flexibility, a dedicated framework was developed, enabling fine-grained parameter control and application programmability through a centralised configuration panel.
On top of this platform, four applications have been developed and currently coexist within the network, illustrating the new platform's capabilities.
The article discusses the test-bed architecture and deployment challenges, as well as the environmental interaction and vibration monitoring applications.
An experimental evaluation of these applications' capacity limits and performance shows that in-network processing must be used with the vibration monitoring application to avoid congestion collapse.
Furthermore, a minimum amount of time is needed to complete processing tasks with the Environmental Interaction Application (approximately 200 ms in our test topology).

A performance oriented two-loop control approach is proposed for a class of multiple-input-multiple-output (MIMO) systems with input saturation, state constraints, matched parametric uncertainties and input disturbances.
In the inner loop, a constrained adaptive robust control (ARC) law is synthesized to achieve the required robust tracking performances with respect to on-line replanned trajectory in the presence of input saturation and various types of matched uncertainties.
In the outer loop, a replanned trajectory is generated by solving a constrained optimization algorithm online to minimize the converging time of the overall system response to the desired trajectory while not violating various constraints.
Interaction of the two loops is explicitly characterized by a set of inequalities that the design variables of each loop have to satisfy.
It is theoretically shown that the resulting closed-loop system can track feasible desired trajectories with a guaranteed converging time and steady-state tracking accuracy without violating the state constraints.
Since the system in study is most appropriate to describe the dynamics of the robotic systems, the control of a two-axis planar robotic manipulator is used as an application example.
Comparative simulation results demonstrate the advantage of the proposed approach over the traditional approaches in practical applications.

In automated electroplating lines, computer-controlled hoists are used to transfer parts from a processing resource to another one.
Products are mounted into carriers and immersed sequentially in a series of tanks following a given sequence.
The aim of this study is to propose an algorithm to solve the two-hoist cyclic scheduling problem.
This problem consists in finding a repetitive sequence of hoists' moves, while avoiding collision between the hoists which share a common track.
The objective is to minimize the period of this repetitive cycle.
First, a set of sequences is generated.
After that, for each sequence, an algorithm is applied to assign moves to hoists.
Finally, a mixed integer linear programming model is formulated by taking into account the two sequences of hoists moves.
Then it is solved to give the starting times of moves.

Recently, vector quantization has become noted as a highly efficient coding method of image and voice data.
So far, many of the highly efficient coding problems, or service coding problems, have been studied separately from channel coding problems.
This paper reconsiders vector quantization jointly optimizing source coding and channel coding, and proposes a new vector quantizer for noisy channels.
Vector quantizers for binary symmetric channels are designed for memoryless Gaussian source, Gauss-Markov source and the real images, and are compared with the conventional vector quantizer which does not take account of channel errors.
As a result, it is shown that the performance of the proposed vector quantizer is improved without adding the redundancy for error correction, and the improvement of the performance of the proposed vector quantizer for noisy channels over the conventional vector quantizer becomes significant for highly correlated sources and longer block length.

Abstract We focus on the computational complexity of regular simple path queries (RSPQs).
We consider the following problem RSPQ(L) for a regular language L: given an edge-labeled digraph G and two nodes x and y, is there a simple path from x to y that forms a word belonging to L?
We fully characterize the frontier between tractability and intractability for RSPQ ( L ) .
More precisely, we prove RSPQ ( L ) is either , -complete or -complete depending on the language L. We also provide a simple characterization of the tractable fragment in terms of regular expressions.
Finally, we also discuss the complexity of deciding whether a language L belongs to the fragment above.
We consider several alternative representations of L: DFAs, NFAs or regular expressions, and prove that this problem is -complete for the first representation and -complete for the other two.

The Grasping Affordance Model (GAM) introduced here provides a computational account of perceptual processes enabling one to identify grasping action possibilities from visual scenes.
GAM identifies the core of affordance perception with visuo-motor transformations enabling one to associate features of visually presented objects to a collection of hand grasping configurations.
This account is coherent with neuroscientific models of relevant visuo-motor functions and their localization in the monkey brain.
GAM differs from other computational models of biological grasping affordances in the way of modeling focus, functional account, and tested abilities.
Notably, by learning to associate object features to hand shapes, GAM generalizes its grasp identification abilities to a variety of previously unseen objects.
Even though GAM information processing does not involve semantic memory access and full-fledged object recognition, perceptions of (grasping) affordances are mediated there by substantive computational mechanisms which include learning of object parts, selective analysis of visual scenes, and guessing from experience.

The ever increasing need for I/O bandwidth will be met with ever larger arrays of disks.
These arrays require redundancy to protect against data loss.
This paper examines alternative choices for encodings, or codes, that reliably store information in disk arrays.
Codes are selected to maximize mean time to data loss or minimize disks containing redundant data, but are all constrained to minimize performance penalties associated with updating information or recovering from catastrophic disk failures.
We also codes that give highly reliable data storage with low redundant data overhead for arrays of 1000 information disks.

We apply an unsupervised machine learning ap- proach for Internet traffic identification and compare the results with that of a previously applied supervised machine learning approach.
Our unsupervised approach uses an Expectation Max- imization (EM) based clustering algorithm and the supervised approach uses the NaBayes classifier.
We find the unsu- pervised clustering technique has an accuracy up to 91% and outperform the supervised technique by up to 9%.
We also find that the unsupervised technique can be used to discover traffic from previously unknown applications and has the potential to become an excellent tool for exploring Internet traffic.
I.
INTRODUCTION Accurate classification of Internet traffic is important in many areas such as network design, network management, and network security.
One key challenge in this area is to adapt to the dynamic nature of Internet traffic.
Increasingly, new applications are being deployed on the Internet; some new applications such as peer-to-peer (P2P) file sharing and online gaming are becoming popular.
With the evolution of Internet traffic, both in terms of number and type of applications, however, traditional classification techniques such as those based on well-known port numbers or packet payload analysis are either no longer effective for all types of network traffic or are otherwise unable to deploy because of privacy or security concerns for the data.
A promising approach that has recently received some attention is traffic classification using machine learning tech- niques (1)-(4).
These approaches assume that the applications typically send data in some sort of pattern; these patterns can be used as a means of identification which would allow the connections to be classified by traffic class.
To find these patterns, flow statistics (such as mean packet size, flow length, and total number of packets) available using only TCP/IP headers are needed.
This allows the classification technique to avoid the use of port numbers and packet payload information in the classification process.
In this paper, we apply an unsupervised learning technique (EM clustering) for the Internet traffic classification problem and compare the results with that of a previously applied supervised machine learning approach.
The unsupervised clus- tering approach uses an Expectation Maximization (EM) algo- rithm (5) that is different in that it classifies unlabeled training data into groups called "clusters" based on similarity.
The NaBayes classifier has been previously shown to have high accuracy for Internet traffic classification (2).
In parallel work, Zander et al.
focus on using the EM clustering approach to build the classification model (4).
We complement their work by using the EM clustering approach to build a classifier and show that this classifier outperforms the Na¨ Bayes classifier in terms of classification accuracy.
We also analyze the time required to build the classification models for both approaches as a function of the size of the training data set.
We also explore the clusters found by the EM approach and find that the majority of the connections are in a subset of the total clusters.
The rest of this paper is organized as follows.
Section II presents related work.
In Section III, the background on the algorithms used in the Na¨ive Bayes and EM clustering approaches are covered.
In Section IV, we introduce the data sets used in our work and present our experimental results.
Section V discusses the advantages and disadvantages of the approaches.
Section VI presents our conclusions and describes future work avenues.

One of the major problems facing systems for Computer Aided Design (CAD), Architecture Engineering and Construction (AEC) and Geographic Information Systems (GIS) applications today is the lack of interoperability among the various systems.
When integrating software applications, substantial difficulties can arise in translating information from one application to the other.
In this paper, we focus on semantic difficulties that arise in software integration.
Applications may use different terminologies to describe the same domain.
Even when applications use the same terminology, they often associate different semantics with the terms.
This obstructs information exchange among applications.
To circumvent this obstacle, we need some way of explicitly specifying the semantics for each terminology in an unambiguous fashion.
Ontologies can provide such specification.
It will be the task of this paper to explain what ontologies are and how they can be used to facilitate interoperability between software systems used in computer aided design, architecture engineering and construction, and geographic information processing.

We propose a cost equation to measure the performance of a communications network handling several classes of disparate data.
Specifically, it accounts for how well the network is satisfying the time constraint requirements of each class of data.
To compute this measure for the token passing bus, we develop the necessary analysis to compute the queue statistics and delay distribution for the highest priority class.
Our analysis is based on the formulation of an M/G/l vacation model with a gated, limited server.
We provide results from a simulation study of this model to validation our analysis.

Many projections envision a future in which the Internet is populated with a vast number of Web-accessible XML files—a “World-Wide Database”.
Recently, there has been a great deal of research into XML query languages to enable the execution of database-style queries over these XML files.
However, merely being an XML query-processing engine does not render a system suitable for querying the Internet.
A truly useful system must provide mechanisms to (a) find the XML files that are relevant to a given query, and (b) deal with remote data sources that either provide unpredictable data access and transfer rates, or are infinite streams, or both.
The Niagara Internet Query System was designed from the bottom-up to provide these mechanisms.
It finds relevant XML documents by using a novel collaboration between the Niagara XML-QL query processor and the Niagara “text-in-context” XML search engine.
To handle infinite streams and data sources with unpredictable rates, it supports a “get partial” operation on blocking operators in order to produce partial query results, and inserts synchronization packets at critical points in the operator tree to guarantee the consistency of (partial) results.
The Niagara Internet Query System is public domain software that can be found at http://www-db.cs.wisc.edu/niagara/.
Category: Research.

The qualitative behaviour of iterates of a map can be very complicated.
One approach to these phenomena starts with the simplest situation, the case where the map has a fixed point.
Under parameter variations, the fixed point typically moves until a bifurcation value is reached and one of three possible more complex phenomena is encountered.
These are fold, flip and Neimark - Sacker bifurcations; they are called codimension one phenomena because they generically appear in problems with one free parameter.

Abstract Traumatic brain injury (TBI) is a leading cause of sustained impairment in military and civilian populations.
However, mild (and some moderate) TBI can be difficult to diagnose because the injuries are often not detectable on conventional MRI or CT.
Injured brain tissues in TBI patients generate abnormal low-frequency magnetic activity (ALFMA, peaked at 1–4 Hz) that can be measured and localized by magnetoencephalography (MEG).
We developed a new automated MEG low-frequency source imaging method and applied this method in 45 mild TBI (23 from combat-related blasts, and 22 from non-blast causes) and 10 moderate TBI patients (non-blast causes).
Seventeen of the patients with mild TBI from blasts had tertiary injuries resulting from the blast.
The results show our method detected abnormalities at the rates of 87% for the mild TBI group (blast-induced plus non-blast causes) and 100% for the moderate group.
Among the mild TBI patients, the rates of abnormalities were 96% and 77% for the blast and non-blast TBI groups, respectively.
The spatial characteristics of abnormal slow-wave generation measured by Z scores in the mild blast TBI group significantly correlated with those in non-blast mild TBI group.
Among 96 cortical regions, the likelihood of abnormal slow-wave generation was less in the mild TBI patients with blast than in the mild non-blast TBI patients, suggesting possible protective effects due to the military helmet and armor.
Finally, the number of cortical regions that generated abnormal slow-waves correlated significantly with the total post-concussive symptom scores in TBI patients.
This study provides a foundation for using MEG low-frequency source imaging to support the clinical diagnosis of TBI.

Precise segmentation and identification of thoracic vertebrae is important for many medical imaging applications whereas it remains challenging due to vertebra's complex shape and varied neighboring structures.
In this paper, a new method based on learned bonestructure edge detectors and a coarse-to-fine deformable surface model is proposed to segment and identify vertebrae in 3D CT thoracic images.
In the training stage, a discriminative classifier for object-specific edge detection is trained using steerable features and statistical shape models for 12 thoracic vertebrae are also learned.
In the run-time, we design a new coarse-to-fine, two-stage segmentation strategy: subregions of a vertebra first deforms together as a group; then vertebra mesh vertices in a smaller neighborhood move group-wise, to progressively drive the deformable model towards edge response maps by optimizing a probability cost function.
In this manner, the smoothness and topology of vertebra's shapes are guaranteed.
This algorithm performs successfully with reliable mean point-to-surface errors 0.95±0.91 mm on 40 volumes.
Consequently a vertebra identification scheme is also proposed via mean surface meshes matching.
We achieve a success rate of 73.1% using a single vertebra, and over 95% for 8 or more vertebra which is comparable or slightly better than state-of-the-art [1].

We classify the features that seem to be encountered in real vehicle routing problems.
Given these features, we try to indicate which features cause the greatest difficulty and which modeling approaches allow us to represent the greatest range of practical considerations or features.

In order to retrieve an image from a large image database, the descriptor should be invariant to scale and rotation.
It must also have enough discriminating power and immunity to noise for retrieval from a large image database.
The Zernike moment descriptor has many desirable properties such as rotation invariance, robustness to noise, expression efficiency, fast computation and multi-level representation for describing the shapes of patterns.
In this paper, we show that the Zernike moment can be used as an effective descriptor of global shape of an image in a large image database.
The experimental results conducted on a database of about 6,000 images in terms of exact matching under various transformations and the similarity-based retrieval show that the proposed shape descriptor is very effective in representing shapes.

Abstract While prior research has examined the relationship between narcissism and self-promoting behaviors on social media (e.g., posting selfies), little is known about the extent to which individuals' level of narcissism relates to how involved they are in other people's feedback (e.g., comments and “likes”) received on their selfies, or how observant and responsive they are to other people's selfie postings.
The present study investigates how narcissism relates to such selfie-related behaviors, as well as overall evaluation of selfie-posting behavior and intention to post selfies in the future.
By employing a total of 315 Korean subjects who take and post selfies on social networking sites, the present study indicates that individuals higher in narcissism are more likely to evaluate selfie-posting behavior favorably, be involved in the feedback provided by others, and be observant of other people's selfies.
However, level of narcissism did not moderate the relationship between how much one observes ot...

We introduce and study two classifications refining the polynomial hierarchy.
Both extend the difference hierarchy over NP and are analogs of some hierarchies from recursion theory.
We answer some natural questions on the introduced classifications, e.g.
we extend the result of J.Kadin that the difference hierarchy over NP does not collapse (if the polynomial hierarchy does not collapse).

Researchers face many challenges in representing biological data, including: (1) inherent complexity of biological data, (2) domain knowledge barrier, (3) constantly evolving knowledge, and (4) lack of expert data-modeling skills.
We have studied how to represent biological sequences and sequence-related genomics concepts using logical data structure.
From our multiple experiences in genomic data modeling, we present results in three areas: genomic schema elements, genomic schema fragments, and genomic data modeling lessons.
A genomic schema element is a data model that contains only one basic biological sequence notion.
Genomic schema elements provide biology data modelers with baseline thoughts in genomic data modeling.
A genomic schema fragment is a data model that contains only one genomic topic area.
Genomic schema fragments provide biology data modelers with successful design solutions that they can adapt to fit their own problem's needs.
Genomic data modeling lessons address issues particularly important to genomic data modeling such as modeling contextual information, modeling intermediate and derived data, modeling inconsistent data, and modeling categorical rules.
Genomic data modeling lessons provide novice biology data modelers with enriched principles from content-neutral data modeling techniques.
In all, we have demonstrated how to manage evolving genomic knowledge concepts and discovery results using data modeling techniques extended into the genomics domain.

images reveal more meaningful information to the human observers rather than grayscale ones.
Regardless of the advantages of the existing well-known objective image quality measures, one of the common and major limitations of these measures is that they evaluate the quality of grayscale images only and don't make use of color information.
In this paper we propose an improved method for image quality assessment that adds a color comparison to the criteria of the well-known Multiscale Structural Similarity index (MSSIM).
We evaluated the new color image quality measure through human subjective experiments.
Our human subjective evaluation data contains 25 reference images and 875 test images produced by five popular color quantization algorithms.
Each of the quantized images was evaluated by twenty two subjects and more than 19200 individual human quality judgments were carried out to obtain the final mean opinion scores.
We also tested the proposed method on TID2008 image database to further verify our results.
These results indicate that adding color comparison improves MSSIM for many distortions in TID2008 and for assessing quantized images in our database.
Keywordsquality assessment, Structural similarity index, Color quantization.

We consider asynchronous shared memory distributed systems, and investigate coordination problems in this model.
We provide a waitfree randomized consensus protocol that requires an expected O(n2 log n) atomic operations.

A fuzzy controller with online learning capability is reported in this paper.
The controller learns from a standard proportional plus derivative (PD) controller.
It is implicitly assumed that the tuning parameters of the PD controller are already known.
The learning is realized via Wang's table lookup scheme.
The controllers are applied successfully to control an open-loop unstable system, i.e., the ball and plate system.
Experimental studies have demonstrated the performance of the proposed controller.

Abstract Purpose Healthcare technology is meant to reduce medication errors.
The objective of this study was to assess unintended errors related to technologies in the medication use process.
Methods Medication incidents reported from 2006 to 2010 in a main tertiary care hospital were analysed by a pharmacist and technology-related errors were identified.
Technology-related errors were further classified as socio-technical errors and device errors.
This analysis was conducted using data from medication incident reports which may represent only a small proportion of medication errors that actually takes place in a hospital.
Hence, interpretation of results must be tentative.
Results 1538 medication incidents were reported.
17.1% of all incidents were technology-related, of which only 1.9% were device errors, whereas most were socio-technical errors (98.1%).
Of these, 61.2% were linked to computerised prescription order entry, 23.2% to bar-coded patient identification labels, 7.2% to infusion pumps, 6.8% to computer-aided dispensing label generation and 1.5% to other technologies.
The immediate causes for technology-related errors included, poor interface between user and computer (68.1%), improper procedures or rule violations (22.1%), poor interface between user and infusion pump (4.9%), technical defects (1.9%) and others (3.0%).
In 11.4% of the technology-related incidents, the error was detected after the drug had been administered.
Conclusions A considerable proportion of all incidents were technology-related.
Most errors were due to socio-technical issues.
Unintended and unanticipated errors may happen when using technologies.
Therefore, when using technologies, system improvement, awareness, training and monitoring are needed to minimise medication errors.

Abstract In designing synthetic discriminant function (SDF) filters, the usual choice for the correlation constraint is a real-valued constant for all training images of a given class.
However, this choice of constraints results in a filter that recognizes all images in the convex hull of the training set, which is generally undesirable.
The use of appropriate complex-valued constraints, though, produces an SDF filter which recognizes only images near the boundary of this convex hull and thus provides improved discrimination performance.
This improvement in discrimination can be accomplished without degrading the generalization performance of the filter.

The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings.
However, it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables.
State-of-the-art methods thus do not scale to problems with more than 20,000 variables.
In this paper, we develop an algorithm BIGQUIC, which can solve 1 million dimensional l1-regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory.
In order to do so, we carefully exploit the underlying structure of the problem.
Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components.
In spite of these modifications, we are able to theoretically analyze our procedure and show that BIGQUIC can achieve super-linear or even quadratic convergence rates.

We investigated the relationship between heart rate variability (HRV) and flow.We conducted a driving-simulator experiment to induce boredom, flow, and anxiety.HRV differed significantly between the three mental states.A negative linear relation exists between low-frequency component of HRV and flow.Flow is associated with baroreflex modulation of cardiovascular activity.
We present the results of an experimental investigation on the relationship between heart rate variability (HRV) and flow in adults exposed to computer-simulated tasks with different demand level manipulations: a balanced skill-demand level (fit) to induce flow, too high demands to induce anxiety, and too low demands to induce boredom.
Eighteen participants were exposed to three simulated driving tasks that differed in their demand levels.
During all tasks, the participants' heart rates were monitored and flow was measured after each task by means of a questionnaire.
Our results show that high-frequency HRV (HF-HRV) and low-frequency HRV (LF-HRV) differed between the three experimental conditions and an increase in demand level caused a decrease in HF-HRV and LF-HRV.
Furthermore, experiencing flow in a balanced skill-demand task was associated with a decreased LFHRV activity compared to being engaged in a task with too high demands (anxiety condition), in which higher levels of flow were related to moderate parasympathetic activity (HF-HRV) as well as to moderate baroreflex function (LF-HRV).
Our results contribute to a better understanding of the psychophysiology of flow and further demonstrate how virtual environments such a driving simulator can be effectively used to investigate psychological constructs such as flow or anxiety.

This paper presents two criteria for the termination of tree automata completion.
Tree automata completion is a technique for computing a tree automaton recognizing or over-approximating the set of terms reachable w.r.t.
a term rewriting system.
The first criterion is based on the structure of the term rewriting system itself.
We prove that for most of the known classes of linear rewriting systems preserving regularity, the tree automata completion is terminating.
Moreover, it outputs a tree automaton recognizing exactly the set of reachable terms.
When the term rewriting system is outside of such classes, the set of reachable terms can be approximated using a set of equations defining an abstraction.
The second criterion, which holds for any left-linear term rewriting system, defines sufficient restrictions on the set of equations for the tree automata completion to terminate.
We then show how to take advantage of this second criterion to use completion as a new static analysis technique for functional programs.
Some examples are demonstrated using the Timbuk completion tool.

This paper is concerned with the problem of global synchronization for complex networks with hybrid coupling.
An innovative approach is proposed to develop delay-dependent criteria for the system, which makes use of many relaxed information to construct Lyapunov functional, and employs an integral equation method and matrix expansion method to handle multitude Kronecker product terms.
The method in this paper increases the number of arbitrary matrix, and alleviates the requirements of the positive definiteness of the matrix which should be considered in many existing references.
This leads to significant improvement in the performance of the synchronization results.
Finally, a chaotic synchronization example is provided to show the effectiveness of the proposed results.

There are many uncertain problems in practical production and life which need decisions made with soft sets and fuzzy soft sets.
However, the basis of evaluation of the decision method is single and simple, the same decision problem can obtain different results from using a different evaluation basis.
In this paper, in order to obtain the right result, we discuss fuzzy soft set decision problems.
A new algorithm based on grey relational analysis is presented.
The evaluation bases of the new algorithm are multiple.
There is more information in a decision result based on multiple evaluation bases, which is more easily accepted and logical to one's thinking.
For the two cases examined, the results show that the new algorithm is efficient for solving decision problems.

Admission control and routing in circuit-switched networks are investigated with two types of traffic: wideband with delayed call set up and narrowband operating in loss mode.
Markov decision theory is used to construct a state-dependent routing policy.
Optimality conditions for a load sharing routing are derived from the first-order Kuhn-Tucker equations.
These models together with a direct routing scheme are used in a simulation study to determine the best approach for control of traffic with delayed call setup.
State-dependent routing is found to be superior to direct routing and load sharing routing, although its operation is much more complex.
>

Abstract How does ICT integration in learning processes depend on the support or lack of support from local authorities?
Data from the SITES M2 case studies sheds light on how Danish schools attempted to meet the ‘Act on the Folkeskole’ regarding the integration of ICT in all subjects.
This paper draws on the school's perspective based on interviews with the Principals, teachers and ICT coordinators, and the schools' ICT plans and strategies.
The schools' perspective is then seen in relationship to the respective municipalities, ICT strategies and national ICT policy in a discussion of the implications of ICT integration on the decentralised Danish folkeskole system.

A design-for-testability scheme for detecting CMOS analog faults was reported by Favalli et al.
(see ibid., vol.25, no.5, p.1239-46, 1990).
The authors propose two alternative designs, one for small circuits and another for large circuits, which require significantly less area overhead (about 1/4 to 1/3) than that of Favalli's design.
With the proposed modification in the first design, the untestable problem, which occurred in Favalli's design, can be alleviated.
Furthermore, the proposed schemes are also fit to be implemented in VLSI circuits.
>

The authors introduce a circuit partitioning method based on analysis of reconvergent fan-out.
A corolla is defined as a set of overlapping reconvergent fan-out regions.
The authors partition the circuit into a set of disjoint corollas and use the corollas to resynthesize the circuit.
The authors develop the notion of resynthesis potential of a logic circuit and use it to select corollas that resynthesize with most gain.
It is shown that resynthesis of large benchmark circuits using the corollas consistently reduces transistor pairs and layout area while improving delay and testability.
The use of don't cares to further minimize the corollas in the local context and the global context is explored.
>

The availability of unlimited text-to-speech synthesis systems provides the potential for remote access to databases over existing telephone systems.
This paper describes some of the intelligibility and user interface problems associated with such applications and reports work aimed at understanding and solving these problems.
In particular, it describes an approach to intelligibility problems based on the understanding and manipulation of listeners’ adaptation to synthetic speech, and user interface work examining use of a simulated travel information system.

From the Publisher: ::: For those engineers and scientists who use computers to solve their problems only to discover new, subtle problems in their results, this book is a welcome quick guide to trouble-shooting.
Offering practical advice on detecting and removing the insidious bugs that plague finite-precision calculations, real Computing outlines techniques for preserving significant figures, avoiding extraneous solutions (those ridiculous "answers" that turn up all too often), and finding efficient iterative processes for solving nonlinear equations.Anyone who computes with real numbers (for example, floating-point numbers stored with limited precision) tends to pick up a few computing "tricks"--techniques that increase the frequency of useful answers.
But where there might be ample guidance for a computor grappling with linear problems, there is little help for someone negotiating the nonlinear world--and it is this need that Forman Acton addresses.
His book presents a wealth of examples and exercises (with answers) to help a reader develop problemformulating skills--thus learning to avoid the common pitfalls that software packages seldom detect.
It presumes some experience with standard numerical methods--but for beginners in real computing, it will lend a touch of realism to topics often slighted in introductory texts.

Abstract Based on recent theories of the L2 reading process that have focused on an interactive approach, i.e.
the utilization of both top-down and bottom-up processing, this paper is concerned with the question of how reading comprehension can be facilitated with a multimedia application for language learning.
On the macro level, the effect of a dynamic visual advance organizer is investigated.
On the micro level, the effects of multimedia annotations for single vocabulary items are studied.
In addition, the relationship between vocabulary acquisition and reading comprehension is examined.
To test our hypotheses three studies with a total of 160 students were conducted using the multimedia application CyberBuch.
The results indicate that a dynamic visual advance organizer does aid in overall comprehension and that annotations of individual vocabulary items consisting of both visual and verbal information help more than verbal information only.
Also, a moderate correlation between vocabulary knowledge and reading comprehension was found.
These results support the dual coding theory and its extension to multimedia learning and underline the importance of visual information in addition to verbal information to support both top-down and bottom-up processing in reading in a foreign language.

The introduction of multispectral imaging in pathology problems such as the identification of prostatic cancer is recent.
Unlike conventional RGB color space, it allows the acquisition of large number of spectral bands within the visible spectrum.
This results in a feature vector of size greater than 100.
For such high dimensionality problems, pattern recognition techniques suffer from the well-known curse-of-dimensionality problem.
The two well known techniques to solve this problem are feature extraction and feature selection.
A feature selection technique using tabu search with an intermediate-term memory is proposed.
The cost of a feature subset is measured by leave-one-out correct-classification rate of a nearest-neighbor (1-NN) classifier.
Experiments have been carried out on textured multispectral images taken at 16 spectral channels and the results have been compared with a reported classical feature extraction technique.

Abstract The relevant theory of discrete B-splines with associated new algorithms is extended to provide a framework for understanding and implementing general subdivision schemes for nonuniform B-splines.
The new derived polygon corresponding to an arbitrary refinement of the knot vector for an existing B-spline curve, including multiplicities, is shown to be formed by successive evaluations of the discrete B-spline defined by the original vertices, the original knot vector, and the new refined knot vector.
Existing subdivision algorithms can be seen as proper special cases.
General subdivision has widespread applications in computer-aided geometric design, computer graphics, and numerical analysis.
The new algorithms resulting from the new theory lead to a unification of the display model, the analysis model, and other needed models into a single geometric model from which other necessary models are easily derived.
New sample algorithms for interference calculation, contouring, surface rendering, and other important calculations are presented.

Many group communications require a security infrastructure that ensures multiple levels of access control for group members.
While most existing group key management schemes are designed for single level access control, we present a multi-group key management scheme that achieves hierarchical group access control.
Particularly, we design an integrated key graph that maintains keying material for all members with different access privileges.
It also incorporates new functionalities that are not present in conventional multicast key management, such as user relocation on the key graph.
Analysis is performed to evaluate the storage and communication overhead associated key management.
Comprehensive simulations are performed for various application scenarios where users statistical behavior is modelled using a discrete Markov chain.
Compared with applying existing key management schemes directly to the hierarchical access control problem, the proposed scheme significantly reduces the overhead associated with key management and achieves better scalability.

Many recently developed control schemes for robotic manipulators require as inputs the desired position, velocity, and in some cases, acceleration of each joint of the manipulator.
However, it is most natural to specify the desired trajectory of the end effector in Cartesian coordinates.
Thus it is desirable to have a command generator which has as input a desired Cartesian trajectory, and as output a vector of joint positions, velocities, and accelerations that correspond to the demanded trajectory.
Such a command generator is presented in the form of a nonlinear feedback system that has the advantage of being related to a linear system.
The linear system can be used to compute precise bounds on the performance of the nonlinear system.
Simulation results for a nonspherical wrist manipulator are given.
>

Organisations are highly interested in collecting and analysing customer data to enhance their service offerings and customer interaction.
However, individuals increasingly fear how such practices may negatively affect them.
Although previous studies have investigated individuals’ concerns about information privacy practices, the adverse consequences people associate with external actors accessing their personal information remain unclear.
To mitigate customers’ fears, organisations need to know which adverse consequences individuals are afraid of and how to address those negative perceptions.
To investigate this topic, we conducted 22 focus groups with 119 participants.
We developed a comprehensive conceptualisation and categorisation of individuals’ perceived adverse consequences of access to their information that includes seven types of consequences: psychological, social, career-related, physical, resource-related, prosecution-related, and freedom-related.
Although individuals may limit their interactions with an organisation owing to consequences they associate with both the organisation and other actors, organisations can apply preventive and corrective mechanisms to mitigate some of these negative perceptions.
However, organisations’ scope of influence is limited and some fears may be mitigated only by individuals themselves or government regulation, if at all.

Abstract We propose a strongly polynomial algorithm for the minimum cost tension problem.
Our algorithm is inspired by Goldberg and Tarjan's ideas which deal with the minimum cost circulation problem.
It consists of repeatedly canceling minimum mean-cost residual cuts.
Hence, we had to consider two other combinatorial optimization problems which possess interesting practical applications: the min-cost and min-mean-cost residual cut problems (Hadjiat and Maurras, 1995).

Abstract Due to anatomical variability across subjects many brain mapping experiments have analysis focused on a few particular regions of interest so as to circumvent the problem of sub-optimal statistics resulting from the lack of anatomical correspondence across subjects.
Since the topographic distribution of experimental effects across the cortex is also often of interest, two separate analyses are often conducted, one on the regions of interest alone, as well as a separate ‘whole brain’ analysis with sub-optimal spatial correspondence across brains.
In this paper we present a new group alignment procedure which incorporates, from each subject, both macro-anatomical (curvature) information and functional information from standard localizer experiments.
After specifying appropriate parameters to weight anatomical and functional alignment forces, we were able to create a group cortical reconstruction which was well aligned in terms of both anatomical and functional areas.
We observed an increase in the overlap of functional areas as well as an improvement in group statistics following this integrated alignment procedure.
We propose that, using this alignment scheme, two separate analyses may not be necessary as both analyses can be integrated into a single procedure.
After an integrated structural and functional alignment one is able to carry out a whole brain analysis with improved statistical sensitivity due to the reduction in spatial variation in the location of functional regions of interest which fCBA accomplishes.
Furthermore, regions in the vicinity of localised and aligned regions-of-interest will also benefit from the integrated alignment.

In this paper, an Adaptive Neuro-Fuzzy Inference System (ANFIS) based Sensor fault detection and isolation for Continuous Stirred Tank Reactor (CSTR) is proposed.
CSTR is a highly nonlinear process exhibiting stable and unstable steady state at different operating regions.
Fault detection (FD) of such a complicated CSTR process is a mind boggling problem.
In this paper, an ANFIS based 'dedicated observer' scheme is dealt along with statistical methods for the detection of the fault.
The result shows the feasibility of using the proposed method for the detection of sensor faults in CSTR.

We present a novel atom–atom potential derived from a database of protein–ligand complexes.
First, we clarify the similarities and differences between two statistical potentials described in the literature, PMF and Drugscore.
We highlight shortcomings caused by an important factor unaccounted for in their reference states, and describe a new potential, which we name the Astex Statistical Potential (ASP).
ASP's reference state considers the difference in exposure of protein atom types towards ligand binding sites.
We show that this new potential predicts binding affinities with an accuracy similar to that of Goldscore and Chemscore.
We investigate the influence of the choice of reference state by constructing two additional statistical potentials that differ from ASP only in this respect.
The reference states in these two potentials are defined along the lines of Drugscore and PMF.
In docking experiments, the potential using the new reference state proposed for ASP gives better success rates than when these literature reference states were used; a success rate similar to the established scoring functions Goldscore and Chemscore is achieved with ASP.
This is the case both for a large, general validation set of protein–ligand structures and for small test sets of actives against four pharmaceutically relevant targets.
Virtual screening experiments for these targets show less discrimination between the different reference states in terms of enrichment.
In addition, we describe how statistical potentials can be used in the construction of targeted scoring functions.
Examples are given for cdk2, using four different targeted scoring functions, biased towards increasingly large target-specific databases.
Using these targeted scoring functions, docking success rates as well as enrichments are significantly better than for the general ASP scoring function.
Results improve with the number of structures used in the construction of the target scoring functions, thus illustrating that these targeted ASP potentials can be continuously improved as new structural data become available.
Proteins 2005.
© 2005 Wiley-Liss, Inc.

Abstract Every concept learning system produces hypotheses that are written in some sort of constrained language called the concept description language, and for most learning systems, the concept description language is fixed.
This paper describes a learning system that makes a large part of the concept description language an explicit input, and discusses some of the possible applications of providing this additional input.
In particular, we discuss a technique for learning a logic program such that the antecedent of each clause in the program can be generated by a special antecedent description language; it is shown that this technique can be used to make use of many different types of background knowledge, including constraints on how predicates can be used, programming cliches, overgeneral theories, incomplete theories, and theories syntactically close to the target theory.
The approach thus unifies many of the problems previously studied in the field of knowledge-based learning.

The acquisition of data stored on cloud services has become increasingly important to digital forensic investigations.
Apple, Inc. continues to expand the capabilities of its cloud service, iCloud.
As such, it is critical to determine an effective means for forensic acquisition of data from this service and its effect on the original file data and metadata.This research examined files acquired from the iCloud service via the native Mac OS X system synchronization with the service.
The goal was to determine the operating system locations of iCloud-synched files.
Once located, the secondary goal was to determine if the file hash values match those of the original files and whether file metadata, particularly timestamps, are altered.

We discuss the dynamic programming approach to finding an optimal sequence of a set of tasks when the tasks are related by precedence restrictions.
We describe how to use this approach in problems where no explicit precedence relations exist.
Computer implementation considerations played an important role in its development.
Computational results indicate that, when the curse of dimensionality can be dispelled, dynamic programming can be a useful procedure for large sequencing problems.

It has been previously shown that a real-time decomposition of the incoming signal into a set of partially uncorrelated components via an orthogonal transform, and a subsequent adaptation on these individual components, leads to faster convergence rates.
Here, transform domain processing is characterized by the effect of the transform on the shape of the mean-square error surface.
It is shown that the effect of an ideal transform is to convert equal error contours that are initially hyperellipses in the parameter space into hyperspheres.
Five specific real-valued orthogonal transforms are compared in terms of learning characteristics and computational complexity.
Since the Karhunen-Loeve transform (KLT) is the ideal transform for this application, and since the KLT is defined in terms of the statistics of the input signal, it is certain that no fixed-parameter transform can deliver optimal learning characteristics for all input signals.
However, the simulations suggest that transforms can be found which give much improved performance in a given situation.
>

An improved flexible contact lens is provided having hydrophilic and lipophobic properties which allow extended duration of wear by eliminating or greatly reducing the risk of rupture of the natural tear film produced by the cornea.
These properties are achieved by crosslinking a selected water soluble polymer which has been grafted onto a pre-formed substrate, for example crosslinking polyvinyl pyrrolidone which has been previously grafted onto a silicone substrate.
The grafting and the crosslinking are accomplished in separate stages.

This article examines the adoption of free wireless Internet parks (iPark) by Qatari citizens as a means of accessing electronic services from public parks.
The Qatari government has launched the free wireless Internet parks concept under their national electronic government (e-government) initiative with a view to providing free Internet access for all citizens whilst enjoying the outdoors.
By offering free wireless internet access, the Qatari government hopes to increase the accessibility of e-government services and encourage their citizens to actively participate in the global information society with a view to bridging the digital divide.
The adoption and diffusion of iPark services will depend on user acceptance and the availability of wireless technology.
This article examines an extended technology acceptance model (TAM) that proposes individual differences and technology complexity in order to determine perceived usefulness and perceived ease of the iPark initiative by using a survey-based study.
The article provides a discussion on the key findings, research implications, limitations, and future directions for the iPark initiative in Qatar.

BACKGROUND ::: Magnetoencephalography (MEG) is a method of studying brain activity via recordings of the magnetic field generated by neural activity.
Modern MEG systems employ an array of low critical-temperature superconducting quantum interference devices (low-Tc SQUIDs) that surround the head.
The geometric distribution of these arrays is optimized by maximizing the information content available to the system in brain activity recordings according to Shannon's theory of noisy channel capacity.
::: ::: ::: NEW METHOD ::: Herein, we present a theoretical comparison of the performance of low- and high-Tc SQUID-based multichannel systems in recordings of brain activity.
::: ::: ::: RESULTS ::: We find a high-Tc SQUID magnetometer-based multichannel system is capable of extracting at least 40% more information than an equivalent low-Tc SQUID system.
The results suggest more information can be extracted from high-Tc SQUID MEG recordings (despite higher sensor noise levels than their low-Tc counterparts) because of the closer proximity to neural sources in the brain.
::: ::: ::: COMPARISON WITH EXISTING METHODS ::: We have duplicated previous results in terms of total information of multichannel low-Tc SQUID arrays for MEG.
High-Tc SQUID technology theoretically outperforms its conventional low-Tc counterpart in MEG recordings.
::: ::: ::: CONCLUSIONS ::: A full-head high-Tc SQUID-based MEG system's potential for extraction of more information about neural activity can be used to, e.g., develop better diagnostic and monitoring techniques for brain disease and enhance our understanding of the working human brain.

Abstract Neuronal network activity in the developing brain is generated in a discontinuous manner.
In the visual cortex during the period of physiological blindness of immaturity, this activity mainly comprises retinally triggered spindle bursts or Ca2 + clusters thought to contribute to the activity-dependent construction of cortical circuits.
In spite of potentially important developmental functions, the spatial structure of these activity patterns remains largely unclear.
In order to address this issue, we here used three-dimensional two-photon Ca2 + imaging in the visual cortex of neonatal mice at postnatal days (P) 3–4 in vivo.
Large-scale voxel imaging covering a cortical depth of 200 μm revealed that Ca2 + clusters, identified as spindle bursts in simultaneous extracellular recordings, recruit cortical glutamatergic neurons of the upper cortical plate (CP) in a column-like manner.
Specifically, the majority of Ca2 + clusters exhibit prominent horizontal confinement and high intra-cluster density of activation involving the entire depth of the upper CP.
Moreover, using simultaneous Ca2 + imaging from hundreds of neurons at single-cellular resolution, we demonstrate that the degree of neuronal co-activation within Ca2 + clusters displays substantial heterogeneity.
We further provide evidence that co-activated cells within Ca2 + clusters are spatially distributed in a non-stochastic manner.
In summary, our data support the conclusion that dense coding in the form of column-like Ca2 + clusters is a characteristic property of network activity in the developing visual neocortex.
Such knowledge is expected to be relevant for a refined understanding of how specific spatiotemporal characteristics of early network activity instruct the development of cortical circuits.

Recently, Zhang and Wang proposed a steganographic scheme by exploiting modification direction (EMD) to embed one secret digit d in the base-(2xn+1) notational system into a group of n cover pixels at a time.
Therefore, the hiding capacity of the EMD method is log"2(2xn+1)/n bit per pixel (bpp).
In addition, its visual quality is not optimal.
To overcome the drawbacks of the EMD method, we propose a novel steganographic scheme by exploiting eight modification directions to hide several secret bits into a cover pixel pair at a time.
By this way, the proposed method can achieve various hiding capacities of 1, 2, 3, 4, and 4.5 bpp and good visual qualities of 52.39, 46.75, 40.83, 34.83, and 31.70dB, respectively.
The experimental results show that the proposed method outperforms three recently published works, namely Mielikainen's, Zhang and Wang's, and Yang et al.
's methods.

An anti-lock braking system control is a rather difficult problem due to its strongly nonlinear and uncertain characteristics.
To overcome these difficulties, robust control methods should be employed such as a sliding mode control.
The aim of this paper is to give a short overview of sliding mode control techniques implemented in the control of ABS.
The most used control algorithms are applied to a quarter vehicle model to demonstrate the advantages of this control approach.
Fast convergence and good performances of the designed controllers are verified through digital simulations and validated in real time applications using a laboratory experimental setup.

Successfully implementing patient care information systems (PCIS) in health care organizations appears to be a difficult task.
After critically examining the very notions of 'success' and 'failure', and after discussing the problematic nature of lists of 'critical success- or failure factors', this paper discusses three myths that often hamper implementation processes.
Alternative insights are presented, and illustrated with concrete examples.
First of all, the implementation of a PCIS is a process of mutual transformation; the organization and the technology transform each other during the implementation process.
When this is foreseen, PCIS implementations can be intended strategically to help transform the organization.
Second, such a process can only get off the ground when properly supported by both central management and future users.
A top down framework for the implementation is crucial to turn user-input into a coherent steering force, creating a solid basis for organizational transformation.
Finally, the management of IS implementation processes is a careful balancing act between initiating organizational change, and drawing upon IS as a change agent, without attempting to pre-specify and control this process.
Accepting, and even drawing upon, this inevitable uncertainty might be the hardest lesson to learn.

The stochastic block model (SBM) is a random graph model with planted clusters.
It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences.
This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection).
The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds.
The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods.
A few open problems are also discussed.

In this paper we present our approach in learner modelling for CSCL environments.
Learner models provide the information needed in order to support awareness and promote opportunities of effective collaboration and learning in a networked community of practice.
The learner model is proposed as a set of beliefs hold by a software agent about the capabilities, commitments and learning goals of the learner.
By exchanging their beliefs about the capabilities of their learners the software agents promote the creation of zones of proximal development in the learning group, by proposing tasks based on the group-based knowledge frontier of their learners, which represents the assistance and learning possibilities of the learner in a community of practice.

The Hodgkin-Huxley Equations.- Dendrites.- Dynamics.- The Variety of Channels.- Bursting Oscillations.- Propagating Action Potentials.- Synaptic Channels.- Neural Oscillators: Weak Coupling.- Neuronal Networks: Fast/Slow Analysis.- Noise.- Firing Rate Models.- Spatially Distributed Networks.

There have been numerous applications of statistical methods to the field of education, mostly in educational measurement.
This article provides short descriptions of several topics in statistics that have found applications to education and provides examples of applications of some of these topics to education.

We attack the problem of recovering an image (a function of two variables) from experimentally available integrals of its grayness over thin strips.
This problem is of great importance in a large number of scientific areas.
An important version of the problem in medicine is that of obtaining the exact density distribution within the human body from X-ray projections.
This paper proposes an algorithm for finding an optimal Bayesian estimate of an unknown image from the projection data, shows that the algorithm is geometrically convergent, reports on its computer implementation, and demonstrates its performance on a medical problem.

Uses alternating convex projection techniques to compute the closest positive definite Toeplitz matrix that satisfies certain inequality constraints to a specified symmetric matrix.
Some applications to signal processing and control problems are discussed.
>

A hardware accelerator for self-organizing feature maps is presented.
The system is based on the universal rapid prototyping system RAPTOR2000 that has been developed by the authors.
The prototyping system consists of a motherboard and up to six application specific modules.
The motherboard provides the necessary communication infrastructure for the application specific functionality that is implented on the modules.
RAPTOR2000 is linked to its host - a standard personal computer or workstation - via PCI bus.
For the simulation of self-organizing maps a module has been disigned for the RAPTOR2000 system, that embodies an FPGA of the Xilinx Virtex series and optionally up to 128 MBytes of SDRAM.
A speed-up of about 60 is achieved with five FPGA modules on the RAPTOR2000 system compared to a software implementation on a state of the art personal computer for typical applications of self-organizing maps.

Cytochrome P450 3A4 (CYP3A4) metabolizes more than 50% of clinically used drugs and is often involved in adverse drug–drug interactions.
It displays atypical binding and kinetic behavior toward a number of ligands characterized by a sigmoidal shape of the corresponding titration curves, which is indicative of a positive homotropic cooperativity.
This requires a participation of at least two ligand molecules, whereby the binding of the first ligand molecule increases the affinity of CYP3A4 for the binding of the second ligand molecule.
In the current study, a combination of molecular dynamics simulations and free-energy calculations was applied to elucidate the physicochemical origin of the observed positive homotropic cooperativity in ketoconazole binding to CYP3A4.
The binding of the first ketoconazole molecule was established to increase the affinity for the binding of the second ketoconazole molecule by 5 kJ mol–1, which explains and quantifies the experimentally observed cooperative behavior of CYP3A4...

Abstract In this paper, a new approach for the diagnosis of the subjects with Parkinson's disease (PD) from the healthy control subjects is proposed.
This method uses the measurements of gait signals using the ground reaction forces under usual walking of the subjects.
These measurements were computed using 8 sensors placed underneath of each foot.
The absolute value of the difference between the force measurements were calculated for each sensor at each time and these signals went through a short-time Fourier transform (STFT) and several features were extracted from the spectrum of the signals.
The histogram of these features was computed and the bin selection was performed using the feature discriminant ratio (FDR) method.
Then the chi-square distance between the reduced histograms was computed and it formed a kernel for support vector machines (SVMs) for classification.
The results on 93 subjects with PD and 73 healthy control subjects show that the proposed approach obtains an accuracy of 91.20% for the diagnosis of the PD using gait signals.

Thank you for reading applied decision support with soft computing.
Maybe you have knowledge that, people have search hundreds times for their favorite books like this applied decision support with soft computing, but end up in harmful downloads.
Rather than enjoying a good book with a cup of tea in the afternoon, instead they are facing with some infectious bugs inside their computer.
applied decision support with soft computing is available in our digital library an online access to it is set as public so you can get it instantly.
Our digital library spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one.
Kindly say, the applied decision support with soft computing is universally compatible with any devices to read.

A novel algorithm of parallel particle swarm optimization with island population model is proposed to improve the performance of particle swarm optimization algorithm for application to large-scale problems and multi-variable solutions.The parallel particle swarm optimization algorithm is designed and implemented using an idea of island population model.The experimental results show that not only the solving efficiency is raised but also the restraining premature convergence is enhanced in the parallel algorithm.Comparing with classical particle swarm optimization,the performance of the proposed algorithm is greatly improved consequently.

The spatiotemporal databases concern about the time-varying spatial attributes.
And one of the important research areas is tracking and managing moving objects for the location-based services.
Many location-aware applications have arisen in various areas including mobile communications, traffic control and military command and control (C2) systems.
However, managing exact geometric location information is difficult to be achieved due to continual change of moving objects' locations.In this paper we propose the Bst-tree that utilizes the concept of multiversion B-trees.
It provides an indexing method for future location queries based on the dual transformation.
This approach can be applied for the range query on moving object's trajectories specifically in the mobile communication systems.
Also we present a dynamic management algorithm that determines the appropriate update interval probabilistically induced by various mobility patterns to guarantee the query performance.

One major challenge of contention-based Medium-Access Control (MAC) for sensor networks is to reduce energy waste in idle listening time without global synchronisation of active/sleep duty cycles.
This paper presents Distributed Power Scheduling (DPS), a medium access control protocol that is specifically designed for a sensor network that supports data aggregation.
DPS integrates data aggregation process into power-mode scheduling in the MAC layer and effectively reduces packet delay.
Two enhancements, adaptive aggregation and adaptive sleeping, dynamically adjust aggregation period and sleeping time, and greatly reduce idle listening period.
DPS requires only local synchronisation and supports self-configuration.
Detailed protocol description is presented.
Formal analysis presents upper bounds on expected frame time (duty cycle period)and expected packet delay.
Simulation study shows that the two enhancements in DPS achieved 28% reduction in idle listening time, which resulted in 20% energy reduction, as compared to the basic DPS protocol.

About 12 years of work with a specific type of learning pattern-recognition system are reviewed.
The principles and characteristics of the scheme, which is based on random-access-memory implementation, are discussed in some detail.
Methods of improving performance and cost-optimising pattern recognisers are presented, together with case studies in a variety of fields including the recognition of alphanumerics, chemical data and faults in digital circuit boards.

Web site quality is now considered a critical factor to attract customers' attention and build loyalty.
Based on a review of the literature, this study focuses on five dimensions of Web quality — usability, web site design, service quality, information quality, and enjoyment.
The aim of this research is to identify the development of Web site quality in four UK airlines based on the research dimensions during the period from 1999 to 2005.
For this purpose, the survey utilised the Web archive (www.archive.org) for retrospective analysis (n=120).
According to the findings, the four airlines have improved their Web site quality on the four dimensions in quite different ways during the period from 1999 to 2005.
One of the main conclusions of this study is that the airlines have specific advantages from their particular mix of Web site attributes.
The paper rounds off with conclusions and implications for research and practice.

It is shotwt that 5y utilising phase information in an impedance imaging system using induced currents, images of permittivity distribution c8n be reconstructed.
Results are presented which show good agreement between simulated and measured data.
Furthermore the results demonstrate that permittivity images can reveal objects that are almost invisible in a conductivity image.

Presently available methods to analyze the link between explanatory variables and an ordered categorical response implicitly assume independence.
This hypothesis is no longer valid when data are collected over time.
We assume temporal dependence and introduce autocorrelation using a latent-variable formulation.
Due to intractable distributions, we resort to Gibbs sampling for statistical inference within the Bayesian paradigm.
Variable selection is also addressed and appears as a straightforward byproduct of this framework.
We illustrate the method by analyzing on-line quality data that possess such autocorrelation.

During business analysis, business activities are modeled and analyzed.
Redefined models become the blueprints for improved business activities.
The cost to produce models of the organization is high and model accuracy is important.
Involvement from knowledgeable participants and stakeholders is desirable during business modeling and analysis.
Traditional modeling approaches limit direct participation to a small handful of participants.
This paper discusses the development and evaluation of an electronic meeting system (EMS) based activity modeling tool.
Modeling efforts supported by this new approach are compared with modeling efforts supported by analysts with a single-user tool.
The results of this comparison reveal that the EMS-based modeling tool allows a greater number of individuals to participate efficiently in model development.
Models are developed between 175 percent and 251 percent faster with the new approach than with the traditional approach.
Specific features are discussed that help relatively novice modelers work with analysts to develop models of reasonable quality.
Measures are set forth that can be used to assess modeling efficiency and quality.

Social presence is considered to be a major design principle in computer-mediated communication and an important determinant of online community participation.
In this research, a multidimensional conceptualization of social presence that is more specific to online communities was developed and the potential of such a conceptualization to augment motivational theory was investigated to provide a better explanation of online community participation.
More specifically, motivational theory and social presence theory are integrated to develop a model explaining online community participation.
Different from prior research where social presence was assumed to be fully mediated through motivation, a direct effect of social presence on participation is hypothesized over and above those effects that are mediated by motivational variables.
Three social presence dimensions of particular relevance to online communities (i.e., awareness, affective social presence, and cognitive social presence) are identified and inv...

The Hamming-stability perceptron learning rule (PHSL) is proposed for the Hopfield content-addressable memories based on three well recognized criteria, which amount to widely expanding the basin of attraction around each desired attractor.
Extensive experiments convincingly show that the PHSL does take good care of three optimal criteria.
>

A receiver which employs coherent, or synchronous, detection must have knowledge of the phase of the received signal.
In general, the receiver acquires this knowledge from sighals received previously over the channel.
The result of this measurement process is a noisy phase reference which is then used by the receiver in the detection of the incoming signals.
In this paper the effect of using baud decisions to direct the phase measurement process (decision-directed measurement) is investigated by means of Monte Carlo computer simulation of a coherent, binary communication system employing either orthogonal or phase-reversal signaling.
The results are compared with those obtained from similar systems using nondecision-directed measurement.
Analytical verification of the computer results is provided for particular cases.
Error rates are given at several signal-to-noise ratios.
The central conclusion of this study is that detection using a phase reference obtained through the decision-directed technique outlined herein results in system error rates which are generally lower, at all signal-to-noise ratios, than error rates of corresponding nondecision-directed phase measurement schemes.
A "run-away" phenomenon, in which the receiver by committing errors loses the reference phase and never regains it, is not encountered.

Each new print copy includes Navigate 2 Advantage Access that unlocks a comprehensive and interactive eBook, student practice activities and assessments, a full suite of instructor resources, and learning analytics reporting tools.
Databases Illuminated, Third Edition combines database theory with a practical approach to database design and implementation.
Strong pedagogical features, including accessible language, real-world examples, downloadable code, and engaging hands-on projects and lab exercises create a text with a unique combination of theory and student-oriented activities.
Providing an integrated, modern approach to databases, Databases Illuminated, Third Edition is the essential text for students in this expanding field.
The text is packaged with a full suite of instructor resources, including a Test Bank, Solutions to Projects and Exercises, Lecture Slides in PowerPoint format, and Additional Projects.
With Navigate 2, technology and content combine to expand the reach of your classroom.
Whether you teach an online, hybrid, or traditional classroom-based course, Navigate 2 delivers unbeatable value.
Experience Navigate 2 today at www.jblnavigate.com/2 Key features of the Third Edition include: A new chapter on the emerging topics of Big Data, NoSQL, and NewSQL Revised and expanded coverage on database security, including SQL injection Extended coverage of SQL, with additional material on temporal database queries and JDBC New features and downloadable SQL code for Oracle 12c, MySQL, and SQL Server Updated information on social and ethical issues addressing new regulations and global issues Use of the open source software LibreOffice, which is available worldwide at no cost Access to Navigate 2 online learning materials including a comprehensive and interactive eBook, student practice activities and assessments, learning analytics reporting tools, and more

Robots interacting with other agents in rich information landscapes and complex dynamic physical environments require sophisticated and robust concept and knowledge management capabilities if they are to solve problems, communicate, learn and exhibit intelligent behaviours.
In this paper we describe how conceptual spaces provide a powerful substrate upon which to build effective concept and knowledge management capabilities that integrate information from multiple sensory and symbolic sources.
We use SONY AIBO robots and the robot soccer domain to illustrate our framework and approach.
The conceptual spaces framework allows robots to build rich and grounded world models from a wide variety of internal and external knowledge resources, e.g.
sensors, ontologies, databases, knowledge bases, the semantic Web, Web services, and other agents.
Conceptual spaces provide an important and effective bridge between the perceptual level and the symbolic level by grounding sensory information to objects

In this paper, a three-mode subspace technique based on higher order singular value decomposition (HOSVD) is presented.
This technique is then used in the context of wave separation.
It can be regarded as the extension to three-mode arrays of the well-known subspace technique proposed by Eckart and Young (Psychometrica 1 (1936) 211) for matrices.
Three-mode data sets are increasingly encountered in signal processing and are classically processed using matrix algebra techniques.
The proposed approach aims to process naturally three-mode data with multilinear algebra tools.
So in the proposed algorithms, the structure of the data set is preserved and no reorganization is performed on it.
The choice of HOSVD for subspace method is explained, studying the rank definition for three-mode arrays and orthogonality between subspaces.
A projector formulation for three-mode signal and noise subspaces is also given and the improvement of separation with the three-mode approach over a componentwise approach is shown.
We study two applications for the proposed Higher Order Subspace approach: the reverberation problem in sonar, and the polarized seismo-acoustic wave separation problem.
For the first application, we propose a three-mode version of the Principal Component Inverse algorithm (IEEE Trans.
Aerospace Electron.
Systems 30(1) (1994) 55).
We apply the proposed technique on simulated data as well as on real sonar data where the three modes are angle, delay and distance.
For the second application, we consider the polarization of the seismic wave as the third mode (in addition to time and distance modes) and show the resulting improvement of wave separation using the proposed Higher Order approach.

This article aims at illustrating how Information and Communication Technologies (ICT) could be used to exploit and disseminate Cultural Heritage, providing enriching learning experiences for different targets of users, especially young people.
In fact, by the immersion in virtual museums or reconstructed worlds, users can build different paths of fruition interacting with 3D objects as in a videogame.
In this way, a superimposed and interchangeable view of the real find and its virtual reconstruction for a global vision is allowed.
Particularly effective for arising interest and curiosity in the users are mobile devices (i.e.
Personal Digital Assistants, pocket PCs, smart-phones) integrated with GIS and GPS, which can provide combined real and virtual information based on usersâ€™ location by a Virtual Navigation System.
The case study of Calabrian Magna Graecia (Italy) is presented, with particular reference to the projects â€œVirtualMuseum Net of Magna Graeciaâ€ and â€œNETConnectâ€ .

Layered Sigmoid Belief Networks are directed graphical models in which the local conditional probabilities are parameterised by weighted sums of parental states.
Learning and inference in such networks are generally intractable, and approximations need to be considered.
Progress in learning these networks has been made by using variational procedures.
We demonstrate, however, that variational procedures can be inappropriate for the equally important issue of inference - that is, calculating marginals of the network.
We introduce an alternative procedure, based on assuming that the weighted input to a node is approximately Gaussian distributed.
Our approach goes beyond previous Gaussian field assumptions in that we take into account correlations between parents of nodes.
This procedure is specialized for calculating marginals and is significantly faster and simpler than the variational procedure.

Background: In this study, we empirically evaluated the consistency and accuracy of five different methods to detect differentially expressed genes (DEGs) based on microarray data.
Methods: Five different methods were compared, including the t-test, significance analysis of microarrays (SAM), the empirical Bayes t-test (eBayes), t-tests relative to a threshold (TREAT), and assumption adequacy averaging (AAA).
The percentage of overlapping genes (POG) and the percentage of overlapping genes related (POGR) scores were used to rank the different methods on their ability to maintain a consistent list of DEGs both within the same data set and across two different data sets concerning the same disease.
The power of each method was evaluated based on a simulation approach which mimics the multivariate distribution of the original microarray data.
Results: For smaller sample sizes (6 or less per group), moderated versions of the t-test (SAM, eBayes, and TREAT) were superior in terms of both power and consistency relative to the t-test and AAA, with TREAT having the highest consistency in each scenario.
Differences in consistency were most pronounced for comparisons between two different data sets for the same disease.
For larger sample sizes AAA had the highest power for detecting small effect sizes, while TREAT had the lowest.
Discussion: For smaller sample sizes moderated versions of the t-test can generally be recommended, while for larger sample sizes selection of a method to detect DEGs may involve a compromise between consistency and power.

Abstract We study the existence of pseudo S -asymptotically ω -periodic mild solutions for a class of abstract fractional differential equation.

Code-similarity is the actual indicator of plagiarism in the context of programming assignments.
However, the experiences of practical software development have empirically confirmed the existences of other causes for code-similarity.
Existing practices usually overemphasis the casual relationship between code-similarity and plagiarism, but ignore the importance to make students understand other causes that also contribute to code-similarity.
This paper presents an active learning method to involve students and instructors collaboratively in finding causes of code-similarity occurred in programming assignments.
The result shows that most causes occurred in programming assignments are positive.
Students can learn the different causes of code-similarity with pros and cons during conducting the active learning method.

Abstract : Analytical methods for the development of Reynolds stress models in turbulence are reviewed in detail.
Zero, one and two equation models are discussed along with second-order closures.
A strong case is made for the superior predictive capabilities of second-order closure models in comparison to the simpler models.
The central points of the paper are illustrated by examples from both homogeneous and inhomogeneous turbulence.
A discussion of the author's views concerning the progress made in Reynolds stress modeling is also provided along with a brief history of the subject.
(jhd)

Where and how should access control be exercised over cyber-physical systems?
Here, Vint Cerf deliberates a possible way to secure such systems.

The Gauss-Newton algorithm is often used to minimize a nonlinear least-squares loss function instead of the original Newton-Raphson algorithm.
The main reason is the fact that only first-order derivatives are needed to construct the Jacobian matrix.
Some applications as, for instance multivariable system identification, give rise to "weighted" nonlinear least-squares problems for which it can become quite hard to obtain an analytical expression of the Jacobian matrix.
To overcome that struggle, a pseudo-Jacobian matrix is introduced, which leaves the stationary points untouched and can be calculated analytically.
Moreover, by slightly changing the pseudo-Jacobian matrix, a better approximation of the Hessian can be obtained resulting in faster convergence.

The acceptability and usability of intrusion detection systems get seriously affected with data imbalance in network traffic.
A large number of false alarms mean a lot in terms of the acceptability of intrusion detection systems.
The reason for the increase in false alerts is that the normal traffic abound.
Even with highly accurate intrusion detection systems, the effective detection rate of the minority attack types will be unacceptably low, and those attack types are often the most serious ones.
Thus, high accuracy is not necessarily an indicator of high model quality, and therein lays the accuracy paradox of predictive analytics.
The cost of missing an attack is higher than the cost of false alarms.
The aim of this work is to provide an architecture that enables available intrusion detection systems to work together towards creating a more realistic model of the state of a network.
The data-dependent decision fusion architecture presented in this paper learns from the data and then appropriately gives weighting to the decisions of various intrusion detection systems.
The fusion enriches these weighted decisions to provide a single decision, which is better than those of the existing intrusion detection systems.
It is also shown that our technique is more flexible and also outperforms other existing fusion techniques such as OR, AND, SVM, and ANN.
This method reduces the false positive rate and improves the overall detection rate and, also, the detection rate of minority class types in particular.
For illustrative purposes, two different data sets, namely the DARPA 1999 data set as well as the real-time network traffic embedded with attacks, have been used.
Copyright © 2012 John Wiley & Sons, Ltd.

The security of traditional identity-based signatures wholly depends on the security of secret keys.
Exposure of secret keys requires reissuing all previously assigned signatures.
This limitation becomes more obvious today as key exposure is more common with increasing use of mobile and unprotected devices.
Under this background, mitigating the damage of key exposure in identity-based signatures is an important problem.
To deal with this problem, we propose to integrate forward security into identity-based signatures.
In this paper, we firstly formalize the definition and security notions for forward-secure identity-based signature scheme, and then construct an efficient scheme.
All parameters in our scheme have, at most, log-squared complexity in terms of the total number of time periods.
The scheme is provably secure without random oracles.

The pattern of nucleolar organizer regions in bone marrow smears of 40 patients with acute myeloid leukemia (AML), 27 with acute lymphoid leukemia (ALL) and 15 control cases were studied.
All normal blasts as well as leukemic blasts revealed at least one cluster of AgNORs.
In AML the mean number of clusters (2.87 +/- 0.48) and dots (0.23 +/- 0.17) per cell was significantly lower than in normal myeloblasts (3.27 +/- 0.14 resp.
0.34 +/- 0.12).
In ALL the number of clusters (2.82 +/- 0.62) was not significantly different from that of AML blasts.
While in all control cases the distribution of the number of clusters per cell within a case could be approximated by a modified Poisson equation, this did not hold for 9 ALL and in 13 AML patients.
The histogram of those cases showed a lower dispersion.
In ALL, but not in AML, the number of clusters per cell correlated with the peripheral leukocyte count.
The present results strengthen the concept that clusters and dots should be regarded as separate entities and suggest that their pattern is related to duration of cell cycle as it has been shown in other tissues.

Abstract This paper deals with a system of rational difference equations x n + 1 = a y n + b c y n + d , y n + 1 = a x n + b c x n + d , n = 0 , 1 , 2 , … , where a , b , c , d are real numbers with c ≠ 0 and a d − b c ≠ 0 .
We establish a representation formula of solutions of the system and classify global behavior of solutions when no initial values belong to the forbidden set of the system.

Information sharing stands for a two-way activity in which information is given and received in the same context.
The present study reviews information sharing from the viewpoint of information giving.
The empirical analysis draws on interviews with twenty environmental activists in Finland, 2005.
Three major motives for information giving in non-work contexts were identified: first, seredipitous altruism to provide help to other people, second, pursuit of the ends of seeking information by proxy, and third, duty-driven needs characteristic of persons elected to positions of trust.
Since in most cases information giving was driven by altruistic motives, the lack of reciprocity did not in practice weaken the motives for information giving.
However, in the case of sensitive information, information giving tends to be restricted by calculations of the risk of information leakage against benefits obtained from the personally rewarding experience of providing important information to others.

The process of gene assembly in ciliates, an ancient group of organisms, is one of the most complex instances of DNA manipulation known in any organisms.
This process is fascinating from the computational point of view, with ciliates even using the linked list data structure.
Three molecular operations (ld,hi, and dlad) have been postulated for the gene assembly process.
We initiate here the study of parallelism of this process by investigating several natural questions, such as: when can a number of operations be applied in parallel to a gene pattern, or how many steps are needed to assemble in parallel a micronuclear gene.
We believe that the study of parallelism contributes to a better understanding of the nature of gene assembly, and in particular it provides a new insight in the complexity of this process.

The purpose of this study was to develop a scale for measuring teachers' perceptions towards ICTs in teaching-learning process in the classroom.
The sample of the study consisted of volunteering Turkish teachers (nź=ź200).
This study developed a new scale for measuring teachers' perceptions towards ICTs in teaching-learning process.
In order to test the validity of the scale, the exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) were carried out in the research.
A result of the EFA, the scale consisted of three factors: attitude, usage, and belief with 25 items.
It was also seen that there were positive correlations amongst the three factors of the scale.
Cronbach's Alpha reliability coefficient value was found as 0.92 and Spearman-Brown split-half correlation value was found as 0.85 in the study.
It was seen that reliability coefficient values of the factors of in the scale ranged between 0.88 and 0.72 in the research.
Lastly, as a result of the CFA, it was understood that the obtained values (Δź2 (nź=ź200)/dfź=ź4.85/3; GFIź=ź0.96; AGFIź=ź0.94; RMSEAź=ź0.026; CFIź=ź0.97; TLIź=ź0.98) confirmed the three-factor structure of the scale.
In the literature, no scale similar to teachers' perception of ICT was found.Scale had a structure of three factors with 25 items.There were positive correlations amongst the factors.The scale was found to be a valid and reliable instrument.

Abstract Let ¢G = (G, ⊗, ⩽) be a linearly ordered, commutative group and ⊕ be defined by a ⊕ b = min(a, b) for all a, b ϵ G. Extend ⊕, ⊗ to matrices and vectors as in conventional linear algebra.
An n × n matrix A with columns A1,…,An is called regular if ∑ jϵU ⊕ λ j ⊗ A j = ∑ jϵV ⊕ λ j ⊗ A j does not hold for any λ1,…,λn ϵ G, σ ≠ U, V ⊆ {1, 2,…, n}, U ∩ V = σ.
We show that the problem of checking regularity is polynomially equivalent to the even cycle problem.
We also present two other types of regularity which can be checked in O(n3) operations.

The primary aim of any anti-collision protocols is to identify tags quickly, as doing so ensures that a Radio Frequency IDentification (RFID) reader incurs minimal energy wastage and achieves high identification rate.
To date, researchers have proposed various protocols to minimize tag collisions and idle slots-key factors that determine a reader's read rate and energy expenditure.
Most of these protocols, however, are designed for single reader systems.
To this end, we propose E^2MAC, an energy efficient, distributed Medium Access Control (MAC) protocol for identifying and monitoring tags in RFID-enhanced wireless sensor networks.
E^2MAC exploits the low power capability of a ultra-wideband transceiver and distinct pulses to address the reader collision problem.
In addition, it uses ResMon, an enhanced dynamic frame slotted Aloha protocol to read and monitor tags.
Lastly, E^2MAC uses a novel load balancing algorithm to amortize the cost of reading and monitoring tags to multiple readers.
These E^2MAC features ensure that the contention level at each reader is kept at a minimum and distributed fairly.
As a result, E^2MAC has a high reading rate and low energy consumption.
In addition, E^2MAC helps in minimizing the impact of the tag orientation problem, where a tag becomes unreadable if its antenna is parallel to a reader's field lines.
In particular, the use of multiple readers increases spatial diversity and hence increases the likelihood that a tag is readable by at least one reader.
Our simulation results show E^2MAC to have very low energy consumption, reading delay and per-reader collision.
More importantly, system designers have the flexibility to lower these metrics further with additional readers, bigger frame sizes, or by dividing tags into small groups.

ABSTRACT We present an interactive demonstration of histogram distance-based radio tomographic imaging (HD-RTI), a device-free localization (DFL) system that uses measurements of received signal strength (RSS) on static links in a wireless network to estimate the locations of people who do not participate in the system by wearing any radio device in the deployment area.
Compared to prior methods of RSS-based DFL, using a histogram difference metric is a very accurate method to quantify the change in RSS on the link compared to historical metrics.
The new method is remarkably accurate, and works with lower node densities than prior methods.

The ongoing wireless communication evolution offers improvements for industrial applications where traditional wireline solutions causes prohibitive problems in terms of cost and feasibility.
Many ...

In this paper we present statistical modeling of large scale data networks.
We explore network properties such as node degree distribution and clustering coefficient for analyzing different networks i.e.
directed unweighted, directed weighted, undirected weighted and undirected unweighted.
With thousands of data points, typical statistical graphics become cluttered and hard to read.
A common workaround is to subsample the data to produce a less cluttered view.
Considering the need of better interactive visualization of larger databases we are trying to develop an analyst-centered linearly scalable algorithm in MATLAB for statistical modeling and visualization of large scale big data network.
We are testing our scalable algorithm for real world large scale data networks.

Thank you very much for downloading the id casebook case studies in instructional design.
Maybe you have knowledge that, people have look hundreds times for their favorite books like this the id casebook case studies in instructional design, but end up in harmful downloads.
Rather than reading a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their computer.

We studied the effect of luminance and illuminance on visual fatigue and arousal.We employed an approach based on eye, performance and subjective measures.Higher levels of screen luminance increase visual fatigue.Higher levels of either screen luminance or ambient illuminance increase arousal.Findings might have practical implications for adaptive brightness solutions.
We investigated the conjoint effect of screen luminance and ambient illuminance on visual fatigue and arousal during prolonged digital reading (one hour) by means of a multidimensional approach based on eye, performance and subjective measures.
Two levels of screen luminance (low, high) and two levels of ambient illuminance (low, high) were tested in a 2×2 between-subjects design in which participants were arbitrarily allocated to four groups, one for each combined level of luminance and illuminance.
Results showed that reading under high levels of screen luminance increases visual fatigue, as reflected by a decrease of eye blinks.
Concerning arousal, exposure to higher levels of either luminance or illuminance increased alertness and performance.
Faster saccades, increased reading speed and less microsaccades were found under high screen luminance.
Fewer regressive saccades and shorter reaction times were observed under high ambient illuminance.
However, the reason why some of these measures are sensitive to screen luminance while other to ambient illuminance remains unknown.
These findings might have practical implications for the implementation of adaptive brightness solutions and for the online detection of both visual fatigue and arousal levels during digital reading.

The expressive power of branching time logics is studied in the framework of the theory of ω-automata and ω-languages.
The systems CTL* (computation tree logic) and ECTL* (extended computation tree logic) are characterized in terms of star-free, resp.
regular ω-languages.
A further characterization of CTL* by a "non-counting property" for sets of trees shows that it is decidable whether an ECTL*-formula can be written as a CTL*-formula.

This paper shows how primary decompositions of an ideal can give useful descriptions of components of a graph arising in problems from combinatorics, statistics, and operations research.
We begin this introduction with the general formulation.
Then we give the simplest interesting example of our theory, followed by a statistical example similar to that which provided our original motivation.
Later on we study the primary decompositions corresponding to some natural combinatorial problems.

To develop and evaluate active safety systems, reference scenarios need to be identified and analyzed.
In this research, a methodology for the deduction of reference scenarios and a simulation approach to evaluate the effectiveness of those systems is presented.
Using the example of preventive pedestrian safety, accident data of the United States and Germany were analyzed to identify representative scenarios.
Data of both countries show similar results and a combined set of scenarios was derived which could function as specifications for new systems, input parameters for simulation models and representative test scenarios for real world and simulator testing.

In order for an automatic information retrieval system to effectively retrieve documents related to a given subject area, the content of each document in the system''s database must be represented accurately.
This study examines the hypothesis that better representations of document content can be constructed if the content analysis method takes into consideration the syntactic structure of document and query texts.
Two methods of automatically generating phrases for use as content indicators have been implemented and tested experimentally.
The non-syntactic (or statistical) method is based on simple text characteristics such as word frequency and the proximity of words in text.
The syntactic method uses augmented phrase structure rules (production rules) to selectively extract phrases from parse trees generated by an automatic syntactic analyzer.
Experimental results show that the effect of non-syntactic phrase indexing is inconsistent.
For the five collections tested, increases in average precision ranged from 22.7% to 2.2% over simple, single term indexing.
The syntactic phrase indexing method was tested on two collections.
Precision figures averaged over all test queries indicate that non-syntactic phrase indexing performs significantly better than syntactic phrase indexing for one collection, but that the difference is insignificant for the other collection.
More detailed analysis of individual queries, however, indicates that the performance of both methods is highly variable, and that there is evidence that syntax-based indexing has certain benefits not available with the non-syntactic approach.
Possible improvements of both methods of phrase indexing are considered.
It is concluded that the prospects for improving the syntax-based approach to document indexing are better than for the non-syntactic approach.
The PLNLP system was used for syntactic analysis of document and query texts, and for implementing the syntax-based phrase construction rules.
The SMART information retrieval system was used for retrieval experimentation.

There are two kinds of uncertainty in science: structural and parametric uncertainty.
Parametric uncertainty means that we know the relevant factors and their interrelations for a given phenomenon, but miss the exact (initial) values of these factors.
Structural uncertainty means that we are unsure if we know all the relevant factors and their interdependencies.
Using this conceptual distinction it is clear that every simulation model for tactical wargaming is necessarily affected by massive structural uncertainty, since the "fog of war" has not lifted much since the days of Clausewitz.
The term "fog" seeks to capture the uncertainty regarding own capabilities, adversary capabilities and intents, as well as many other factors.
However, if war is seen that way, it is necessarily insufficient to base decisions on frameworks that only deal with parametric uncertainty.
The classical approach to make decisions, rational analysis, has therefore severe limitations for this application.
The major consequence of structural uncertainty for simulation supported decision making is that stochastic parameter variation and subsequent statistical analysis are inadequate as the sole basis for critical decisions.
The paper discusses this calamity and, as a solution, suggests tactical wargaming, a method based on assumption based planning, scenario planning, expert intuition and exploratory simulation.
This approach significantly differs from standard wargaming, which, in the author's view, is too focused on parametric uncertainty.

The supervised classification of fuzzy data obtained from a random experiment is discussed.
The data generation process is modelled through random fuzzy sets which, from a formal point of view, can be identified with certain function-valued random elements.
First, one of the most versatile discriminant approaches in the context of functional data analysis is adapted to the specific case of interest.
In this way, discriminant analysis based on nonparametric kernel density estimation is discussed.
In general, this criterion is shown not to be optimal and to require large sample sizes.
To avoid such inconveniences, a simpler approach which eludes the density estimation by considering conditional probabilities on certain balls is introduced.
The approaches are applied to two experiments; one concerning fuzzy perceptions and linguistic labels and another one concerning flood analysis.
The methods are tested against linear discriminant analysis and random K-fold cross validation.

In this letter, we propose a generic and efficient algorithm that can construct both asymmetrical and symmetrical reversible variable-length codes (RVLCs).
Starting from a given Huffman code, the construction is based on two developed codeword selection mechanisms, for the symmetrical case and the asymmetrical case, respectively; it is shown that the two mechanisms possess simple features and can generate efficient RVLCs easily.
In addition, two new asymmetrical RVLCs are constructed and shown to be very efficient for further reducing the coding overheads in MPEG-4 when operating in the reversible decoding mode.

From the Publisher: ::: Concurrent Systems answers the need for a book on concurrent programming which serves to integrate operating systems and database concepts, and provides a foundation for later courses in these areas.

We propose an evolutionary algorithm (EA) that applies to the capacitated vehicle routing problem (CVRP).
The EA uses edge assembly crossover (EAX) which was originally designed for the traveling salesman problem (TSP).
EAX can be straightforwardly extended to the CVRP if the constraint of the vehicle capacity is not considered.
To address the constraint violation, the penalty function method with 2-opt and Interchange neighborhoods is incorporated into the EA.
Moreover, a local search is also incorporated into the EA.
The experimental results demonstrate that the proposed EA can effectively find the best-known solutions on Christofides benchmark.
Moreover, our EA found ten new best solutions for Golden instances in a reasonable computation time.

The present research is motivated by the observation that if the period T of a certain binary sequence is a prime, then its linear complexity will be bounded from below by the order of 2 modulo T, i.e., LC⩾OrdT(2).
A class of generators with state periods T(q, n)=q·2n−1 are constructed for q=3, 5, 7, 9 and arbitrary n on the basis of a pair of m-sequence generators with the same number of stages, each controlling the clock of the other (bilateral stop-and-go clock control).
A new test is derived to find the primes among the numbers T(q, n) with the cases 3 | q and 3 | q treated in a unified manner.
The orders of 2 modulo some of the primes T(q, n) are given and some additional cryptographic and implementational remarks are made.

This paper shares experiences of the authors with combining Multiple Criteria Decision Making (MCDM) and techniques from soft systems approaches for decision support at particular stages of complex problem solving.
These are based on their involvement in three projects within the Information and Communications Technology sector.
The justification for the combined use of MCDM and separate techniques from systems thinking is discussed.
The contribution of this paper is in the demonstration of how combinations of methods that are parts of soft systems methodologies and MCDM may support multiple perspective representations of complex managerial problems, and in the lessons learned from the three cases.

The prediction of motion for use in engineering design must rely both on empirical and theoretical methods.
The methods complement on another, for the empirical analyses provide checks on the theory and help to determine the parameters necessary in the theoretical predictions.
In this paper, I will give short discussions of both the empirical and theoretical methods for predicting strong motion that I and my colleagues have been doing over the last several years.

The crosstalk phenomenon, wich occurs between transmission lines, is caused by electromagnetic fields of currents flowing through the lines.
Crosstalk between two bent lines is studied by using a set of solutions of modified telegrapher's equations.
By expressing electromagnetic fields in terms of voltages and currents in the line ends, the resultant network function in the form of an ABCD matrix is obtained.
Electromagnetic fields caused by currents flowing in risers at transmission line ends are taken into account in addition to those fields in line sections.
The validity of the proposed approach was confirmed by comparing experimental results with computed results and those simulated by a commercial electromagnetic solver for some bent-line models.

Rules in active database systems can be very difficult to program due to the unstructured and unpredictable nature of rule processing.
We provide static analysis techniques for predicting whether a given rule set is guaranteed to terminate and whether rule execution is confluent (guaranteed to have a unique final state).
Our methods are based on previous techniques for analyzing rules in active database systems.
We improve considerably on the previous techniques by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not make this determination.
Our improved analysis is based on a “propagation” algorithm, which uses an extended relational algebra to accurately determine when the action of one rule can affect the condition of another, and determine when rule actions commute.
We consider both conditon-action rules and event-condition-action-rules, making our approach widely applicable to relational active database rule languages and to the trigger language in the SQL:1999 standard.

The dimensionally uniform Jacobian matrix of a novel three-limbed, six degree-of-freedom (DOF) minimanipulator is used to derive its dimensionally uniform stiffness matrix.
The minimanipulator limbs are inextensible and its actuators are base-mounted.
The lower ends of the limbs are connected to bidirectional linear stepper motors that are constrained to move on a base plane.
The minimanipulator is capable of providing high positional resolution and high stiffness.
It is shown that, at a central configuration, the stiffness matrix of the minimanipulator can be decoupled (diagona-lized), if proper design parameters are chosen.
It is also shown that the stiffness of the minimanipulator is higher than that of the Stewart platform.
Guidelines for obtaining large minimanipulator stiffness values are established.
© 1995 John Wiley & Sons, Inc.

Abstract The shorter term beat-to-beat heart rate data collected from the general population are often interrupted by artifacts, and an arbitrary exclusion of such individuals from analysis may significantly reduce the sample size and/or introduce selection bias.
A computer algorithm was developed to label as artifacts any data points outside the upper and lower limits generated by a 5-beat moving average ±25% (or set manually by an operator using a mouse) and to impute beat-to-beat heart rate throughout an artifact period to preserve the timing relationships of the adjacent, uncorrupted heart rate data.
The algorithm applies Fast Fourier Transformation to the smoothed data to estimate low-frequency (LF; 0.025–0.15 Hz) and high-frequency (HF; 0.16–0.35 Hz) spectral powers and the HF/LF ratio as conventional indices of sympathetic, vagal, and vagal–sympathetic balance components, respectively.
We applied this algorithm to resting, supine, 2-min beat-to-beat heart rate data collected in the population-based Atherosclerosis Risk in Communities study to assess the performance (success rate) of the algorithm ( N = 526) and the inter- and intra-data-operator repeatability of using this computer algorithm ( N = 108).
Eighty-eight percent (88%) of the records could be smoothed by the computer-generated limits, an additional 4.8% by manually set limits, and 7.4% of the data could not be processed due to a large number of artifacts in the beginning or the end of the records.
For the repeatability study, 108 records were selected at random, and two trained data operators applied this algorithm to the same records twice within a 6-month interval of each process (blinded to each other's results and their own prior results).
The inter-data-operator reliability coefficients were 0.86, 0.92, and 0.90 for the HF, LF, and HF/LF components, respectively.
The average intra-data-operator reliability coefficients were 0.99, 0.99, and 0.98 for the HF, LF, and HF/LF components, respectively.
These results indicate that this computer algorithm is efficient and highly repeatable in processing short-term beat-to-beat heart rate data collected from the general population, given that the data operators are trained according to standardized protocol.

Abstract A framework for constructing a cognitive model of users' information searching behaviour is described.
The motivation for the framework is to create explanatory and predictive theories of information searching to improve the design of information retrieval (IR) systems.
The framework proposes a taxonomy of components for process models of the information seeking task, information need types and knowledge sources necessary to support the task.
The framework is developed into a preliminary version of a cognitive theory of information searching by the addition of strategies and correspondence rules which predict user behaviour in different task stages according to information need types, facilities provided by the IR system and knowledge held by the user.
The theory is evaluated by using claims analysis based on empirical observations of users information retrieval and by a walkthrough of an IR session to investigate how well the theory can account for empirical evidence.
Results show that the theory can indicate the expert strategies which should be followed in different task contexts but predictions of actual user behaviour are less accurate.
The future possibilities for employing the theoretical model as a tutorial advisor for information retrieval and as an evaluation method for IR systems are reviewed.
The role and potential of cognitive theories of user task-action in Information Retrieval and Human Computer Interaction are discussed.

This article reports on an experiment that examines the influence of message framing and anchor points, and the joint effect of these two information cues on Internet consumers' judgments regarding attitude, purchase intention and willingness to pay.
The role of participants' subjective knowledge is also evaluated.
The experimental results suggest that message framing, which describes a product's attribute in positive or negative terms, significantly influences participants' attitude toward and their intention to buy the product.
In addition, participants' willingness to pay was significantly influenced by the presentation of anchors embedded in banner advertisements.
Further, a significant interaction effect for message framing and anchor points indicate that their congruence enhances the effects of information presentation on people's responses.
Specifically, describing a product attribute in positive terms along with a high anchor point induces more favorable response than any other framing and anchoring combinations.
Finally, online shoppers who are low in product knowledge are more susceptible to framing and anchoring influences.
The findings provide guidance for designing appropriate product and price cues to induce Internet consumer responses that favor online retailers.

Discusses the development of a hybrid classifier of SDSs (short duration signals) in the underwater acoustic environment.
The classifier is envisioned to include both neural-based and non-neural networks.
The authors present a comparison of performance of two neural-based and two non-neural-based classifiers for four signal extractors.
It was found that for DARPA Data Set I, no single classifier is superior to the others; however, a hybrid set of classifiers yields the best performance.
The neural network classifiers are based on RBFs (radial basis functions) and the MLP (multilayer perceptron) trained with backpropagation.
The classical classification techniques of k-nearest neighbor and the Fisher linear discriminant are used for comparison.
The signal extraction methods used include two versions of wavelets, autoregressive spectral coefficients and a linear combination of a wavelet with an autoregressive representation.
>

The author traces developments in computer art worldwide from 1965, when the first computer art exhibitions were held by scientists, through succeeding periods in which artists collaborated with scientists to create computer programs for artistic purposes.
The end of the first decade of computer art was marked by economic, technological and programming advances that allowed artists more direct access to computers, high quality images and virtually unlimited color choices.

Ant Colony Optimization (ACO) Algorithm is a novel search algorithm which simulates the social behavior of ant colony depending on pheromone′s communication.Based on the analysis of shortcomings of basic ACO such as lack and lag of collaboration among ants,this paper proposes a new ACO which is more faithful to real ant colony system.By setting up the pheromone diffusion model,this algorithm improves the collaboration among ants which are nearby.The simulation results for TSP problem show the validity of it.

The purpose of this paper is to obtain the distribution of the number of lost packets within a sequence of n consecutive packet arrivals into a finite buffer M/M/1 queue.
We obtain explicit expressions for the multidimensional generating function of these probabilities based on a recursive scheme introduced by Cidon et al.
(1993).
We then analyze the loss probabilities of a whole message, and analyze the effect of adding redundant packets.
We show that in both heavy traffic as well as in light traffic conditions, adding redundant packets results in decreasing the message loss probabilities.

Traditional approaches for research project selection by government funding agencies mainly focus on the matching of research relevance by keywords or disciplines.
Other research relevant information such as social connections (e.g., collaboration and co-authorship) and productivity (e.g., quality, quantity, and citations of published journal articles) of researchers is largely ignored.
To overcome these limitations, this paper proposes a social network-empowered research analytics framework (RAF) for research project selections.
Scholarmate.com, a professional research social network with easy access to research relevant information, serves as a platform to build researcher profiles from three dimensions, i.e., relevance, productivity and connectivity.
Building upon profiles of both proposals and researchers, we develop a unique matching algorithm to assist decision makers (e.g.
panel chairs or division managers) in optimizing the assignment of reviewers to research project proposals.
The proposed framework is implemented and tested by the largest government funding agency in China to aid the grant proposal evaluation process.
The new system generated significant economic benefits including great cost savings and quality improvement in the proposal evaluation process.

ABSTRACTSeveral probability distributions have been proposed in the literature, especially with the aim of obtaining models that are more flexible relative to the behaviors of the density and hazard rate functions.
Recently, a new generalization of the Lindley distribution was proposed by Ghitany et al.
(2013), called power Lindley distribution.
Another generalization was proposed by Sharma et al.
(2015a), known as inverse Lindley distribution.
In this paper a distribution is obtained from these two generalizations and named as inverse power Lindley distribution.
Some properties of this distribution and study of the behavior of maximum likelihood estimators are presented and discussed.
It is also applied considering two real data sets and compared with the fits obtained for already-known distributions.
When applied, the inverse power Lindley distribution was found to be a good alternative for modeling survival data.

This paper explores notions and rationales of gift exchange among participants of the social networking site ‘LiveJournal.’ While gift exchanges can be framed as a form of power relations, this paper argues that they also have the potential to function as a way of forming and maintaining social bonds, and of maintaining individual and collective identity within the virtual social space.

An operator for the composition of two processes, where one process has priority over the other process, is studied.
Processes are described by action systems, and data refinement is used for transforming processes.
The operator is shown to be compositional, i.e.
monotonic with respect to refinement.
It is argued that this operator is adequate for modelling priorities as found in programming languages and operating systems.
Rules for introducing priorities and for raising and lowering priorities of processes are given.
Dynamic priorities are modelled with special priority variables which can be freely mixed with other variables and the prioritising operator in program development.
A number of applications show the use of prioritising composition for modelling and specification in general.

Previous work shows that, as the buyer's uncertainty about the quality of some good or service increases, so does the tendency to purchase that good or service via embedded transactions, rather than from strangers.
While this previous work explains variation in embedded exchange across different types of purchases, it does not address variation in embedded exchange across persons.
Our research integrates the embeddedness and trust literatures to explain variation in within-network exchanges based on an interaction of the purchaser's generalized trust and the level of uncertainty entailed in the purchase (i.e., whether there exists an incentive for the seller to misrepresent the quality of some good or service).
For purchases involving uncertainty, low-trusters will tend to forgo risky transactions with strangers, opting instead for the increased certainty of embedded markets.
High-trusters, on the other hand, will be more likely to transact with strangers (despite the increased risk), from whom they can often find better deals.
We should not expect any differences between high- and low-trusters for products that do not entail uncertainty.
Results from two data sources, responses from a nationally representative survey of the U.S. population and behavioral responses in new laboratory experiments, provide support for the arguments.

Abstract A dominating set in a graph G = ( V , E ) is a set of vertices D such that every vertex in V − D removal results in a disconnected graph.
A block in a graph G is a maximal connected subgraph of G having no cutvertices.
A cactus is a graph in which each block is either an edge or a cycle.
In this paper we present a linear time algorithm for finding a minimum order dominating set in a cactus.

One-class classification, known also as learning in the absence of counterexamples, is one of the most challenging problems in the contemporary machine learning.
The scope of the paper focuses on creating a one-class multiple classifier systems with diverse classifiers in the pool.
An approach is proposed in which an ensemble of one-class classifiers, instead of a single one, is used for the target class recognition.
The paper introduces diversity measures dedicated to the selection of such specific classifiers for the committee.
Therefore the influence of heterogeneity assurance on the overall classification performance is examined.
Experimental investigations prove that diversity measures for one-class classifiers are a promising research direction.
Additionally the paper investigates the lack of benchmark datasets for one-class problems and proposes an unified approach for training and testing one-class classifiers on publicly available multi-class datasets.

The aim of this paper is to report for the first time the 1000 most common words and lemmas of Modern Greek and some of their quantitative characteristics.
The frequency word list produced is based on the Hellenic National Corpus (HNC), a corpus of Modern Greek language consisting of about 13 million words of written texts.
In particular, we investigate the application of Zipf’s law in both the 1000 most common words and lemmas.
In addition we examine the frequency distribution of the grammatical categories in the 1000 most common words and lemmas as well as the average word length in the whole HNC and the growth of the average word length as a function of the number of the most common words.

Transporting hybrid coded video over wireless channel is very challenging.
On the one hand, wireless link is much more error prone than wired network due to time varying channel conditions such as fading and multipath interference.
On the other hand, hybrid coded video is very vulnerable to error propagation when transmitted over error prone channels.
Numerous researches have been conducted to enhance error robustness for wireless video transmission.
Among them, many schemes use retransmission to reduce packet loss rate and improve reconstructed video quality.
However, retransmission is delay constrained due to the low delay nature of real-time video.
Packet loss is inevitable even when retransmission is employed.
In the proposed architecture, a novel error recovery scheme is introduced which switches adaptively between ACK and NACK modes according to channel conditions.
Video proxy server at the base station is designed to make the retransmission and feedback based error recovery method more effective.
State-of-the-art H.264 is used as video encoder since it provides not only high coding efficiency but also multi-frame which plays a key role in the framework.
Simulation results demonstrate the effectiveness of this architecture.

We consider Maxwell's equations together with a nonlinear dissipative magnetic law described by the Landau-Lifshitz-Gilbert equation.
The paper is devoted to the numerical study of this problem.
We introduce two algorithms for the time discretization.
The modulus of magnetization is conserved in both cases.
Assuming a sufficiently smooth electromagnetic field, we derive the error estimates for both approximation schemes.

Multi-view video coding (MVC) uses various prediction modes and exhaustive mode decision to achieve high coding efficiency.
However, the introduced heavy computational complexity becomes the bottleneck of the practical application of MVC.
For this, an efficient early Direct mode decision for MVC is proposed in this paper.
Based on the observation that the Direct mode is highly possible to be the optimal mode, the proposed method first computes the rate distortion (RD) cost of the Direct mode and compares this RD cost value with an adaptive threshold for providing an early termination chance as follows.
If this RD cost value is smaller than the adaptive threshold, the Direct mode will be selected as the optimal mode and the checking process of the remaining modes will be skipped; otherwise, all the modes will be checked to select the one with the minimum RD cost as the optimal mode.
Note that the above-mentioned adaptive threshold is determined as the median prediction value of a set of thresholds, which are derived by using the spatial, temporal and inter-view correlations between the current macroblock (MB) and its neighboring MBs, respectively.
Experimental results have demonstrated that the proposed method is able to significantly reduce the computational complexity of MVC with negligible loss of coding efficiency, compared with the exhaustive mode decision in MVC.

This paper presents a robust digital watermarking scheme for copyright protection of digital images.
Four subimages are obtained from the host image by using subsampling.
A binary watermark image is permutated using chaotic map and then is embedded into the DCT coefficients of two of the subimages which are selected according to a random coordinates sequence generated by a secret key.
Watermark extraction is simply by comparing the DCT coefficients of the watermarked subimages and does not need original image.
Experimental results show good robustness and security of the proposed scheme.

A fully integrated 0.18- $$\upmu \hbox {m}$$ μ m CMOS LC-tank voltage-controlled oscillator (VCO) suitable for low-voltage and low-power S-band wireless applications is proposed in this paper.
In order to meet the requirement of low voltage applications, a differential configuration with two cross-coupled pairs by adopting admittance-transforming technique is employed.
By using forward-body-biased metal oxide semiconductor field effect transistors, the proposed VCO can operate at 0.4 V supply voltage.
Despite the low power supply near threshold voltage, the VCO achieves wide tuning range by using a voltage-boosting circuit and the standard mode PMOS varactors in the proposed oscillator architecture.
The simulation results show that the proposed VCO achieves phase noise of $$-$$ - 120.1 dBc/Hz at 1 MHz offset and 39.3 % tuning range while consuming only $$594~\upmu \hbox {W}$$ 594 μ W in 0.4 V supply.
Figure-of-merit with tuning range of the proposed VCO is $$-$$ - 192.1 dB at 3 GHz.

It is well known that the expected maximum length of nonzero carry propagation in the addition of two uniformly distributed binary numbers of n-digits each is less than log2 n. The propagation of both zero and nonzero carry is required in the employment of asynchronous self-timing addition.
For the addition of two n-digit binary numbers which are uniformly distributed, a simple recursive algorithm is readily derived for the exact determination of the expected maximum length of zero or nonzero carry propagation.

In this paper the recursive instrumental variable-approximate maximum likelihood (IV-AML) method of time-series analysis for single-input, single-output systems is extended to the characterization of multivariable (multi-input, multi-output systems) using techniques of statistical feature selection to circumvent, in an approximate fashion, some of the problems of characterizing multi-variable stochastic disturbances.
A method of dynamic system structure identification and parameter estimation based on this multi-variable IV-AML procedure is also outlined and applied to the problem of modelling the dynamic relationship between biochemical oxygen demand (BOD) and dissolved oxygen (DO) in a non-tidal river system on the basis of daily field data collected over an extended period of time.

Abstract In the design of computer-aided diagnosis systems for lung cancer diagnosis, an appropriate and accurate segmentation of the pulmonary nodules in computerized tomography (CT) is one of the most relevant and difficult tasks.
An accurate segmentation is crucial for the posterior measurement of nodule characteristics and for lung cancer diagnosis.
This paper proposes different approaches that use Hessian-based strategies for lung nodule segmentation in chest CT scans.
We propose a multiscale segmentation process that uses the central medialness adaptive principle, a Hessian-based strategy that was originally formulated for tubular extraction but it also provides good segmentation results in blob-like structures as is the case of lung nodules.
We compared this proposal with a well established Hessian-based strategy that calculates the Shape Index (SI) and Curvedness (CV).
We adapted the SI and CV approach for multiscale nodule segmentation.
Moreover, we propose the combination of both strategies by combining the results, in order to take benefit of the advantages of both strategies.
Different cases with pulmonary nodules from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) database were taken and used to analyze and validate the approaches.
The chest CT images present a large variability in nodule characteristics and image conditions.
Our proposals provide an accurate lung nodule segmentation, similar to radiologists performance.
Our Hessian-based approaches were validated with 569 solid and mostly solid nodules demonstrating that these novel strategies have good results when compared with the radiologists segmentations, providing accurate pulmonary nodule volumes for posterior characterization and appropriate diagnosis.

GNSS satellite signal deformations cause individual biases in the pseudorange observable.
These biases are typically mitigated by using external correction values.
However, the biases are the result of the correlator’s tracking response and thus depend on correlator and front-end design of the receiver.
As a result, different receivers are likely to exhibit inconsistent biases, for which the available corrections may not be applicable.
The residual errors lead to adverse effects for pseudorange-based positioning applications and can also hinder carrier-phase ambiguity resolution.
Bias inconsistencies of modern GNSS receivers are studied in a zero-baseline test.
Differential pseudorange biases between receivers are computed for GPS, GLONASS, Galileo and BeiDou satellites for all commonly tracked signals.
The effects on pseudorange-based positioning applications and carrier-phase ambiguity resolution are discussed.

Hardware implementation of programmable neural networks requires multiplications of input analogue voltages and constant values.
In the Letter a very efficient implementation of cellular neural networks with digital templates is presented.

Abstract A construction algorithm which considers some practical aspects of the machine layout problem is presented.
The methodology considers machine dimensions and their input and output locations with flow data.
The layout is generated on a continuum, considering the best configuration and orientation of input and output locations of machines to minimize the material flow cost.
The location procedure searches along the boundary of existing machines.
A bi-criterion optimization approach is proposed to overcome one of the weaknesses inherited by construction procedures.
The suggested construction procedure is implemented on a microcomputer and applied to solve some test problems with encouraging results.

The objective of this article is to present a didactic example of Structural Equation Modeling using the software SmartPLS 2.0 M3.
The program mentioned uses the method of Partial Least Squares and seeks to address the following situations frequently observed in marketing research: Absence of symmetric distributions of variables measured by a theory still in its beginning phase or with little “consolidation”, formative models, and/or a limited amount of data.
The growing use of SmartPLS has demonstrated its robustness and the applicability of the model in the areas that are being studied.

Experienced and novice programmers often use program examples they have created or learned in the past to solve new programming tasks.
Experienced teachers of programming-related courses know and use the power of example in their teaching.
This paper presents WebEx a Webbased tool for exploring programming examples that enables teachers to use example-based programming approach with heterogeneous classes.
WebEx maximizes learning opportunity for every student, gives every student a second chance in understanding key programming examples, and encourages every student to explore example programs.

The recent proliferation of computer networks has stimulated the emergence of thousands of online communities.
Facebook, which has grown to 175-million users in five years and recently surpassed megasite MySpace to become the world's largest social networking site, is a classic example.
As the importance of online communities continues to grow, a good understanding of their success factors for building and sustaining a community becomes crucial.
In this article, we apply social capital theories to examine the interactions among individuals and trust building at the initial development of an online community.
Specifically, we postulate that offline social capital can be transplanted into an online community (small or large) to foster the development of trust and social norms that make a community thrive.
We conduct two experimental studies: one in the context of real-world, small-scale online communities, and the other in the context of computer-simulated large-scale online communities.
Results from these ...

The rapid advance of technology has permitted the creation of vast amounts of information, both on and off the Internet.
The public is only just beginning to realize how this information, especially personal information, may be used in ways that may not be acceptable.
Laws across different countries are often conflicting, making it difficult to control how personal information is being used and how individual privacy is being violated.
The solution to this problem lies somewhere between government, industry, and the individual.
This paper discusses the current state of personal privacy in each of these three areas.

Chaotic PSO algorithm is proposed to solve NP-hard IPPS problem.Ten chaotic maps are implemented to avoid premature convergence to local optimum.Makespan, balanced level of machine utilization and mean flow time are observed.Five experimental studies show that cPSO outperforms GA, SA, and hybrid algorithm.Scheduling plans are tested by mobile robot within a laboratory environment.
Process planning and scheduling are two of the most important manufacturing functions traditionally performed separately and sequentially.
These functions being complementary and interrelated, their integration is essential for the optimal utilization of manufacturing resources.
Such integration is also significant for improving the performance of the modern manufacturing system.
A variety of alternative manufacturing resources (machine tools, cutting tools, tool access directions, etc.)
causes integrated process planning and scheduling (IPPS) problem to be strongly NP-hard (non deterministic polynomial) in terms of combinatorial optimization.
Therefore, an optimal solution for the problem is searched in a vast search space.
In order to explore the search space comprehensively and avoid being trapped into local optima, this paper focuses on using the method based on the particle swarm optimization algorithm and chaos theory (cPSO).
The initial solutions for the IPPS problem are presented in the form of the particles of cPSO algorithm.
The particle encoding/decoding scheme is also proposed in this paper.
Flexible process and scheduling plans are presented using AND/OR network and five flexibility types: machine, tool, tool access direction (TAD), process, and sequence flexibility.
Optimal process plans are obtained by multi-objective optimization of production time and production cost.
On the other hand, optimal scheduling plans are generated based on three objective functions: makespan, balanced level of machine utilization, and mean flow time.
The proposed cPSO algorithm is implemented in Matlab environment and verified extensively using five experimental studies.
The experimental results show that the proposed algorithm outperforms genetic algorithm (GA), simulated annealing (SA) based approach, and hybrid algorithm.
Moreover, the scheduling plans obtained by the proposed methodology are additionally tested by Khepera II mobile robot using a laboratory model of manufacturing environment.

Two probabilistic hit-and-run algorithms are presented to detect nonredundant constraints in a full dimensional system of linear inequalities.
The algorithms proceed by generating a random sequence of interior points whose limiting distribution is uniform, and by searching for a nonredundant constraint in the direction of a random vector from each point in the sequence.
In the hypersphere directions algorithm the direction vector is drawn from a uniform distribution on a hypersphere.
In the computationally superior coordinate directions algorithm a search is carried out along one of the coordinate vectors.
The algorithms are terminated through the use of a Bayesian stopping rule.
Computational experience with the algorithms and the stopping rule will be reported.

In this paper, we develop a parallel-structured real-coded genetic algorithm (RCGA), named the RGA-RDD, for numerical optimization.
Technically, the proposed RGA-RDD integrates three specially designed evolutionary operators - the Ranking Selection (RS), Direction-Based Crossover (DBX), and the Dynamic Random Mutation (DRM) - as a whole to mimic a specific evolutionary process.
Unlike the conventional RCGAs that perform evolutionary operators in a series framework, the RGA-RDD embeds a coordinator in the inner parallel loop to organize the operations of the DBX and DRM so that a higher possibility of locating the global optimum is ensured.
Besides, based on the results of a systematic parametric analysis, we provide a parameter selection guideline for the settings of the proposed RGA-RDD.
Furthermore, a data-driven optimization scheme, which incorporates the uniform design for design of experiments and a shape-tunable neural network for auxiliary decision support, is applied to search for an optimal set of the algorithm parameters.
The effectiveness and applicability of the proposed RGA-RDD are demonstrated through a variety of benchmarked optimization problems, followed by comprehensive comparisons with some existing state-of-the-art evolutionary algorithms.
Extensive simulation results reveal that the performance of the proposed RGA-RDD is superior to comparative methods in locating the global optimum for real-parameter optimization problems, especially for unsolved multimodal and high-dimensional hybrid functions.

This document describes an extension to Generalized MPLS (Multi- ::: Protocol Label Switching) signaling to support communication of alarm ::: information.
GMPLS signaling already supports the control of alarm ::: reporting, but not the communication of alarm information.
This ::: document presents both a functional description and GMPLS-RSVP ::: specifics of such an extension.
This document also proposes ::: modification of the RSVP ERROR_SPEC object.
This document updates RFC ::: 3473, "Generalized Multi-Protocol Label Switching (GMPLS) Signaling ::: Resource ReserVation Protocol-Traffic Engineering (RSVP-TE) ::: Extensions", through the addition of new, optional protocol elements.
::: It does not change, and is fully backward compatible with, the ::: procedures specified in RFC 3473.
[STANDARDS-TRACK]

Abstract.
There is a growing need for high-resolution land surface parameters as land surface models are being applied at increasingly higher spatial resolution offline as well as in regional and global models.
The default land surface parameters for the most recent version of the Community Land Model (i.e.
CLM 4.0) are at 0.5° or coarser resolutions, released with the Community Earth System Model (CESM).
Plant Functional Types (PFTs), vegetation properties such as Leaf Area Index (LAI), Stem Area Index (SAI), and non-vegetated land covers were developed using remotely sensed datasets retrieved in late 1990's and the beginning of this century.
In this study, we developed new land surface parameters for CLM 4.0, specifically PFTs, LAI, SAI and non-vegetated land cover composition, at 0.05° resolution globally based on the most recent MODIS land cover and improved MODIS LAI products.
Compared to the current CLM 4.0 parameters, the new parameters produced a decreased coverage by bare soil and trees, but an increased coverage by shrub, grass, and cropland.
The new parameters result in a decrease in global seasonal LAI, with the biggest decrease in boreal forests; however, the new parameters also show a large increase in LAI in tropical forest.
Differences between the new and the current parameters are mainly caused by changes in the sources of remotely sensed data and the representation of land cover in the source data.
Advantages and disadvantages of each dataset were discussed in order to provide guidance on the use of the data.
The new high-resolution land surface parameters have been used in a coupled land-atmosphere model (WRF-CLM) applied to the western US to demonstrate their use in high-resolution modeling.
A remapping method from the latitude/longitude grid of the CLM data to the WRF grids with map projection was also demonstrated.
Future work will include global offline CLM simulations to examine the impacts of source data resolution and subsequent land parameter changes on simulated land surface processes.

Technology in the field of digital media generates huge amounts of non-textual information, audio, video, and images, along with more familiar textual information.
The potential for exchange and retrieval of information is vast and daunting.
The key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall).
The traditional solution employs keyword-based search.
The only documents retrieved are those containing user specified keywords.
But many documents convey desired semantic information without containing these keywords.
This limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms.
Recall is increased, but at the expense of deteriorating precision.
::: One can overcome this problem by indexing documents according to meanings rather than words, although this will entail a way of converting words to meanings and the creation of an index structure.
We have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies.
An ontology is a collection of concepts and their interrelationships, which provide an abstract view of an application domain.
With regard to the converting words to meaning the key issue is to identify appropriate concepts that both describes and identifies documents, as well as language employed in user requests.
This dissertation describes an automatic mechanism for selecting these concepts.
An important novelty is a scalable disambiguation algorithm which prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation.
We also propose an automatic query expansion mechanism that deals with user requests expressed in natural language.
This mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form.
::: Focusing on audio data, we have constructed a demonstration prototype.
We have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall.
The techniques employed can be applied to the problem of information selection in all media types.

This paper examines the use of coarse-grained multithreading to lessen the negative impact of memory access latencies on the performance of uniprocessor on-line transaction processing systems.
It considers the effect of switching threads on cache misses in a two-level cache system.
It also examines several different thread-switch policies.
The results suggest that multithreading with a small number (3–5) of active threads can significantly improve the performance of such commercial environments.

Abstract The paper is part of a research effort that focuses on the provision of more efficient and effective design methods for broadcast modelling systems.
It presents an interactive deformation technique called AxDf (Axial Deformations).
Based on the paradigm of the modelling tool, the axial-deformations technique allows deformations, such as bending, scaling, twisting and stretching, that can be controlled with a 3D axis to be easily specified.
Moreover, AxDf can easily be combined with other existing deformation techniques.

In this paper we discuss cone-based hypervolume indicators (CHI) that generalize the classical hypervolume indicator (HI) in Pareto optimization.
A family of polyhedral cones with scalable opening angle γ is studied.
These γ-cones can be efficiently constructed and have a number of favorable properties.
It is shown that for γ-cones dominance can be checked efficiently and the CHI computation can be reduced to the computation of the HI in linear time with respect to the number of points μ in an approximation set.
Besides, individual contributions to these can be computed using a similar transformation to the case of Pareto dominance cones.

Abstract In this paper the projection hybrid FV/FE method presented in [1] is extended to account for species transport equations.
Furthermore, turbulent regimes are also considered thanks to the k–e model.
Regarding the transport diffusion stage new schemes of high order of accuracy are developed.
The CVC Kolgan-type scheme and ADER methodology are extended to 3D.
The latter is modified in order to profit from the dual mesh employed by the projection algorithm and the derivatives involved in the diffusion term are discretized using a Galerkin approach.
The accuracy and stability analysis of the new method are carried out for the advection–diffusion–reaction equation.
Within the projection stage the pressure correction is computed by a piecewise linear finite element method.
Numerical results are presented, aimed at verifying the formal order of accuracy of the scheme and to assess the performance of the method on several realistic test problems.

A fast algorithm is presented for training multilayer perceptrons as an alternative to the backpropagation algorithm.
The number of iterations required by the new algorithm to converge is less than 20% of what is required by the backpropagation algorithm.
Also, it is less affected by the choice of initial weights and setup parameters.
The algorithm uses a modified form of the backpropagation algorithm to minimize the mean-squared error between the desired and actual outputs with respect to the inputs to the nonlinearities.
This is in contrast to the standard algorithm which minimizes the mean-squared error with respect to the weights.
Error signals, generated by the modified backpropagation algorithm, are used to estimate the inputs to the nonlinearities, which along with the input vectors to the respective nodes, are used to produce an updated set of weights through a system of linear equations at each node.
These systems of linear equations are solved using a Kalman filter at each layer.
>

This paper proposes a modelling approach suitable for formalizing fault tolerant systems, taking into account different fault scenarios.
Verification of the properties of such systems is then performed using model checking.
A general framework for the formal specification and verification of fault tolerant systems is defined starting from these principles, and experience with its application to two case studies is then presented.
Copyright © 2002 John Wiley & Sons, Ltd.

In this paper a VC-1 decoder Single Instruction Multiple Data (SIMD) implementation on a multi-core architecture is presented.
The STMicroelectronics P2012 platform with its development tools is used.
Our aim is to address High Definition (1920x1080) Advanced Profile VC-1 video streams in real time.
Several solutions taken to increase decoder performances through a better exploitation of the Data Level Parallelism (DLP) will be shown.
P2012 is a project in development, but some result achieved with the platform simulator will be shown.

Professional development is imperative for the currency and relevancy of a proficient teaching workforce in distance education, and in turn, the quality of programs being delivered.
As participation in distance education within Australian universities is growing, with increasing numbers of academics being required to teach, casual employment of academic staff is often undertaken.
As a consequence, casual academics now undertake significant amounts of university teaching loads.
In addition, although employed on a casual basis, casual academics often have ongoing relationships with the university, spanning more than one teaching period and in some cases, even years.
Professional development is crucial to ensure the replenishment and vivacity of the university, its academics and students.
Consequently, the goal of this constructivist grounded theoretical study was to explore the experiences of casual distance education academics, from which professional development emerged as a key theme.
Twelve casual acade...

Three important factors that determine user performance during database retrieval are representation realism, expressive ease, and task complexity.
Representation realism is the level of abstraction used when formulating queries.
Expressive ease is the syntactic flexibility permitted when formulating queries.
Task complexity is the level of difficulty of queries.
A controlled laboratory experiment was conducted to assess the effects of these three factors on user productivity during database retrieval.
The independent variables were representation realism (high versus low), expressive ease (high versus low), and query complexity (simple versus complex).
The dependent variables were query accuracy and query time.
Results show that all these three factors significantly affected user performance during database retrieval.
However, their relative impact on query accuracy and query time differed.
Moreover, these factors interacted in unique ways to moderate query accuracy and query time.
Besides verifying prior empirical findings, these results offer several suggestions for future research and development work in the area of database retrieval.

During December 1993 a microwave radiometer belonging to the Central Aerological Observatory (CAO) Moscow was operated at the Meteorological Research Unit, Cardington, UK.
The radiometer has a single channel with a central frequency of 61.0 GHz, bandwidth of 2 GHz and sensitivity 0.04 K for a 1 s integration time.
The antenna (beam-width 6°) was scanned from nadir to horizontal in 9° steps.
Calibration was done every scan cycle using an electrically switched load.
The information potentially available from such a radiometer is discussed.
The measurements made at Cardington are compared with in situ observations made by radiosondes and by the Meteorological Office's tethered balloon system.
Microwave brightness temperatures calculated from the radiosonde ascents are compared with observations.
These were found to agree with a mean difference of 0.1 K and with a standard deviation of <0.3 K. Similarly retrieved temperature profiles up to 500 m were found to agree with the radiosonde profiles within 1 K rms.
During the comparison period there were no strong low-level inversions to test fully the radiometer's retrieval capability.
Copyright © 1998 Royal Meteorological Society

Abstract : This report is concerned with issues of man-machine interaction in decision support system for high-level decision makers.
It discusses components that such systems should have, what the current state of the art is with respect to such systems, and how current research in artificial intelligence is leading toward solving the remaining problems.
Topics covered include natural language syntax and semantics, models of the beliefs and goals of the user, and knowledge-based helpful systems.
(Author)

Abstract In this paper we construct a resolvable ( k ( k + 1), k , k −1)-BIBD for each k ⩾3, k a prime or prime power.
For k ⩾ 5 such a design was previously unknown.
In fact, for these parameters we show that there are at least k − 1 nonisomorphic resolutions of the constructed design and that a pair of orthogonal resolutions exist.

We present a method to derive a solution to the combined frame and ramification problems for certain classes of theories of action written in the situation calculus.
The theories of action considered include the causal laws of the domain, in the form of a set of effect axioms, as well as a set of ramification state constraints.
The causal laws state the direct effects that actions have on the world, and ramification state constraints allow one to derive indirect effects of actions on the domain.
::: ::: ::: ::: To solve the combined frame and ramification problems, the causal laws and ramification state constraints are replaced by a set of successor state axioms.
Given a state of the world, these axioms uniquely determine the truth value of dynamic properties after an action is performed.
In this article, we extend previous work by formulating an approach for the mechanical generation of these successor state axioms.
We make use of the notions of implicate and support that have been developed in the context of propositional theories.
The approach works for classes of syntactically restricted sets of ramification state constraints.

Abstract Functional MRI (fMRI) of ongoing brain activity at rest i.e.
without any overt-directed behavior has revealed patterns of coherent activity, so called resting-state functional networks.
The dynamical organization of nodes into these functional networks is closely related to the underlying structural connections.
However, functional correlations have also been observed between cortical regions without apparent neural links, and mechanisms generating functional connectivity between distant cortical regions are largely unknown.
It has been suggested that indirect connections and collective effects governed by the network properties of the cortex play a significant role.
We use numerical simulations to investigate these mechanisms with reference to remote synchronization and network symmetry.
Neural activity and the inferred hemodynamic response of the network nodes are modeled as sets of self-sustained oscillators, which are embedded in topologies of complex functional brain interactions.
The coupling topology is based on connectivity maps derived from fMRI and DTI experiments.
Consequently, our network model includes important information on whether direct or indirect neural connections exist between functionally associated regions.
In the simulated functional networks, remote synchrony between pairs of nodes clearly arises from symmetry in the interactions, which are quantified by the number of shared neighbors.
A larger joint neighborhood positively correlates with a higher level of synchrony.
Therefore, our results indicate that a large overlapping neighborhood in complex networks of brain interactions gives rise to functional similarity between distant cortical regions.

Group decision making (GDM) with intuitionistic fuzzy preference relations (IFPRs) has been an important and active research topic recently, in which one of the most challenging issues is how to reach the group consensus so as to get the best decision.
As the uniform consensus is often unachievable in practice, in order to achieve the consensus, the existing method needs to remove the experts with the most different opinions from the decision group.
It has two drawbacks: the first is the loss of the valuable judgments and opinions of the removed experts.
This is especially harmful in practice where most experts or decision makers often have the biased knowledge in the sense of in-depth expertise in some aspects and naive views in other aspects.
The second is demotivating the experts in GDM.
To overcome these weaknesses in the existing method, this paper presents an enhanced consensus reaching process for GDM with IFPRs, which only removes some opinions of an expert for alternative(s) instead of removing the expert from the decision group.
A numerical example concerning the selection of outstanding PhD students for China Scholarship Council is given to show the feasibility and effectiveness of the enhanced consensus reaching process.

A numerical methodology for hydrodynamic design in centrifugal pumps is developed and tested, considering the possibility to increase the hydraulic efficiency of the pump impeller by improving the blades shape.
The simulation and analysis of the incompressible turbulent flow through the test impeller is performed with a commercial CFD code, and the numerical results are in agreement with the corresponding measurements in a laboratory pump impeller.
A parametric study is carried out to examine the influence of some blade design parameters on the performance and the efficiency of the impeller, like the blade length, the inlet height, and the leading edge inclination.
The values of the above parameters that maximize the hydraulic efficiency of the impeller are then derived by a multiparametric optimization methodology, using a stochastic evolutionary algorithm software.
The optimal impeller design exhibits a remarkable efficiency increase compared to the initial impeller design.

Abstract We propose session-ocaml , a novel library for session-typed concurrent/distributed programming in OCaml.
Our technique solely relies on parametric polymorphism, which can encode core session type structures with strong static guarantees.
Our key ideas are: (1) polarised session types, which give an alternative formulation of duality enabling OCaml to automatically infer an appropriate session type in a session with a reasonable notational overhead; and (2) a parameterised monad with a data structure called ‘slots’ manipulated with lenses, which can statically enforce session linearity including delegations.
We introduce a notational extension to enhance the session linearity for integrating the session types into the functional programming style.
We show applications of session-ocaml to a travel agency use case and an SMTP protocol implementation.
Furthermore, we evaluate the performance of on a number of benchmarks.

A horseshoe which is capable of automatically adapting itself to hoof growth and hoof impact.
The horseshoe includes a series of tiles which are arranged in a row in substantially end-to-end relation and which form a substantially U-shaped assembly.
The tiles are spaced slightly from each other so as to define between themselves gaps which separate the tiles from each other.
An elongated filamentary element is connected to the tiles and extends across the gaps therebetween so as to connect the tiles to each other, and this filamentary element is flexible so that the tiles can freely adapt themselves to growth of the hoof.
The tiles are normally located in a common plane but are free to move transversely of the latter plane in response to impact forces.

A new distributed mutual exclusion algorithm is proposed in this paper.
This algorithm is a centralized token-based algorithm.
An improvement of the synchronization delay for centralized mutual exclusion in distributed systems is achieved by modifying the standard algorithm for centralized mutual exclusion by a forwarding mechanism that allows tokens to be passed directly between requesters instead of always going through a centralized coordinator.
Performance metrics such as synchronization delay, messages per request, and response time under light and heavy loads, are presented.
Analysis shows that this algorithm outperforms existing algorithms and its performance is close to the lower bounds of these metrics.

We consider an adversarial formulation of the problem of predicting a time series with square loss.
The aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect.
Our approach allows natural measures of smoothness such as the squared norm of increments.
More generally, we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms.
We derive the minimax strategy for all problems of this type and show that it can be implemented efficiently.
The optimal predictions are linear in the previous observations.
We obtain an explicit expression for the regret in terms of the parameters defining the problem.
For typical, simple definitions of smoothness, the computation of the optimal predictions involves only sparse matrices.
In the case of norm-constrained data, where the smoothness is defined in terms of the squared norm of the comparator's increments, we show that the regret grows as T/√ λT, where T is the length of the game and λT is an increasing limit on comparator smoothness.

A great majority of the EU citizens already owns a cellular phone.
An increasing part of these phones are smartphones with a broadband internet connection.
This growing network of smart internet enabled devices could act as a dense sensing network, as well as a tool for individual informing and tasking of mobile citizens and volunteers.

Let G be a mixed graph and let L(G) be the Laplacian matrix of the graph G. The first eigenvalue and the first eigenvectors of G are respectively referred to the least nonzero eigenvalue and the corresponding eigenvectors of L(G).
In this paper we focus on the properties of the first eigenvalue and the first eigenvectors of a nonsingular unicyclic mixed graph (abbreviated to a NUM graph).
We introduce the notion of characteristic set associated with the first eigenvectors, and then obtain some results on the sign structure of the first eigenvectors.
By these results we determine the unique graph which minimizes the first eigenvalue over all NUM graphs of fixed order and fixed girth, and the unique graph which minimizes the first eigenvalue over all NUM graphs of fixed order.

We present a simple bijection between diagonally convex directed (DCD) polyominoes with n diagonals and plane trees with 2n edges in which every vertex has even degree (even trees), which specializes to a bijection between parallelogram polyominoes and full binary trees.
Next we consider a natural definition of symmetry for DCD-polyominoes, even trees, ternary trees, and non-crossing trees, and show that the number of symmetric objects of a given size is the same in all four cases.

Earlier work of Bixby, Cunningham, and Topkis is extended to give a combinatorial algorithm for the problem of minimizing a submodular function, for which the amount of work is bounded by a polynomial in the size of the underlying set and the largest function value (not its length).

We establish that a Vertex Replacement set of graphs, i.e., a set of graphs generated by a C-edNCE or, equivalently, by a separated handle rewriting graph grammar is Hyperedge Replacement, i.e., is generated by a hyperedge replacement graph grammar, iff its graphs do not contain arbitrary large complete bipartite graphs Kn, n as subgraphs.
Another equivalent condition is that its graphs have a number of edges that is linearly bounded in terms of the number of vertices.
These properties are decidable by means of an appropriate extension of the theorem by Parikh that characterizes the commutative images of context-free languages.
We extend these results to hypergraphs.

In a storage system where individual storage nodes are prone to failure, the redundant storage of data in a distributed manner across multiple nodes is a must to ensure reliability.
Reed-Solomon codes possess the reconstruction property under which the stored data can be recovered by connecting to any k of the n nodes in the network across which data is dispersed.
This property can be shown to lead to vastly improved network reliability over simple replication schemes.
Also of interest in such storage systems is the minimization of the repair bandwidth, i.e., the amount of data needed to be downloaded from the network in order to repair a single failed node.
Reed-Solomon codes perform poorly here as they require the entire data to be downloaded.
Regenerating codes are a new class of codes which minimize the repair bandwidth while retaining the reconstruction property.
This paper provides an overview of regenerating codes including a discussion on the explicit construction of optimum codes.

We propose an automatic technique to segment scar and classify the myocardial tissue of the left ventricle from Delay Enhancement (DE) MRI.
The method uses multiple region growing with two types of regions and automatic seed initialization.
The region growing criteria is based on intensity distance and the seed initialization is based on a thresholding technique.
We refine the obtained segmentation with some morphological operators and geometrical constraints to further define the infarcted area.
Thanks to the use of two types of regions when performing the region growing, we are able to segment and classify the healthy and pathological tissues.
We have also a third type of tissue in our classification, which includes tissue areas that deserve special attention from medical experts: border-zone tissue or myocardial segmentation errors.

In this work we have developed a formal ontology for perdurants suitable for representing interlocking institutional worlds in the general area of interoperating information systems.
The formal ontology is a specialization of the perdurant elements of the DOLCE and Bunge-Wand-Weber universal formal ontologies using an abstract material ontology based on the theory of speech acts embodied in a method of information systems design taken from the literature.
The perdurant formal ontology is necessarily integrated with an endurant formal ontology.
The one used is taken from the literature.
Like the formal endurant ontology used, our formal perdurant ontology is represented as a UML profile, enabling us to re-use the vast structure of UML.
The formal endurant ontology was able to re-use the UML Instances model, so did not have to make specific provision for individuals.
Our formal perdurant ontology was not able to do something comparable, because UML does not have a concept of a static description of instances of perdurants.
We therefore needed to develop an instances model, based on the dynamic behavior typical of database transaction management systems which must preserve the integrity of a database under conditions of catastrophic system failure.
Finally, we looked at the question of how institutional worlds interlock using our formal ontology, and discovered that the process is far more complex than importing objects from one ontology into another.

For typical optimization problems, the design space of interest is well defined: It is a subset of Rn, where n is the number of (continuous) variables.
Constraints are often introduced to eliminate infeasible regions of this space from consideration.
Many engineering design problems can be formulated as search in such a design space.
For configuration design problems, however, the design space is much more difficult to define precisely, particularly when constraints are present.
Configuration design spaces are discrete and combinatorial in nature, but not necessarily purely combinatorial, as certain combinations represent infeasible designs.
One of our primary design objectives is to drastically reduce the effort to explore large combinatorial design spaces.
We believe it is imperative to develop methods for mathematically defining design spaces for configuration design.
The purpose of this paper is to outline our approach to defining configuration design spaces for engineering design, with an emphasis on the mathematics of the spaces and their combinations into larger spaces that more completely capture design requirements.
Specifically, we introduce design spaces that model physical connectivity, functionality, and assemblability considerations for a representative product family, a class of coffeemakers.
Then, we show how these spaces can be combined into a “common” product variety design space.
We demonstrate how constraints can be defined and applied to these spaces so that feasible design regions can be directly modeled.
Additionally, we explore the topological and combinatorial properties of these spaces.
The application of this design space modeling methodology is illustrated using the coffeemaker product family.

Two levels of an innovative adaptive switching pattern (ASP) for use in the control of induction machines are described.
The ASP is based on a tolerance band control strategy resulting in nearly sinusoidal stator currents.
The first level (ASP1) significantly increases the switching time thereby eliminating the very fast switching sometimes experienced.
The second level (ASP2) reduces the number of double commutations by one or two orders of magnitude.
The price for applying ASP is only a small, irregular, consequential increase in the current error.
A rotating reference frame fixed to the rotor flux is applied.
This makes ASP especially suitable for application in the field-oriented control of current controlled voltage source inverter (CC-VSI) fed induction motor drives.
The theoretical background supported by conclusive simulation results illustrates clearly the significant benefits of ASP over the regular switching pattern often used.
To complete the picture a short survey of the various techniques used in the speed control of induction machines is presented in the introduction.
>

We summarize the data mining competition associated with IJCRS’15 conference – IJCRS’15 Data Challenge: Mining Data from Coal Mines, organized at Knowledge Pit web platform.
The topic of this competition was related to the problem of active safety monitoring in underground corridors.
In particular, the task was to design an efficient method of predicting dangerous concentrations of methane in longwalls of a Polish coal mine.
We describe the scope and motivation for the competition.
We also report the course of the contest and briefly discuss a few of the most interesting solutions submitted by participants.
Finally, we reveal our plans for the future research within this important subject.

Schizophrenia is a mental disorder affecting 1-2% of the population and it is estimated 12-16% of hospital beds in Australia are occupied by patients with psychosis.
The suicide rate for patients with this diagnosis is higher than that of the general population.
Any technique which enhances training and treatment of this disorder will have a significant societal and economic impact.
A significant research project using Virtual Reality (VR), in which both visual and auditory hallucinations are simulated, is currently being undertaken at the University of Queensland.
The virtual environments created by the new software are expected to enhance the experiential learning outcomes of medical students by enabling them to experience the inner world of a patient with psychosis.
In addition the Virtual Environment has the potential to provide a technologically advanced therapeutic setting where behavioral, exposure therapies can be conducted with exactly controlled exposure stimuli with an expected reduction in risk of harm.
This paper reports on the current work of the project, previous stages of software development and future educational and clinical applications of the Virtual Environments.

We show that the bilinear forms graphsHq(n,d) of diameterd?
3 are characterized as distance-regular graphs by their parameters provided that eithern?d+3 andq?
3, orn?d+4 andq=2.
As a corollary of the method used, we can show the following.
If ?
is a distance-regular graph with classical parameters (d,q,?,?)
and diameterd?3, then either ?
is a Johnson graph, a Grassmann graph, a Hamming graph, or a bilinear forms graph, or ?
is bounded in terms ofd,qand ?.

This article provides a systematised account of safety engineering practices that clarifies their relation to the goal of safety engineering, namely to increase safety.
We list 24 principles referred to in the literature of safety engineering, dividing them into four major categories: Inherently safe design, Safety reserves, Safe fail and Procedural safeguards.
It emerges from this systematisation that important aspects of these methods can be better understood with the help of the distinction between risk and uncertainty.

The internet is increasingly used in psychological research to solicit participants and collect data.
This paper includes two studies examining the quality of data obtained via web-based methods administered either inside or outside the lab.
Both studies used item recognition accuracy as a proxy for attention to questions.
Study 1 examined the extent to which undergraduate participants (N?=?504) read and attended to questions either inside or outside the lab.
Study 2 (N?=?744) replicated Study 1, added a Mechanical Turk sample, and examined attention to non-intuitive survey instructions.
Results indicated that participants demonstrated good item recognition, regardless of locale or sample; however, small sex effects on accuracy were found in both studies.
Specifically, women were more accurate at identifying previously seen items than men in both Study 1 and Study 2.
In Study 2, Mechanical Turk participants were more likely to read instructions than undergraduate participants, regardless of whether they participated inside or outside of the lab.
The findings support the use of the internet for sampling purposes as well as survey administration, and suggest that researchers use care when studies include non-intuitive instructions.
Undergraduate and MTurk participants did not differ in item recognition accuracy.Participants attended to survey items equally well in and outside of the lab.Women's recognition accuracy was higher than men's, especially when unsupervised.MTurk participants were more likely than undergraduates to read instructions.Results support using the web for subject recruitment and survey administration.

Abstract Simulation of radiation hydrodynamics in two spatial dimensions is developed, having in mind, in particular, target design for indirectly driven inertial confinement energy (IFE) and the interpretation of related experiments.
Intense radiation pulses by laser or particle beams heat high-Z target configurations of different geometries and lead to a regime which is optically thick in some regions and optically thin in others.
A diffusion description is inadequate in this situation.
A new numerical code has been developed which describes hydrodynamics in two spatial dimensions (cylindrical R – Z geometry) and radiation transport along rays in three dimensions with the 4 π solid angle discretized in direction.
Matter moves on a non-structured mesh composed of trilateral and quadrilateral elements.
Radiation flux of a given direction enters on two (one) sides of a triangle and leaves on the opposite side(s) in proportion to the viewing angles depending on the geometry.
This scheme allows to propagate sharply edged beams without ray tracing, though at the price of some lateral diffusion.
The algorithm treats correctly both the optically thin and optically thick regimes.
A symmetric semi-implicit (SSI) method is used to guarantee numerical stability.
Program summary Program title: MULTI2D Catalogue identifier: AECV_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AECV_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No.
of lines in distributed program, including test data, etc.
: 151 098 No.
of bytes in distributed program, including test data, etc.
: 889 622 Distribution format: tar.gz Programming language: C Computer: PC (32 bits architecture) Operating system: Linux/Unix RAM: 2 Mbytes Word size: 32 bits Classification: 19.7 External routines: X-window standard library (libX11.so) and corresponding heading files (X11/*.h) are required.
Nature of problem: In inertial confinement fusion and related experiments with lasers and particle beams, energy transport by thermal radiation becomes important.
Under these conditions, the radiation field strongly interacts with the hydrodynamic motion through emission and absorption processes.
Solution method: The equations of radiation transfer coupled with Lagrangian hydrodynamics, heat diffusion and beam tracing (laser or ions) are solved, in two-dimensional axial-symmetric geometry ( R – Z coordinates) using a fractional step scheme.
Radiation transfer is solved with angular resolution.
Matter properties are either interpolated from tables (equations-of-state and opacities) or computed by user routines (conductivities and beam attenuation).
Restrictions: The code has been designed for typical conditions prevailing in inertial confinement fusion (ns time scale, matter states close to local thermodynamical equilibrium, negligible radiation pressure, …).
Although a wider range of situations can be treated, extrapolations to regions beyond this design range need special care.
Unusual features: A special computer language, called r94 , is used at top levels of the code.
These parts have to be converted to standard C by a translation program (supplied as part of the package).
Due to the complexity of code (hydro-code, grid generation, user interface, graphic post-processor, translator program, installation scripts) extensive manuals are supplied as part of the package.
Running time: 567 seconds for the example supplied.

The advance of micro/nanotechnology in energy-harvesting, micropower, electronic devices, and transducers for automobile and aerospace applications has led to the need for accurate thermomechanical characterization of micro/nano-scale materials to ensure their reliability and performance.
This persistent need has driven various efforts to develop innovative experimental techniques that overcome the critical challenges associated with precise mechanical and thermal control of micro/nano-scale specimens during material characterization.
Here we review recent progress in the development of thermomechanical testing methods from miniaturized versions of conventional macroscopic test systems to the current state of the art of in situ uniaxial testing capabilities in electron microscopes utilizing either indentation-based microcompression or integrated microsystems.
We discuss the major advantages/disadvantages of these methods with respect to specimen size, range of temperature control, ease of experimentation and resolution of the measurements.
We also identify key challenges in each method.
Finally, we summarize some of the important discoveries that have been made using in situ thermomechanical testing and the exciting research opportunities still to come in micro/nano-scale materials.

Abstract Security protocols such as IPSec, SSL and VPNs used in many communication systems employ various cryptographic algorithms in order to protect the data from malicious attacks.
Thanks to public-key cryptography, a public channel which is exposed to security risks can be used for secure communication in such protocols without needing to agree on a shared key at the beginning of the communication.
Public-key cryptosystems such as RSA, Rabin and ElGamal cryptosystems are used for various security services such as key exchange and key distribution between communicating nodes and many authentication protocols.
Such public-key cryptosystems usually depend on modular arithmetic operations including modular multiplication and exponentiation.
These mathematical operations are computationally intensive and fundamental arithmetic operations which are intensively used in many fields including cryptography, number theory, finite field arithmetic, and so on.
This paper is devoted to the analysis of modular arithmetic operations and the improvement of the computation of modular multiplication and exponentiation from hardware design perspective based on FPGA.
Two of the well-known algorithms namely Montgomery modular multiplication and Karatsuba algorithms are exploited together within our high-speed pipelined hardware architecture.
Our proposed design presents an efficient solution for a range of applications where area and performance are both important.
The proposed coprocessor offers scalability which means that it supports different security levels with a cost of performance.
We also build a system-on-chip design using Xilinx’s latest Zynq-7000 family extensible processing platform to show how our proposed design improve the processing time of modular arithmetic operations for embedded systems.

The performance of the modified unscented Kalman filter (UKF) for nonlinear stochastic discrete-time system with linear measurement equation is investigated.
It is proved that under certain conditions, the estimation error of the UKF remains bounded.
Furthermore, it is shown that the design of noise covariance matrix plays an important role in improving the stability of the algorithm.
Error behavior of the UKF is then derived in terms of mean square error (MSE), and the Cramer-Rao lower bound (CRLB) is introduced as a performance measure.
The modified UKF is found to approach the CRLB if the difference between the real noise covariance matrix and the selected one is small enough.
These results are verified by using Monte Carlo simulations on two example systems.

Before a program can fail, a software fault must be executed, that execution must alter the data state, and the incorrect data state must propagate to state that results directly in an incorrect output.
This paper describes a tool called PISCES (developed by Reliable Software Technologies Corporation) for predicting the probability that faults in a particular program location will accomplish all three of these steps causing program failure.
PISCES is a tool that is used during software verification and validation to predict a program's testability.
>

The formulation of policies requires the selection and configuration of effective and acceptable courses of action to reach explicit goals.
A one-size-fits-all policy is unlikely to achieve the desired goals; as a result, the identification of a suite of alternative policies, together with clear indications of their trade-offs, is crucial to accommodate the diversity of stakeholders’ preferences.
At present, the formulation of transport policies is done manually; this fact, together with the size of the space of possible policies, results in a large part of that space being left unexplored.
A six-step framework to explore the space of alternative transport policies in order to achieve environmental targets is proposed.
The process starts with a user-defined set of specific policy measures, using them as building blocks in the generation of alternative policy packages, clusters and future imagesaccording to the user’s preferences and goals.The analysis framework is based on the visioning and backcasting approach used in the VIBAT report (Banister & Hickman, 2006a).
The framework is being implemented as a prototype decision support system around a case study: the formulation and analysis of policies required to achieve CO2 emission targets for the transport sector in the UK.
Important insights on how to develop the framework have also been elicited from engineering design.
The goal is to accelerate the task of policy-making and improve the effectiveness of the resulting policies.The proposed method and computer implementation is fundamentally different from the tools commonly used in the transport sector and is intended to assist (not replace) transport policy makers, and complement (not substitute nor compete with) existing mathematical modelling tools.
This research constitutes the first step towards the development of a general family of computer-based systems that support the design of policies to achieve environmental targets – not only for transport, but also for other sectors such as energy and water.

Mobile devices have been increasingly utilized in informal learning because of their high degree of portability; mobile guide systems (or electronic guidebooks) have also been adopted in museum learning, including those that combine learning strategies and the general audio- visual guide systems.
To gain a deeper understanding of the features and limitations of these guide systems in a museum-learning context and also to provide new designs that better guide learners in interacting with peers and exhibitions, in-depth exploration of learners' actual visits and analyses of their behavioural patterns is crucial.
This study was based on empirical observation and analysis of the learning behaviours (recorded on video) of 65 elementary-school students who were placed into three groups: mobile guide with problem-solving strategy, audio-visual mobile guide and paper-based learning-sheet guide.
By coding and analysing the video and conducting sequential analysis and frequency analysis of learning-related discussion content, behavioural interaction patterns were determined by which the features and limitations of the different types of guides were compared.
Among the findings, it was discovered that the students in the problem-solving mobile guide group showed a higher level of two-way interactions with their peers and the exhibits, as well as more learning-related discussions.
Relevant suggestions for teachers, researchers and guide-systems developers are also given.

Agent-based models are becoming indispensable tools in building design, safety assessments as well as management of emergency egress.
However, reliable calibrations of these models should be mandatory before they are used in practice.
To improve the database for model calibration we present results from two experiments at a T-junction and a corner.
In such structures the dynamics of pedestrian streams is complex and up to now not studied systematically.
To understand it deeply, series of well-controlled laboratory experiments are conducted.
The Voronoi method, which is used to analyze the experiments, allows high resolution and small fluctuation in time and space.
From the results, it is found that the fundamental diagrams of pedestrian flow in T-junction are not the same before and after merging.
At the same density, the velocities of pedestrians before merging are smaller than that after merging.
To analyze whether turning or merging of the stream is responsible for this discrepancy, we compare the fundamental diagrams of pedestrian flow in T-junction with the flow at a single corner.
The fundamental diagrams of the streams in front and behind the corner agree well and are also in accordance with that from T-junction flow after merging.
Besides, space-resolved measurements for the density, velocity profiles are obtained using Voronoi method.
These maps offer important information about dangerous spots and thus enable to improve egress management and facility design.

Abstract The initial contribution in this paper begins with proposing a robust and secure DWT, DCT and SVD based multiple watermarking techniques for protecting digital contents over unsecure social networks.
The proposed technique initially decomposes the host image into third level DWT where the vertical frequency band (LH2) at second level and low frequency band (LL3) at the third level DWT is selected for embedding image and text watermark respectively.
Further, the proposed method addresses the issue of ownership identity authentication, multiple watermarks are embedded instead of single watermark into the same multimedia objects simultaneously, which offer the extra level of security and reduced storage and bandwidth requirements in the important applications areas such as E-health, secure multimedia contents on online social network, secured E-Voting systems, digital cinema, education and insurance companies, driver’s license /passport.
Moreover, the robustness image watermark is also enhanced by using Back Propagation Neural Network (BPNN), which is applied on extracted watermark to minimize the distortion effects on the watermarked image.
In addition, the method addresses the issue of channel noise distortions in the identity information.
This has been achieved using error correcting codes (ECCs) for encoding the text watermark before embedding into the host image.
The effects of Hamming and BCH codes on the robustness of personal identity information in the form of text watermark and the cover image quality have been investigated.
Further, to enhance the security of the host and watermarks the selective encryption is applied on watermarked image, where only the important multimedia data is encrypted.
The proposed method has been extensively tested and analyzed against known attacks.
Based on experimental results, it is established that the proposed technique achieves superior performance in respect of, robustness, security and capacity with acceptable visual quality of the watermarked image as compared to reported techniques.
Finally, we have evaluated the image quality of the watermarked image by subjective method.
Therefore, the proposed method may find potential solutions in prevention of personal identity theft and unauthorized multimedia content sharing on online social networks/open channel.

ABSTRACT The advent of Robotic Process Automation (RPA) has the potential to disrupt the traditional audit model.
With its capability to automate rules-based tasks that are repetitive and manual, R...

In this work we have studied the applicability of genetic algorithms to the optimization of elevator group control parameters.
The goal was to minimize the average passenger waiting time in a simulated office building.
The elevator group control consists of a set of elevators and controllers.
At the highest level of the system hierarchy the elevator group controller decides which elevator serves which call.
The decision is based on the actual calls, state of the elevators (location, direction, load) and on the estimation of the traffic situation like: incoming, interfloor, outgoing, which gives some idea how the calls will be distributed within a few minutes time period.
Especially in office buildings the traffic type depends on the time of the day.
This allocation problem, which seems to be quite simple, turns out to be a very difficult control problem in practise and is one of the features how elevator companies can competed with one another.

Abstract The most common approach for incorporating discontinuities in visual reconstruction problems makes use of Bayesian techniques, based on Markov random field models, coupled with stochastic relaxation and simulated annealing.
Despite their convergence properties and flexibility in exploiting a priori knowledge on physical and geometric features of discontinuities, stochastic relaxation algorithms often present insurmountable computational complexity.
Recently, considerable attention has been given to suboptimal deterministic algorithms, which can provide solutions with much lower computational costs.
These algorithms consider the discontinuities implicitly rather than explicitly and have been mostly derived when there are no interactions between two or more discontinuities in the image model.
In this paper we propose an algorithm that allows for interacting discontinuities, in order to exploit the constraint that discontinuities must be connected and thin.
The algorithm, called E-GNC, can be considered an extension of the graduated nonconvexity (GNC), first proposed by Blake and Zisserman for noninteracting discontinuities.
When applied to the problem of image reconstruction from sparse and noisy data, the method is shown to give satisfactory results with a low number of iterations.

This thesis deals with the problem of how to design a linear and time invariant controller (continuous- or discrete-time) for a SISO- or SIMO-system with amplitude constraints in the actuator.
One ...

In this paper, we briefly address the application of the standard principal component analysis (PCA) technique to fault detection and identification.
Based on an analysis of the existing test statistic, we propose a new test statistic, which is similar to the Hawkin's T 2h statistic but without the numerical drawback.
In comparison with the SPE index, the threshold setting associated with the new statistic is computationally simpler.
Our further study is dedicated to the analysis of fault sensitivity.
We consider the off-set and scaling faults, and evaluate the test statistic by viewing its sensitivity to the faults.
Our final study focuses on identifying off-set and scaling faults.
To this end, two algorithms are proposed.
This paper also includes some critical remarks on the application of the PCA technique to fault diagnosis.

In this paper, a case study investigating the experiences from evolution and modification of reusable assets in product-line architectures is presented involving two Swedish companies, Axis Communications AB and Securitas Larm AB.
Key persons in these organisations have been interviewed and information has been collected from documents and other sources.
The study identified problems related to multiple versions of reusable assets, dependencies between assets and the use of assets in new contexts.
The problem causes have been identified and analysed, including the early intertwining of functionality, the organizational model, the time to market pressure, the lack of economic models and the lack of encapsulation boundaries and required interfaces.

This paper describes the development of a video-based continuous sign language recognition system using Hidden Markov Models (HMM).
The system aims for automatic signer dependent recognition of sign language sentences, based on a lexicon of 52 signs of German Sign Language.
A single colour video camera is used for image recording.
The recognition is based on Hidden Markov Models concentrating on manual sign parameters.
As an additional component, a stochastic language model is utilised, which considers uni- and bigram probabilities of single and successive signs.
The system achieves an accuracy of 95% using a bigram language model.

Median is a general concept of capturing the essential information of a given set of objects.
In this work we adopt this concept to the problem of learning, or synthesis, of representative graphical symbols from given examples.
Graphical symbols are represented by graphs.
This way the learning task is transformed into that of computing the generalized median of a given set of graphs, which is a novel graph matching problem and solved by a genetic algorithm.

This paper proposes the nonnegative-lasso method for variable selection in high dimensional sparse linear regression models with the nonnegative constraints on the coefficients.
This method is an extension of Lasso and is shown to have variable selection consistency and estimation consistency under certain condition similar to Irrepresentable Condition in Lasso.
To get the solution of the nonnegative-lasso, many algorithms such as Lars, coordinate decent can be used, among which multiplicative updates approach is preferred since it is faster and simpler.
The constrained index tracking problem in stock market without short sales is studied in the latter part.
The tracking results indicate that nonnegative-lasso can get small tracking error and is successful in assets selection.

This paper develops the utility gradient (or martingale) approach for computing portfolio and consumption plans that maximize stochastic differential utility (SDU), a continuous-time version of recursive utility due to Duffie and Epstein (1992a).
The setting is that of a general stochastic investment opportunity set with Brownian information (making some of the results novel in the time-additive case, as well).
We characterize the first order conditions of optimality as a system of forward-backward SDE's, and for the Markovian case we show how to solve this system in terms of a system of quasilinear parabolic PDE's and forward only SDE's, which is amenable to numerical computation.
Another contribution is a proof of existence, uniqueness, and basic properties for a parametric class of homothetic SDU that can be thought of as a continuous-time version of the CES Kreps-Porteus utilities studied by Epstein and Zin (1989).
For this class, we show that the solution method simplifies significantly, resulting in closed form solutions in terms of a single backward SDE (without imposing a Markovian structure).
The latter can be easily computed, as we will illustrate with a number of tractable concrete examples involving the type of "affine" state price dynamics that are familiar from the term structure literature.

Summary Lightweight IP (lwIP) is an open source TCP/IP networking stack for embedded systems.
The Xilinx® Software Development Kit (SDK) provides lwIP software customized to run on various Xilinx embedded systems that can be MicroBlaze™ or PowerPC® processor based.
The Xilinx SDK provided lwIP software can also be run on ARM®-based Xilinx Zynq®-7000 All Programmable (AP) SoC.
The information in this application note applies to MicroBlaze processors and ARM-based Zynq-7000 AP SoC systems.
This document describes how to use the lwIP library to add networking capability to an embedded system.
In particular, lwIP is utilized to develop these applications: echo server, web server, Trivial File Transfer Protocol (TFTP) server, and receive and transmit throughput tests.
You can download the Reference Design Files for this application note from the Xilinx website.
For detailed information about the design files, see Reference Design.

Before spreadsheets, modeling of business concerns required some competency in mathematics (algebra, calculus, statistics, and probability) and in computer programming, skills that are rather intimidating for the average business executive and management school student.
However, the spreadsheet and the personal computer revolution challenged that paradigm.
With its simple intuitive interface, direct interactivity, and universal presence, the humble spreadsheet has made business modeling much easier and has been considered by many analysts as the tool of choice for exploring business opportunities.
Many university professors have already adopted spreadsheets as their computing platform in support of teaching business mathematics, statistics, and management science courses.
Though some business modeling skills can be learned when spreadsheets are used in the courses, they are often secondary to the task of delivering the main subject content.
Working out business challenges in the real world, however, requi...

Abstract In the paper we discuss the use of viewpoint specifications, a technique which concentrates on making large specifications more understandable.
Rather than specifying the whole system at once, a system is described using several self-contained partial specifications, which may then be amalgamated to give a description of the complete system.
Amalgamation is taken to be a composite process in which the data and operations of the constituent viewpoints are separately considered.
The approach is illustrated in terms of Z specifications.

Thank you for downloading spice a guide to circuit simulation and analysis using pspice.
As you may know, people have look numerous times for their chosen books like this spice a guide to circuit simulation and analysis using pspice, but end up in malicious downloads.
Rather than enjoying a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their desktop computer.

The special characteristics of the National Assessment data which affect the validity of conventional techniques of statistical analysis are considered.
In contrast to the assumptions underlying standard methods of statistical analysis, the NAEP samples are obtained via a stratified multi-stage probability sampling design in which clusters of students are selected and in which certain subpopulations are sampled at a higher rate.
The resulting samples have different statistical characteristics from those of a simple random sample.
Analytic techniques which take the structure of the data into account are discussed and their properties explored.
A second feature of the National Assessment data arises from the fact that multiple matrix sampling techniques are used for the assignment of subsets of the pool of exercises to subsamples of students.
National Assessment accounts for the effect of the missing data on its scaling models through the use of multiple imputation procedures; creating vectors of “plausible values” for the scale-scores.
The use of plausible values and their effect on analysis are illustrated.
The analyses are illustrated with results from the NAEP assessments of mathematics.

We consider a target illuminated by MIMO radar in an urban terrain where the measurement consists of multiple re- flected signals.
A robust particle filter to estimate the target state in this setting is presented.

Abstract The two-dimensional (2-D) autocorrelation function (ACF) of an image statistically characterizes the spatial pattern within that image and presents a powerful tool for fabric analysis.
It determines shape preferred orientation, degree of alignment, and distribution anisotropy of image objects.
We present here a fast, user-friendly, MS-DOS based computer program, AUTO, to calculate the 2-D ACF of a digital monochrome image.
AUTO displays an image on the screen and allows selection of a portion of the image for autocorrelation.
Rapid calculation of ACF values is achieved by using a fast Fourier transform (FFT) routine according to the convolution theorem.
The spatial distribution of ACF values is contoured for quantitative analysis of fabric anisotropy.
Applications of this technique include the determination of grain or pore fabric of geological specimens, strain analysis, and the interpretation of petrophysical properties.

Abstract Sufficient conditions are obtained for the existence of a unique, globally attractive, strictly positive (componentwise), almost periodic solution of a non-autonomous, almost periodic competitive two-species model with a stage structure in one species.
An example together with its numeric simulations shows the feasibility of our main results.
Our result generalizes the main results of Zeng et al.
[G.Z.
Zeng, L.S.
Chen, L.H.
Sun, Y. Liu, Permanence and the existence of the periodic solution of the non-autonomous two-species competitive model with stage structure, Adv.
Complex Syst., 7 (3&4) (2004) 385–393]

Purpose – The purpose of this paper is to examine how pregnant women with type 1 diabetes integrate new information technology (IT) into their health management activities, using activity theory as an analytical framework.
Design/methodology/approach – The research is a multiple case design, based on interviews with 15 women with type 1 diabetes who were pregnant, considering pregnancy, or had recently given birth.
A thematic analysis, sensitised by activity theory, was used to analyse the data.
Findings – Health management in this setting involves negotiations and contradictions across boundaries of interacting activities.
Participants play an active role in managing their health and using new IT tools in particular ways to support their health management.
Using new technologies creates both opportunities and challenges.
IT-enabled healthcare devices and other information systems open up new treatment possibilities, but also generate new contradictions between interacting activity systems.
Research limit...

Carry-save-adder (CSA) is one of the most widely used components for fast arithmetic in industry.
This paper provides a solution to the problem of finding an optimal-timing allocation of CSAs in arithmetic circuits.
Namely, we present a polynomial time algorithm which finds an optimal-timing CSA allocation for a given arithmetic expression.
We then extend our result for CSA allocation to the problem of optimizing arithmetic expressions across the boundary of design hierarchy by introducing a new concept, called auxiliary ports.
Our algorithm can be used to carry out the CSA allocation step optimally and automatically and this can be done within the context of a standard RTL synthesis environment.

The synchronization problem of a neutral complex dynamical network (NCDN) with distributed delay, Markovian jump parameters and partially unknown transition rates via sampled-data controller is investigated in this paper.
The retarded, neutral and distributed delays are considered to be interval mode-dependent and time-varying, while the sampling period is assumed to be time-varying and bounded.
By the interval dividing approach, a new augmented stochastic Lyapunov functional is constructed, which contains some triple-integral terms to reduce the conservativeness.
Then the delay-range-dependent and rate-dependent exponential stability conditions for the closed-loop error system are obtained by the Lyapunov-Krasovskii stability theory, integral matrix inequalities and reciprocally convex lemma.
Based on these new stability conditions, the sampled-data exponential synchronization controllers are found in terms of the solutions to linear matrix inequalities.
Finally, numerical examples are given to demonstrate the feasibility and effectiveness of the proposed theoretic result.

Interictal epileptiform discharges, or 'interictal spikes', are the hallmark of epilepsy.
Still, there is growing evidence that oscillatory activity—whether in the gamma band (30–120 Hz) or at higher frequencies is another important marker of hyperexcitable tissues.
A major difficulty arises from the fact that interictal spikes and oscillations overlap in the frequency domain.
This hampers the correct delineation of the cortex producing pathological oscillations by simple filtering.
Here, we propose a nonlinear technique for fitting the spike waveform in order to remove it, resulting in a 'despiked' signal.
This strategy was first applied to simulated data inspired from real stereo-electroencephalographic (SEEG) signals, then to real data.
We show that despiking leads to a better space-time-frequency analysis of the oscillatory part of the signal.
Thus, in the real SEEG signals, the spatio-temporal maps show a buildup of gamma oscillations during the preictal period in the despiked signals, whereas in the original signals this activity is masked by spikes.
Despiking is thus a promising venue for a better characterization of oscillatory activity in electrophysiology of epilepsy.

Stochastic electroencephalogram (EEG) signals are known to be nonstationary and often multicomponential.
Detecting and extracting their components may help clinicians to localize brain neurological dysfunctionalities for patients with motor control disorders due to the fact that movement-related cortical activities are reflected in spectral EEG changes.
A new algorithm for EEG signal components detection from its time-frequency distribution (TFD) has been proposed in this paper.
The algorithm utilizes the modification of the Renyi entropy-based technique for number of components estimation, called short-term Renyi entropy (STRE), and upgraded by an iterative algorithm which was shown to enhance existing approaches.
Combined with instantaneous frequency (IF) estimation, the proposed method was applied to EEG signal analysis both in noise-free and noisy environments for limb movements EEG signals, and was shown to be an efficient technique providing spectral description of brain activities at each electrode location up to moderate additive noise levels.
Furthermore, the obtained information concerning the number of EEG signal components and their IFs show potentials to enhance diagnostics and treatment of neurological disorders for patients with motor control illnesses.

Wavelet neural networks(WNN) are a class of neural networks consisting of wavelets.
A novel learning method based on immune genetic algorithm(IGA) for continuous wavelet neural networks is presented in this paper.
Through adopting multi-encoding, this algorithm can optimize the structure and the parameters of WNN in the same training process.
Simulation results show that WNN with novel algorithm has a comparatively simple structure and enhance the probability for global optimization.
The study also indicates that the proposed method has the potential to solve a wide range of neural network construction and training problems in a systematic and robust way.

Abstract Many biomedical applications, such as the design of customized orthopaedic implants, require accurate mathematical models of bone geometry.
The surface geometry is often generated by fitting closed parametric curves, or contours, to the edge points extracted from a sequence of evenly spaced planar images acquired using computed tomography (CT), magnetic resonance imaging (MRI), or ultrasound imaging.
The Bernstein basis function (BBF) network described in this paper is a novel neural network approach to performing functional approximation tasks such as curve and surface fitting.
In essence, the BBF architecture is a two-layer basis function network that performs a weighted summation of nonlinear Bernstein polynomials.
The weight values generated during network training are equivalent to the control points needed to create a smooth closed Bezier curve in a variety of commercially available computer-aided design software.
Modifying the number of basis neurons in the architecture is equivalent to changing the degree of the Bernstein polynomials.
An increase in the number of neurons will improve the curve fit, however, too many neurons will diminish the network's ability to generate a smooth approximation of the cross-sectional boundary data.
Additional constraints are imposed on the learning algorithm in order to ensure positional and tangential continuity for the closed curve.
A simulation study and real world experiment are presented to show the effectiveness of this functional approximation method for reverse engineering bone structures from serial medical imagery.

This paper develops a two-stage automatic algorithm for fairing C2-continuous cubic parametric B-splines under convexity, tolerance and end constraints.
The first stage is a global procedure, yielding a C2 cubic B-spline which satisfies the local-convexity, local-tolerance and end constraints imposed by the designer.
The second stage is a local finefairing procedure employing an iterative knot-removal knotreinsertion technique, which adopts the curvature-slope discontinuity as the fairness measure of a C2 spline.
This procedure preserves the convexity and end properties of the output of the first stage and, moreover, it embodies a globaltolerance constraint.
The performance of the algorithm is discussed for four data sets.

AbstractThe measurements most often made and quoted in connexion with photographic materials relate to the mean response to light.
For any radiation detector, such responsivity characteristics are important where cost or environment severely limit the level of technology that can be applied; for example, respectively, in commercial darkrooms or in rocket missiles.
In other circumstances responsivity is mainly a matter of convenience.
The quality of the results is then limited in practice, as it always is in principle, by signal-to-noise relations.
Knowledge of the properties, often called detectivity characteristics, which define these relations is therefore needed for the rational use of the material.
The definition and use of the quantities called noise-equivalent quantum efficiency and noise-equivalent quantum storage are recalled.
For any condition of exposure they represent abstractions from the sinusoidal response factors (acceptance factors) and granularity spectra.
More subtle abstractions are abl...

Thank you for downloading system design a practical guide with specc.
As you may know, people have look hundreds times for their chosen books like this system design a practical guide with specc, but end up in malicious downloads.
Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some infectious virus inside their desktop computer.
system design a practical guide with specc is available in our book collection an online access to it is set as public so you can get it instantly.
Our books collection saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one.
Merely said, the system design a practical guide with specc is universally compatible with any devices to read.

All human languages have words that can mean different things in different contexts, such words with multiple meanings are potentially “ambiguous”.
The process of “deciding which of several meanings of a term is intended in a given context” is known as “word sense disambiguation (WSD)”.
This paper presents a method of WSD that assigns a target word the sense that is most related to the senses of its neighbor words.
We explore the use of measures of relatedness between word senses based on a novel hybrid approach.
First, we investigate how to “literally” and “regularly” express a “concept”.
We apply set algebra to WordNet’s synsets cooperating with WordNet’s word ontology.
In this way we establish regular rules for constructing various representations (lexical notations) of a concept using Boolean operators and word forms in various synset(s) defined in WordNet.
Then we establish a formal mechanism for quantifying and estimating the semantic relatedness between concepts—we facilitate “concept distribution statistics” to determine the degree of semantic relatedness between two lexically expressed con- cepts.
The experimental results showed good performance on Semcor, a subset of Brown corpus.
We observe that measures of semantic relatedness are useful sources of information for WSD.

In response to the challenges facing engineering educators today, the latest developments in computer technology were applied to instrumentation and communication, two essential areas of the engineering curricula.
Hypermedia-based resource modules containing generic information on these two areas were developed.
Users can navigate the modules using either a networked tree or direct access method.
This allows students to learn independently, at their own pace, and in a nonlinear way.
The resource modules can be used in measurements and technical communication courses, in almost all engineering laboratories, and in capstone design projects.
Since the resource modules contain generic information and are put on CD-ROMs, they can be used in other engineering programs.
Preliminary results indicate that the modules were informative and helpful to students and also allowed them to learn material independently and at their own pace.

The prospects for automatically identifying two-word multiwords ::: in corpora have been explored in depth, and there are now ::: well-established methods in widespread use.
(We use ::: ‘multiwords’ to include collocations, colligations, idioms and ::: set phrases etc.)
But many multiwords are of more than two ::: words and research for items of three and more words has been ::: less successful.
We present three complementary strategies, all ::: implemented and available in the Sketch Engine.
The first, ::: ‘multiword sketches’, starts from the word sketch for a word ::: and lets a user click on a collocate to see the third words ::: that go with the node and collocate.
In the word sketch for ::: take, one collocate is care.
We can click on that to find ::: ensure, avoid: take care to ensure, take care to avoid.
The ::: second, ‘commonest match’, will find these full expressions, ::: including the to.
We look at all the examples of a collocation ::: (represented as a pair/triple of lemmas plus grammatical ::: relation(s)) and find the commonest forms and order of the ::: lemmas, plus any other words typically found in that same ::: collocation.
For baby and bathwater we find throw the baby out ::: with the bathwater.
The third, ‘multi level tokenization’, ::: allows intelligent handling of items like in front of, which ::: are, arguably, best treated as a single token, so lets us find ::: its collocates: mirror, camera, crowd.
While the methods have ::: been tested and exemplified with English, we believe they will ::: work well for many languages.

This paper addresses a fundamental short-run resource-allocation problem confronting retail distribution: simultaneously finding the specific brands, from many, that should be displayed, and the amount of retail product-display area that should be assigned to these brands, in order to maximize the retail institution's profit.
The paper decomposes total market demand according to the various levels of brand preference that could conceivably exist in final markets, and then, employs an algorithm, similar to the one used to solve the fixed-charge problem, to find the optimal brand mix and display-area allocation.

Abstract The responsiveness to non-invasive neuromodulation protocols shows high inter-individual variability, the reasons of which remain poorly understood.
We here tested whether the response to intermittent theta-burst stimulation (iTBS) – an effective repetitive transcranial magnetic stimulation (rTMS) protocol for increasing cortical excitability – depends on network properties of the cortical motor system.
We furthermore investigated whether the responsiveness to iTBS is dose-dependent.
To this end, we used a sham-stimulation controlled, single-blinded within-subject design testing for the relationship between iTBS aftereffects and (i) motor-evoked potentials (MEPs) as well as (ii) resting-state functional connectivity (rsFC) in 16 healthy subjects.
In each session, three blocks of iTBS were applied, separated by 15 min.
We found that non-responders (subjects not showing an MEP increase of ≥ 10% after one iTBS block) featured stronger rsFC between the stimulated primary motor cortex (M1) and premotor areas before stimulation compared to responders.
However, only the group of responders showed increases in rsFC and MEPs, while most non-responders remained close to baseline levels after all three blocks of iTBS.
Importantly, there was still a large amount of variability in both groups.
Our data suggest that responsiveness to iTBS at the local level (i.e., M1 excitability) depends upon the pre-interventional network connectivity of the stimulated region.
Of note, increasing iTBS dose did not turn non-responders into responders.
The finding that higher levels of pre-interventional connectivity precluded a response to iTBS could reflect a ceiling effect underlying non-responsiveness to iTBS at the systems level.

In this paper, we present a general framework for designing approximation schemes for combinatorial optimization problems in which the objective function is a combination of more than one function.
Examples of such problems include those in which the objective function is a product or ratio of two linear functions, parallel machine scheduling problems with the makespan objective, robust versions of weighted multiobjective optimization problems, and assortment optimization problems with logit choice models.
The main idea behind our approximation schemes is the construction of an approximate Pareto-optimal frontier of the functions that constitute the given objective.
Using this idea, we give the first fully polynomial-time approximation schemes for the max-min resource allocation problem with a fixed number of agents, combinatorial optimization problems in which the objective function is the sum of a fixed number of ratios of linear functions, or the product of a fixed number of linear functions, and assor...

For d and k > 2, d-dimensional k-tape Turing machines cannot simulate d-dimensional Turing machines with k heads on l tape in real time.

In this paper, a novel two-dimensional unconditionally stable finite-difference time-domain (2D US-FDTD) method based on the Crank–Nicolson (CN) scheme is proposed.
In particular, in the proposed 2D US-FDTD method the field components are defined at only two time steps n and n + 1; and the original time-dependent Maxwell's equations of the Crank–Nicolson scheme are solved by introducing a proper intermediate value for a field component.
Compared to the ADI-FDTD method, the US-FDTD method offers the following two advantages: (i) the left-hand and right-hand sides of the original updating equations are balanced (with regards to time steps) as accurately as possible and (ii) only a single iteration that requires fewer updating equations is needed for the field development.
The numerical performance of the proposed US-FDTD method over the ADI-FDTD algorithm is demonstrated through numerical examples.
In particular, it is shown that the asymmetric effect (that is, the asymmetric result is obtained even for exactly symmetric computational setups), which always appears in the ADI-FDTD algorithm, is not observed in the US-FDTD method.
© 2003 Wiley Periodicals, Inc. Microwave Opt Technol Lett 38: 457–462, 2003; Published online in Wiley InterScience (www.interscience.wiley.com).
DOI 10.1002/mop.11089

Despite the frequent application of noise reduction in hearing aids, there is little research on user preference for different settings of noise reduction.
We therefore measured individual preference for noise reduction strength for speech that was embedded in background noise.
In a laboratory experiment, three types of noise (speech shaped stationary noise, party babble, and traffic noise) were processed with two single- channel noise reduction algorithms.
Ten normal-hearing and seven hearing-impaired subjects participated.
The preference for strength of noise reduction differed between the noise types and this was consistent for the two different noise reduction algorithms.
The inter-individual spread between hearing-impaired listeners was as large as between normal-hearing listeners and as a consequence we found no systematic differences between the groups.
These results support earlier findings that an individual tuning of noise reduction parameters is important.
Furthermore, the results suggest that it could be beneficial to adaptively change the setting of noise reduction in a hearing aid, depending on the type of background noise.

The integrity I(G) of a noncomplete connected graph G is a measure of network invulnerability and is defined by I(G) = min{|S + m(G - S)}, where S and m(G - S) denote the the subset of V and the order of the largest component of G - S, respectively.
In this paper, we determine the integrity and some other parameters of middle graphs of some classes of graphs.

Purpose – The purpose of this paper is to present the proposal of a Product Line (PL)-based approach for Business Process Management (BPM) projects that cover the entire BPM lifecycle and proposes integrating it with dynamic techniques still not used together.
Design/methodology/approach – The authors carried out this work using the design science research methodology.
The authors assessed the proposed approach using a classification procedure created through a series of specific attributes, which enables a comparison of the proposed integrated approach with related works selected from a systematic literature review.
Findings – The comparative assessment has shown that the proposed approach presents the most comprehensive solution than any other similar one suggested for the same purpose, mainly in terms of the coverage of the entire BPM lifecycle and dynamic techniques.
Research limitations/implications – Due to the high-level conceptual nature of the proposed approach, the authors could not evaluate it ...

For daily burn wound care and therapeutic physical therapy skin stretching procedures, powerful pain medications alone are often inadequate.
This feasibility study provides the first evidence that entering an immersive virtual environment using very inexpensive (∼$400) wide field of view Oculus Rift Virtual Reality (VR) goggles can elicit a strong illusion of presence and reduce pain during VR.
The patient was an 11-year-old male with severe electrical and flash burns on his head, shoulders, arms, and feet (36 percent total body surface area (TBSA), 27 percent TBSA were third-degree burns).
He spent one 20-minute occupational therapy session with no VR, one with VR on day 2, and a final session with no VR on day 3.
His rating of pain intensity during therapy dropped from severely painful during no VR to moderately painful during VR.
Pain unpleasantness dropped from moderately unpleasant during no VR to mildly unpleasant during VR.
He reported going "completely inside the computer generated world", and had more fun during VR.
Results are consistent with a growing literature showing reductions in pain during VR.
Although case studies are scientifically inconclusive by nature, these preliminary results suggest that the Oculus Rift VR goggles merit more attention as a potential treatment for acute procedural pain of burn patients.
Availability of inexpensive but highly immersive VR goggles would significantly improve cost effectiveness and increase dissemination of VR pain distraction, making VR available to many more patients, potentially even at home, for pain control as well as a wide range of other VR therapy applications.
This is the first clinical data on PubMed to show the use of Oculus Rift for any medical application.

Abstract Recommender systems for online broadcasting become important as the number of channels has been increasing.
In online broadcasting, to provide accurate recommendation, recommender systems should take time factors as well as users’ condition into account, but the conventional systems don’t.
This paper proposes a real-time recommender system for online broadcasting called RecTime which considers time factors and preferences simultaneously.
Specifically, RecTime employs a 4-d tensor factorization, which considers two more dimensions regarding the time factors, while typical collaboriative filtering methods only consider two dimensions, users and items.
By factorizing the 4-d tensor, the system naturally identifies the recommendation time and the items at the same time.
In our experiments on real-world data, RecTime properly models users’ watching patterns and significantly outperforms previous methods in terms of the accuracy on the recommendation time as well as the items.

Supply Chain Event Management (SCEM) is an approach aiming to fill the gap between Supply Chain Planning and Supply Chain Execution.
The goal of SCEM applications is to monitor the states of supply chains by observing specific events and exceptions in real-time and alerting managers if problems occur.
This paper presents an architecture for a mobile SCEM system based on software agents, Auto-ID technologies, and mobile computing.
It introduces the main layers and components of this architecture.
A special focus is placed on web services, mobile user interfaces, and integration of a multi-agent platform as innovative solutions to improve SCEM.

The paper presents a novel echo-hiding method for audio watermarking.
The method is quite different from previous echo-hiding methods since it presents a new echo kernel which introduces a forward kernel as well as a backward kernel.
The new kernel, a combination of the backward and forward kernels, can enhance considerably the watermark detection rate.
Thus, it is possible to reduce echo amplitude.
The paper proves mathematically that the combination of kernels improves watermark performance.
Experimental results show that the proposed watermarking scheme is much better than previous echo-hiding schemes in terms of detection rate and imperceptibility.

In a discussion concerning the future of the library, Minsky and Feigenbaum endorsed the idea for books to ‘talk to each other’.
Creating knowledge or doing inference by connecting related documents is an important aspect of undiscovered public knowledge, an important concept initiated by Swanson.
However, so far few computer models exist in this regard.
In this paper, we describe such a model which connects documents.
We first explain the meaning of integration of scientific texts by connecting documents.
Our fundamental idea is then illustrated by an example and further described by a model.
Some features of this model are discussed.
A brief comparison with some other computer models is also provided.

Many researchers have suggested that schematic processes and prototype formation are important in face recognition (Goldstein & Chance, 1974; Malpass, 1975).
A prototype is thought of as the “best” exemplar of a set or category of objects.
Among it’s properties is a high probability of being identified as a familiar entity in the context of the category it typifies.
In the case of recognition, category prototypes have a high likelihood of being “recognized” as entities having been previously seen, even if they have actually not been previously seen.
It is thought that this is because more than any other category member, they summarize, or represent, attributes of the category.

There was a time that data warehouse implementations were relatively isolated within the IT infrastructure of an organization.
Data extract programs snuck out in the middle of the night or over a weekend to steal away with data from the opera­tional systems and load it into the informational repository (data warehouse).
A select group of knowledge workers (usually in marketing, finance, strategic plan­ning, etc.)
had access to this information for reporting and analysis, but the re­quirements for connectivity to large numbers of users outside the corporate ivory tower and interoperability with production systems were minimal With the expanded scope of successful data warehouses to encompass tactical decision making in addition to traditional decision support applications (cf.
Brobst, Rarey 2001), the need for better integration into the mainstream of IT infrastructure has become essential.
As a result, Enterprise Application Integra­tion (EAI) has emerged as a critical component in the architecture of advanced data warehouse implementations.
When properly deployed, EAI provides a vehi­cle for transitioning from back office decision making to tactical decision support on the front-lines to impact execution of the enterprise strategy.
EAI plays a parti­cularly important role when implementing the extreme data freshness service levels required for Active Data Warehousing

Results from a field trial involving the use of USB iKeys as a secure access mechanism for remote access of patient medical records from a central server are reported.
These are discussed within the context of eHealth generally, where technological considerations can easily be outweighed by concerns of patient privacy, security and confidentiality.

This paper makes a case for extending the techniques of automatic extraction of man-made objects from aerial images to include all primary elements (objects) of the urban landscape.
Techniques to visualize urban landscapes for use in urban landscape planning and design are reaching a mature and productive stage of development.
A lack of affordable, usable three dimensional digital data is holding back the use of these techniques in everyday practice.

In this work we propose a new architecture for musical distortion.
WaveShaping (a sound synthesis technique from the mid-late 1970s) is used here as the basic distortion generation element.
WaveShaping can easily and precisely generate upper partials from a flat-envelope pure sinusoidal signal.
The extent and distribution of the overtones series can be adjusted at will.
In order to apply WS techniques to distort real world audio signals, a multiband front-end is used.
The front-end filterbank extracts simple (ideally monotonal) nonfull band signals, whose amplitude envelope is flattened previous to WS.
This way, WS can be adjusted (to set the relative magnitude of newly generated overtones and to avoid aliasing components) and applied in a per-band basis.
As another benefit, the structure makes intermodulation distortion less of a problem, balancing the end result towards “harmonic” distortion rather than “metallic” or “buzzing”.
The proposal must be understood in terms of a “distortion- synthesiser” rath...

The authors have measured an Arnstein gap, that is, a significant difference between desired and actual levels of citizen participation in planning processes.
This Arnstein gap exists because even well-intentioned professionals have an unrealistic expectation of achieving consensus across large planning scales.
Further, it is often hoped or believed that technologies of representation will somehow accomplish consensus.
The authors argue this is not possible without developing a stronger theoretical framework for their deployment in planning in democratic societies.
The purpose of this research is to move the public closer to the center of the public infrastructure planning and design process in a productive, efficient, and more satisfactory manner, that is, to close the Arnstein gap.
The authors adapt a participatory framework, called structured public involvement (SPI), for integrating visualization and geospatial technologies into large-scale public involvement in planning domains.
The authors discuss how SPI using the casewise visual evaluation method is applied in collaboration with planners.
A case study is presented of integrated transportation and land-use planning for an Indiana city.
The results demonstrate that SPI achieves high levels of stakeholder satisfaction in addition to providing high-quality planning and design guidance for professionals.

Automation of forging processes is important for both safety and efficiency in today's advanced manufacturing operations.
This work supports the development of an Intelligent Open Die Forging System which will integrate state-of-the-art modelling techniques, automatic die selection and sequencing, full system dynamic simulation, automatic machine programming and coordination, and sensor-based process control to enable the production of more general and complex workpiece geometries than are achievable using current forging methods.
Effective automation of this open die forging system requires the coordination and control of the major system components: press, robot, and furnace.
In particular, forces exerted on the robot through its manipulation of the workpiece during forging must be minimized to avoid damage to the manipulator mechanism.
In this paper, the application of neural networks for compliance control of the forging robot to minimize these forces is investigated.
Effectiveness of the neural network-based compliance control module is evaluated through a full dynamic system simulation, which will later form a central part of the complete Intelligent Forging System.
Dynamic simulation of the robot is achieved using an efficient O(N) recursive algorithm, while material flow of the workpiece is modeled with a finite element approach.
Simulation and timing results for the complete processing system for a specific open die forging example are presented.

We study the convergence behavior of a sequence of stationary points of a parametric NLP which regularizes a mathematical program with equilibrium constraints (MPEC) in the form of complementarity conditions.
Accumulation points are feasible points of the MPEC; they are C-stationary if the MPEC linear independence constraint qualification holds; they are M-stationary if, in addition, an approaching subsequence satisfies second order necessary conditions, and they are B-stationary if, in addition, an upper level strict complementarity condition holds.
These results complement recent results of Fukushima and Pang [Convergence of a smoothing continuation method for mathematical programs with equilibrium constraints, in Ill-posed Variational Problems and Regularization Techniques, Springer-Verlag, New York, 1999].
We further show that every local minimizer of the MPEC which satisfies the linear independence, upper level strict complementarity, and a second order optimality condition can be embedded into a locally unique piecewise smooth curve of local minimizers of the parametric NLP.

Quantization errors in discrete-cosine-transform (DCT) video compression are known as DCT residues.
Knowledge on their distribution is essential in understanding rate-distortion (R-D) behaviors of generic video coding.
Traditional R-D analysis adopted a simplified distortion model.
Those distortion models took only quantization parameter into account.
They lack adaptability to variation of video sources, as the distribution of coding errors also depends on the statistics of video source.
Another common approach models the distribution of DCT residues by fitting experimental data from coded pictures to conjectured statistical distributions, but it did not provide insights into what gives rise to the distribution of DCT residues.
This paper intends to quantify the distribution of DCT residues with respect to video source and with respect to the quantization strategy by understanding the quantization of DCT frequency components.
Moreover, it is applied to derive an R-D model to show the advantage of the proposed distribution model.

Bankruptcy prediction has been a topic of research for decades, both within the financial and the academic world.
The implementations of international financial and accounting standards, such as Basel II and IFRS, as well as the recent credit crisis, have accentuated this topic even further.
This paper describes both regularized and non-linear kernel variants of traditional discriminant analysis techniques, such as logistic regression, Fisher discriminant analysis (FDA) and quadratic discriminant analysis (QDA).
Next to a systematic description of these variants, we contribute to the literature by introducing kernel QDA and providing a comprehensive benchmarking study of these classification techniques and their regularized and kernel versions for bankruptcy prediction using 10 real-life data sets.
Performance is compared in terms of binary classification accuracy, relevant for evaluating yes/no credit decisions and in terms of classification accuracy, relevant for pricing differentiated credit granting.
The results clearly indicate the significant improvement for kernel variants in both percentage correctly classified (PCC) test instances and area under the ROC curve (AUC), and indicate that bankruptcy problems are weakly non-linear.
On average, the best performance is achieved by LSSVM, closely followed by kernel quadratic discriminant analysis.
Given the high impact of small improvements in performance, we show the relevance and importance of considering kernel techniques within this setting.
Further experiments with backwards input selection improve our results even further.
Finally, we experimentally investigate the relative ranking of the different categories of variables: liquidity, solvency, profitability and various, and as such provide new insights into the relative importance of these categories for predicting financial distress.

Higher-order logic programming languages such as Elf extend first-order logic programming in two ways: first-order terms are replaced with (dependently) typed A-terms and the body of clauses may contain implication and universal quantification.
In this paper, we describe tabled higher-order logic programming where some redundant computation is eliminated by memoizing sub-computation and re-using its result later.
This work extends Tamaki and Sato's search strategy based on memoization to the higher-order setting.
We give a proof-theoretic characterization of tabling based on uniform proofs and prove soundness of the resulting interpreter.
Based on it, we have implemented a prototype of a tabled logic programming interpreter for Elf.

The paper describes a exploratory procedure for data analysis for use within GIS.
The objective is to search digital map databases for the presence of geographical relationships that may be useful for descriptive purposes, as a pointer towards areas for further investigation, and as a means of generating hypotheses for subsequent testing.
A prototype Geographical Correlates Exploration Machine (GCEM) is demonstrated by searching for possible linkages between children with leukaemia and a selection of environmental coverages.
Arc Info is used for the GIS parts and a Cray X-MP/48 supercomputer for the analysis.
Ultimately, it is envisaged that GCEM will be able to run entirely within a GIS workstation environment.

Abstract Information workers as yet have not fully understood the relationship existing between information and development.
After a short exposition of this relationship, in which the author indicates that information should be seen as one of the essential know‐how resources, this article argues the value and necessity of information for development.
An attempt is made to classify the various areas where information is needed for development, as well as the information systems and infrastructures available and/or required to provide for the different needs.
A number of reasons are given as to why information has not yet played the role in development that it could play.
In conclusion, some guidelines are given for optimal use of information as a resource for development.

The linear exponential distribution is a very well-known distribution for modeling lifetime data in reliability and medical studies.We introduce in this paper a new four-parameter generalized version of the transmuted generalized linear exponential distribution.We provide a comprehensive account of the mathematical properties of the new distributions.
In particular, A closed-form expressions for the density, cumulative distribution ,quantile and median of the distribution is given.
Also, the rth order moment and moment generating function are derived.
The maximum likelihood estimation of the unknown parameters is discussed.
Real data are used to determine whether the TGLED is better than other well-known distributions in modeling lifetime data or not.

Computer-assisted instruction (CAI) has been shown to enhance rote memory skills and improve higher order critical thinking skills.
The challenge now is to identify what aspects of CAI improve which specific higher-order skills.
This study focuses on the effectiveness of using CAI to teach logarithmic graphing and dimensional analysis.
Two groups of ninth graders participated in a one-class period laboratory.
Experiment 1 compared a fully automated computer laboratory to an equivalent paper-and-pencil exercise.
Experiment 2 compared the same automated computer laboratory in Experiment 1 with a revised, less automated computer version.
Both the paper-and-pencil exercise and the less automated computer exercise required students to perform basic mathematical calculations.
The results from a post-test revealed that very few students were able to master the complex task of dimensional analysis, but students who took the paper-based and revised, less automated version scored higher overall.
These results imply that students required to perform basic calculations had a better understanding of the lab as a whole.
These results suggest that until students master basic skills, they do not have the cognitive resources to concentrate on higher-order concepts.
This is supported by cognitive load theory.

Desharnais, Gupta, Jagadeesan and Panangaden introduced a family of behavioural pseudometrics for probabilistic transition systems.
These pseudometrics are a quantitative analogue of probabilistic bisimilarity.
Distance zero captures probabilistic bisimilarity.
Each pseudometric has a discount factor, a real number in the interval (0, 1].
The smaller the discount factor, the more the future is discounted.
If the discount factor is one, then the future is not discounted at all.
Desharnais et al.
showed that the behavioural distances can be calculated up to any desired degree of accuracy if the discount factor is smaller than one.
In this paper, we show that the distances can also be approximated if the future is not discounted.
A key ingredient of our algorithm is Tarski's decision procedure for the first order theory over real closed fields.
By exploiting the Kantorovich-Rubinstein duality theorem we can restrict to the existential fragment for which more efficient decision procedures exist.

With increasing popularity of the Internet and tremendous amount of on-line text, automatic document classification is important for organizing huge amounts of data.
Readers can know the subject of many document fields by reading only some specific Field Association (FA) words.
Document fields can be decided efficiently if there are many FA words and if the frequency rate is high.
This paper proposes a method for automatically building new FA words.
A WWW search engine is used to extract FA word candidates from document corpora.
New FA word candidates in each field are automatically compared with previously determined FA words.
Then new FA words are appended to an FA word dictionary.
From the experiential results, our new system can automatically appended around 44% of new FA words to the existence FA word dictionary.
Moreover, the concentration ratio 0.9 is also effective for extracting relevant FA words that needed for the system design to build FA words automatically.

We prove that for superadditive games a necessary and sufficient condition for the bargaining set to coincide with the core is that the monotonic cover of the excess game induced by a payoff be balanced for each imputation in the bargaining set.
We present some new results obtained by verifying this condition for specific classes of games.
For N-zero-monotonic games we show that the same condition required at each kernel element is also necessary and sufficient for the kernel to be contained in the core.
We also give examples showing that to maintain these characterizations, the respective assumptions on the games cannot be lifted.

Meijer, G.R., L.O.
Hertzberger, T.L.
Mal, E. Gaussens, and F. Arlabosse, Exception handling system for autonomous robots based on PES, Robotics and Autonomous Systems 7 (1991) 197-209.
To reach autonomy of a robot system, the robot needs to be able to adapt itself to the changing environment.
One of the key capabilities required for such a system is exception handling.
We realized exception handling mechanisms for autonomous robots with a Procedural Expert System (PES).
The work described in this paper is based on the Exception Handling Model (EHM), a model describing the handling of exceptions within the framework of an a priori plan.
Exception handling provides the robot with capabilities to monitor its operations.
Upon the detection of an exception, a diagnosis can be performed which leads to a classification of the exception at hand.
Task rescheduling and recovery planning is performed based on heuristic knowledge of the application.
Modelling of a robot assembly application and the realization of EHM with PES are described in the paper.

Abstract Weigl, M., Siemiȧatkowska, B., Sikorski, K.A.
and Borkowski, A., Grid-based mapping for autonomous mobile robot, Robotics and Autonomous Systems, 11 (1993) 13–21.
A mapping module for the mobile robot equipped with the ultrasonic range finder is presented.
The environment is described by a grid of cells that can be either free or occupied.
A two-stage processing of data coming from the sonar is proposed: first, the readings are filtered and composed into the local model, then the latter is aggregated into the global map.
In order to account for random errors the formulae based upon Shafer theory are employed.
The proposed procedure is able to reproduce correctly the indoor environment as documented by the results of tests performed on a prototype robot.

In this paper, we consider single-machine due window assignment and scheduling with a common flow allowance and controllable job processing times, subject to unlimited or limited resource availability.
Due window assignment with a common flow allowance means that each job has a job-dependent due window, the starting time and completion time of which are equal to its actual processing time plus the job-independent parameters q1 and q2, respectively, which are common to all the jobs.
The processing time of each job is either a linear or a convex function of the amount of a common continuously divisible resource allocated to the job.
We study five versions of the problem that differ in terms of the objective function and processing time function being used.
We provide structural properties of the optimal schedules and polynomial-time solution algorithms for the considered problems.

We focus on two criticisms of Bulk Synchronous Parallelism (BSP): that delaying communication until specific points in a program causes poor performance, and that frequent barrier synchronisations are too expensive for high-performance parallel computing.
We show that these criticisms are misguided, not just about BSP but about parallel programming in general, because they are based on misconceptions about the origins of poor performance.
The main implication for parallel programming is that higher levels of abstraction do not only make software construction easier—they also make high-performance implementation easier.

We consider a three-terminal state-dependent relay channel with the channel state available noncausally at only the source.
Such a model may be of interest for node cooperation in the framework of cognition, i.e., collaborative signal transmission involving cognitive and noncognitive radios.
We study the capacity of this communication model.
One principal problem is caused by the relay's not knowing the channel state.
For the discrete memoryless (DM) model, we establish two lower bounds and an upper bound on channel capacity.
The first lower bound is obtained by a coding scheme in which the source describes the state of the channel to the relay and destination, which then exploit the gained description for a better communication of the source's information message.
The coding scheme for the second lower bound remedies the relay's not knowing the states of the channel by first computing, at the source, the appropriate input that the relay would send had the relay known the states of the channel, and then transmitting this appropriate input to the relay.
The relay simply guesses the sent input and sends it in the next block.
The upper bound accounts for not knowing the state at the relay and destination.
For the general Gaussian model, we derive lower bounds on the channel capacity by exploiting ideas in the spirit of those we use for the DM model; and we show that these bounds are optimal for small and large noise at the relay irrespective to the strength of the interference.
Furthermore, we also consider a relay model with orthogonal channels from the source to the relay and from the source and relay to the destination in which the source input component that is heard by the relay does not depend on the channel states.
We establish a better upper bound for both DM and Gaussian cases and we also characterize the capacity in a number of special cases.

Abstract We review the theory and practice of the multilayer perceptron.
We aim at addressing a range of issues which are important from the point of view of applying this approach to practical problems.
A number of examples are given, illustrating how the multilayer perceptron compares to alternative, conventional approaches.
The application fields of classification and regression are especially considered.
Questions of implementation, i.e.
of multilayer perceptron architecture, dynamics, and related aspects, are discussed.
Recent studies, which are particularly relevant to the areas of discriminant analysis, and function mapping, are cited.

In this thesis, I study the spectral characteristics of large dynamic networks and formulate the spectral evolution model.
The spectral evolution model applies to networks that evolve over time, and describes their spectral decompositions such as the eigenvalue and singular value decomposition.
The spectral evolution model states that over time, the eigenvalues of a network change while its eigenvectors stay approximately constant.
::: I validate the spectral evolution model empirically on over a hundred network datasets, and theoretically by showing that it generalizes arncertain number of known link prediction functions, including graph kernels, path counting methods, rank reduction and triangle closing.
The collection of datasets I use contains 118 distinct network datasets.
One dataset, the signed social network of the Slashdot Zoo, was specifically extracted during work on this thesis.
I also show that the spectral evolution model can be understood as a generalization of the preferential attachment model, if we consider growth in latent dimensions of a network individually.
As applications of the spectral evolution model, I introduce two new link prediction algorithms that can be used for recommender systems, search engines, collaborative filtering, rating prediction, link sign prediction and more.
::: The first link prediction algorithm reduces to a one-dimensional curve fitting problem from which a spectral transformation is learned.
The second method uses extrapolation of eigenvalues to predict future eigenvalues.
As special cases, I show that the spectral evolution model applies to directed, undirected, weighted, unweighted, signed and bipartite networks.
For signed graphs, I introduce new applications of the Laplacian matrix for graph drawing, spectral clustering, and describe new Laplacian graph kernels.
I also define the algebraic conflict, a measure of the conflict present in a signed graph based on the signed graph Laplacian.
I describe the problem of link sign prediction spectrally, and introduce the signed resistance distance.
For bipartite and directed graphs, I introduce the hyperbolic sine and odd Neumann kernels, which generalize the exponential and Neumann kernels for undirected unipartite graphs.
I show that the problem of directed and bipartite link prediction are related by the fact that both can be solved by considering spectral evolution in the singular value decomposition.

Abstract In this paper, we investigate the existence of positive solutions for multi-point boundary value problems on the half-line.
Under various weaker conditions, we establish various results on the existence of multiple positive solutions for multi-point boundary value problems on the half-line by applying the fixed-point theorem of cone expansion and compression type due to Krasnosel’skill in a special function space.
Especially, we allow the nonlinear terms to have suitable singularities.
Our results significantly extend and improve many known results even for nonsingular cases.

Wireless microsensor networks usually consist of a large number of small sensor nodes with limited onboard energy supply and deployed densely in a given area for information harvesting purposes.
To reduce energy consumption and prolong network lifetime, clustering techniques are often used, among which the grid-based ones are very popular due to their simplicity and scalability.
In this paper, we analyze and evaluate the energy-optimal grid size for a grid-based clustering and routing scheme proposed specifically for wireless microsensor networks.
Through numerical and simulation results, we reveal the tradeoff generic to all grid-based clustering schemes.
In addition, we propose a randomized technique to further prolong the network lifetime and discuss other energy-saving opportunities.
This paper provides some insights into the intrinsic limits of grid-based clustering schemes for wireless micro sensor networks.

Abstract We present a fully spectral methodology for magnetohydrodynamic (MHD) calculations in a whole sphere.
The use of Jones–Worland polynomials for the radial expansion guarantees that the physical variables remain infinitely differentiable throughout the spherical volume.
Furthermore, we present a mathematically motivated and systematic strategy to relax the very stringent time step constraint that is present close to the origin when a spherical harmonic expansion is used for the angular direction.
The new constraint allows for significant savings even on relatively simple solutions as demonstrated on the so-called full sphere benchmark, a specific problem with a very accurately-known solution.
The numerical implementation uses a 2D data decomposition which allows it to scale to thousands of cores on present-day high performance computing systems.
In addition to validation results, we also present three new whole sphere dynamo solutions that present a relatively simple structure.

Abstract We prove that three graphs of order n and size less than or equal to n −3 can be packed (edge-disjointly) into the complete graph K n .

Abstract This paper considers a fluid queueing system, fed by N independent sources that alternate between silence and activity periods.
We assume that the distribution of the activity periods of one or more sources is a regularly varying function of index ζ.
We show that its fat tail gives rise to an even fatter tail of the buffer content distribution, viz., one that is regularly varying of index ζ + 1.
In the special case that ζ ϵ (−2, −1), which implies long-range dependence of the input process, the buffer content does not even have a finite first moment.
As a queueing-theoretic by-product of the analysis of the case of N identical sources, with N → ∞, we show that the busy period of an M/G/∞ queue is regularly varying of index ζ iff the service time distribution is regularly varying of index ζ.

Purpose – Data mining (DM) is used to improve the performance of manufacturing quality control activity, and reduces productivity loss.
The purpose of this paper is to discover useful hidden patterns from fabric data to reduce the amount of defective goods and increase overall quality.Design/methodology/approach – This research examines the improvement of manufacturing process via DM techniques.
The paper explores the use of different preprocessing and DM techniques (rough sets theory, attribute relevance analysis, anomaly detection analysis, decision trees and rule induction) in carpet manufacturing as the real world application problem.
SPSS Clementine Programme, Rosetta Toolkit, ASP (Active Server Pages) and VBScript programming language are used.Findings – The most important variables of attributes that are effective in product quality are determined.
A decision tree (DT) and decision rules are generated.
Therefore, the faults in the process are detected.
An on‐line programme is generated and the mode...

It is well known that the trapezoidal rule converges geometrically when applied to analytic functions on periodic intervals or the real line.
The mathematics and history of this phenomenon are reviewed, and it is shown that far from being a curiosity, it is linked with computational methods all across scientific computing, including algorithms related to inverse Laplace transforms, special functions, complex analysis, rational approximation, integral equations, and the computation of functions and eigenvalues of matrices and operators.

Computer systems are modeled before construction to minimize errors and performance bottlenecks.
A common modeling approach is to build software models of computer system components, and use realistic trace data as input.
This methodology is commonly referred to as trace-driven simulation.
Trace-driven simulation can be very accurate if both the system model and input trace data represent the systems under test.
The accuracy of the model is typically under the control of the research, but little or no trace data is available that accurately represents current of future workloads.
The objective of this work is to describe the Brigham Young University Address Collection Hardware (BACH) and illustrate the types of traces that we can collect.
These traces will be made available to others in a national trace repository(http://traces.byu.edu).
We also provide some cache performance statistics for the SPEC 2000 integer benchmarks.

Software Engineering - From Auxiliary to Key Technology.- The Relevance of the Software Pioneers for sd&m.- From the Stack Principle to ALGOL.- Sequentielle Formelubersetzung.- Verfahren zur automatischen Verarbeitung von kodierten Daten und Rechenmaschinen zur Ausubung des Verfahrens.- The Roots of Object Orientation: The Simula Language.- Class and Subclass Declarations.- Pascal and Its Successors.- The Programming Language Pascal.- Program Development by Stepwise Refinement.- The IBM Operating System/360.- The Functional Structure of OS/360.- Graphical User Interfaces.- B-Trees and Databases, Past and Future.- Organization and Maintenance of Large Ordered Indexes.- A Relational Model of Data for Large Shared Data Banks.- Entity-Relationship Modeling: Historical Events Future Trends and Lessons Learned.- The Entity Relationship Model - Toward a Unified View of Data.- EWD 1308: What Led to "Notes on Structured Programming".- Solution of a Problem in Concurrent Programming Control.- Go To Statement Considered Harmful.- Assertions: A Personal Perspective.- An Axiomatic Basis for Computer Programming.- Proof of Correctness of Data Representations.- The Secret History of information Hiding.- On the Criteria to Be Used in Decomposing Systems into Modules.- On a "Buzzword": Hierarchical Structure.- Abstract Data Types, Then and Now.- Abstract Data Types and the Development of Data Structures.- JSP in Perspective.- Constructive Methods of Program Design.- Structured Analysis: Beginnings of a New Discipline.- Structure Analysis and System Specification.- A History of Software Inspections.- Design and Code Inspections to Reduce Errors in Program Development.- Advances in Software Inspections.- Early Experiences in Software Economics.- Software Engineering Economics.- Design Patterns - Ten Years Later.- Design Patterns: Abstraction and Reuse of Object-Oriented Design.

Exciting new developments in hardware and software have made computer-based multimedia presentations a reality.
Here is a comprehensive guide that teaches people what they need to know to create and produce multimedia products with high-tech tools.
Includes a reference section of resources: books, magazines, products, and organizations.

The outline of this chapter is as follows.
In Section 1.2 we discuss the philosophical and logical origins of abduction and induction.
In Section 1.3 we analyse previous work on abduction and induction in the context of logic programming\indexlogic programming and artificial intelligence, and attempt a (partial) synthesis of this work.
Section 1.4 considers the integration of abduction and induction in artificial intelligence, and Section 1.5 concludes.

LTE supports Orthogonal Frequency Division Multiple Access (OFDMA) communication system where frequency reuse of one is used, i.e.
all cells/sectors operate on the same frequency channel to maximize spectral efficiency.
However, due to heavy Co-channel Interference (CCI) in frequency reuse one deployment, UEs at the cell edge may suffer degradation in connection quality.
With LTE, UEs operate on subchannels, which only occupy a small fraction of the whole channel bandwidth; the cell edge interference problem can be easily addressed by appropriately configuring subchannel usage without resorting to traditional frequency planning.

To understand collective motion of real neural networks very well, we investigate collective phase synchronization of small world chaotic Hindmarsh-Rose (HR) neural networks.
By numerical simulations, we conclude that small world chaotic HR neural networks can achieve collective phase synchronization.
Furthermore, it is shown that phase synchronization of small world chaotic HR neural networks is dependent on the coupling strength,the connection topology (which is determined by the probability ]3), as well as the coupling number.
These phenomena are important to guide us to understand the synchronization of real neural networks.

In this paper, we investigate a class of memristor-based BAM neural networks with time-varying delays.
Under the framework of Filippov solutions, boundedness and ultimate boundedness of solutions of memristor-based BAM neural networks are guaranteed by Chain rule and inequalities technique.
Moreover, a new method involving Yoshizawa-like theorem is favorably employed to acquire the existence of periodic solution.
By applying the theory of set-valued maps and functional differential inclusions, an available Lyapunov functional and some new testable algebraic criteria are derived for ensuring the uniqueness and global exponential stability of periodic solution of memristor-based BAM neural networks.
The obtained results expand and complement some previous work on memristor-based BAM neural networks.
Finally, a numerical example is provided to show the applicability and effectiveness of our theoretical results.

Abstract The human brain is organized as a dynamic network, in which both regional brain activity and inter-regional connectivity support high-level cognitive processes, such as reading.
However, it is still largely unknown how the functional brain network organizes to enable fast and effortless reading processing in the native language (L1) but not in a non-proficient second language (L2), and whether the mechanisms underlying local activity are associated with connectivity dynamics in large-scale brain networks.
In the present study, we combined activation-based and multivariate graph-theory analysis with functional magnetic resonance imaging data to address these questions.
Chinese–English unbalanced bilinguals read narratives for comprehension in Chinese (L1) and in English (L2).
Compared with L2, reading in L1 evoked greater brain activation and recruited a more globally efficient but less clustered network organization.
Regions with both increased network efficiency and enhanced brain activation in L1 reading were mostly located in the fronto-temporal reading-related network (RN), whereas regions with decreased global network efficiency, increased clustering, and more deactivation in L2 reading were identified in the default mode network (DMN).
Moreover, functional network efficiency was closely associated with local brain activation, and such associations were also modulated by reading efficiency in the two languages.
Our results demonstrate that an economical and integrative brain network topology is associated with efficient reading, and further reveal a dynamic association between network efficiency and local activation for both RN and DMN.
These findings underscore the importance of considering interregional connectivity when interpreting local BOLD signal changes in bilingual reading.

One of the successful methods in classification problems is feature selection.
Feature selection algorithms; try to classify an instance with lower dimension, instead of huge number of required features, with higher and acceptable accuracy.
In fact an instance may contain useless features which might result to misclassification.
An appropriate feature selection methods tries to increase the effect of significant features while ignores insignificant subset of features.
In this work feature selection is formulated as an optimization problem and a novel feature selection procedure in order to achieve to a better classification results is proposed.
Experiments over a standard benchmark demonstrate that applying Bee Colony Optimization in the context of feature selection is a feasible approach and improves the classification results.

In this work, we present a new method for the restoration of images degraded by noise and spatially invariant blur.
In the proposed method, the original image restoration problem is replaced by an equality constrained minimization problem.
A quasi-Newton method is applied to the first-order optimality conditions of the constrained problem.
In each quasi-Newton iteration, the hessian of the Lagrangian is approximated by a circulant matrix and the Fast Fourier Transform is used to compute the quasi-Newton step.
The quasi-Newton iteration is terminated according to the discrepancy principle.
Results of numerical experiments are presented to illustrate the effectiveness and usefulness of the proposed method.

We test an uninvestigated proposition from spiral of silence theory that fear of social isolation (FSI) prompts people to seek out information about the climate of public opinion.
Taking a trait-based individual difference perspective, the authors develop and validate a measure of FSI that is less likely to produce the interpretational problems that plague existing measures.
Then, using data from eight countries spread across four continents, the authors examine whether those who fear social isolation to a greater extent are more likely to attend to a particular source of information in the social environment about public opinion—mass media reports of public opinion polls.
Study results support spiral of silence theory’s prediction—FSI does appear to motivate people to ascertain what the public thinks.
However, there may be some cultural boundaries to this process.

This action research project was conducted to improve reading comprehension with second grade and third grade students.
The teacher researchers intended to improve reading comprehension by using higher-order thinking skills such as predicting, making connections, visualizing, inferring, questioning, and summarizing.
In their classrooms the teacher researchers modeled these strategies through the think-aloud process and graphic organizers.
This was followed by students using these strategies through whole class, small group, and independent practice.
The teacher researchers gathered information prior to implementing the reading strategy interventions.
The Metacomprehension Strategy Index indicated a lack of student knowledge of strategies to use before, during, and after reading.
The State Snapshot of Early Literacy given to the second grade students identified 9 of the 16 students below target level.
The Test Ready Test given to the third grade students indicated 10 of the 17 students were at risk for reading comprehension failure.
The information gathered by the teacher researchers after the interventions had been modeled and practiced showed improvement with the second and third grade students.
The postintervention scores for the Metacomprehension Strategy Index showed a significant increase in students’ knowledge of the reading comprehension strategies.
The State Snapshot of Early Literacy post-intervention scores indicated only 6 of the 16 second grade students remained below target level for reading comprehension.
The Test Ready Test given to third grade students indicated only 2 of the 16 students had post-intervention scores that were at risk for reading comprehension failure.

Achieving long battery lives or even self sustainability has been a long standing challenge for designing mobile devices.
This paper seamlessly integrates two promising energy-saving technologies, namely mobile computation offloading (MCO) and microwave power transfer (MPT), and proposes a novel design framework of wirelessly powered MCO.
Consider a single-user system where a base station (BS) either transfers power to or offloads computation from a mobile.
Two mobile operation modes, namely local computation and offloading, are optimized separately for maximizing the mobile energy savings.
For local computation, the non-convex problem of optimizing the CPU- cycle frequencies under the deadline and energy causality constraints is solved via convex relaxation.
The optimal CPU- cycle frequencies are shown to have different forms depending on the BS transmission power.
For offloading, the time duration before the deadline is divided for separate MPT and offloading and the optimal division is derived in a closed form.
By combining above results, the optimal offloading decision is analyzed with respect to the deadline, data-input size and BS transmission power and validated by simulation.

Given a collection of closed subspaces of a Hilbert space, the method of alternating projections produces a sequence which converges to the orthogonal projection onto the intersection of the subspaces.
A large class of problems in medical and geophysical image reconstruction can be solved using this method.
A sharp error bound will enable the userto estimate accurately the number of iterations necessary to achieve a desired relative error.
We obtain the sharpest possible upper bound for the case of two subspaces, and the sharpest known upper bound for more than two subspaces.

Abstract The purpose of this paper is manifold.
In a first part, we present a new alternating least squares (ALS)-based method for estimating the matrix factors of a Kronecker product, the so-called Kronecker ALS (KALS) method.
Four other methods are also briefly described.
In a second part, we consider the design of multiple-input multiple-output (MIMO) wireless communication systems using tensor modelling.
Eight systems are presented in a unified way, and their theoretical performance is compared in terms of maximal diversity gain.
Exploiting a Kronecker product of symbol and channel matrices, and applying the algorithms introduced in the first part, we propose three semi-blind and two supervised receivers, called Kronecker receivers, for jointly estimating the channel and the transmitted symbols.
Necessary identifiability conditions are established.
Finally, extensive Monte Carlo simulation results are provided to compare the performance of three tensor-based systems, on the one hand, and of the five proposed Kronecker receivers for the tensor space-time-frequency (TSTF) coding system, on the other hand.

We have developed a full-function multiple sequence editor (ESEE) for use with IBM-PC compatible microcomputers.
ESEE permits the simultaneous editing, manipulation and display of several macromolecular sequences with the same degree of ease and generality afforded by commercial word processors, but with correct line wrapping.
In addition to multiple sequence alignment, ESEE can serve as a universal front-end for analysis programs and as a utility for producing publication-quality figures

The application of different window functions to the STDFT method for gene prediction that identifies the well known period-three property is explored.
Window functions are employed to suppress spectral noise originating from non-coding regions in the DNA sequence.
Results are obtained on the choice of a suitable window and a method for tailoring a window to a particular gene is proposed.

It is argued here that we must establish a demonstrably sound, comprehensive, rigorously formalized theoretical foundation upon which to base practical computer-aided architectural design software-development efforts, and a general approach to this task is suggested.
First, the basic types of primitives, structures, and operations that we might employ are considered.
Next, the concept of a formal architectural language is developed.
Finally, design synthesis is viewed as a process of searching within such a language to find a particular design which, under specified algorithms that establish the semantics of the language, has acceptable interpretations.
The implications of a program of formalization of architectural knowledge along these lines for research, development, teaching, and design practice are discussed.

The development of support systems for surgery significantly increases the likelihood of obtaining satisfactory results.
In the case of fracture reduction interventions these systems enable surgery planning, training, monitoring and assessment.
They allow improvement of fracture stabilization, a minimizing of health risks and a reduction of surgery time.
Planning a bone fracture reduction by means of a computer assisted simulation involves several semiautomatic or automatic steps.
The simulation deals with the correct position of osseous fragments and fixation devices for a fracture reduction.
Currently, to the best of our knowledge there is no computer assisted methods to plan an entire fracture reduction process.
This paper presents an overall scheme of the computer based process for planning a bone fracture reduction, as described above, and details its main steps, the most common proposed techniques and their main shortcomings.
In addition, challenges and new trends of this research field are depicted and analyzed.

We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released.
The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license.
Each media object in the dataset is represented by several pieces of metadata, e.g.
Flickr identifier, owner name, camera, title, tags, geo, media source.
The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.
In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development.
We further present several new challenges in multimedia research that can now be expanded upon with our dataset.

Texture is a fundamental characteristic of many types of images, and texture representation is one of the essential and challenging problems in computer vision and pattern recognition which has attracted extensive research attention.
Since 2000, texture representations based on Bag of Words (BoW) and on Convolutional Neural Networks (CNNs) have been extensively studied with impressive performance.
Given this period of remarkable evolution, this paper aims to present a comprehensive survey of advances in texture representation over the last two decades.
More than 200 major publications are cited in this survey covering different aspects of the research, which includes (i) problem description; (ii) recent advances in the broad categories of BoW-based, CNN-based and attribute-based methods; and (iii) evaluation issues, specifically benchmark datasets and state of the art results.
In retrospect of what has been achieved so far, the survey discusses open challenges and directions for future research.

A synthesis of the work of three noted authors provides a framework for collaborative decisions built on the foundation of decision analysis.
A Nobel Prize winner provides a psychological foundation for the framework, an authority on harnessing the collective wisdom of organizations argues for the necessity of a mechanism for the aggregation of the decision makers’ understandings, and a former senior executive for a Fortune 500 company describes a series of structured dialogues that supports the aggregation of understandings.The resulting collaborative decision process aggregates, rather than compromises, the understandings of decision makers.
It makes explicit the aggregation of individuals’ understandings of the frame of the decision to be made, the alternatives to be considered, the sources of value and risk, and, finally, the reasons for the resulting collaborative choice.In collaborative decision making, we do not strive for an optimum, a compromise, or a satisficing solution.
Rather, collaborative decision making results in a significantly more valuable choice than the alternatives envisioned by any of the decision makers through the aggregation understandings.
Though the collaborative choice was not envisioned by the decision makers, each feels ownership of it and explicitly agrees to implement it.

In this paper, using the concept of A-statistical convergence for double real sequences, we obtain a Korovkin type-approximation theorem for double sequences of positive linear operators defined on the space of all 2@p-periodic and real valued continuous functions on the real two-dimensional space.
Furthermore, we display an application which shows that our new result is stronger than its classical version.
Also, we study rates of A-statistical convergence of a double sequence of positive linear operators acting on this space.
Finally, displaying an example, it is shown that our statistical rates are more efficient than the classical aspects in the approximation theory.

Most of us recognize from personal experience that, when they are designed properly, multicolor displays are not only more pleasing aesthetically than monochrome displays, but also convey more information and/or do so at higher rates.
(Perhaps this is why they are more pleasing.)
The catch is that color must be used “properly”; therefore, much of the applied color-vision research that has been performed has been directed toward increasing our understanding of how to do this.
This chapter reviews some of the topics that have attracted special interest and, where it is appropriate, makes recommendations based upon the research findings.

Part 1 The context of mathematics education: ethnomathematics and its place in the history and pedagogy of mathematics, U.D'Ambrosio mathematics and ballistics, J.J.M.Bos mathematics education in its cultural context, A.Bishop foundations of eurocentrism in mathematics, G.Joseph mathematics in a social context math within education as praxis versus math with education as hegemony, M.Fasheh folk mathematics, E.Maier hidden messages, J.Maxwell mathematics education - an industrial viewpoint, T.S.Wilkinson the computer as a cultural influence in mathematical learning, R.Noss contextualising mathematics - towards a theoretical map, P.Dowling.
Part 2 Mathematics in the workplace - research views: mathematics and workplace research, M.Harris and J.Evans finding the maths in work, M.Harris the role of number in work and training, D.Mathews skills versus understanding, R.Straser, et al mathematics in and out of school, D.Carraher theories of practice, J.Evans and M.Harris.
Part 3 Mathematics in the workplace - user views: an industry and mathematics - one view from courtaulds, S.Ingham CAPITB - the work of the clothing and allied products industry training board, K.Pye the gendering of work, J.Holland.
Part 4 School mathematics in context: CPVE, S.Sullivan problems solving in the workplace, P.Drake the mathematics and TVEI project, A.Grey work reclaimed - status mathematics in non-elitist contexts, M.Harris and C.Paechter.
Postscript the maths in work project, M.Harris.

Following either a text-based, synchronous computer-mediated conversation (CMC) or a face-to-face dyadic interaction, 80 participants rated their partners' personality profile.
Impressions were assessed in terms of both their breadth (the comprehensiveness of the impression) and intensity (the magnitude of the attributions).
Results indicated that impressions formed in the CMC environment were less detailed but more intense than those formed face-to-face.
These data provide support for theories that, in addition to acknowledging the unique constraints and characteristics of CMC, consider the cognitive strategies and heuristics involved in the impression formation process.
The differential impact of a text-based medium on trait-specific impressions (e.g., extraversion, neuroticism) is also discussed in the context of a cross-modal approach to impression formation.

Abstract In this paper, a robust and secure image hashing scheme based on salient structure features is proposed, which can be applied in image authentication and retrieval.
In order to acquire the fixed length of image hash, the pre-processing for image regularization is first conducted on input image.
Salient edge detection is then applied on the secondary image, and a series of non-overlapping blocks containing the richest structural information in the secondary image are selectively sampled according to the edge binary map.
Dominant DCT coefficients of the sampled blocks with their corresponding position information are retrieved as the robust features.
After the compression with dimensionality reduction for the concatenated features, the final hash can be produced.
Experimental results show that the proposed scheme has better performances of perceptual robustness and discrimination compared with some state-of-the-art schemes.

This paper addresses issues of knowledge transfer through OEM (original equipment manufacture) sub-contracting in high-tech industries in Taiwan.
Major issues explored in this paper include: how a firm in Taiwan establishes an OEM relationship with its world-class competitors, how the relationship was maintained, and how knowledge transfer was facilitated under such a relationship.
Findings of our study suggest that the Taiwanese firms deliberately chose to collaborate with their world-class competitors in order to learn from them.
Under OEM sub-contracting, knowledge transfer took place in a systematic fashion.
The types of knowledge transferred were diverse, and the transferred knowledge influenced profoundly the firms' operations or even their ways of thinking.
Such knowledge transfer enhanced the firms' absorptive capacity, and could be divided into several phases.
Benign contexts in which mutual trust developed between the firms and their OEM buyers facilitated the transfer of knowledge.
Propositions for further research are proposed.

This paper presents a model for the performance prediction of FFT algorithms executed on a shared-memory parallel computer consisting of N processors an the same number of memory modules.
The model applies a deterministic analysis to estimate the communication delay through the interconnection network by assuming that all requests arrive at the network in bursts.
Our results indicate that the communication delay is significantly affected by the method applied to allocate data to memory modules.
For the case in which all data items referenced by a processor during an iteration are allocated to a single memory module, the best-case communication time complexity grows as O[(log N) 2 /N].
The worst-case communication time complexity for this case, obtained by a different allocation of data to memory modules, is increased to O[(log N)/√N] due to high network contention.
For the case in which the data items referenced by different processors during an iteration are allocated to the same memory module, the communication time complexity is further increased to O(log N) since all N requests generated by processors are serialized at a single memory module.
The methods developed in this paper can be applied for the performance prediction of other well-structured parallel iterative algorithms.

Review of applications of algorithms in bio-inspired computing.Brief description of algorithms without mathematical notations.Brief description of scope of applications of the algorithms.Identification of algorithms whose applications may be explored.Identification of algorithms on which theory development may be explored.
With the explosion of data generation, getting optimal solutions to data driven problems is increasingly becoming a challenge, if not impossible.
It is increasingly being recognised that applications of intelligent bio-inspired algorithms are necessary for addressing highly complex problems to provide working solutions in time, especially with dynamic problem definitions, fluctuations in constraints, incomplete or imperfect information and limited computation capacity.
More and more such intelligent algorithms are thus being explored for solving different complex problems.
While some studies are exploring the application of these algorithms in a novel context, other studies are incrementally improving the algorithm itself.
However, the fast growth in the domain makes researchers unaware of the progresses across different approaches and hence awareness across algorithms is increasingly reducing, due to which the literature on bio-inspired computing is skewed towards few algorithms only (like neural networks, genetic algorithms, particle swarm and ant colony optimization).
To address this concern, we identify the popularly used algorithms within the domain of bio-inspired algorithms and discuss their principles, developments and scope of application.
Specifically, we have discussed the neural networks, genetic algorithm, particle swarm, ant colony optimization, artificial bee colony, bacterial foraging, cuckoo search, firefly, leaping frog, bat algorithm, flower pollination and artificial plant optimization algorithm.
Further objectives which could be addressed by these twelve algorithms have also be identified and discussed.
This review would pave the path for future studies to choose algorithms based on fitment.
We have also identified other bio-inspired algorithms, where there are a lot of scope in theory development and applications, due to the absence of significant literature.

Abstract A tournament T on any set X is a dyadic relation such that for any x , y ∈ X (a) ( x , x ) ∉ T and (b) if x ≠ y then ( x , y ) ∈ T iff ( y , x ) ∉ T . The score vector of T is the cardinal valued function defined by R ( x ) = |{ y ∈ X : ( x , y ) ∈ T }|.
We present theorems for infinite tournaments analogous to Landau's necessary and sufficient conditions that a vector be the score vector for some finite tournament.
Included also is a new proof of Landau's theorem based on a simple application of the “marriage” theorem.

Analyzing And Driving Verification: An Executive's Guide.- The Verification Crisis.- Automated Metric-Driven Processes.- Roles in a Verification Project.- Overview of a Verification Project.- Verification Technologies.- Managing The Verification Process.- Verification Planning.- Capturing Metrics.- Regression Management.- Revision Control and Change Integration.- Debug.- Executing The Verification Process.- Coverage Metrics.- Modeling and Architectural Verification.- Assertion-Based Verification.- Dynamic Simulation-Based Verification.- System Verification.- Mixed Analog and Digital Verification.- Design for Test.- Case Studies And Commentaries.- Metric-Driven Design Verification: Why Is My Customer a Better Verification Engineer Than Me?.- Metric-Driven Methodology Speeds the Verification of a Complex Network Processor.- Developing a Coverage-Driven SoC Methodology.- From Panic-Driven to Plan-Driven Verification Managing the Transition.- Verification of a Next-Generation Single-Chip Analog TV and Digital TV ASIC.- Management IP: New Frontier Providing Value Enterprise-Wide.- Adelante VD3204x Core, SubSystem, and SoC Verification.- SystemC-based Virtual SoC: An Integrated System-Level and Block-Level Verification Approach from Simulation to Coemulation.- Is Your System-Level Project Benefiting from Collaboration or Headed to Chaos?.

Emphasizing the processes by which learners can write free from the constraints of other people's ideas, this text presents a new approach to teaching.
It examines the writing process, cultural assumptions, ways of improving and evaluating writing and using the included exercises.

Purpose ::: ::: To develop data acquisition and image reconstruction methods to enable high-resolution 1H MR spectroscopic imaging (MRSI) of the brain, using the recently proposed subspace-based spectroscopic imaging framework called SPICE (SPectroscopic Imaging by exploiting spatiospectral CorrElation).
::: ::: ::: ::: Theory and Methods ::: ::: SPICE is characterized by the use of a subspace model for both data acquisition and image reconstruction.
For data acquisition, we propose a novel spatiospectral encoding scheme that provides hybrid data sets for determining the subspace structure and for image reconstruction using the subspace model.
More specifically, we use a hybrid chemical shift imaging /echo-planar spectroscopic imaging sequence for two-dimensional (2D) MRSI and a dual-density, dual-speed echo-planar spectroscopic imaging sequence for three-dimensional (3D) MRSI.
For image reconstruction, we propose a method that can determine the subspace structure and the high-resolution spatiospectral reconstruction from the hybrid data sets generated by the proposed sequences, incorporating field inhomogeneity correction and edge-preserving regularization.
::: ::: ::: ::: Results ::: ::: Phantom and in vivo brain experiments were performed to evaluate the performance of the proposed method.
For 2D MRSI experiments, SPICE is able to produce high-SNR spatiospectral distributions with an approximately 3 mm nominal in-plane resolution from a 10-min acquisition.
For 3D MRSI experiments, SPICE is able to achieve an approximately 3 mm in-plane and 4 mm through-plane resolution in about 25 min.
::: ::: ::: ::: Conclusion ::: ::: Special data acquisition and reconstruction methods have been developed for high-resolution 1H-MRSI of the brain using SPICE.
Using these methods, SPICE is able to produce spatiospectral distributions of 1H metabolites in the brain with high spatial resolution, while maintaining a good SNR.
These capabilities should prove useful for practical applications of SPICE.
Magn Reson Med, 2015.
© 2015 Wiley Periodicals, Inc.

In this paper, a linearized $L1$-Galerkin finite element method is proposed to solve the multidimensional nonlinear time-fractional Schrodinger equation.
In terms of a temporal-spatial error splitting argument, we prove that the finite element approximations in the $L^2$-norm and $L^\infty$-norm are bounded without any time-step size conditions.
More importantly, by using a discrete fractional Gronwall-type inequality, optimal error estimates of the numerical schemes are obtained unconditionally, while the classical analysis for multidimensional nonlinear fractional problems always required certain time-step restrictions dependent on the spatial mesh size.
Numerical examples are given to illustrate our theoretical results.

ABSTRACT The first part outlines a pedagogical framework for educational agents in non-pedagogical roles.
We draw on work by Scho¨n (reflective practitioner), Lave and Wenger (community of practice), and Henderson (virtual practicum) to describe such a general framework.
In the second part, state-of-the-art and trends in e-learning are reviewed to answer the question how research on embodied educational agents can be made to bear on current e-learning, with the goal of improving its quality and to realize the virtual practicum inside existing platforms and standards.

In networked control systems (NCSs) achievable performance is limited by the communication links employed to transmit signals in the loop.
In the present work, we characterise LTI coding systems which optimise performance for various NCS architectures.
We study NCSs where the communication link is situated between plant output and controller, and NCSs where the communication link is located between controller and actuator.
Furthermore, we present a novel NCS architecture, which is based upon the Youla parameterisation.
We show that, which of these architectures gives best performance depends, inter alia, upon characteristics of a related non-networked design, plant disturbances and reference signal.
A key aspect of our work, resides in the utilisation of fixed signal-to-noise ratio channel models which give rise to parsimonious designs, where channel utilisation is kept low.
The results are verified with simulations utilising bit-rate limited channels.

This letter presents a new approach to actively use radar shadow in order to unwrap and geocode objects that cannot be reconstructed by synthetic aperture radar interferometry alone.
This new algorithm may significantly improve phase unwrapping and digital elevation model reconstruction in mountainous topography or in urban areas.
The algorithm is explained and examples are given with data from the Shuttle Radar Topography Mission, where shadow is a serious problem in Alpine terrain.

In this paper we present a multimicrophone array for adaptive speech processing in a multipath and reverberant environment.
Our proposed system for noise reduction consists of two stages: first the incoherent noise components are removed by Wiener filtering of the received signals.
In a second step the possible coherent noise sources, which cannot be suppressed by the Wiener filtering are removed by an additional multichannel noise canceller.
A theoretical description of the proposed system is given which shows that this method is able to reduce incoherent as well as coherent noise components and is therefore applicable in realistic conditions.

EDDIE, a novel mechatronical emotion-display designed for dynamic non-verbal human-robot interaction is presented.
A special feature are dynamic and realistic emotional state transitions.
Therefore, the emotional state-space based on the circumplex model of affect is directly mapped to joint space.
The display is largely developed and manufactured in a rapid-prototyping process.
Only miniature off-the-shelf mechatronic components are used providing high functionality at low cost.

In this paper, we consider single-machine scheduling problem with controllable processing times and learning effect, i.e., processing times of jobs are controllable variables with linear costs and also are defined as functions of positions in a schedule.
We concentrate on two goals separately, namely minimizing a cost function containing makespan, total completion time, total absolute differences in completion times, and total compression cost and minimizing a cost function containing makespan, total waiting time, total absolute differences in waiting times, and total compression cost.
The problem is modeled as an assignment problem and thus can be solved with the well-known algorithms.

A bipolar LSI linear chip, precisely mated to an in-process laser-trimmed thin-film-on-silicon resistor network, to realize a two-chip 0.003% accurate 12-bit D/A converter in a 24-pin DIP, will be covered.

Overlay networks are expected to be a promising technology for the realization of QoS (Quality of Service) control.
Overlay networks have recently attracted considerable attention due to the following advantages: a new service can be developed in a short duration and it can be started with a low cost.
The definition and necessity of the overlay network is described, and the classification of various current and future overlay networks, particularly according to the QoS feature, is attempted.
In order to realize QoS control, it is considered that routing overlay and session overlay are promising solutions.
In particular, session and overlay networks are explained in detail since new TCP protocols for QoS instead of current TCP protocols that control congestion in the Internet can be used within overlay networks.
However, many open issues such as scalability still need further research and development although overlay networks have many attractive features and possess the potential to become a platform for the deployment of new services.

Most information systems share a common assumption: information seeking is discrete.
Such an assumption neither reflects real-life information seeking processes nor conforms to the perspective of phenomenology, "life is a journey constituted by continuous acquisition of knowledge."
Thus, this study develops and validates a theoretical model that explains successive search experience for essentially the same information problem.
The proposed model is called Multiple Information Seeking Episodes (MISE), which consists of four dimensions: problematic situation, information problem, information seeking process, episodes.
Eight modes of multiple information seeking episodes are identified and specified with properties of the four dimensions of MISE.
The results partially validate MISE by finding that the original MISE model is highly accurate, but less sufficient in characterizing successive searches; all factors in the MISE model are empirically confirmed, but new factors are identified as well.
The revised MISE model is shifted from the user-centered to the interaction-centered perspective, taking into account factors of searcher, system, search activity, search context, information attainment, and information use activities.

cracking codes the rosetta stone and decipherment is available in our book collection an online access to it is set as public so you can get it instantly.
Our digital library spans in multiple locations, allowing you to get the most less latency time to download any of our books like this one.
Kindly say, the cracking codes the rosetta stone and decipherment is universally compatible with any devices to read.

When all the processors are trying to read and write the same main memory, you can do things the right way, the wrong way or the right way but so-slow-nobody-cares way.

A 36 MW submerged-arc ferrosilicon (FeSi) furnace is modeled using the technique of system identification.
The main purpose of the modeling is for the simulation and evaluation of different control schemes.
In particular, the part of the system associated with the control of the three-phase electrode currents, which is done by positioning of the electrodes, is examined.
Data were collected at Icelandic Alloys Ltd., using experiments conducted mostly in open loop.
The positioning of electrodes responding to control signals was estimated and modeled using measurements in closed loop.
Data collection for the disturbance identification was done by keeping the three electrodes fixed and recording the currents.
A first-order autoregressive (AR) model was used to model the disturbance environment.
Different ARX MIMO models were tried using several model orders, combinations of inputs, and input delays.
The results obtained indicate that the simple linear models developed include all the critical factors needed in a simulation of different electrode position based control schemes for the electrode current control of the furnace.

The Internet of Things (IoT) will exponentially increase the scale and the complexity of existing computing and communication systems.
In a world of multi-stakeholder information and assets provision on top of millions of real-time interacting and communicating Things, autonomicity is an imperative property and a grand challenge.
Autonomic Things will allow systems to self-manage the complexity, the dynamicity and the distribution of the IoT.
In order to make Things able to manage themselves and contribute to the global self-management network, we have to empower them with mandatory properties like situational-awareness, knowledge, smartness and social behavior.
In this paper we present the approach that the COSMOS project introduces in order to enable Things to evolve and act in a more autonomous way, becoming more reliable and smarter.

With the advent of off-pump coronary artery bypass grafting and minimally invasive coronary artery bypass grafting, significant efforts have been made to facilitate construction of the graft to coronary anastomosis.
As a result, a number of anastomotic devices have been developed.
While the ideal anastomotic device should be easy to use, to produce a geometrically optimal anastomosis with minimal endothelial damage and minimal blood-exposed non-intimal surface, a number of design constraints apply.
This review collects the available pre-clinical and clinical data for some of the devices with special regard for surgical outcome, patency rate and the need for additional perioperative anticoagulation treatment.

Similarity measures are fundamental tools for identifying relationships within or across patent portfolios.
Many bibliometric indicators are used to determine similarity measures; for example, bibliographic coupling, citation and co-citation, and co-word distribution.
This paper aims to construct a hybrid similarity measure method based on multiple indicators to analyze patent portfolios.
Two models are proposed: categorical similarity and semantic similarity.
The categorical similarity model emphasizes international patent classifications (IPCs), while the semantic similarity model emphasizes textual elements.
We introduce fuzzy set routines to translate the rough technical (sub-) categories of IPCs into defined numeric values, and we calculate the categorical similarities between patent portfolios using membership grade vectors.
In parallel, we identify and highlight core terms in a 3-level tree structure and compute the semantic similarities by comparing the tree-based structures.
A weighting model is designed to consider: 1) the bias that exists between the categorical and semantic similarities, and 2) the weighting or integrating strategy for a hybrid method.
A case study to measure the technological similarities between selected firms in China’s medical device industry is used to demonstrate the reliability our method, and the results indicate the practical meaning of our method in a broad range of informetric applications.

In this study, the authors inter-compared the performance of three satellite-rainfall products in representing the diurnal cycle of rain occurrence and rain rate over the Nile basin in eastern Africa.
These products are the real time (RT) and post-real-time (PRT) (bias adjusted) versions of Tropical Rainfall Measuring Mission (TRMM) and other sources product known as TRMM-3B42 and the National Oceanographic and Atmospheric Administration Climate Prediction Center (NOAA-CPC) product which is based on the CPC morphing technique (CMORPH).
The rainfall diurnal cycles are re-produced using these products with specific focus on assessing effects of geographic location and topographic features.
The performance of the satellite products in representing rainfall diurnal cycle shows large variation over the Nile basin.
The products overestimate rain occurrence over the lakes, islands, and shores and underestimate occurrence over mountain tops.
Overall, CMORPH performs better than TRMM-3B42 RT and TRMM-3B42 PRT in capturing the diurnal cycle of rain rate in Lake Tana basin.
However, the difference between the two products is very small for Lake Victoria basin, where both products perform more favorably.
Over most of the Nile basin areas, the use of fine versus coarse temporal and spatial resolution of the CMORPH product showed large differences for diurnal cycle of rain occurrence than that of rain rate.
Results also show that the bias adjustment of TRMM-3B42 product does not necessarily bring improvements probably since the adjustments are not performed based on local rain gauge data.

The advances in new techniques for correcting presbyopia, such as a small aperture combined with monovision, require an in-depth study of binocular aspects.
In this work, we have studied binocular visual performance of 12 subjects after inducing different degrees of anisocoria combined with two different add powers in the non-dominant eye.
We have analysed visual performance in terms of the visual-discrimination capacity (a function to evaluate the strength of bothersome halos) and the contrast-sensitivity.
The results show a deterioration of the binocular vision when inducing anisocoria and with any add power, with a higher perception of halos, a lower contrast sensitivity and poorer binocular summation of these visual functions on increasing anisocoria.
This deterioration is clinically acceptable in the case of low add power, since positive binocular summation is maintained in contrast sensitivity, and visual discrimination is not altered.

Molecular similarity is a key concept in drug discovery.
It is based on the assumption that structurally similar molecules frequently have similar properties.
Assessment of similarity between small molecules has been highly effective in the discovery and development of various drugs.
Especially, two-dimensional (2D) similarity approaches have been quite popular due to their simplicity, accuracy and efficiency.
Recently, the focus has been shifted towards the development of methods involving the representation and comparison of three-dimensional (3D) conformation of small molecules.
Among the 3D similarity methods, evaluation of shape similarity is now gaining attention for its application not only in virtual screening but also in molecular target prediction, drug repurposing and scaffold hopping.
A wide range of methods have been developed to describe molecular shape and to determine the shape similarity between small molecules.
The most widely used methods include atom distance-based methods, surface-based approaches such as spherical harmonics and 3D Zernike descriptors, atom-centered Gaussian overlay based representations.
Several of these methods demonstrated excellent virtual screening performance not only retrospectively but also prospectively.
In addition to methods assessing the similarity between small molecules, shape similarity approaches have been developed to compare shapes of protein structures and binding pockets.
Additionally, shape comparisons between atomic models and 3D density maps allowed the fitting of atomic models into cryo-electron microscopy maps.
This review aims to summarize the methodological advances in shape similarity assessment highlighting advantages, disadvantages and their application in drug discovery.

In the field of cluster analysis, most of existing algorithms are developed for small data sets, which cannot effectively process the large data sets encountered in data mining.
Moreover, most clustering algorithms consider the contribution of each sample for classification uniformly.
In fact, different samples should be of different contribution for clustering result.
For this purpose, a novel typical-sample-weighted clustering algorithm is proposed for large data sets.
By the atom clustering, the new algorithm extracts the typical samples to reduce the data amount.
Then the extracted samples are weighted by their corresponding typicality and then clustered by the classical fuzzy c-means (FCM) algorithm.
Finally, the Mahalanobis distance is employed to classify each original sample into obtained clusters.
It is obvious that the novel algorithm can improve the speed and robustness of the traditional FCM algorithm.
The experimental results with various test data sets illustrate the effectiveness of the proposed clustering algorithm.

Abstract In this paper, we develop a new multi-focus image fusion method based on saliency detection and multi-scale image decomposition.
Proposed method is very efficient, since the visual saliency explored in this algorithm is able to emphasize visually significant regions.
Unlike most of the multi-scale fusion methods, an average filter is employed in our algorithm for multi-scale image decomposition.
Hence it is computationally simple.
A new weight map construction process based on visual saliency is developed.
Weight maps of this algorithm are capable of detecting and identifying focused and defocused regions of the source images.
We are able to integrate only focused and sharpened regions into the fused image.
Performance of the proposed method is compared with that of the state-of-the-art multi-focus fusion methods.
Proposed method outperforms them in terms of visual quality and fusion metrics.
Our method requires considerably less computational time, thus making it preferable for real time implementation.

Biology learning is, by its very nature, complex.
Living systems are composed of systems nested within systems, each of which has components that interact to produce the emergent behavior of that system and interact in the next larger system.
Living system components can be as small as ions and can participate in systems as large as the biosphere of Earth.
This chapter summarizes a body of research conducted on the use of computer-based simulations and representations for instruction and assessment in human body systems, genetics, and ecosystems.
The strategic use of these representations for fostering and assessing model-based learning, reasoning, and inquiry are discussed, as are the tasks that students can perform with these representations and the evidence that can be gathered when students perform these tasks.
This chapter also presents a theoretical framework that integrates model-based learning with evidence-centered design and describes how it is used to guide the design of simulation-based representations in assessment.
This framework has the potential to transform the experiences and outcomes of biology learning by enabling learners to develop richly connected, useful, and extensible understandings of living systems.

Abstract While researchers have used conversation analysis (CA) methods to understand online talk since the 1990s, to date there has been no systematic review of these studies to better understand this methodological development.
This article presents a comprehensive literature review of 89 peer-reviewed journal articles reporting findings of empirical studies using CA to understand social interaction online.
In this review, we describe who is conducting this type of research, the contexts in which CA has been used to make sense of text-based online talk, and where such studies are being published.
We also identify the “fundamental” conversational structures researchers are drawing upon in making sense of online talk as social interaction.
Findings show that studies are using CA to understand “mundane” conversational contexts, as well as institutional talk from educational, counseling and workplace settings.
The number of such studies are increasing and are being conducted by an international network of researchers across a variety of disciplines.
The data is most often described as synchronous or asynchronous, with a slow increase in attention to social media data.
Publication outlets are mostly language-based and/methodological journals.
Analysis revealed four main aims: (1) comparing online and face-to-face talk, (2) understanding how coherence is maintained, (3) understanding how participants deal with trouble, and (4) understanding how social actions are accomplished asynchronously.
This review contributes to the overall understanding of the methodological development of CA, offering useful insights for those interested in using it to understand social interaction as it occurs online.

Abstract Motor performance is accompanied by neural activity in various cortical and sub-cortical areas.
This intricate network has to be delicately orchestrated.
We analyzed the role of beta synchronization in motor learning using magneto-encephalography combined with electromyography.
Cortico-spinal synchronization in the beta band was found to be of particular importance in establishing bimanual movement patterns in the context of a 3:2 polyrhythmic (isometric) force production task.
Its dynamics correlated highly with the learning of this complex bimanual motor skill.
We submit that the cortical dynamics entrains the spinal motor system by which cortico-spinal beta synchrony serves higher-level motor control functions as primary means of information transfer along the neural axis.

Abstract The Optical Mapping System constructs ordered restriction maps spanning entire genomes through the assembly and analysis of large datasets comprising individually analyzed genomic DNA molecules.
Such restriction maps uniquely reveal mammalian genome structure and variation, but also raise computational and statistical questions beyond those that have been solved in the analysis of smaller, microbial genomes.
We address the problem of how to filter maps that align poorly to a reference genome.
We obtain map-specific thresholds that control errors and improve iterative assembly.
We also show how an optimal self-alignment score provides an accurate approximation to the probability of alignment, which is useful in applications seeking to identify structural genomic abnormalities.

We examine a case of successful integrated BIM-based design in a construction project.We present a set of key factors influential to enable digital collaboration in this project.Key factors identified include: change agents, new roles, cloud computing, contracts, etc.We found integrated design to depend upon changing traditional work practices.The case is an example of BIM implementation and collaborative work in the AEC industry.
Building information modeling (BIM) and related digital innovations can serve as a catalyst for more transparency, tighter integration, and increased productivity in the architecture, engineering, and construction industry.
Yet, many project teams struggle with how to work based on the new technology.
Collaborative design based on shared information systems like BIM requires changing traditional and institutionalized work practices and routines.
A case study of integrated BIM design in a large healthcare construction project serves as an example for how commonly experienced challenges can be overcome.
The project has been awarded BuildingSMART's 2015 award for 'outstanding open BIM practice' making it Norway's role model for BIM practice.
Based on diffusion of innovations theory, we identified the following set of key factors enabling digital collaboration in this project: change agents, new roles and responsibilities, a cloud computing infrastructure, BIM contracts, and a BIM learning environment.
The findings presented in this article may serve as an example for BIM implementation and collaborative work in construction projects.

This article introduces a domain-and application-independent language for representing preferences as part of user profiles.
It also describes the translation of statements of this language to RDF datasets using a new ontology named Framework for Ratings and Preferences FRAP.
The availability of this language and its RDF representation enable the effective exchange of user preferences across different applications in the web environment.
The practical usage and limitations of this approach are also discussed in the article.

We develop a model of interlocking bilateral relationships between upstream firms (manufacturers) that produce differentiated goods and downstream firms (retailers) that compete imperfectly for consumers.
Contract offers and acceptance decisions are private information to the contracting parties.
We show that both exclusive dealing and vertical integration between a manufacturer and a retailer lead to vertical foreclosure, to the detriment of consumers and society.
Finally, we show that firms have indeed an incentive to sign such contracts or to integrate vertically.

ABSTRACTAttenuation of random noise is a major concern in seismic data processing.
This kind of noise is usually characterized by random oscillation in seismic data over the entire time and frequency.
We introduced and evaluated a low-rank and sparse decomposition-based method for seismic random noise attenuation.
The proposed method, which is a trace by trace algorithm, starts by transforming the seismic signal into a new sparse subspace using the synchrosqueezing transform.
Then, the sparse time-frequency representation (TFR) matrix is decomposed into two parts: (a) a low-rank component and (b) a sparse component using bilateral random projection.
Although seismic data are not exactly low-rank in the sparse TFR domain, they can be assumed as being of semi-low-rank or approximately low-rank type.
Hence, we can recover the denoised seismic signal by minimizing the mixed l1‐l2 norms’ objective function by considering the intrinsically semilow-rank property of the seismic data and sparsity feature of random...

Trellis coding using multidimensional quadrature amplitude modulation (QAM) signal sets is investigated.
Finite-size 2D signal sets are presented that have minimum average energy, are 90 degrees rotationally symmetric, and have from 16 to 1024 points.
The best trellis codes using the finite 16-QAM signal set with two, four, six, and eight dimensions are found by computer search (the multidimensional (multi-D) signal set is constructed from the 2-D signal set).
The best moderate complexity trellis codes for infinite lattices with two, four six, and eight dimensions are also found.
The minimum free squared Euclidean distance and number of nearest neighbors for these codes were used as the selection criteria.
Many of the multi-D codes are fully rotationally invariant and give asymptotic coding gains up to 6.0 dB.
From the infinite lattice codes, the best codes for transmitting J, J+1/4, J+1/3, J+1/2, J+2/3, and J+3/4 b/sym (J an integer) are presented.
>

Petri nets have been recognised as a high level formal and graphical specification language for modelling, analysis, and control of concurrent asynchronous distributed systems.
This paper presents a PN model, synthesised by an extended version of the knitting synthesis technique.
This method, as an incremental design approach, establishes the conditions under which the fundamental behavioural properties of the synthesised systems are fulfilled and preserved.
That is, the synthesised models are live, bounded, and reversible (cyclic).
A Petri net with the aforementioned properties is called a well-behaved Petri net system which is guaranteed to operate in a deadlock-free, stable, and cyclic fashion.
Well-behaved Petri net models, synthesised using the proposed method can be compiled into control codes and implemented as real-time controllers for flexible manufacturing systems.
The significance of this paper is due to the application of an extended version of knitting synthesis technique to a real life example of a flexible manufacturing system.

The international logistics centers choice problem is a very important issue in International logistics.
The location choice problem usually involves numbers and words in which all of the criteria are weighted using words and the performance evaluations for all sub-criteria are either numbers or words.
How to aggregate all of these data without losing information is a very daunting task using a type-1 fuzzy set (T1 FS) approach.
This paper applies a new methodology—Perceptual Computer (Per-C)—to help solve this hierarchical multi-person multi-criteria decision making problem.
The Per-C has three components: encoder, computing with words (CWW) engine and decoder.
First, the interval approach (IA) is used to obtain interval type-2 fuzzy set (IT2 FS) word models for the words in a pre-specified vocabulary.
Second, a linguistic weighted average (LWA) is used to aggregate all the data including numbers and words modeled by IT2 FSs.
Finally, a centroid-based ranking method is used to rank the location choices, and a similarity measure is used to obtain similarities of the location choices.
The decision-maker decides the winning location choice as the one with the highest ranking and least similarity to other locations.

This paper proposes a novel dynamic structure neural fuzzy network (DSNFN) to address the adaptive tracking problems of multiple-input-multiple-output (MIMO) uncertain nonlinear systems.
The proposed control scheme uses a four-layer neural fuzzy network (NFN) to estimate system uncertainties online.
The main feature of this DSNFN is that it can either increase or decrease the number of fuzzy rules over time based on tracking errors.
Projection-type adaptation laws for the network parameters are derived from the Lyapunov synthesis approach to ensure network convergence and stable control.
A hybrid control scheme that combines the sliding-mode control and the adaptive bound estimation control with different weights improves system performance by suppressing the influence of external disturbances and approximation errors.
As the employment of the DSNFN, high-quality tracking performance could be achieved in the system.
Furthermore, the trained network avoids the problems of overfitting and underfitting.
Simulations performed on a two-link robot manipulator demonstrate the effectiveness of the proposed control scheme.

This study assesses the current metadata practices and trends in Association of Research Libraries (ARL) libraries, based on the survey Metadata conducted in spring 2007 (SPEC Kit 298: Metadata), a collaborative effort with the staff at the ARL.
The survey investigates how metadata has been implemented in ARL member libraries: what kinds of projects or initiatives have been undertaken, what types of digital objects are associated with metadata, who are creating metadata, what schemas and tools are used to create and manage metadata, and the organizational changes and challenges resulting from the adoption of metadata in the libraries.
The author summarizes her observations of the findings and the main themes that emerged from the metadata practices in libraries.
She assesses the changing context of metadata creation and management and the evolution of metadata workflow and best practices in libraries.
The author also discusses the roles and responsibilities of metadata professionals and the implications o...

Abstract This paper presents a new algorithm based on hybridizing Bat Algorithm (BA) and Artificial Bee Colony (ABC) with Chaotic based Self-Adaptive (CSA) search strategy (CSA-BA-ABC) to solve the large-scale, highly non-linear, non-convex, non-smooth, non-differential, non-continuous, multi-peak and complex Combined Heat and Power Economic Dispatch (CHPED) problems.
The proposed hybrid algorithm has better capability to escape from local optima with faster convergence rate than the standard BA and ABC.
The proposed algorithm works based on the three mechanisms.
The first one is a novel adaptive search mechanism, in which one of the three search phases (BA phase, directed onlooker bee phase and modified scout bee phase) is selected based on the aging level of the individual’s best solution (pbest).
In this regard, ABC’s phases can assist BA phase to search based on deeper exploration /exploitation pattern as an alternative.
In periodic intervals, the second mechanism called as CSA updates algorithm control parameters using chaotic system based on prevailing search efficiency in the swarm.
Lastly, the third mechanism is enhancing the algorithm performance by incorporating individual’s directional information, habitat selection and self-adaptive compensation.
The effectiveness and robustness of the proposed algorithm are tested on a set of 23 benchmark functions and three CHPED problems.
The obtained results by the suggested algorithm in terms of quality solution, computational performance and convergence characteristic are compared with various algorithms to show the ability of the proposed approach and its robustness in finding a better cost- effective solution.

Functional programming languages like F#, Erlang, and Scala are attracting attention as an efficient way to handle the new requirements for programming multi-processor and high-availability applications.
Microsoft's new F# is a true functional language and C# uses functional language features for LINQ and other recent advances.
Real World Functional Programming is a unique tutorial that explores the functional programming model through the F# and C# languages.
The clearly presented ideas and examples teach readers how functional programming differs from other approaches.
It explains how ideas look in F#-a functional language-as well as how they can be successfully used to solve programming problems in C#.
Readers build on what they know about .NET and learn where a functional approach makes the most sense and how to apply it effectively in those cases.
The reader should have a good working knowledge of C#.
No prior exposure to F# or functional programming is required.

Preface The Company of Readers Becoming a Reader: Childhood Years Young Adults and Reading Adult readers Closing Index

Compilers for higher-order programming languages like Scheme, ML, and Lisp can be broadly characterized as either "direct compilers" or "continuation-passing style (CPS) compilers", depending on their main intermediate representation.
Our central result is a precise correspondence between the two compilation strategies.
::: Starting from the theoretical foundations of direct and CPS compilers, we develop relationships between the main components of each compilation strategy: generation of the intermediate representation, simplification of the intermediate representation, code generation, and data flow analysis.
For each component, our results pinpoint the superior compilation strategy, the reason for which it dominates the other strategy, and ways to improve the inferior strategy.
Furthermore, our work suggests a synthesis of the direct and CPS compilation strategies that combines the best aspects of each.
::: The contributions of this thesis include a comprehensive analysis of the properties of the CPS intermediate representation, a new optimal CPS transformation and its inverse, a new intermediate representation for direct compilers, an equivalence between the canonical equational theories for reasoning about continuations and general computational effects, a sound and complete equational axiomatization of the semantics of call-by-value control operators, a methodology for deriving equational logics for imperative languages, and formal relationships between code generators and data flow analyzers for direct and CPS compilers.
These contributions unify concepts in two distinct compilation strategies, and can be used to compare specific compilers.

We introduce the user-centered research that we are conducting using functional deformable research prototypes.
This work has recently crystallized in the demonstration of the Nokia Kinetic Device (figure 1).
In the large design space that opens before us around deformable user interfaces (DUIs), we have chosen to focus on mobile personal interfaces.
We aim to investigate how human factors should influence the transition from rigid to deformable hardware.
In this paper, we propose the topics that a research agenda should cover, and we discuss our research methodology.
We also describe the functional deformable research prototype (called Kinetic DUI-RP) that we are using to conduct our research.
Finally, we present an initial set of design guidelines that future research will develop further.

Reported work on financial time series prediction using neural networks often shows a characteristic one step shift relative to the original data.
This seems to imply a failure of the neural network (NN), because a shift corresponds to a random walk prediction.
Our systematic analysis of different time delay neural networks predictors applied to the detrended S&P 500 time series, indicates that this prediction behavior is not a limitation of the network, but may be a characteristic of the time series.
This suggests that there are no short-term correlations in this stockmarket time series, which is consistent with conventional statistical analysis.

Abstract Proliferation of online social media and the phenomenal growth of online commerce have brought to us the era of big data.
Before this availability of data, models of demand distribution at the product level proved elusive due to the ever shorter product life cycle.
Methods of sales forecast are often conceived in terms of longer-run trends based on weekly, monthly or even quarterly data, even in markets with rapidly changing customer demand such as the fast fashion industry.
Yet short-run models of demand distribution and sales forecasting (aka.
nowcast) are arguably more useful for managers as the majority of their decisions are concerned with day to day discretionary spending and operations.
Observations in the fast fashion market were acquired, for a collection time frame of about 1 month, from a major Chinese e-commerce platform at granular, half-daily intervals.
We developed an efficient method to visualize the demand distributional characteristics; found that big data streams of customer reviews contain useful information for better sales nowcasting; and discussed the current influence pattern of sentiment on sales.
We expect our results to contribute to practical visualization of the demand structure at the product level where the number of products is high and the product life cycle is short; revealing big data streams as a source for better sales nowcasting at the corporate and product level; and better understanding of the influence of online sentiment on sales.
Managers may thus make better decisions concerning inventory management, capacity utilization, and lead and lag times in supply-chain operations.

A drafting implement holder is provided and consists of a receptacle for holding the drafting implements which is attached to the edge of a table top of a drafting table by a mounting structure to maintain the receptacle in a substantially vertical position regardless of the angular position of the table top.

The Performance Model Interchange Format (PMIF) provides a mechanism for transferring the system model information among performance modeling tools requiring only that the tools either internally support PMIF or provide an interface that reads/writes model specifications from/to a file.
This paper presents the latest version of the specification (PMIF 2): a metamodel defining the information requirements and the corresponding XML schema.
It defines the semantic properties for a pmif.xml interchange, the prescribed validation order, errors and warnings, and a tool and Web service implementation.
Import and export prototypes for two different types of tools prove the concept.
Generally available examples are used for repeatability.

An approach to task sequence planning for a generalized robot manufacturing or material handling workcell is described.
Given the descriptions of the objects in this system and all feasible geometric relationships among these objects, an AND/OR net which describes the relationships of all feasible geometric states and associated feasibility criteria for net transitions is generated.
This AND/OR net is mapped into a Petri net which incorporates all feasible sequences of operations.
The resulting Petri net is shown to be bounded and have guaranteed properties of liveness, safeness, and reversibility.
Sequences are found from the reachability tree of the Petri net.
Feasibility criteria for net transitions may be used to generate an extended Petri net representation of lower-level command sequences.
The resulting Petri net representation may be used for online scheduling and control of the system of feasible sequences.
A simulation example of the sequences is described.
>

A simulated ball sport amusement instrument for optionally playing one of several different games.
The instrument has a rectangular frame with vertical side and end walls.
A plurality of hitting units are supported by the walls and have hitting surfaces disposed within the interior of the frame.
Handle levers are connected to ends of the hitting units to control movement of the hitting surfaces.
Holes are provided on the tops of the walls for receiving poles of football or basketball goals, or poles of a net for volleyball.
In one embodiment, the interior of the frame is divided by a grille that has a plurality of openings in which the hitting surfaces of the hitting units are positioned.
The dimensions of the grille openings are smaller than a ball used with the instrument so that the ball is prevented from falling through the playing surface and is always in a position to be hit by one of the hitting units.

The authors propose an integrated approach to the design of combinational logic circuits in which stuck-open faults and path delay faults are detectable by robust tests that detect modeled faults independent of the delays in the circuit under test.
They demonstrate that the proposed designs and tests guarantee the design of CMOS logic circuits in which all path delay faults are locatable.
>

A novel consensus protocol is presented for linear multi-agent systems(MASs) in which all the agents have the same continuous-time linear dynamics with high dimension.
A fast convergence speed can be obtained by using this protocol for the MASs where each agent gets its own state through the full-order observer.
Every agent can also obtain the states of its neighbors?
and its instantaneous neighbors?
to reach consensus.
Necessary and sufficient conditions of consensus for the MASs are proposed to improve the convergence speed.
Finally, the corresponding convergence speeds of different protocols are showed by numerical simulation, and extended to directed topology.

A spectrum analyzer with enhanced sensitivity has been built and used in noise measurements.
It is based on the processing of the input signal by two independent channels in parallel and takes advantage of the incoherent property of the noise in each of the two input stages.
The instrument has demonstrated an improvement in sensitivity of at least 50 dB with respect to a traditional system, and therefore can measure low input signals down to the hundred pV//spl radic/Hz range.

Abstract Nowadays, Wireless Body Area Network (WBAN) is broadly utilized for health monitoring and remote medical care.
When the physiological information collected in WBAN is distributed to cloud computing platform, a new healthcare service mode is enabled by “cloud-assisted WBAN”, where user’s body signals can be stored, processed, managed and analyzed over a long-term period.
Though the provisioning of healthcare services is largely enhanced via cloud-enabled technologies, more challenging issues are raised due to the increased user’s requirements on quality of experience (QoE) in terms of user mobility, content delivery latency, and personalized interaction, etc.
In order to tackle these challenges, this paper presents various solutions including: (1) a novel integration of WBAN with Long Term Evolution (LTE) to support high user mobility; (2) an efficient scheme to distribute contents by leveraging the emerging named data networking (NDN) technology, to support rich media healthcare content delivery without service interruption while achieving low cost and bandwidth saving; (3) the use of adaptive streaming to adjust suitable content size according to the dynamic bandwidth.
The experimental results conducted by OPNET verify the viability of NDN and adaptive streaming to support the healthcare services involving the transmissions of rich media contents between WBAN and internet.

This book chapter discusses the concept of edge-assisted cloud computing and its relation to the emerging domain of “Fog-of-things (FoT)”.
Such systems employ low-power embedded computers to provide local computation close to clients or cloud.
The discussed architectures cover applications in medical, healthcare, wellness and fitness monitoring, geo-information processing, mineral resource management, etc.
Cloud computing can get assistance by transferring some of the processing and decision making to the edge either close to client layer or cloud backend.
Fog of Things refers to an amalgamation of multiple fog nodes that could communicate with each other with the Internet of Things.
The clouds act as the final destination for heavy-weight processing, long-term storage and analysis.
We propose application-specific architectures GeoFog and Fog2Fog that are flexible and user-orientated.
The fog devices act as intermediate intelligent nodes in such systems where these could decide if further processing is required or not.
The preliminary data analysis, signal filtering, data cleaning, feature extraction could be implemented on edge computer leading to a reduction of computational load in the cloud.
In several practical cases, such as tele healthcare of patients with Parkinson’s disease, edge computing may decide not to proceed for data transmission to cloud (Barik et al., in 5th IEEE Global Conference on Signal and Information Processing 2017, IEEE, 2017) [4].
Towards the end of this research paper, we cover the idea of translating machine learning such as clustering, decoding deep neural network models etc.
on fog devices that could lead to scalable inferences.
Fog2Fog communication is discussed with respect to analytical models for power savings.
The book chapter concludes by interesting case studies on real world situations and practical data.
Future pointers to research directions, challenges and strategies to manage these are discussed as well.
We summarize case studies employing proposed architectures in various application areas.
The use of edge devices for processing offloads the cloud leading to an enhanced efficiency and performance.

Cell location, segmentation and feature extraction of cell images are principal tasks of a high-resolution system for automated cytology.
To perform these tasks with high speed, image processing algorithms and the architecture of a processor have to be optimized mutually.
This has led to the development of a fast system for the evaluation of cytologic samples based on an optimized TV microscope, a host minicomputer with different peripheral array processors and digital image storages.
The processors are optimized in speed for two-dimensional local operations to investigate neighborhood relations and morphology in cell images.
Two-dimensional transformations of TV images (288 x 512 x 8 bit) can be carried out within 20 to 200 msec.
The processors are able to realize linear filter functions (correlation, convolution) as well as nonlinear functions (median filtering).
A set of measurements like area, circumference and connectivity can be derived parallely from one image in 20 msec.
The system performs efficient and fast detection and segmentation of cells scanned in one TV frame within one second as well as the extraction of a large number of morphologic features within a few seconds.
Based on these procedures, high-resolution analysis of several thousand cells of a sample within one minute will be possible.

We present a higher order kinetic Monte Carlo methodology suitable to model the evolution of systems in which the transition rates are non-trivial to calculate or in which Monte Carlo moves are likely to be non-productive flicker events.
The second order residence time algorithm first introduced by Athenes et al.
[Phil.
Mag.
A 76 (1997) 565] is rederived from the n-fold way algorithm of Bortz et al.
[J. Comput.
Phys.
17 (1975) 10] as a fully stochastic algorithm.
The second order algorithm can be dynamically called when necessary to eliminate unproductive flickering between a metastable state and its neighbours.
An algorithm combining elements of the first order and second order methods is shown to be more efficient, in terms of the number of rate calculations, than the first order or second order methods alone while remaining statistically identical.
This efficiency is of prime importance when dealing with computationally expensive rate functions such as those arising from long-range Hamiltonians.
Our algorithm has been developed for use when considering simulations of vacancy diffusion under the influence of elastic stress fields.
We demonstrate the improved efficiency of the method over that of the n-fold way in simulations of vacancy diffusion in alloys.
Our algorithm is seen to be an order of magnitude more efficient than the n-fold way in these simulations.
We show that when magnesium is added to an Al-2at.%Cu alloy, this has the effect of trapping vacancies.
When trapping occurs, we see that our algorithm performs thousands of events for each rate calculation performed.

Graphical models such as Markov random fields have been successfully applied to a wide variety of fields, from computer vision and natural language processing, to computational biology.
Exact probabilistic inference is generally intractable in complex models having many dependencies between the variables.
::: We present new approaches to approximate inference based on linear programming (LP) relaxations.
Our algorithms optimize over the cycle relaxation of the marginal polytope, which we show to be closely related to the first lifting of the Sherali-Adams hierarchy, and is significantly tighter than the pairwise LP relaxation.
::: We show how to efficiently optimize over the cycle relaxation using a cutting-plane algorithm that iteratively introduces constraints into the relaxation.
We provide a criterion to determine which constraints would be most helpful in tightening the relaxation, and give efficient algorithms for solving the search problem of finding the best cycle constraint to add according to this criterion.
::: By solving the LP relaxations in the dual, we obtain efficient message-passing algorithms that, when the relaxations are tight, can provably find the most likely (MAP) configuration.
Our algorithms succeed at finding the MAP configuration in protein side-chain placement, protein design, and stereo vision problems.
(Copies available exclusively from MIT Libraries, Rm.
14-0551, Cambridge, MA 02139-4307.
Ph.
617-253-5668; Fax 617-253-1690.)

Recent years have seen greatly increasing interests in voice over IP in wireless LANs, in which the IEEE 802.11 distributed coordination function protocol or enhanced DCF protocol is used.
However, since both DCF and EDCF are contention-based medium access control protocols, it is difficult for them to support the strict QoS requirement for VoIP.
Therefore, in this article we propose a novel call admission control scheme that runs at the MAC layer to support VoIP services.
The call admission control mechanism regulates voice traffic to efficiently coordinate medium contention among voice sources.
The rate control mechanism regulates non-voice traffic to control its impact on the performance of voice traffic.
Extensive simulations demonstrate that the proposed schemes can well support statistical QoS guarantees for voice traffic and maintain stable high throughput for non-voice traffic at the same time.

The question of what are the necessary elements to integrate a robotics system that would enable it to carry out a task, i.e.
pick-up and transport objects in an unknown environment, is addressed.
One of the major concerns is to insure adequate data throughput and fast communication between modules within the system, so that haptic tasks can be adequately carried out.
The communication issues involved in the development of such a system are discussed.
>

Solutions of the spectral Navier equation in the linearized theory of elasticity that satisfy the Herglotz boundness condition at infinity are introduced.
The leading asymptotic terms in a neighborhood of infinity provide the far-field patterns, and the Herglotz norm is expressed as the sum of the $L^2 $-norms of these patterns over the unit sphere.
Basic integral representations that connect the solid spherical Navier eigenvectors to the vector spherical harmonics over the unit sphere are utilized to prove the fundamental representation theorem for the elastic Herglotz solutions in full space.
It is shown that the longitudinal and the transverse Herglotz kernels are exactly the corresponding far-field patterns of the irrotational and the solenoidal parts of the displacement field.
Particular methods to obtain the displacement field from the far-field patterns, and vice versa, are also described.

The Requirement Engineering is the most important phase of the software development life cycle which is used to translate the imprecise, incomplete needs and wishes of the potential users of software into complete, precise and formal specifications.
These specifications can be decomposed on application of a data mining techniques, clustering.
The process of clustering the requirements allows reducing the cost of software development and maintenance.
In this research two most frequently used algorithms in clustering namely k means and fuzzy c means are used.
The output generated is then analyzed for evaluating the performance of the two clustering algorithms.
The requirements specified by the different stakeholders of the library are used as the input.
The data mining tool WEKA was used for clustering.
The clustering algorithms were then analyzed for accuracy and performance.
On analysis the fuzzy c means algorithm was found to be more suitable for clustering of library requirements.
The results proved to be satisfactory.

Abstract This paper addresses the trajectory tracking problem for a quadcopter system under nominal and fault-affected scenarios (in the latter case, due to stuck actuator(s)).
Differential flatness is employed for trajectory generation and control design.
The particularity resides in that a full parametrization of the states and inputs is given without any assumptions or simplifications on the quadcopter dynamics.
Furthermore, using the properties of flatness and a combination between computed torque control and feedback linearization, a two-layer control design is proposed.
The tracking performances and stability gurantees are analyzed for nominal and faulty functioning under extensive simulations.

A box for storing a battery or the like comprises a plurality of upstanding panels (11-14) defining a storage compartment (16) and a removable cover (18) disposed over the compartment.
A door (24) is movably mounted adjacent to the compartment and when closed firmly by a latch (26) holds the cover in place to attenuate noise emanating in the storage box.
A sound suppressor (37) extends inwardly from the door to overlie the cover to engage and hold the cover against a support (20-23) and to attenuate noise.

A radio system includes a fairly large number of transmitting-receiving stations which are spatially distributed over an area to be covered.
Each of the stations preferably serves a fairly large number of individual subscribers in such a way that the individual subscriber can reach a subscriber who may be connected to another transmitting-receiving station.
Radio frequency channels with different frequencies are provided at least for the speech channels of adjacent exchanges and positioned in a frequency range which is fixed for the system.
The transmission of the individual speech channel is provided in accordance with digital methods, preferably by means of converting the signals of the individual speech channel into a PCM or a pulse delta modulation signal.
Preferably, the bandwidth for the individual signal is reduced through the use of a Vocoder.
The transmitter of an exchange transmits its information toward the other exchanges, through which a connection is to be established to the called party, by way of an omnidirectional antenna which effects little grouping in the plane spatially determined by the exchange.
Directional antennas are provided for receiving the radio frequency signals and preferably provide a relatively high degree of grouping in the horizontal plane and in the vertical plane.
The transmitter therefore covers all exchanges which must be reached by way of one antenna while the receiver in each exchange will respectively receive the transmitters of the other exchanges by way of its associated directional antenna.

Abstract A survey of the principal schemes in the literature suggested that a new way of addressing the problem of signature recognition be formulated in order to find a satisfactory solution for eliminating random forgeries.
A fundamental problem in the field of off-line signature recognition is the lack of a pertinent shape representation or shape factor.
This paper introduces a novel idea for a dynamic signature recognition system.
An initial attempt is presented to demonstrate the data glove as an effective high-bandwidth data entry device for signature recognition.
GloveSignature is a virtual-reality-based environment to support the signing process.
The proposed approach retains the power to discriminate against forgeries.
This paper extends the use of instrumented data gloves—gloves equipped with sensors for detecting finger bend and hand position and orientation for recognizing hand signatures.
Several researchers have already explored the use of gloves in other application areas but using the gloves for the recognition of hand signatures has never been reported.
An attempt is made in this research to explore the feasibility of using the 5th Glove in on-line signature recognition.
Two hundred signatures were collected from 20 subjects, and features were extracted.
We demonstrate the effectiveness of a hybrid technique, which is based on both the most discriminating eigenfeatures and the self-organizing maps (SOFMs) for signature recognition.

Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained.
In single-label classification tasks, groups are differentiated based on the value of the target variable.
In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed.
This paper investigates stratification in the multi-label data context.
It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria.
The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.

Let $A(x)=(A_{ij}(x))$ be a square matrix with $A_{ij}$ being a polynomial in $x$.
This paper proposes "combinatorial relaxation" type algorithms for computing the degree of the determinant, $\delta(A) = \deg_x \det A(x)$, based on its combinatorial upper bound $\widehat \delta(A)$, which is defined in terms of the maximum weight of a perfect matching in an associated graph.
The graph is bipartite for a general square matrix $A$ and nonbipartite for a skew-symmetric $A$.
The algorithm transforms $A$ to another matrix $A'$ for which $\delta(A) = \delta(A') = \widehat \delta(A')$ with successive elementary operations.
The algorithm is efficient, making full use of the fast algorithms for weighted matchings; it is combinatorial in almost all cases (or generically) and invokes algebraic elimination routines only when accidental numerical cancellations occur.
::: It is shown in passing that for a (skew-)symmetric polynomial matrix $A(x)$ there exists a unimodular matrix $U(x)$ such that $A'(x)=U(x) A(x) U(x)^\tp$ satisfies $\delta(A) = \delta(A') = \widehat \delta(A')$.

In this paper some analytical solutions are given for a two-dimensional advection equation with anisotropic dispersion.
Chemical decay or adsorption-like reaction inside the liquid phase is considered.
Bessel function expansion is used to solve the second order PDE model with different initial conditions, corresponding to usual experimental practices.
Cylindrical geometry is considered since large columns could be adopted to investigate both anisotropic dispersion and adsorption/desorption kinetic mechanisms.

Personal attitudes are a major factor to affect individual information technology usage.
The purpose of this study is to explore the role of individual attitudes toward computers and the Internet usage for assisting job performance.
This research applies the 3-TUM approach to understand individual attitudes toward computers and the Internet usage.
After statistical analysis, the results provide a support that the 3-TUM is appropriate model for investigating faculty and staff perceptions toward computers and Internet.
In addition, the results also support that using computers and the Internet may assist individual job performance.
Furthermore; this study offer evidence that when individuals have more self-efficacy and feel computers and the Internet are more useful, then they have more behavioral intention to use and learn computers and the Internet for assisting their job performance.

In this article we present our ideas for an integrating activity for archival research on the Holocaust.
We analyse how we can improve Holocaust-related collection descriptions for research, which we will make available online, and how EHRI provides travel grants for transnational access to existing infrastructures in Holocaust research.
Both approaches help us overcome that Holocaust-related material is geographically dispersed and address the challenges for historical research stemming from the way documentation on the Holocaust has been attempted up to now.
We have chosen to implement the EHRI integrated information resource using graph databases.
With their emphasis on relationships, graph databases are particularly well suited for historical research in particular and humanities research in general.
We analyse the architecture and implementation details of this novel approach and show how graph databases integrate with traditional ways of searching and browsing historical collections.
This way, we su...

Material behaviour computes form.
In the physical world, material form is always inseparably connected to internal constraints and external forces; in the virtual space of digital design, though, form and force are usually treated as separate entities – divided into processes of geometric form generation and subsequent engineering simulation.
Using the example of the interdisciplinary ICD/ITKE Research Pavilion, constructed at the University of Stuttgart in 2010, Moritz Fleischmann, Jan Knippers, Julian Lienhard, Achim Menges and Simon Schleicher explain how feedback between computational design, advanced simulation and robotic fabrication expands the design space towards previously unexplored architectural possibilities.
Copyright © 2012 John Wiley & Sons, Ltd.

We present a novel hardware-accelerated texture advection algorithm to visualize the motion of two-dimensional unsteady flows.
Making use of several proposed extensions to the OpenGL-1.2 specification, we demonstrate animations of over 65,000 particles at 2 frames/sec on an SGI Octane with EMXI graphics.
High image quality is achieved by careful attention to edge effects, noise frequency, and image enhancement.
We provide a detailed description of the hardware implementation, including temporal and spatial coherence techniques, dye advection techniques, and feature extraction.

Blind Source Separation (BSS) methods, like Independent Component Analysis (ICA), show good performance in the analysis of fMRI data.
However, the independence assumption used in ICA, may be violated in practice.
Hence, it is important to develop algorithm which can fully exploit the characteristics of fMRI data and use more reliable assumptions.
In this paper, we propose an fMRI data analysis method which exploits the sparsity of source components in a signal dictionary.
The proposed method, derived as a two-stage method, is established by reformulating the blind separation problem as a sparse approximation problem.
First, a priori selection of a particular dictionary, in which the source components are assumed to be sparsely representable.
By choosing a particular dictionary (like wavelet dictionary), the source components, which can be well sparsified in the selected dictionary, are estimated more accurately.
Second, the source components are extracted by exploiting their sparse representability.
The extracted signal components are applied to find consistent task related (CTR) component, activation voxels of CTR, and performance of neural decoding.
Numerical results show that compared to ICA based method, the proposed method can extract more useful information from fMRI data, and higher performance on voxel selection and neural decoding can be achieved by using the separated sources.

In a visibility representation (VR for short) of a plane graph G, each vertex of G is represented by a horizontal line segment such that the line segments representing any two adjacent vertices of G are joined by a vertical line segment.
Rosenstiehl and Tarjan [Rectilinear planar layouts and bipolar orientations of planar graphs, Discrete Comput.
Geom.
1 (1986) 343], Tamassia and Tollis [An unified approach to visibility representations of planar graphs, Discrete Comput.
Geom.
1 (1986) 321] independently gave linear time VR algorithms for 2-connected plane graph.
Afterwards, one of the main concerns for VR is the size of the representation.
In this paper, we prove that any plane graph G has a VR with height bounded by ⌊5n/6⌋.
This improves the previously known bound ⌊15n/16⌋.
We also construct a plane graph G with n vertices where any VR of G requires a size of (⌊2n/3⌋) × (⌊4n/3⌋ - 3).
Our result provides an answer to Kant's open question about whether there exists a plane graph G such that all of its VR require width greater that cn, where c > 1 [G. Kant, A more compact visibility representation, Internat.
J. Comput.
Geom.
Appl.
7 (1997) 197].

This paper presents a computer-aided geometric design approach to realize a new genre of 3D puzzle, namely the 3D Polyomino puzzle.
We base our puzzle pieces on the family of 2D shapes known as polyominoes in recreational mathematics, and construct the 3D puzzle model by covering its geometry with polyominolike shapes.
We first apply quad-based surface parametrization to the input solid, and tile the parametrized surface with polyominoes.
Then, we construct a nonintersecting offset surface inside the input solid and shape the puzzle pieces to fit inside a thick shell volume.
Finally, we develop a family of associated techniques for precisely constructing the geometry of individual puzzle pieces, including the ring-based ordering scheme, the motion space analysis technique, and the tab and blank construction method.
The final completed puzzle model is guaranteed to be not only buildable, but also interlocking and maintainable.

An outer bound to the capacity region of the two-receiver discrete memoryless broadcast channel is given.
The outer bound is tight for all cases where the capacity region is known.
When specialized to the case of no common information, this outer bound is shown to be contained in the Korner-Marton outer bound.
This containment is shown to be strict for the binary skew-symmetric broadcast channel.
Thus, this outer bound is in general tighter than all other known outer bounds on the discrete memoryless broadcast channel.

Excessive executive compensation is one of the prevailing topics in the business press.
The "managerial power approach", which is at the center of numerous publications of Bebchuk and Fried (Pay without performance: the unfulfilled promise of executive compensation, Harvard University Press, Cambridge, 2004) and Bebchuk and Grinstein (Oxford Rev Econ Policy 21:283---303, 2005), claims that chief executive officers (CEOs) corrupt the pay-setting process by influencing the board of directors.
This paper analyzes the role of the board of directors in the pay-setting process for the CEO's compensation schedule in an optimal contracting framework.
The board's role in our model is to provide information about the CEO's ability to the shareholders.
We assume that the CEO and other stakeholders, such as employees' representatives, can influence the board to act in their respective favor when reporting to the shareholders.
We show how the CEO's influence on the board alters the optimal contract and the CEO's incentives.
Most interestingly, the influence of an interest group on the board can be the reason for high pay schedules, even though that was not the objective of the interest group.

The multi-pile vehicle routing problem is a particular combination of loading and routing problems, in which items have to be loaded into different piles within vehicles, and then delivered with minimum cost.
The problem is motivated by a real-world timber distribution problem, and is of both theoretical and practical interest.
In this paper, we first develop heuristic and exact methods to solve the loading problem.
We then include these methods into a tailored combination of Variable Neighborhood Search and Branch-and-Cut, to solve the overall problem.
Extensive computational results show how the resulting algorithms are capable of solving to optimality a large number of small-size instances, and of consistently outperforming previous algorithms from the literature on large-size and real-world instances.

Abstract We utilize the Lyapunov function method to analyze stability of continuous nonlinear neural networks with delays and obtain some new sufficient conditions ensuring the globally asymptotic stability independent of delays.
Three main conditions imposed on the weighting matrices are established.
(i).
The spectral radius ρ ( M −1 (| W 0 |+| W τ |) K ) M −1 (| W 0 |+| W τ |) K + P −1 ((| W 0 |+| W τ |) KM −1 ) T P ‖ ∞ μ 2 ( W 0 )+‖ W τ ‖ 2, F m / k ).
These three conditions are independent to each other.
The delayed Hopfield network, Bidirectional associative memory network and cellular neural network are special cases of the network model considered in this paper.
So we improve some previous works of other researchers.

The main goal of external beam radiotherapy is the treatment of tumours, while sparing, as much as possible, surrounding healthy tissues.
In order to master and optimize the dose distribution within the patient, dosimetric planning has to be carried out.
Thus, for determining the most accurate dose distribution during treatment planning, a compromise must be found between the precision and the speed of calculation.
Current techniques, using analytic methods, models and databases, are rapid but lack precision.
Enhanced precision can be achieved by using calculation codes based, for example, on Monte Carlo methods.
However, in spite of all efforts to optimize speed (methods and computer improvements), Monte Carlo based methods remain painfully slow.
A newer way to handle all of these problems is to use a new approach in dosimetric calculation by employing neural networks.
Neural networks (Wu and Zhu 2000 Phys.
Med.
Biol.
45 913–22) provide the advantages of those various approaches while avoiding their main inconveniences, i.e., time-consumption calculations.
This permits us to obtain quick and accurate results during clinical treatment planning.
Currently, results obtained for a single depth–dose calculation using a Monte Carlo based code (such as BEAM (Rogers et al 2003 NRCC Report PIRS-0509(A) rev G)) require hours of computing.
By contrast, the practical use of neural networks (Mathieu et al 2003 Proceedings Journees Scientifiques Francophones, SFRP) provides almost instant results and quite low errors (less than 2%) for a two-dimensional dosimetric map.

The network architecture of an RFID system affects the operating performance and scalability of the entire system.
The related researches about RFID system could be made easier as the bridge-based and the gateway-based network architecture.
This study considers a scenario for an RFID object tracking system designs a hierarchical RFID network system and evaluates the operating performance of different RFID system architectures.
The results of performance evaluations demonstrate that the proposed hierarchical RFID network architecture reduces the network and database system loading by 41.8% and 83.2%, respectively.
The operation of RFID object tracking over the hierarchical network architecture is indicated to be highly efficient and scalable.

A classification of semifield planes of order q4 with kernel Fq2 and center Fq is given.
For q an odd prime, this proves the conjecture stated in [M. Cordero, R. Figueroa, On the semifield planes of order 54 and dimension 2 over the kernel, Note Mat.
(in press)].
Also, we extend the classification of semifield planes lifted from Desarguesian planes of order q2, q odd, obtained in [M. Cordero, R. Figueroa, On some new classes of semifield planes, Osaka J.
Math.
30 (1993) 171-178], to the even characteristic case.

We investigate the solution of instances of the uncapacitated facility location problem with at most 3000 potential facility locations and similar number of customers.
We use heuristics that produce a feasible integer solution and a lower bound on the optimum.
In particular, we present a new heuristic whose gap from optimality was generally below 1%.
The heuristic combines the volume algorithm and a recent approximation algorithm based on randomized rounding.
Our computational experiments show that our heuristic compares favorably against DUALOC.

During the last few decades, several multi-criteria decision analysis methods have been proposed to help in selecting the best compromise alternatives.
Among them, analytic hierarchy process (AHP) and its applications have attracted much attention from academics and practitioners.
However, since the early 1980s, critics have raised questions regarding its proper use.
One of them concerns the unacceptable changes in the ranks of the alternatives, called rank reversal, upon changing the structure of the decision.
Several modifications were suggested to preserve ranks.
In this paper, a classification scheme and a comprehensive literature review are presented in order to uncover, classify and interpret the current research on AHP methodologies and rank reversals.
On the basis of the scheme, 61 scholarly papers from 18 journals are categorized into specific areas.
The specific areas include the papers on the topics of adding/deleting alternatives and the papers published in adding/deleting criteria.
The scholarly papers are also classified by (1) year of publication, (2) journal of publication, (3) authors' geographic location and (4) using the AHP in association with other methods.
It is hoped that the paper can meet the needs of researchers and practitioners for convenient references of AHP methodologies and rank reversals and hence promote the future of rank reversal research.
Copyright © 2012 John Wiley & Sons, Ltd.

Abstract Advances in computer technology have brought the feasibility of intelligent system components to fuitation.
Within the manufacturing environment, the regulation of parts' value-adding operations from entry into the system to completion as a finished product now can be individualized to depend on part specific information.
The new CADENCE system, introduced in this research, manages the physical parts' operations and the information flow within manufacturing.
Integration of both the physical flow dependencies and the information flow is provided by a negotiation procedure.
CADENCE goes beyond previously proposed task bidding structures by utilizing distributed decision making in managing the flow of individual intelligent parts.

The research and development of biomedical imaging techniques requires more number of image data from medical image acquisition devices, like computed tomography CT, magnetic resonance imaging MRI, positron emission technology, and single photon emission computed tomography.
Multimodal image fusion is the process of combining information from various images to get the maximum amount of content captured by a single image acquisition device at different angles and different times or stages.
This article analyses and compares the performance of different existing image fusion techniques for the clinical images in the medical field.
The fusion techniques compared are simple or pixel-based fusion, pyramid-based fusion, and transform-based fusion techniques.
Four set of CT and MRI images are used for the above fusion techniques.
The performance of the fused results is measured with seven parameters.
The experimental results show that out of seven parameters the values of four parameters, such as average difference, mean difference, root mean square error, and standard deviation are minimum and the values of remaining three parameters, such as peak signal to noise ratio, entropy, and mutual information are maximum.
From the experimental results, it is clear that out of 14 fusion techniques taken for survey, image fusion using dual tree complex wavelet transform gives better fusion result for the clinical CT and MRI images.
Advantages and limitations of all the techniques are discussed with their experimental results and their relevance.
© 2014 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 24, 193-202, 2014.

Semiconductor enterprises should continue to provide more capacity to meet the demands of the highly competitive semiconductor markets.
Photolithography is usually the bottleneck process with the most expensive equipment in a semiconductor wafer fabrication system.
Usually, photolithography area controls the performance of whole semiconductor manufacturing system.
To improve the performances of the photolithography area with dynamic dispatching combination rules, a dynamic scheduling method based on Kohonen neural network (KNN) was proposed in this paper.
A dynamic scheduling framework is also proposed.
The method has been integrated into the dynamic scheduling framework.
A KNN-based sample learning algorithm for selecting best combination rules is presented.
Finally, the results of simulation experiments indicate that the proposed method is effective and feasible in real-time scheduling of semiconductor fabrication system under both closed-loop release policy and open-loop release policy.

Program bugs remain a major challenge for software developers and various tools have been proposed to help with their localisation and elimination.
Most present-day tools are based either on over-approximating techniques that can prove safety but may report false positives, or on under-approximating techniques that can find real bugs but with possible false negatives.
In this paper, we propose a dual static analysis that is based only on over-approximation.
Its main novelty is to concurrently derive conditions that lead to either success or failure outcomes and thus we provide a comprehensive solution for both proving safety and finding real program bugs.
We have proven the soundness of our approach and have implemented a prototype system that is validated by a set of experiments.

Currently, there are many system requirements to guarantee the scalability, fault-tolerance, and high performance in the rapidly evolving research fields such as Grid and P2P technologies.
Due to the dynamic nature of the Grid computing and P2P in networks, the behavior of system might be very unpredictable in respect of reliability.
In this paper, we propose the Group Peer based P2P Grid system architecture with k-Redundancy Scheme that can satisfy these requirements.
With respect to load balancing, this P2P Grid system clearly shows that redundancy scheme of group peer guarantees the reliability for resource or service discovery.
Through the theoretical analysis and simulation, we discuss the performance of the proposed scheme.

We present a pipeline for the personalization of model-based Purkinje fast conduction system using fast electrophysiological models and optical mapping data acquired from ex-vivo porcine hearts.
The regional density of the Purkinje terminals as well as the latest endocardial activation time were the parameters personalized in an iterative procedure maximizing the similarity between the outcome of the electrophysiological simulations and measurements obtained from optical mapping data.
We used a fast wave-front Eikonal-based electrophysiological model that generated the depolarization time maps that were subsequently compared with measurements at each iteration of the optimization stage.
The pacing site given by the experimental data and the optimized Purkinje system were introduced into the electrophysiological model.
We obtained a regional distribution of Purkinje end-terminals in agreement with findings in the literature.
Nevertheless, remaining differences between simulations and measurements after personalization suggest that epicardial data obtained from optical mapping data might not be sufficient to optimize the Purkinje system, which is basically located at the endocardium.
On the other hand, the developed pipeline could also be used with endocardial data on electrical activation provided by non-contact or contact mapping system.

Abstract This paper is concerned with the problem of constructing a controllable graph subject to some practical edge constraints.
Specifically, suppose the total amount of vertices and the upper bounds on the graph diameter or on the vertex degree are given.
We consider the problem of exploring a class of feasible graphs that satisfy the constraints.
Using the hybrid of a path graph and an antiregular graph we propose a simple and systematic method to generate a class of controllable graphs whose diameters or degrees cover the full possible ranges.
The method to select the control vector to ensure the controllability of the combined graph is also proposed.
Numerical examples are provided to demonstrate our results.

In credit card portfolio management, predicting the cardholders’ behavior is a key to reduce the charge off risk of credit card issuers.
As a promising data mining approach, multiple criteria linear programming (MCLP) has been successfully applied to classify credit cardholders’ behavior into two or multiple-groups for business intelligence.
The objective of this paper is to study the stability of MCLP in classifying credit cardholders’ behavior by using cross-validation and ensemble techniques.
An overview of the two-group MCLP model formulation and a description of the dataset used in this paper are introduced first.
Then cross-validation and ensemble methods are tested respectively.
As the results demonstrated, the classification rates of cross-validation and ensemble methods are close to the rates of using MCLP alone.
In other words, MCLP is a relatively stable method in classifying credit cardholders’ behavior.

We examine three-dimensional (3-D) wavelet coding of video with arbitrary regions of support (AROS).
A critically sampled wavelet transform is applied to the AROS and a modified 3-D set partitioning in hierarchical trees (SPIHT) algorithm is used to quantize and code the wavelet coefficients in the AROS only.
Experiments show that, for typical MPEG-4 pre-segmented sequences, our proposed method can achieve a gain of up to 5.6 dB in average PSNR at the same rate over 3-D SPIHT coding of regular volumes that embed the AROS of the given video sequences.

Abstract The proposed approach inherited the paradigm in particle swarm optimization (PSO) to implement a chaotic search around global best position ( gbest ) and enhanced by K-means clustering algorithm, named KCPSO.
K-means with clustering property in PSO resulted in rapid convergence while chaotic search with ergodicity characteristic in PSO contributed to refine gbest .
Experimental results indicated that the proposed KCPSO approach could evidently speed up convergence and successfully solving complex multidimensional problems.
Besides, KCPSO was compared with canonical PSO in performance.
And, a case study was also employed to demonstrate the validity of the proposed approach.

Abstract This paper concerns determining a new class of statistics for Catalan lattice paths that satisfy the Narayana distribution and account for certain specified behavior of the paths at the constraining boundary.
Each new statistic is shown to be related, usually bijectively, to either a known or another new statistic having the Narayana distribution.

Abstract We give an account of the ‘partition conjecture’ which relates two classic results of convex geometry — Radon's theorem and Tverberg's theorem — in an abstract setting.
We also examine the validity of the conjecture in certain special cases.

The fact that people freely express their opinions and ideas in no more than 140 characters makes Twitter one of the most prevalent social networking websites in the world.
Being popular in Saudi Arabia, we believe that tweets are a good source to capture the public's sentiment, especially since the country is in a fractious region.
Going over the challenges and the difficulties that the Arabic tweets present - using Saudi Arabia as a basis - we propose our solution.
A typical problem is the practice of tweeting in dialectical Arabic.
Based on our observation we recommend a hybrid approach that combines semantic orientation and machine learning techniques.
Through this approach, the lexical-based classifier will label the training data, a time-consuming task often prepared manually.
The output of the lexical classifier will be used as training data for the SVM machine learning classifier.
The experiments show that our hybrid approach improved the F-measure of the lexical classifier by 5.76% while the accuracy jumped by 16.41%, achieving an overall F-measure and accuracy of 84 and 84.01% respectively.

The paper is aimed at generating optimal gait cycles in the sagittal plane of a biped, the locomotion system of which has anthropomorphic characteristics.
Both single and double support phases are globally optimised, considering incompletely specified transition postural configurations from one phase to the other.
An impactless heel-touch is prescribed.
Full dynamic models are developed for both gait phases.
They are completed by specific constraints attached to the unilaterality of contact with the supporting ground.A parametric optimisation method is implemented.
The biped joint coordinates are approximated by cubic splines functions connected at uniformly distributed knots along the motion time.
The finite set of unknowns consists of the joint coordinate values at knots, some gait pattern parameters at phase transitions, and the motion time of each phase.
The step length is adjusted to the prescribed gait speed by the optimisation process.
Numerical simulations concerning slow and fast optimal gaits are presented and discussed.

The Fourier series representation of the quantization error sawtooth yields exact expressions and convenient approximations for all intermodulation (IM) and distortion components produced by quantization of the sum of two sinusoids whose respective amplitudes are A and a.
The mean-squared values of the IM components are also calculated in the case where A and a fluctuate over several quantization steps.
When A and a are many times the quantization-step size Q, these mean-squared values turn out to be approximately Q4/(180 π2Aa) except for high-order IM.
The quantization is generally assumed to be uniform, but nonuniform quantization is also discussed.
The case of A \gg Q and a \ll Q is considered as well as that of a = 0.
The inclusion of even a small amount of additive noise in the input, however, is found to reduce the IM and distortion to undetectable levels, thus ensuring that IM cannot be mistaken for an imput signal unless, contrary to assumption, the quantization staircase is curved, i.e., the quantization is nonlinear.
Hence, not many quantization bits are needed in order to avoid IM problems.

This paper studies feedback stabilization for networked control systems (NCSs) over quantized fading channels placed at the plant input, which cover both logarithmic quantization and packet drop in the actuator channel.
The notion of mean-square (MS) stability is developed in the input-output setting, and the MS stabilizability is studied for both single-input (SI) and multi-input (MI) systems under state feedback.
A necessary and sufficient condition is derived for the MS stabilizability of the NCS over the quantized fading channel by using channel resource allocation.

Classifying spam is a topic of ongoing research in the area of natural language processing, especially with the increase in the usage of the Internet for social networking.
This has given rise to the increase in spam activity by the spammers who try to take commercial or non-commercial advantage by sending the spam messages.
In this paper, we have implemented an evolving area of technique known as deep learning technique.
A special architecture known as Long Short Term Memory (LSTM), a variant of the Recursive Neural Network (RNN) is used for spam classification.
It has an ability to learn abstract features unlike traditional classifiers, where the features are hand-crafted.
Before using the LSTM for classification task, the text is converted into semantic word vectors with the help of word2vec, WordNet and ConceptNet.
The classification results are compared with the benchmark classifiers like SVM, Naive Bayes, ANN, k-NN and Random Forest.
Two corpuses are used for comparison of results: SMS Spam Collection dataset and Twitter dataset.
The results are evaluated using metrics like Accuracy and F measure.
The evaluation of the results shows that LSTM is able to outperform traditional machine learning methods for detection of spam with a considerable margin.

Computer vision and image processing technology have been rapidly developed and widely applied in many fields.
There are many potential applications in modern agriculture.
In this paper, a novel vegetable disease and insect pest recognition method is proposed based on the current computer vision and image processing methods.
To investigate the vegetable disease and insect pest state, it is convenient to use images captured using smart phones for judgment.
To implement this application, the disease area and the insect number on the leaves should be detected and figured out.
So a new extraction and classification algorithm is firstly introduced to recognize leaves from images.
Then a region-labeling algorithm is applied to calculate the insect number and disease areas in the segmented images.
To deal with the areas of adhesion, a mathematical morphology algorithm is used for separating the objects.
The proposed method is implemented on mobile smart devices and tested with field experiments.
The experimental...

Enterprise resource planning (ERP) software is a dominant approach for dealing with legacy information system problems.
In order to avoid invalidating maintenance and development support from the ERP vendor, most organizations reengineer their business processes in line with those implicit within the software.
Regardless, some customization is typically required.
This paper presents two case studies of ERP projects where customizations have been performed.
The case analysis suggests that while customizations can give true organizational benefits, careful consideration is required to determine whether a customization is viable given its potential impact upon future maintenance.
Copyright © 2001 John Wiley & Sons, Ltd.

We compare three mathematical programming modeling languages, GAMS, OMNI and MathPro.
To understand the properties of these languages, we formulate four linear programs in each language.
The formulations are representative of the kinds of model structures one encounters in practice.
Each of the languages focuses on a different view of linear programs.
GAMS approximates algebra, OMNI uses the activity view and MathPro uses a block schematic.
We summarize our experiences with the languages and suggest areas for further enhancement.

Abstract The paper deals with the balanced truncation and coprime factors reduction of Markovian jump linear (MJL) systems, which can have mode-varying state, input, and output dimensions.
We develop machinery for balancing mean square stable MJL system realizations using generalized Gramians and strict Lyapunov inequalities, and provide an improved a priori upper bound on the error induced in the balanced truncation process.
We also generalize the coprime factors reduction method and, in doing so, extend the applicability of the balanced truncation technique to the class of mean square stabilizable and detectable MJL systems.
We provide tools to establish mean square stabilizability and detectability of the considered MJL systems.
In addition, a notion of right-coprime factorization of MJL systems and methods to construct such factorizations are given.
The error measure in the coprime factors reduction approach, while still norm-based, does not directly capture the mismatch between the nominal system and the reduced-order model, as is the case in the balanced truncation approach where mean square stable models are considered.
Instead, the error measure is given in terms of the distance between the coprime factors realizations, and thus has an interpretation in terms of robust feedback stability.
The paper concludes with an illustrative example which demonstrates how to apply the coprime factors model reduction approach.

In the past, several model checking algorithms have been proposed to verify probabilistic reactive systems.
The techniques to combat the state-explosion problem have mainly concentrated on symbolic methods with variants of decision diagrams or abstraction methods.
In this paper, we show how partial order reduction with a variant of Peled's ample set method can be applied in the context of LTL model checking for probabilistic systems modelled by Markov decision processes.

Filmmaking is a unique, illustrated guide to and appraisal of the basic narrative and structural techniques employed, either consciously or intuitively, by all successful filmmakers.
Filled with examples from popular films, the book seamlessly joins theory and practice in an accessible approach to filmmaking that should find an eager audience among filmmakers (directors, editors, cinematographers, producers and screenwriters).
Bob Foss is co-writer of the screenplay for Disney's Shipwrecked.

In this work, we present a new multi-parametric magnetic resonance imaging (MP-MRI) texture feature model for automatic detection of prostate cancer.
In addition to commonly used imaging sequences in conventional MP-MRI, namely T2-weighted MRI (T2w) and diffusion-weighted imaging (DWI), our proposed MP-MRI texture feature model uses computed high-b DWI (CHB-DWI) and a new diffusion imaging sequence called correlated diffusion imaging (CDI).
A set of texture features was calculated for both the conventional MP-MRI and new MP-MRI texture feature model.
We evaluated the performance of the proposed MP-MRI texture feature model via leave-one-patient-out cross-validation using a Bayesian classifier trained on cancerous and healthy tissue samples obtained from real clinical MP-MRI datasets.
The proposed MP-MRI texture feature model outperformed the conventional model (i.e., T2w+DWI) with regard to cancer detection accuracy.

We prove that any bridgeless graph G = (V, E), |E| = m, |V| = N, admits a cycle cover of total length at most m + 54(n − 1).
We give a quick survey of the related problems and establish some properties for the vertex covering problem and for shortest coverings of cographic matroids.

EAM based incremental learning algorithm is presented.FCBF is adopted to select the minimum redundant features adaptively.EAM gets higher accuracy rate than FAM.EAM can learn new data incrementally without forgetting the original knowledge.
In this paper, an Ellipsoid ARTMAP (EAM) network model based on incremental learning algorithm is proposed to realize online learning and tool condition monitoring.
The main characteristic of EAM model is that hyper-ellipsoid is used for geometric representation of categories which can depict the sample distribution robustly and accurately.
Meanwhile, adaptive resonance based strategy can realize the update of the hyper-ellipsoid node locally and monotonically.
Therefore, the model has strong incremental learning ability, which guarantees the constructed classifier can learn new knowledge without forgetting the original information.
Based on incremental EAM model, a tool condition monitoring system is realized.
In this system, features are firstly extracted from the force and vibration signals to depict dynamic features of tool wear process.
Then, fast correlation based filter (FCBF) method is introduced to select the minimum redundant features adaptively so as to decrease the feature redundancy and improve classifier robustness.
Based on the selected features, EAM based incremental classifier is constructed to realize recognition of the tool wear states.
To show the effectiveness of the proposed method, multi-teeth milling experiments of Ti-6Al-4V alloy were carried out.
Moreover, to estimate the generation error of the classifiers accurately, a five-fold cross validation method is utilized.
By comparison with the commonly used Fuzzy ARTMAP (FAM) classifier, it can be shown that the averaging recognition rate of EAM initial classifier can reach 98.67%, which is higher than FAM.
Moreover, the incremental learning ability of EAM is also analyzed and compared with FAM using the new data coming from different cutting passes and tool wear category.
The results show that the updated EAM classifier can get higher classification accuracy on the original knowledge while realizing effective online learning of the new knowledge.

Software platforms’ success largely depends on complementors’ willingness to repeatedly invest their time and effort to the development of platform applications that attract users and increase the platform’s installed base.
But how can platform providers encourage desirable behaviours by complementors (i.e., application developers) in the absence of formal roles and hierarchical control structures?
Although previous studies of software-based platforms have identified openness as critical instrument at the macro (i.e., platform) level and have provided initial attempts to measure the construct, no research has been dedicated to comprehensively conceptualize and operationalize platform openness at the micro level from the perspective of application developers.
To go beyond these preliminary findings and to theorize about the nature and effects of platform openness as perceived by application developers, we develop a construct called perceived platform openness (PPO).
Drawing on recently advanced scale development methodologies, we conceptualize PPO as a multidimensional construct and empirically validate it with important consequent variables linked to developers’ continuous platform contributions.
Empirical evidence from several rounds of qualitative and quantitative steps supports the conceptual validity of the construct and empirical relevance of the scale across different smartphone platform contexts (i.e., Apple iOS and Google Android).
Researchers will benefit from the study’s systematic and comprehensive conceptualization of PPO, how it is measured, and how it relates to critical application developer beliefs and attitudes.
Platform managers may use our results to target the underlying facets of PPO most likely to contribute to the platform’s long-term goals.

Modified Look-Locker imaging is frequently used for T1 mapping of the myocardium.
However, the specific effect of various MRI parameters (e.g., encoding scheme, modifications of flip angle, heart rate, T2, and inversion times) on the accuracy of T1 measurement has not been studied through Bloch simulations.
In this work, modified Look-Locker imaging was characterized through a numerical solution for Bloch equations.
MRI sequence parameters that may affect T1 accuracy were systematically varied in the simulation.
For validation, phantoms were constructed with various T2 and T1 times and compared with Bloch equation simulations.
Human volunteers were also evaluated with various pulse sequences parameters to assess the validity of the numerical simulations.
There was close agreement between simulated T1 times and T1 times measured in phantoms and volunteers.
Lower T2 times (i.e., <30 ms) resulted in errors greater than 5% for T1 determination.
Increasing maximum inversion time value improved T1 accuracy particularly for precontrast myocardial T1.
Balanced steady-state free precession k space centric encoding improved accuracy for short T1 times (post gadolinium), but linear encoding provided improved accuracy for precontrast T1 values.
Lower flip angles are preferred if the signal-to-noise ratio is sufficiently high.
Bloch simulations for modified Look-Locker imaging provide an accurate method to comprehensively quantify the effect of pulse sequence parameters on T1 accuracy.
As an alternative to otherwise lengthy phantom studies or human studies, such simulations may be useful to optimize the modified Look-Locker imaging sequence and compare differences in T1-derived measurements from different scanners or institutions.
Magn Reson Med, 2013.
© 2012 Wiley Periodicals, Inc.

Abstract In this paper, a new algorithm is proposed to carry out fuzzy clustering without making assumptions on initial guesses.
The search for good clustering is made by a specific cluster-validity criterion.
This tool has been tested on six data sets.

Associations are a widely used construct of object-oriented languages.
However, the meaning of associations for conceptual modelling of application domains remains unclear.
This paper employs ontological analysis to first examine the software semantics of the association construct, and shows that they cannot be transferred to conceptual modelling.
The paper then explores associations as 'semantic connections' between objects and shows that this meaning cannot be transferred to conceptual modelling either.
::: ::: As an alternative to the use of associations, the paper proposes using shared properties, a construct that is rooted directly in ontology.
An example from a case study demonstrates how this is applied.
The paper then shows an efficient implementation in object-oriented programming languages to maintain seamless transitions between analysis, design, and implementation.

Collaborative tagging systems (CTSs), also known as folksonomies, have grown in popularity on the Web and social tagging has become an important feature of many Web 2.0 services.
It has been argued that the power of tagging lies in the ability for people to freely determine the appropriate tags for resources without having to rely on a predefined lexicon or hierarchy.
The free-form nature of tagging causes a number of problems in this social classification scheme, such as synonymy and morphological variety.
However, social tagging can be a valuable source of information to help in the organization of Web resources.
In this paper we present an empirical analysis carried out to determine the importance of social tagging in Web page classification.
Experimental results showed that tag-based classification outperformed classifiers based on full-text of documents.

The paper deals with the method of tool path generation for 5-axis control machining using a ball end mill.
5-axis control machining has been used for aircraft parts as well as for complicated shapes such as mold and dies.
However, most of the present CAM systems for 5-axis control machining have limited functions in terms of tool collision, workpiece shapes and machining methods.
For that reason, in many cases the optimal cutter location (CL) data cannot be obtained or considerable time is consumed.
To solve this problem, we applied a 3-dimensional configuration space (C-Space) and showed the relationship between all tool positions and postures and the existence of tool collision.
The method of tool path generation devised in the study enables users to generate CL data reflecting their own machining strategy such as smooth change in tool posture and the state of machining without considering the gouging.
The validity of this method was experimentally confirmed by successfully milling an impeller without tool collision occurring.

The potential approach of value theory is extended with respect to a new characterizing property called conservation giving a clear interpretation of the potential.
Many analogues between game theory and physics are presented.
Particularly, there is a theorem of conservation of energy analogous to the highly important one in classical mechanics.
Moreover, the Shapley-Value gets a new interpretation as the marginal contribution to a certain average in contrast to that as an average marginal contribution instead.
The Banzhaf-Index can also be uniquely characterized by this approach.
Finally, all results are extended to games with a continuum of players of finitely many types.

This paper presents an approach to visualize and analyze 3D building information models within virtual 3D city models.
Building information models (BIMs) formalize and represent detailed information related to the lifecycle of buildings, e.g., information about composition, facilities, equipment, usage, maintenance, and workflows such as rescue scenarios.
Complementary, virtual 3D city models represent objects and phenomena of urban areas, typically at a lower level of information detail; virtual 3D city models as a general framework and platform for spatial data allow us to seamlessly combine GIS and BIM data.
In our approach, BIM data and 3D geodata, both provided by possibly distributed, heterogeneous web services, are efficiently integrated by the underlying real-time 3D geovisualization system.
To facilitate insights into complex spatial scenarios, two configurable BIM-specific visualization techniques have been developed, which map BIM data onto 3D building graphics variables respectively geometrically distort 3D building representations.
The visualization functionality can itself be accessed as a specialized web 3D perspective view service.
We demonstrate our approach by a fire and rescue scenario for a part of a 3D campus model.

Although the fields of data visualization and automated knowledge discovery (AKD) share many goals, workers in each field have been reluctant to adopt the tools and methods of the other field.
Many AKD researchers discourage the use of visualization tools because they believe that dependence on human steering will impede the development of numerical or analytical descriptions of complex data.
Many visualization researchers are concerned that their present platforms are being pushed to the limits of their performance by the most advanced visualization techniques and are therefore unwilling to incur the perceived overhead of having a database system mediate access to the data.
We argue that these attitudes are somewhat short-sighted and that the techniques of these two communities are complementary.
We discuss a specific visualization system that we have developed and describe the obstacles that must be overcome in integrating it into an AKD system.
© 1992 John Wiley & Sons, Inc.

This paper proposes a probabilistic approach to diagnose intermittent faults in a wireless sensor network (WSN).
Though system level diagnosis has become an important technique to diagnose faults in wired networks, its applicability to wireless network has not been well studied.
Though, intermittent faults are most probable in WSN, diagnosis of such faults has not been well explored.
The algorithm proposed in this paper has been evaluated through analytical means and is simulated assuming a 512 sensor nodes interconnected to form an arbitrary network topology.
The results show that accurate diagnosis of intermittent faults in WSN is feasible.

Conventional computers do not readily lend themselves to picture processing.
Digital image manipulation by conventional computer is accomplished only at a tremendous cost in time and conceptual distraction.
Computer image processing is the activity of modifying a picture such that retrieval of relevant pictorially encoded information becomes trivial.
Algorithm development for image processing is an alternating sequence of inspired creative visualizations of desired processed results and the formal procedures implementing the desired process on a particular image processing system.
But our process of creative visualization is of pictures as a whole.
Implementation of the visualized image manipulation by conventional computer requires fragmentation of the pictorial concept into information units matched to the word oriented capabilities of general purpose machines.
Conventional computer image processing could be broadly categorized as manipulation of pixel states rather than pictorial content.

Abstract ::: Purpose.
To provide a comprehensive description of the direct measurement of forces and moments applied on the socket of transfemoral amputees during daily living activities.
Methods.
The forces and the moments applied on the socket of one female transfemoral amputee were measured with a commercial transducer at a sampling frequency of 200 Hz and recorded at distance using a wireless modem to transmit the data.
The subject was asked to walk in a straight line and around a circle as well as to ascend and to descend a slope and stairs.
The subject was instructed to perform each activity at her natural pace and as she would usually perform it during daily life.
::: Results.
The results were based on a high number of gait cycles of the prosthetic leg for each activity.
For instance, 62 gait cycles were measured during level walking in a straight line.
Ascending a slope produced a larger moment around the medio-lateral axis than walking over the entire support phase.
Also, walking around a circle produced a higher moment about the long axis of the socket than walking during the push off phase of the support.
The mean stride frequency during descending a slope was higher than straight level walking.
All the other activities presented a slower mean stride frequency than straight level walking.
The impulse on the three axes was similar or smaller than walking in a straight line for all of the activities except for walking around a circle on the medio-lateral axis, as well as ascending a slope and stairs and walking around a circle on the long axis.
::: Conclusion.
An apparatus to directly measure the actual forces and moments applied to the socket of the transfemoral amputees during an unlimited number of steps and a wide range of activities is presented.
The apparatus presented here could be largely used by multi-disciplinary teams including engineers, prosthetists and physiotherapists facing the challenge of safely restoring the locomotion of transfemoral amputees fitted with a conventional socket or osseointegrated implant in particular.

Unstructured volume grids are ubiquitous in scientific computing, and have received substantial interest from the scientific visualization community.
In this paper, we take a point-based approach to rendering unstructured grids.
In particular, we present a novel method of approximating these irregular elements with point-based primitives amenable to existing hardware acceleration techniques.
To improve interactivity to large datasets, we have adapted a level-of-detail strategy.
We use a well-known quantitative metric to analyze the image quality achieved by the final rendering.

Sensor networks have increased the amount and variety of temporal data available, requiring the definition of new techniques for data mining.
Related research typically ad- dresses the problems of indexing, clustering, classification, summarization, and anomaly detection.
They present many ways for describing and comparing time series, but they fo- cus on their values.
This paper concentrates on a new as- pect - that of describing oscillation patterns.
It presents a technique for time series similarity search, based on multi- ple temporal scales, defining a descriptor that uses the an- gular coefficients from a linear segmentation of the curve that represents the evolution of the analyzed series.
Prelim- inary experiments with real datasets showed that our ap- proach correctly characterizes the oscillation of time series.

Abstract The paper presents an application of principal component analysis (PCA) to ECG processing.
For this purpose the ECG beats are time-aligned and stored in the columns of an auxiliary matrix.
The matrix, considered as a set of multidimensional variables, undergoes PCA.
Reconstruction of the respective columns on the basis of a low dimensional principal subspace leads to the enhancement of the stored ECG beats.
A few modifications of this classical approach to ECG signal filtering by means of a multivariate analysis are introduced.
The first one is based on replacing the classical PCA by its robust extension.
The second consists in replacing the analysis of the whole synchronized beats by the analysis of shorter signal segments.
This creates the background for the third modification, which introduces the concept of variable dimensions of the subspaces corresponding to different parts of ECG beats.
The experiments performed show that introduction of the respective modifications significantly improves the classical approach to ECG processing by application of principal component analysis.

Abstract : The Extensible Modeling and Simulation Framework (XMSF) is defined as a composable set of standards, profiles and recommended practices for web-based modeling and simulation.
Markup languages based on XML, Internet technologies, and Web services are combining to enable a new generation of distributed M&S applications to emerge, develop and interoperate.
The purpose of this task is to develop the technical framework, coordinate with existing public standardization efforts, and demonstrate distributed exemplars of the developed framework.
This report describes work accomplished in 2004 by the XMSF project team consisting of Naval Postgraduate School Modeling, Virtual Environments, and Simulation (MOVES) Institute, George Mason University (GMU), Science Applications International Corporation (SAIC), and Old Dominion University/Virginia Modeling, Analysis, and Simulation Center (ODU/VMASC).
Principal areas of work included ongoing community education and development activities through conference and workshop participation, liaison with relevant standards organizations, and development of exemplar applications demonstrating the technical approaches and applicable standards relevant to XMSF precepts.
The report proposes follow-on efforts for 2005, including continuation of current efforts and further standardization work with multicasting, web-enabled IEEE 1516 Run-Time Infrastructure, data modeling in C2 and M&S systems, and executable architectures.

Abstract Imbalance classification is one of the most challenging research problems in machine learning.
Techniques for two-class imbalance classification are relatively mature nowadays, yet multi-class imbalance learning is still an open problem.
Moreover, the community lacks a suitable software tool that can integrate the major works in the field.
In this paper, we present Multi-Imbalance, an open source software package for multi-class imbalanced data classification.
It provides users with seven different categories of multi-class imbalance learning algorithms, including the latest advances in the field.
The source codes and documentations for Multi-Imbalance are publicly available at https://github.com/chongshengzhang/Multi_Imbalance .

A large body of work has been devoted to identifying community structure in networks.
A community is often though of as a set of nodes that has more connections between its members than to the remainder of the network.
In this paper, we characterize as a function of size the statistical and structural properties of such sets of nodes.
We define the network community profile plot, which characterizes the "best" possible community - according to the conductance measure - over a wide range of size scales, and we study over 70 large sparse real-world networks taken from a wide range of application domains.
Our results suggest a significantly more refined picture of community structure in large real-world networks than has been appreciated previously.
Our most striking finding is that in nearly every network dataset we examined, we observe tight but almost trivial communities at very small scales, and at larger size scales, the best possible communities gradually "blend in" with the rest of the network and thus become less "community-like."
This behavior is not explained, even at a qualitative level, by any of the commonly-used network generation models.
Moreover, this behavior is exactly the opposite of what one would expect based on experience with and intuition from expander graphs, from graphs that are well-embeddable in a low-dimensional structure, and from small social networks that have served as testbeds of community detection algorithms.
We have found, however, that a generative model, in which new edges are added via an iterative "forest fire" burning process, is able to produce graphs exhibiting a network community structure similar to our observations.

For classifying multispectral satellite images, a multilayer perceptron (MLP) is trained using either (i) ground truth data or (ii) the output of a K-means clustering program or (iii) both, as applied to certain representative parts of the given data set.
In the second case, different sets of clustered image outputs, which have been checked against actual ground truth data wherever available, are used for testing the MLP.
The cover classes are, typically, different types of (a) vegetation (including forests and agriculture); (b) soil (including mountains, highways and rocky terrain); and (c) water bodies (including lakes).
Since the extent of ground truth may not be sufficient for training neural networks, the proposed procedure (of using clustered output images) is believed to be novel and advantageous.
Moreover, it is found that the MLP offers an accuracy of more than 99% when applied to the multispectral satellite images in our library.
As importantly, comparison with some recent results shows that the proposed application of the MLP leads to a more accurate and faster classification of multispectral image data.

Purpose – It has been argued that behavioural models of technology acceptance do not serve equally across cultures.
This study aims to extend technology acceptance model (TAM) to suit in a developing country context.
The model attempts to identify the relationship between social norms, management support and moderating factors voluntariness and experience.Design/methodology/approach – The study used a quantitative methodology to investigate the correlational paths.
A cross‐sectional survey was completed by 504 academics working in higher educational institutes of Pakistan.
The data were analysed using structural equation modelling (SEM) based on partial least squares (PLS) methods.Findings – The extended model achieved acceptable fit and most of the hypothesised paths were significant.
Determinant perceived usefulness was an important construct of the internet acceptance, i.e.
R2=34 per cent more than behavioural intention.
Moderator experience diminished the impact of usefulness on behaviour usage.Practi...

We study problems in which each of finitely many agents must be allocated a single object, based on the agents' rankings of pure outcomes.
A random allocation is ordinally efficient if it is not ordinally dominated in the sense of there being another random assignment that gives each agent a first order stochastically dominant distribution of objects.
We show that any ordinally efficient random assignment maximizes the sum of expected utilities for some vector of vNM utility functions that are consistent with the given ordinal preferences.
One method of proof uses a new version of the separating hyperplane theorem for polyhedra.
(C) 2002 Elsevier Science (USA).

Abstract Program partitioning and scheduling are essential steps in programming non-shared-memory computer systems.
Partitioning is the separation of program operations into sequential tasks, and scheduling is the assignment of tasks to processors.
To be effective, automatic methods require an accurate representation of the model of computation and the target architecture.
Current partitioning methods assume today′s most prevalent models - macro dataflow and a homogeneous/two-level multicomputer system.
Based on communication channels, neither model represents well the emerging class of NUMA multiprocessor computer systems consisting of hierarchical read/write memories.
Consequently, the partitions generated by extant methods do not execute well on these systems.
In this paper, we extend the conventional graph representation of the macro-dataflow model to enable mapping heuristics to consider the complex conununication options supported by NUMA architectures.
We describe two such heuristics.
Simulated execution times of program graphs show that our model and heuristics generate higher quality program mappings than current methods for NUMA architectures.

A synthetic review is given of the research developed in the past 10 years in the field of short-range communications for intelligent transportation systems.
Moreover, the role of new packet mobile radio (such as the General Packet Radio Service) and third-generation systems in the possible provision of services for the field of ITS is discussed; more precisely, the services for driving safety are addressed.
The research programs under development in Italy and Japan which can have an impact on the selection of suitable techniques for ITS services are presented.

Abstract In this paper we show that it can be beneficial to have a high-level vision component that guides the reasoning of the whole vision system when interpreting a dynamic and uncertain world.
This guidance is provided by an attentional mechanism that exploits knowledge of the specific problem being solved.
Here we develop a general framework for such an attentional mechanism and its application to understanding dynamic scenes.
This attentional mechanism can enable a vision system to perform a given domain task while expending minimal resources.
We have developed a component that uses Bayesian networks combined with a deictic representation to select what, when and how to use processed data from a fixed camera.
We apply two forms of Bayesian network, which (1) create a dynamic structure to reflect the spatial organisation of the data and (2) measure task relatedness.
Together these give attentional focus making the reasoning performed relevant to the task.

This paper proposes a general approach to subdivision algorithms used in interactive computer aided design for splines which are linear combinations of translates of any box splines.
We show how these algorithms can be used for efficient generation of the corresponding spline surfaces.
Our results extend several known special cases.

Abstract The Stochastically Excited Linear Prediction (SELP) algorithm for speech coding offers good performance at bit rates as low as 4.8 kbit/s.
Linear Predictive Coding (LPC) techniques remove the short-term correlation from the speech.
A pitch loop removes long-term correlation, producing a noise-like residual, which is vector quantized.
Information describing the LPC filter coefficients, the long-term predictor, and the vector quantization is transmitted.
In this paper, we describe improvements to the SELP algorithm which result in better speech quality and higher computational efficiency.
In its closed-loop form, the pitch loop can be interpreted as a vector quantization of the desired excitation signal with an adaptive codebook populated by previous excitation sequences.
To better model the non-stationarity of speech we extend this adaptive codebook with a special set of candidate vectors which are transform of other codebook entries.
The second stage vector quantization is performed using a fixed stochastic codebook.
In its original form, the SELP algorithm requires excessive computational effort.
We employ a new recursive algorithm which performs a very fast search through the adaptive codebook.
In this method, we modify the error criterion, and exploit the resulting symmetries.
The same fast vector quantization procedure is applied to the stochastic codebook.

Every day Italian TV viewers are exposed to a vast amount of heavily mediated audiovisual texts translated into their native language which contain a wide array of references to all the specific aspects and features of the source languages and cultures.
Dubbese is the hybrid language used by the Italian dubbing industry to transpose both fictional and non-fictional foreign TV and cinema productions.
This paper will illustrate the results of a large scale research project based upon a corpus of over 300 hours of dubbed TV programmes which set out to assess what Italian TV viewers perceive and understand of the culture-specific references contained in dubbed filmic products .

Abstract The brain's serotonergic (5-HT) system has been implicated in controlling impulsive behavior and attentional orienting and linked to impulse control and anxiety related disorders.
However, interactions between genotypical variation and responses to serotonergic drugs impede both treatment efficacy and neuroscientific research.
We examine behavioral and electrophysiological responses to acute intravenous administration of a selective serotonin reuptake inhibitor (SSRI) while controlling for major genetic differences regarding 5-HT transporter (5-HTT) genotypes.
Out of a genotyped sample of healthy Caucasian subjects (n = 878) two extreme-groups regarding 5-HTT genotypes were selected (n = 32).
A homozygous high-expressing group based on tri-allelic 5-HTTLPR and rs25532 (L AC /L AC = LL) was compared to homozygous S allele carriers (SS).
Both groups were administered a low dose of citalopram (10 mg) intravenously in a double blind crossover fashion and performed a novelty NoGo paradigm while high density EEG was recorded.
Interactions between drug and genotype were seen on both behavioral and neurophysiological levels.
Reaction slowing following inhibitory events was decreased by the administration of citalopram in the LL but not SS group.
This was accompanied by decreases in the amplitude of the inhibitory N2 EEG component and the P3b in the LL group, which was not seen in the SS group.
SS subjects showed an increase in P3a amplitudes following SSRI administration to any type of deviant stimulus possibly reflecting increased attentional capture.
The acute SSRI response on inhibitory processes and attentional orienting interacts with genotypes regulating 5-HTT gene expression.
SS subjects may show increased attentional side effects reflected in increases in P3a amplitudes which could contribute to treatment discontinuation.
Inhibitory processes and their neural correlates are affected only in LL subjects.
These findings may indicate an underlying mechanism that could relate genotypical differences to altered side effect profiles and drug responses and are compatible with a non-monotonic relationship between 5-HT levels and optimal functioning.

Developing and maintaining a digital library requires substantial investments that are not simply a matter of technological decisions, but include also organizational issues (user roles, workflows, types of contents, etc.).
These issues are often handled by approaches based on a physical perspective that treats the stored information either in terms of data formats or physical space needed to archive them.
All these perspectives completely ignore the semantic aspects of the digital contents.
In this paper, we address such a semantic perspective.
More specifically, we propose a service-oriented architecture that explicitly includes a semantic layer which provides primitive services to the applications built on top of the digital library.
As part of this layer, a specific component is described: the PIRATES framework.
This module assists end users to complete several tasks concerning the retrieval of the most relevant content with respect to a description of their information needs (a search query, a user profile, etc.).
Techniques of user modeling, adaptive personalization, and knowledge representation are exploited to build the PIRATES services in order to fill the gap existing between traditional and semantic digital libraries.

This paper presents parallel algorithms for priority queue operations on a p-processor EREW-PRAM.
The algorithms are based on a new data structure, the Min-path Heap (MH), which is obtained as an extension of the traditional binary-heap organization.
Using an MH, it is shown that insertion of a new item or deletion of the smallest item from a priority queue of n elements can be performed in O log n/p + log log n) parallel time, while construction of an MH from a set of n items takes O(n/p+log n) time.
The given algorithms for insertion and deletion achieve the best possible running time for any number of processors p, with p ∈ O(log n/log log n), while the MH construction algorithm employs up to Θ(n/log n) processors optimally.

Two more or less standard methods exist for the systematic, logical construction of classical mathematics, the so-called theory of types, due in the main to Russell, and the Zermelo axiomatic set theory.
In systems based upon either of these, the connective of membership, “e”, plays a fundamental role.
Usually although not always it figures as a primitive or undefined symbol.
Following the familiar simplification of Russell's theory, let us mean by a logical type in the strict sense any one of the following: (i) the totality consisting exclusively of individuals, (ii) the totality consisting exclusively of classes whose members are exclusively individuals, (iii) the totality consisting exclusively of classes whose members are exclusively classes whose members in turn are exclusively individuals, and so on.
Any entity from (ii) is said to be one type higher than any entity from (i), any entity from (iii), one type higher than any entity from (ii), and so on.
In systems based upon this simplified theory of types, the only significant atomic formulae involving “e” are those asserting the membership of an entity in an entity one type higher.
Thus any expression of the form “( x∈y )” is meaningless except where “ y ” denotes an entity of just one type higher than the type of the entity denoted by “ x ” It is by means of general type restrictions of this kind that the Russell and other paradoxes are avoided.

Abstract We are all creatures of habit; the way we think and the views we take are conditioned by our education, society as a whole, and, at a much deeper level, our cultural memories or instinct.

The successful deployment of wireless local area networks (WLAN) for high speed data transmission and cellular systems for wide coverage and global roaming has emerged to be a complementary platform for wireless data communications.
In order to fully exploit potentials in 3G/WLAN integration, authentication of roaming users crossing different networks, must be coupled with mobility management, which is a challenging, yet not resolved issue.
The focus of this paper is on state-of-art solutions to Wi-Fi and cellular networks based on IP infrastructure.
Moreover, we introduce a new authentication architecture for fast authentication during inter-networking handoff and large-scale heterogeneous networks.
We show that the new architecture can reduce authentication latency significantly and be adaptive to user mobility and traffic.
Copyright © 2005 John Wiley & Sons, Ltd.

Under ideal conditions, complementary code pairs produce no sidelobes.
In practice, sidelobes are produced, among other things, when the period of the Doppler frequency shift as well as the time of coherence of clear air turbulence are comparable (or smaller) than the interpulse period.
The intensity of sidelobes increases with the radar operating frequency and becomes a real problem in the upper VHF and UHF bands.
In this paper, a new technique for reducing sidelobes originating from atmospheric characteristics for clear air radar systems using pulse coding is presented.
For this, a generalized analytic expression for a sidelobe suppression factor applicable to any number of binary code sequences is first derived.
This is then used to develop the technique, which in the case of complementary codes consists of manipulating the order of transmission of the code sequences.
For such codes, improvements in sidelobe suppression of the order of 80 dB on the VHF frequency band are obtained.
The modifications required to implement the technique into existing systems are very few and simple.

Cars, aircraft, mobile cell phones, ships, tanks, and mobile robots all have the common property that they are moving objects.
A kinematic representation can be used to describe the location of these objects as a function of time.
For example, a moving point can be represented by the function p(t) e O 0 p (t − t 0 )O, where O 0 is the start location, t 0 is the start time, and O is its velocity vector.
Instead of storing the location of the object at a given time in a database, the coefficients of the function are stored.
When an object's behavior changes enough so that the function describing its location is no longer accurate, the function coefficients for the object are updated.
Because the location of each object is represented as a function of time, spatial query results can change even when no transactions update the database.
We present efficient algorithms to maintain k-nearest neighbor, and spatial join queries in this domain as time advances and updates occur.
We assume no previous knowledge of what the updates will be before they occur.
We experimentally compare these new algorithms with more straight forward adaptations of previous work to support updates.
Experiments are conducted using synthetic uniformly distributed data, and real aircraft flight data.
The primary metric of comparison is the number of I/O disk accesses needed to maintain the query results and the supporting data structures.

Abstract This paper studies the distributed consensus tracking problem of linear higher-order multi-agent systems with switching directed topologies and occasionally missing control inputs.
In this framework, the underlying topology of dynamic agents may switch among several directed graphs, each having a directed spanning tree rooted at the leader.
Furthermore, the control inputs to the followers may be temporally missed due to actuator failures and network-induced packet loss.
To guarantee asymptotic consensus tracking in such a multi-agent system, several distributed controllers are constructed based only on the relative state information of neighboring agents.
By appropriately constructing a switching Lyapunov function and using tools from the M -matrix theory, some sufficient conditions for achieving distributed consensus tracking are provided.
Finally, some numerical simulations are given to illustrate the theoretical analysis.

1 Machine Intelligence Unit and Center for Soft Computing Research, Indian Statistical Institute, 203 B.T.
Road, Kolkata 700108, INDIA.
ash@isical.ac.in 2 Department of Information and Communication Technology, Fakir Mohan University, Vyasa Vihar, Balasore 756019, INDIA.
satchi.lapa@gmail.com 3 Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, INDIA.
susmitaghoshju@gmail.com

Cronbach’s α is widely used in social science research to estimate the internal consistency of reliability of a measurement scale.
However, when items are not strictly parallel, the Cronbach’s α coefficient provides a lower-bound estimate of true reliability, and this estimate may be further biased downward when items are dichotomous.
The estimation of standardized Cronbach’s α for a scale with dichotomous items can be improved by using the upper bound of coefficient ϕ. SAS and SPSS macros have been developed in this article to obtain standardized Cronbach’s α via this method.
The simulation analysis showed that Cronbach’s α from upper-bound ϕ might be appropriate for estimating the real reliability when standardized Cronbach’s α is problematic.

Checkpointing and rollback recovery is a very effective technique to tolerate the occurrence of failures.
Usually, checkpoint data is saved on disk, however, in some situations the time to write the data to disk can represent a considerable performance overhead.
Alternative solutions would make use of main memory to maintain the checkpoint data.
The paper starts by presenting two main memory checkpointing schemes: neighbour based and parity checkpointing.
Both schemes have been implemented and evaluated in a commercial parallel machine.
The results show that neighbour based checkpointing presents a very low performance overhead and assures a fast recovery for partial failures.
However, it is not able to tolerate multiple and total failures of the system.
To solve this shortcoming the authors propose a two-level stable storage integrating the use of neighbour based with disk based checkpointing.
This approach combines the advantages of the two schemes: the efficiency of diskless checkpointing with the high reliability of disk based checkpointing.

In this article, the authors examine how American youths' contributions to three online worlds - participatory culture, political consumerism, and civic engagement - function as possible gateways to their increased political participation.
Youth involvement in these three online worlds suggests that the Internet creates opportunities for youth involvement in politics and provides a measure of motivation, facilitation, and invitation for that involvement.
Nonetheless, for reasons discussed, it remains an open question whether youths will take full advantage of these opportunities.

The current work sets out to enhance our knowledge of changes or lack of changes in the speech signal when people are being deceptive.
In particular, the study attempted to investigate the appropriateness of using speech cues in detecting deception.
Truthful, deceptive and control speech was elicited from five speakers during an interview setting.
The data was subjected to acoustic analysis and results are presented on a range of speech parameters including fundamental frequency (f0), overall intensity and mean vowel formants F1, F2 and F3.
A significant correlation could not be established for any of the acoustic features examined.
Directions for future work are highlighted.

Scientific performance is often evaluated by bibliometric indicators such as publication counts or citations.
But this may neglect other relevant outputs from research units.
An optimal evaluation would measure each dimension separately, but this would be costly.
Luckily, cluster analyses show that units which specialise in other types of research activities (such as knowledge transfer or education of doctoral students), do not completely ignore publication-related activities.
Publication-related outputs can also be disaggregated into quality (measured by citations) and quantity (measured by counts) dimensions.
Thus, the performance of research groups can be screened using the Integral Citation (a new bibliometric indicator) which combines the quality and quantity dimensions.
Units at the extremes need to be studied in more detail, to avoid measurement biases.
Copyright , Beech Tree Publishing.

Seasonal coastal upwelling in the U.S. Mid-Atlantic coastal ocean normally occurs during the summer months because of generally alongshore southerly wind episodes.
Southerly winds force an offshore surface Ekman flow over the inner continental shelf.
Colder and nutrient-rich waters from below upwell toward the surface replacing offshore-flowing surface waters.
Synthetic Aperture Radar (SAR) observations from the European Remote Sensing (ERS) satellite ERS-2 before and after upwelling-favorable wind episodes in early summer 1996 along the New Jersey coast are presented.
Lower backscatter conditions appearing in the SAR imagery after the onset of upwelling demonstrate the influence of the upwelling regime on the sea surface roughness.
Satellite sea surface temperature (SST) observations and in-situ sea temperature vertical profiles confirm upwelling conditions.
Three key mechanisms are suggested to explain the lower radar returns observed under upwelling conditions, an increase in the atmospheric marine boundary layer stability, an increase in the viscosity of surface waters, and the presence of biogenic surfactants in the upwelling region.

To release the connection problem between different kinds of wireless communication technologies in conventional smart home system, a new direct-connected intelligent home gateway between two different wireless communication technologies was designed.
This gateway was based on ARM920T embedded processor S3C2440A, ARM Linux operation system and new communication technology combining Zigbee and Wireless-Fidelity ( Wi-Fi) , was considered to achieve the personalized objective of remote monitoring, home security, home appliances control and so on.
An embedded Web server was built in ARM9 processor, the Wi-Fi module and Zigbee module were expanded to achieve the construction of wireless network and the connection to Internet, and the hardware structure and software process were shown too.
The experimental results show that, the packet loss rate and response time of this new gateway are less than the other gateways like wire line network and Wi-Fi bluetooth wireless network, and it can be applied to the smart home system.

In Optimality Theory, constraints come in two types, which are distinguished by their mode of evaluation.
Categorical constraints are either satisfied or not ; a categorical constraint assigns no more than one violation-mark, unless there are several violating structures in the form under evaluation.
Gradient constraints evaluate extent of deviation ; they can assign multiple marks even when there is just a single instance of the non-conforming structure.
This article proposes a restrictive definition of what an OT constraint is, from which it follows that all constraints must be categorical.
The various gradient constraints that have been proposed are examined, and it is argued that none is necessary and many have undesirable consequences.

We describe new, efficient algorithms for layout modification and phase assignment for dark field alternating-type phase shifting masks in the single exposure regime.
We make the following contributions.
First, we suggest new two-coloring and compaction approach that simultaneously optimizes layout and phase assignment which is based on planar embedding of an associated conflict graph.
We also describe additional approaches to cooptimization of layout and phase assignment for alternating PSM.
Second, we give optimal and fast algorithms to minimize the number of phase conflicts that must be removed to ensure two colorability of the conflict graph.
We reduce this problem to the T-join problem which asks for a minimum weight edge set A such that a node u is incident to an odd number of edges of A if u belongs to a given node subset T of a weighted graph.
Third, we suggest several practical algorithms for the T-join problem.
In sparse graphs, our algorithms are faster than previously known methods.
Computational experience with industrial VLSI layout benchmarks shows the advantages of the new algorithms.

The DRAM-Based Reconfigurable Acceleration Fabric (DRAF) uses commodity DRAM technology to implement a bit-level, reconfigurable fabric that improves area density by 10 times and power consumption by more than 3 times over conventional field-programmable gate arrays.
Latency overlapping and multicontext support allow DRAF to meet the performance and density requirements of demanding applications in datacenter and mobile environments.

In this paper we study ruled Weingarten surfaces in the Galilean space.
Weingarten surfaces are surfaces having a nontrivial funcional relation between their Gaussian and mean curvature.
We describe some further examples of Weingarten surfaces.

A Turing machine multiplies binary integers on-line if it receives its inputs, low-order digit first, and produces the jth digit of the product before reading in the (j+1)st digits of the two inputs.
We present a general method for converting any off-line multiplication algorithm which forms the product of two n-digit binary numbers in time F(n) into an on-line method which uses time only O(F(n) log n), assuming that F is monotone and satisfies n@?F(n)@?F(2n)/2@?kF(n) for some constant k. Applying this technique to the fast multiplication algorithm of Schonhage and Strassen gives an upper bound of O(n (log n)^2 loglog n) for on-line multiplication of integers.
A refinement of the technique yields an optimal method for on-line multiplication by certain sparse integers.
Other applications are to the on-line computation of products of polynomials, recognition of palindromes, and multiplication by a constant.

A class of algorithms called stochastic fairness queuing is presented.
The algorithms are probabilistic variants of fairness queuing.
They do not require an exact mapping and thus are suitable for high-speed software or firmware implementation.
The algorithms span a broad range of CPU, memory, and fairness tradeoffs.
It is shown that the worst-case execution-speed stochastic fairness queuing is faster than the best-case execution speed of all of the implementations of fair queuing presented.
This advantage is larger for protocols with longer addresses, e.g.
the ISO protocol suite.
>

Abstract The special issue on Modeling and Performance Analysis of Networking and Collaborative Systems by leveraging current networks and collaborative applications.
In addition, the incorporation of the latest and powerful technologies, based on distributed infrastructure, is also explored for the enhancement of these applications, resulting in complex and entangled systems that pose new issues and challenges, in terms of efficiency, security, mobility, and so on.
The goal is to respond to the need for methods and tools for performance analysis and evaluation of current complex collaborative and networking systems and applications.
To this direction, this special issue provides latest research on modeling and performance analysis of networking and collaborative systems from different perspectives.

A construction is described which takes an arbitrary set of machines, an arbitray set of tests, and an arbitrary relation on machines and tests defining which machines pass which tests.
It produces a domain of specifications, which is a retract of the lattice of sets of tests (with the subset ordering), and a domain of nondeterministic machines (ndms), which is a retract of the lattice of sets of machines (with the superset ordering).
These two domains are isomorphic.
Simple conditions ensure that they are ω-algebraic.

Abstract Behavioral rewriting differs from standard rewriting in taking account of the weaker inference rules of behavioral logic, but it shares much with standard rewriting, including notions like termination and confluence.
We describe an efficient implementation of behavioral rewriting that uses standard rewriting.
Circular coinductive rewriting combines behavioral rewriting with circular coinduction, giving a surprisingly powerful proof method for behavioral properties; it is implemented in the BOBJ system, which is used in our examples.
These include several lazy functional stream program equivalences and a behavioral refinement.

Abstract Purpose This paper presents a three-year teledermatology evaluation experience.
The aim is to explain the methodology followed, present the evaluation results, discuss critically the issues that emerged during the experience and report the main lessons learned.
Methods A complete design and evaluation methodology was conducted to fully address significant issues arising from other previous teledermatology experiences.
First, system-design requirements and image quality issues were studied.
Then, a detailed clinical concordance study was undertaken to determine the accuracy of diagnoses made using teledermatology in order to assess different dermatological clinics.
Finally, an impact study on the health system was performed.
Furthermore, clinical, technical, social and alignment outcomes were analyzed during the study and at the end of it, in order to understand how emerging factors affected the final setup of the teledermatology system.
Results The most important results reported in this study can be summarized as follows.
(1) A complete web-based environment for teledermatology support was developed as a result of a dynamic evaluation process with clinical personnel.
(2) A total of 120 teleconsultations (82 pediatric and 28 adult) were made during the clinical concordance study.
Concordance analysis was carried out for each dermatological disease group.
High concordance rates were found in pediatrics for inflammatory dermatoses (76%) and also for adults (75%) with infections and infestations.
(3) Physicians were satisfied with the teledermatology system but the time dedicated to consultation in primary care was a limiting factor (19min for each teleconsultation).
(4) An extensive discussion about the successful and the limiting aspects of the teledermatology experience revealed the reasons behind the final decision not to proceed with its implementation.
It was considered not to be aligned with Health Care Organization (HCO) strategy and consequently did not achieve high-level support for its long-term implementation.
Conclusions A high degree of diagnostic accuracy both for pediatric and adult consultations was achieved using the teledermatology system with affordable technical requirements.
Its usefulness for filtering dermatological referrals was also demonstrated in the study.
Nevertheless, other factors such as the reorganization required for the physicians' time schedule, remuneration issues, absence of EHR (electronic health record) integration and lack of interaction with the HCO were important limiting factors.
This led to the conclusion that under the evaluation conditions long-term set-up was not possible.
It was also concluded that HCO participation would have been essential for both the evaluation study and the long-term set-up of the system.

An implementation of the primal-dual predictor-corrector interior point method is specialized to solve block-structured linear programs with side constraints.
The block structure of the constraint matrix is exploited via parallel computation.
The side constraints require the Cholesky factorization of a dense matrix, where a method that exploits parallelism for the dense Cholesky factorization is used.
For testing, multicommodity flow problems were used.
The resulting implementation is 65%–90% efficient, depending on the problem instance.
For a problem with K commodities, an approximate speedup for the interior point method of 0.8K is realized.

Classification algorithms with unbalanced datasets tend to produce high predictive accuracy over the majority class, but poor predictive accuracy over the minority class.
This problem is very common in biomedical data mining.
This paper introduces a Support Vector Machine (SVM)-based optimized feature selection method, to select the most relevant features and maintain an accurate and well-balanced sensitivity-specificity result between unbalanced groups.
A new metric called the balance index (B) is defined to implement this optimization.
The balance index measures the difference between the misclassified data within each class.
The proposed optimized feature selection is applied to the classification of patients' weaning trials from mechanical ventilation: patients with successful trials who were able to maintain spontaneous breathing after 48h and patients who failed to maintain spontaneous breathing and were reconnected to mechanical ventilation after 30min.
Patients are characterized through cardiac and respiratory signals, applying joint symbolic dynamic (JSD) analysis to cardiac interbeat and breath durations.
First, the most suitable parameters (C"+,C"-,@s) are selected to define the appropriate SVM.
Then, the feature selection process is carried out with this SVM, to maintain B lower than 40%.
The best result is obtained using 6 features with an accuracy of 80%, a B of 18.64%, a sensitivity of 74.36% and a specificity of 82.42%.

The execution of robotic tasks in dynamic, unstructured environments requires the generation of motion plans that respect global constraints imposed by the task while avoiding collisions with stationary, moving, and unforeseen obstacles.
This paper presents the elastic strip framework, which addresses this problem by integrating global motion planning methods with a reactive motion execution approach.
To maintain a collision-free trajectory, a given motion plan is incrementally modified to reflect changes in the environment.
This modification can be performed without suspending task behavior.
The elastic strip framework is computationally efficient and can be applied to robots with many degrees of freedom.
The paper also presents experimental results obtained by the implementation of this framework on the the Stanford Mobile Manipulator.

Finger-worn interfaces are a vastly unexplored space for interaction design.
It opens a world of possibilities for solving day-to-day problems, for visually impaired people and sighted people.
In this work we present EyeRing, a novel design and concept of a finger-worn device.
We show how the proposed system may serve for numerous applications for visually impaired people such as recognizing currency notes and navigating, as well as helping sighted people to tour an unknown city or intuitively translate signage.
The ring apparatus is autonomous, however it is counter parted by a mobile phone or computation device to which it connects wirelessly, and an earpiece for information retrieval.
Finally, we will discuss how finger worn sensors may be extended and applied to other domains.

A novel CMIA configuration based on CCII+ is presented.
Some of the advantages of this configuration over the traditional Voltage Mode Instrumentation Amplifier (VMIA) are: a wideband differential voltage gain independent of gain magnitude and a high common mode rejection ratio (CMRR) without requiring well-matched resistors.
Compared to other previously presented current mode instrumentation amplifiers (CMIAs), each of which showed a limited number of special advantages, this topology benefits from most of those individual advantages, those being: small size, circuit simplicity very inexpensive price, complete circuit symmetry, high voltage gain, and CMRR.
In addition, due to the application of the current feedback technique, the topology has acquired further superior qualities such as weaker voltage and current error signals, making it ideal for working with general purpose ICs, higher voltage gains, and much higher CMRR than other topologies.
A suggestion is also given to widen the CMRR bandwidth.
To prove these advantages and superiorities, a detailed theoretical analysis is given as well as some experiments being performed with AD844AN (the general purpose type) inside a Faraday's cage, with an attenuation better than 60 dB.
Both theoretical and experimental investigations are repeated for Wilson and Kaulberg topologies which are claimed to be the best configurations.
Results are compared to prove the advantages of the new topology.

A given Boolean function has its input distributed among many parties.
The aim is to determine which parties to talk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication.
It is shown that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
>

Text Mining has become an important research area, which refers to the application of machine learning (or data mining) techniques in the study of Information Retrieval and Natural Language Processing.
In sense, it is defined as the way of discovering knowledge from ubiquitous text data which are easily accessible over the Internet or the Intranet.
The survey of Text Mining techniques, Text Mining Applications, literature survey of various applications and tools has been presented.
Text Mining techniques like Document clustering and Document Classification have been presented.
Text mining based framework for applications like Summarization, Topic Discovery, Information Extraction, Information Retrieval terms and techniques in each method has been discussed.
Various text mining and data visualization tools for application to patent information like their working mode, capabilities, data sources and result output have been presented.
In depth analysis of algorithm related to classification techniques its advantages and disadvantages and the working mode has been presented.

Cache partitioning and power-gating schemes are major research topics to achieve a high-performance and low-power shared cache for next generation chip multiprocessors(CMPs).
We propose a power-aware cache partitioning mechanism, which is a scheme to realize both low power and high performance using power-gating and cache partitioning at the same time.
The proposed cache mechanism is composed of a way-allocation function and power control function; each function works based on the cache locality assessment.
The performance evaluation results show that the proposed cache mechanism with a performance-oriented parameter setting can reduce energy consumption by 20% while keeping the performance, and the mechanism with an energy-oriented parameter setting can reduce 54% energy consumption with a performance degradation of 13%.
The hardware implementation results indicate that the delay and area overheads to control the proposed mechanism are negligible, and therefore hardly affect both the entire chip design and performance.

We use the Drozd-Warfield structure theorem for finitely presented modules over a serial ring to investigate the model theory of modules over a serial ring, in particular, to give a simple description of pp-formulas and to classify the pure-injective indecomposable modules.
We also study the question of whether every pure-injective indecomposable module over a valuation ring is the hull of a uniserial module.

Surgical resection is the cornerstone of curative therapy for primary and metastatic liver tumors.
For best results, the surgeon must know the location of all hepatic tumor nodules relative to the major vessels that define the liver's surgical anatomy.
Computed tomography is very sensitive for detecting liver tumors, but its planar slices do not fully address the three-dimensional nature of this surgical problem.
We have developed a technique using volume rendering of computed tomography data that provides a preoperative 3D map of the liver showing tumor location relative to key blood vessels.
This technique also has important implications for emerging, minimally invasive therapies.

The chaotic system is widely used in chaotic cryptosystem and chaotic secure communication.
In this paper, a universal method for designing the discrete chaotic system with any desired number of po...

In this paper, we mainly investigate anonymous user authentication scheme using smart card.
We first demonstrate security weaknesses still exist in two such schemes recently propose by Wang et al.
and Tsai et al., respectively according to Wang et al.
's criteria.
Thereafter, we propose an enhanced smart-card-based authentication scheme with user anonymity for providing all the admired requirements at the same time.
Compared with the previous schemes, our scheme is yet efficient both in computation and communication cost.
Moreover, we can prove the security of the proposed scheme in the random oracle model.
Copyright © 2011 John Wiley & Sons, Ltd. ::: ::: (We demonstrate security weaknesses still exist in two such schemes recently propose by Wang et al.
and Tsai et al.
respectively according to Wang et al.
's criteria.
And we propose an enhanced smart-card-based authentication scheme with user anonymity for providing all the admired requirements at the same time.)

We consider a supply chain structure with shipments from an external warehouse directly to retailers and compare two enhancement options: costly transshipment among retailers after demand has been realized vs. cost-free allocation to the retailers from the development of a centralized depot.
Stochastic programming models are developed for both the transshipment and allocation structures.
We study the impact of cost parameters and demand coefficient of variation on both system structures.
Our results show an increasing convex relationship between average costs and demand coefficient of variation, and furthermore that this increase is more pronounced for the allocation structure.
We employ simulation and nonlinear search techniques to computationally compare the cost performance of allocation and transshipment structures under a wide range of system parameters such as demand uncertainty and correlation; lead times from the external warehouse to retailers, from warehouse to central depot, and from depot to retailers; and transshipment, holding, and penalty costs.
The transshipment approach is found to outperform allocation for a broad range of parameter inputs including many situations for which transshipment is not an economically sound decision for a single period.
The insights provided enable the manager to choose whether to invest in reducing lead times or demand uncertainty and assist in the selection of investments across identical and nonidentical retailers.

We resolve an open problem due to Tetsuo Asano, showing how to compute the shortest path in a polygon, given in a read only memory, using sublinear space and subquadratic time.
Specifically, given a simple polygon $P$ with $n$ vertices in a read only memory, and additional working memory of size $m$, the new algorithm computes the shortest path (in $P$) in $O( n^2 / m )$ expected time, assuming $m = O(n/\log^2 n)$.
This requires several new tools, which we believe to be of independent interest.
Specifically, we show that violator space problems, an abstraction of low dimensional linear-programming (and LP-type problems), can be solved using constant space and expected linear time, by modifying Seidel's linear programming algorithm and using pseudo-random sequences.

Abstract In this paper the axioms, of axiomatic design, are extended to the non-probabilistic and repetitive events.
The idea of information, in the classic theories of Fisher and Wiener–Shannon, is a measure only of probabilistic and repetitive events.
The idea of information is broader than the probability.
The Wiener–Shannon's axioms are extended to the non-probabilistic and repetitiveness events.
It is possible the introduction of a theory of information for events not connected to the probability therefore for non-repetitive events.
On the basis of the so-called Laplace's principle of insufficient knowledge, the MaxInf Principle is defined for choose solutions in absence of knowledge.
In this paper the value of information, as a measure of equality of data among a set of values, is applied in axiomatic framework for data analysis in such cases in which the number of functional requirements (FRs) is greater than the design parameter's (DPs) one.
As example is studied an application in which the number of DPs is lower then the number of FRs, and the coupled design cannot be satisfied.
A typical example in which that happens is in the evaluation of the potential failure mechanisms, failure stresses, failure sites, and failure modes, given a product architecture, the comprising products and materials, and the manufacturing processes.
In design analysis it is possible to hypothesise several causes that can affect the normal functionalities of some products/processes’ parts and to individuate several possible effect that those causes can cause.
In ideal analysis, each functional requirement (effect) must be linked to one design parameter (cause), and vice versa each design parameter can satisfy one (or more) functional parameter.
From the system of equations it turns out that with the number of {FR} • In absence of solution is not possible compare anything: is needed at least a solution.
• Using mathematical transformations it is possible to obtain a marginal solution.
Using the idea of information in metric space, in according with Maximum Entropy Principle of Jaynes it is possible to select as solution the distribution that maximise the Shannon entropy measure and simultaneously is consistent with the values of constraints.
So this method allows to solve the Axiomatic framework and to reason for obtain the best design solution.

The exceptional properties of Austempered Ductile Iron (ADI) are widely known and can be ascribed to its austenitic-ferritic microstructure.
Strain hardening of this material is exceptional and an advantage for many applications where high wear resistance is required, as well as the extraordinary combination of ductility and tensile strength.
One reason which restricts the introduction of this material in practical applications is its poor machinability.
This paper describes the material-sided influences on machinability, especially on the acting wear mechanism.
The heat treatment factor austempering time, ADI grade and the alloying elements nickel and molybdenum are varied and investigated in external longitudinal turning operations.

The star-star concentrator location problem (SSCLP), which is a network layout problem, is considered.
SSCLP is formulated as an integer linear programming problem.
The Lagrangian relaxation (LR) method is used to obtain suboptimal solutions (upper bounds) and lower bounds.
Three different LRs are used for SSCLP.
The resulting Lagrangian dual problems are shown to be equivalent to some linear programming problems.
An approximation algorithm is suggested for SSCLP that produces both a feasible solution (upper bound) and a lower bound.
It is shown that if z and zare the lower and upper bounds found, then z/z ≤ k, where k is the concentrator capacity.
Some computational examples with up to 50 terminals and 20 potential concentrator sites are considered.
All the network designs obtained are shown to be within 2.8% of optimal.

We consider isochrons of a periodic orbit in the planar FitzHugh--Nagumo system, that is, the curves of points that converge to the periodic orbit in phase with each other, and we extend this notion to isochrons of a focus equilibrium.
We introduce the notion of backward-time isochrons, which are isochrons of the reversed-time system, and show that a cubic tangency occurs between a set of forward-time and backward-time isochrons.
We call this moment of tangency a cubic isochron foliation tangency (CIFT).
This phenomenon is not a local feature but happens globally throughout the annulus where both sets of isochrons coexist.
We construct and discuss examples for three mechanisms for a CIFT: a global time-scale separation; a perturbation that increases the velocity along trajectories in a local region of phase space; and a canard explosion (in the Van der Pol system).

This paper presents a framework of website quality evaluation for measuring the performance of government websites.
Multiple criteria decision-making (MCDM) is a widely used tool for evaluating and ranking problems containing multiple, usually conflicting criteria.
In line with the multi-dimensional characteristics of website quality, MCDM provides an effective framework for an inter-websites comparison involving the evaluation of multiple attributes.
It thus ranks different websites compared in terms of their overall performance.
This paper models the inter-website comparison problem as an MCDM problem, and presents a practical and selective approach to deal with it.
In addition, fuzzy logic is applied to the subjectivity and vagueness in the assessment process.
The proposed framework is effectively illustrated to rate Turkish government websites.

These notes are concerned with the numerical treatment of the coupling between second order elliptic problems that feature large contrast between their characteristic coefficients.
In particular, we study the application of Nitsche’s method to set up a robust approximation of interface conditions in the framework of the finite element method.
The notes are subdivided in three parts.
Firstly, we review the weak enforcement of Dirichlet boundary conditions with particular attention to Nitsche’s method and we discuss the extension of such technique to the coupling of Poisson equations.
Secondly, we review the application of Nitsche’s method to large contrast problems, discretised on computational meshes that capture the interface of discontinuity between coefficients.
Finally, we extend the previous schemes to the case of unfitted meshes, which occurs when the computational mesh does not conform with the interface between subproblems.

An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point.
In this paper we present an innovative design for the operation of such ad-hoc networks.
The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network.
This amounts to a new sort of routing protocol.
We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks.
Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts.
Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.

This document defines a YANG data model for the configuration and ::: management of Traffic Engineering (TE) interfaces, tunnels and Label ::: Switched Paths (LSPs).
The model is divided into YANG modules that ::: classify data into generic, device-specific, technology agnostic, and ::: technology-specific elements.
This model covers data for ::: configuration, operational state, remote procedural calls, and event ::: notifications.

This study investigated teachers' ability to learn, apply in lesson plans, and retain knowledge of classroom discourse from a single module of a multimedia professional development program, used with 39 K-12 teachers enrolled in two graduate courses.
Data collection and analysis included: (a) teacher development of - and panel review of - lesson plans integrating discourse, and (b) follow-up teacher interviews one year later.
The study found: (a) the video modeling in the module was effective in helping teachers learn, and (b) discourse strategies were learned, applied, and retained.
Implications for professional development with multimedia to teach classroom discourse include: (a) recursive use, (b) reflection, (c) multiple examples, (d) counter examples, and (e) prompts for observation.

Abstract Online travel has developed dramatically during the past three years in China.
This results in a large amount of unstructured data like tourism reviews from which it is hard to extract useful knowledge.
In this paper, a DWWP system consisting of domain-specific new words detection (DW) and word propagation (WP) is presented.
DW deals with the negligence of user-invented new words and converted sentiment words by means of AMI (Assembled Mutual Information).
Inspired by social networks, the new method WP incorporates manually calibrated sentiment scores, semantic and statistical similarity information, which improves the quality of sentiment lexicon in comparison with existing data-driven methods.
Experimental results show that DWWP improves seventeen percentage points compared with graph propagation and four percentage points compared with label propagation in terms of accuracy on Dataset I and Dataset II, respectively.

This paper presents a comprehensive comparative study of artificial neural networks, learning vector quantization and dynamic time warping classification techniques combined with stationary/non-stationary feature extraction for environmental sound recognition.
Results show 70% recognition using mel frequency cepstral coefficients or continuous wavelet transform with dynamic time warping.

The separation of picking and packing processes of a warehouse management system usually brings extra storage buffers and relatively longer operating time.
This paper develops a hybrid algorithm to generate a picking sequence for combining picking and packing operations.
The algorithm includes three elements: container selection, loading configuration, and loading/picking sequence.
A generic warehouse management system with the proposed sequential order picking function for a tea factory in central Taiwan has been implemented to demonstrate the elimination of storage buffers and the reduction of operation time.

In this paper, we consider the concept of the average connectivity of a graph, defined to be the average, over all pairs of vertices, of the maximum number of internally disjoint paths connecting these vertices.
We establish sharp bounds for this parameter in terms of the average degree and improve one of these bounds for bipartite graphs with perfect matchings.
Sharp upper bounds for planar and outerplanar graphs and cartesian products of graphs are established.
Nordhaus-Gaddum-type results for this parameter and relationships between the clique number and chromatic number of a graph are also established.

The Digital Library Curriculum Development Project (http://curric.dlib.vt.edu) team has been developing educational modules and conducting field-tests internationally since January 2006.
There had been three approaches for module development in the past.
The first approach was that the project team members created draft modules (total of 9) and then those modules were reviewed by the experts in the field as well as by other members of the team.
The second approach was that graduate student teams developed modules under the supervision of an instructor and the project team.
Four members in each team collaborated for a single module.
In total four modules were produced in this way.
The last approach was that five graduate students developed a total of five modules, each module reviewed by two students.
The completed modules were posted in Wikiversity.org for wider distribution and collaborative improvements by the community.
The entire list of modules in the Digital Library Educational Framework also can be found in that location.

Nominal Isabelle is a definitional extension of the Isabelle/HOL theorem prover.
It provides a proving infrastructure for reasoning about programming language calculi involving named bound variables (as opposed to de-Bruijn indices).
In this paper we present an extension of Nominal Isabelle for dealing with general bindings, that means term-constructors where multiple variables are bound at once.
Such general bindings are ubiquitous in programming language research and only very poorly supported with single binders, such as lambdaabstractions.
Our extension includes new definitions of a-equivalence and establishes automatically the reasoning infrastructure for a-equated terms.
We also prove strong induction principles that have the usual variable convention already built in.

This paper examines the front-end process of inter-organizational IT innovation.
In particular, it focuses on the nature and role of architectural knowledge.
Such knowledge is important for development of architectures capable of serving the goals of heterogeneous actors and technologies.
Yet, surprisingly little research has been done on how architectural knowledge may be developed through collective achievements.
This paper presents a theoretical model of architectural knowledge development in inter-organizational IT innovation.
Applying this model throughout an action research project within the Swedish transport industry, the paper identifies four dimensions of architectural knowledge that proved important for facilitating an industry-wide ubiquitous computing environment.
The four dimensions are technology capability awareness, use context sensitivity, business model understanding, and boundary-spanning competence.
We conclude the paper by outlining the theoretical and strategy implications of the model and the four dimensions of architectural knowledge.

The aim of this paper is to find feature-patterns related to the autonomy-disability level of elderly people living in nursing homes.
These levels correspond to profiles based on the people's ability to perform activities of daily living like being able to wash, dress and move.
To achieve this aim, an unsupervised approach is used.
In this article, we propose a new clustering approach based on principal component analysis (PCA) to better approximate clusters.
We want to automatically find categories or groups of residents based on their degree of autonomy-disability.
All residents in a group have similar patterns.
The main function of PCA is to explore the links between variables and the similarities between examples (individuals).
The proposed algorithm uses the PCA technique to direct the determination of the clusters with self-organizing partitions by using the Euclidian distance.
The study was carried out in close collaboration with the French cooperative health organization called the ''Mutualite Francaise de la Loire''.
The quantitative data arises from the databases of four different nursing homes located in the city of Saint-Etienne in France.
The study concerns 2271 observations of dependence evaluations corresponding to 628 residents.

F.P.
Preparata et al.
(1967) introduced a graph-theoretical model for fault diagnosis called the PMC model.
The question of determining the sequential diagnosability number of a system in the PMC model has remained open.
The authors formalize the notion of sequential diagnosability.
The question of determining the sequential diagnosability number of a system is addressed by showing that the appropriate decision problem is co-NP complete.
The problem is also shown to be co-NP complete even when restricted to planar graphs both in the weighted and the BGM models.
>

Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling.
Much of this work however does not explicitly incorporate models of low-level human visual attention.
In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes.
Our notion of saliency is inspired by low-level human visual system cues.
We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures.
We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh.
The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes.
compared to using a purely geometric measure of shape.
such as curvature.
We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.

This textbook provides both profound technological knowledge and a comprehensive treatment of essential topics in music processing and music information retrieval.
Including numerous examples, figures, and exercises, this book is suited for students, lecturers, and researchers working in audio engineering, computer science, multimedia, and musicology.
The book consists of eight chapters.
The first two cover foundations of music representations and the Fourier transformconcepts that are then used throughout the book.
In the subsequent chapters, concrete music processing tasks serve as a starting point.
Each of these chapters is organized in a similar fashion and starts with a general description of the music processing scenario at hand before integrating it into a wider context.
It then discussesin a mathematically rigorous wayimportant techniques and algorithms that are generally applicable to a wide range of analysis, classification, and retrieval problems.
At the same time, the techniques are directly applied to a specific music processing task.
By mixing theory and practice, the books goal is to offer detailed technological insights as well as a deep understanding of music processing applications.
Each chapter ends with a section that includes links to the research literature, suggestions for further reading, a list of references, and exercises.
The chapters are organized in a modular fashion, thus offering lecturers and readers many ways to choose, rearrange or supplement the material.
Accordingly, selected chapters or individual sections can easily be integrated into courses on general multimedia, information science, signal processing, music informatics, or the digital humanities.

There is agreement in the literature that affect influences learning.
In turn, addressing affective issues in the recommendation process has shown their ability to increase the performance of recommender systems in non-educational scenarios.
In our work, we combine both research lines and describe the SAERS approach to model affective educational recommendations.
This affective recommendation model has been initially validated with the application of the TORMES methodology to specific educational settings.
We report 29 recommendations elicited in 12 scenarios by applying this methodology.
Moreover, a UML formalized version of the recommendations model which can describe the recommendations elicited is presented in the paper.

We first review the state-of-the-art in development of log P prediction approaches falling in two major categories: substructure-based and property-based methods.
Then, we compare the predictive power of representative methods for one public (N ¼ 266) and two in house datasets from Nycomed (N ¼ 882) and Pfizer (N ¼ 95809).
A total of 30 and 18 methods were tested for public and industrial datasets, respectively.
Accuracy of models declined with the number of nonhydrogen atoms.
The Arithmetic Average Model (AAM), which predicts the same value (the arithmetic mean) for all compounds, was used as a baseline model for comparison.
Methods with Root Mean Squared Error (RMSE) greater than RMSE produced by the AAM were considered as unacceptable.
The majority of analyzed methods produced reasonable results for the public dataset but only seven methods were successful on the both in house datasets.
We proposed a simple equation based on the number of carbon atoms, NC, and the number of hetero atoms, NHET: log P ¼ 1.46(� 0.02) þ 0.11(� 0.001) NC� 0.11(� 0.001) NHET.
This equation outperformed a large number of programs benchmarked in this study.
Factors influencing the accuracy of log P predictions were elucidated and discussed.
2008 Wiley-

Endoscopy is used for inspection of the inner surface of organs such as the colon.
During endoscopic inspection of the colon or colonoscopy, a tiny video camera generates a video signal, which is displayed on a monitor for interpretation in real-time by physicians.
In practice, these images are not typically captured, which may be attributed by lack of fully automated tools for capturing, analysis of important contents, and quick and easy retrieval of these contents.
This paper presents the description and evaluation results of our novel software that uses new metrics based on image color and motion over time to automatically record all images of an individual endoscopic procedure into a single digitized video file.
The software automatically discards out-patient video frames between different endoscopic procedures.
We validated our software system on 2464h of live video (over 265 million frames) from endoscopy units where colonoscopy and upper endoscopy were performed.
Our previous classification method achieved a frame-based sensitivity of 100.00%, but only a specificity of 89.22%.
Our new method achieved a frame-based sensitivity and specificity of 99.90% and 99.97%, a significant improvement.
Our system is robust for day-to-day use in medical practice.

In the last decade, design innovation has gained increasing prominence in the marketplace, with a growing number of firms innovating not only through technology but also through novel product forms i.e., design.
However, while the effect of technological innovation on product sales is a heavily studied topic, a defining theory of how design innovation influences product sales is still missing.
This paper provides demand-and supply-side theories to formulate a set of coherent hypotheses about the effect of design innovativeness, i.e., the degree of novelty in a product's design, on sales' evolution over time.
The hypotheses are tested in two different samples.
In the first, car models introduced in the United States from 1978 to 2006 for a total of 2,757 model-year data are analyzed.
In the second, motorcycle models introduced in the United States from 1980 to 2008 for a total of 2,847 model-year observations are analyzed.
I find that design innovativeness diminishes initial sales' status but increases sales' growth rates.
Furthermore, design and technological innovativeness have a negative interaction effect on sales' initial status, but a positive effect on sales' growth rates.
Finally, brand strength and brand advertising expenditures worsen the negative effect of design innovativeness on initial sales' status, but boost its positive effect on sales' growth rates.

Virtual Enterprise is the potential mode of enterprise in the future.
The risk management for virtual enterprise is the hot research area recently.
There is always less historical data and there are many uncertain factors in virtual enterprise.
Hence, in this paper, the fuzzy synthetic evaluation model for the risk evaluation of virtual enterprise is established according to the characteristics of virtual enterprise.
The example suggests that the method is useful.

Abstract During the last years, an important number of episodes with peak nitrogen dioxide levels occurred in the Paris region.
Modelling air pollution is necessary to predict future episodes and to take decisions for the protection of populations.
Our study proposes a methodology for urban air pollution analysis, as a preliminary stage of its modelling.
This paper focuses on two major points: the estimation of pollutant concentration fields using measures from a monitoring network, and the definition of a reduced number of concentration spatial patterns.
Interpolation by thin plate splines is proposed and the approximation quality is estimated by leave-one-out cross validation tests.
Analysis of large numbers of pollutant fields is difficult: cluster analysis of spatial distributions is proposed.
For this study, Ward's algorithm is selected and applied to AIRPARIF nitrogen dioxide data during peak episodes from January 1993 to December 1997.
Finally, some results and conclusions are presented.

Mapping a problem graph of communicating tasks onto a network graph of processing elements so that the communication distance is minimized and the problem graph is evenly distributed over the network, is an NP-complete problem.
The authors demonstrate that an approximation technique called simulated annealing can be applied to find acceptable solutions to this problem.
For this purpose, they first develop a distance-variance metric to estimate the goodness of a mapping.
The authors then present evidence that, with a regular network topology, there are no significant local minima in the space of possible solutions.
Hence, a faster form of simulated annealing called quenching becomes appropriate for the mapping problem.
>

Online communities (OCs) are seen as important stimulus to electronic business.
However, surprisingly little is known about how the communication activity of their participants develops and changes over time.
A longitudinal study bears the potential to better elaborate the enabling and inhibiting factors of the participant's communication activity in OCs.
To explore these phenomena, we aimed to develop a conceptual framework that serves as a foundation to guide an explorative data analysis of real OCs.
We use the notions of common ground, information overload, interactivity, and social loafing to explain the communication activity of the participants in OCs.
The empirically explored framework will help organizations to support the development of OCs and utilize them in an economically successful way.
Based on a literature review, we developed a first conceptual framework.
Then, we apply it to describe the development of the communication activity and its determinants in an OC hosted by a German financial ...

Abstract Swarm robots have been an active topic in the field of intelligent machines in recent years.
Self-assembly is an important function of swarm robots that provides an effective way for building different kinds of configurations by autonomous docking.
For a self-assembly process, the path planning control is a key technique.
The present paper proposes a CVT (Centroidal Voronoi Tessellation) based intelligent control algorithm for the self-assembly path planning.
It can enable swarm robots to move from an initial virtual region into the target virtual region by a synchronous collaborative scheduling method.
Matlab simulations are performed on the self-assembly control of three typical configurations including the line-shape, cross-shape and H-shape.
The time consumption is compared under different error conditions, the effectiveness of the new algorithm is validated and its limitation is also pointed out.
Overall, this algorithm provides a promising way for the intelligent control of self-assembly path planning of swarm robots.

Graph transformations are frequently used for describing and manipulating models in model driven software development.
There are several frameworks and solutions for graph transformations but in case of large, industrial-sized models, performance still remains a key issue.
We are working on a transformation algorithm that can efficiently use the computational capabilities of the multicore processor architectures.
By applying the transformations in parallel, our aim is to create a method useful in research and industrial projects as well.
The introduced algorithm is not only a theoretical result; we have implemented and applied it on real-world industrial-sized models.
The paper elaborates the test results and compares the performance of the parallel and the sequential execution.
The results of the measurements are analyzed as well as future research directions are given.

Use of consolidation terminals to transport products from various sources to various destinations can take advantage of economies of scale in transportation costs.
Instead of making direct shipments, each source can ship in bulk to one or more consolidation terminals.
There, shipments can be broken down, and material bound for the same destination can be combined.
We consider such a freight transport problem FTP.
For each source-destination pair, it must be decided whether to ship the product directly or via a consolidation terminal.
Shipping costs are piecewise linear concave functions of the volume shipped.
Shipping via a terminal can also incur a linear inventory holding cost.
We seek a minimum cost pattern of direct and indirect i.e., via a terminal shipments.
This is a type of concave cost multiproduct network flow problem.
We can solve this problem optimally if either the source-to-terminal or the terminal-to-destination shipping costs are linear.
In this case, FTP decomposes into a set of concave cost facility location problems CFLP.
In more general cases, heuristic methods that solve sequences of linear problems can be used.
Some computational results are presented.

Abstract In the fast developing world of today, automatic generation control (AGC) plays an incredibly significant role in offering inevident demand of good quality power supply in power system.
To deliver a quality power, AGC system requires an efficient and intelligent control algorithm.
Hence, in this paper, a novel fractional order fuzzy proportional-integral-derivative (FOFPID) controller is proposed for AGC of electric power generating systems.
The proposed controller is tested for the first time on three structures of multi-area multi-source AGC system.
The gains and fractional order parameters such as order of integrator (λ) and differentiator (µ and γ) of FOFPID controller are optimized using bacterial foraging optimization algorithm (BFOA).
Initially, the proposed controller is implemented on a traditional two-area multi-source hydrothermal power system and its effectiveness is established by comparing the results with FOPID, fuzzy PID (FPID) and PI/PID controller based on recently published optimization techniques like hybrid firefly algorithm-pattern search (hFA-PS) and grey wolf optimization (GWO) algorithm.
The approach is further extended to restructured multi-source hydrothermal and thermal gas systems.
It is observed that the dynamic performance of the proposed BFOA optimized FOFPID controller is superior to BFOA optimized FPID/FOPID/PID and differential evolution (DE)/genetic algorithm (GA) optimized PID controllers.
It is also detected that the dynamic responses obtained under different power transactions with/without appropriate generation rate constraint, time delay and governor dead-zone effectively satisfy the AGC requirement in deregulated environment.
Moreover, robustness of the suggested approach is verified against wide variations in the nominal initial loading, system parameters, distribution company participation matrix structure and size and position of uncontracted power demand.

Purpose – As innovative forms of information and communication technologies (ICTs) such as broadband internet are being adopted, the equitable distribution of ICTs has become an issue of concern.
These apprehensions are being attributed to social exclusion that could arise due to the limited internet use among citizens with lower incomes or educational levels.
This research aims to examine the impact of socio‐economic determinants, such as age, gender, education, income and occupation, on the adoption of broadband.Design/methodology/approach – A survey research approach was employed to achieve the overall aim of this research.
The empirical data of the socio‐economic variables was collected employing a postal survey in the UK.Findings – The findings of this research suggest that in terms of broadband adoption, except for gender, all the other socio‐economic variables, including age, education, occupation and income, significantly helped to explain differences between the adopters and non‐adopters of broad...

Image registration of biological tissue is essential for 3D reconstruction, which is important for visualizing and quantifying the 3D relationships between internal structures of an object.
The biological role of DNA organization, which is an extremely complex 3D architecture within the cell nucleus, has come into focus since it has become clear that the chromatin structure in itself functions as a regulator of DNA.
Thus, 3D reconstruction of cell nuclei based on consecutive series of high-resolution ultrathin slices may provide new information about the chromatin structure and its organizational changes during carcinogenesis.
This work focuses mainly on the problem of registering successive serial transmission electron micrographs of ultrathin sections of mouse liver cell nuclei to analyse the 3D chromatin structure.
A five-step semiautomatic interactive registration method is proposed.
The first two steps of the procedure correct the rotation and translation components by using the phase correlation.
The third, fourth and fifth steps correct the global distortion, employing a point mapping method based on different ways of selecting the control points.
In step three, the control points were automatically computed by phase correlating corresponding subimages of the reference and sensed image.
A semiautomatic method is used in the fourth step to select the control points, i.e.
an automated method for computing the centre of mass of manually identified anatomical structures in neighbouring slices.
For the sections which could not be properly corrected by the four steps, a final step is introduced, where control points are manually selected in the reference and sensed images.
An algorithm is proposed to examine the spatial distribution of selected control points.
Four sets of serial sections of mouse liver cell nuclei, each with approximately 100 sections, are registered by the proposed method and also registered manually for the comparison of registration accuracy.
Artificial X-Z and Z-Y sections of registered series were visually compared for the smoothness of the nuclear membrane.
To quantify the registration accuracy and the extent of registration, the correlation coefficient (C) and the overlap index (Co) were computed over the registered structure of interest.
In addition to the visual comparison and the comparison of C and Co, the registered serial sets were compared by 3D GLCM-based texture features in the Z direction.
The results demonstrate that the proposed semiautomatic registration technique achieved accurate results comparable to the manual registration.
The proposed registration method relies only on the operator for rough pinpointing of cellular structures.
Therefore, it should provide better reproducibility, and allow the user to operate the system faster and in a more relaxed manner than in a manual registration.

This paper conducts stochastic comparison on general residual life and general inactivity time of (n − k + 1)-out-of-n systems and investigates the stochastic behavior of the general inactivity time of a system with units having decreasing reversed hazard rate.
These results strengthen some conclusions in both Khaledi and Shaked (2006) and Hu et al.
(2007).

In this article, we present a framework called state-set branching that combines symbolic search based on reduced ordered Binary Decision Diagrams (BDDs) with best-first search, such as A* and greedy best-first search.
The framework relies on an extension of these algorithms from expanding a single state in each iteration to expanding a set of states.
We prove that it is generally sound and optimal for two A* implementations and show how a new BDD technique called branching partitioning can be used to efficiently expand sets of states.
The framework is general.
It applies to any heuristic function, evaluation function, and transition cost function defined over a finite domain.
Moreover, branching partitioning applies to both disjunctive and conjunctive transition relation partitioning.
An extensive experimental evaluation of the two A* implementations proves state-set branching to be a powerful framework.
The algorithms outperform the ordinary A* algorithm in almost all domains.
In addition, they can improve the complexity of A* exponentially and often dominate both A* and blind BDD-based search by several orders of magnitude.
Moreover, they have substantially better performance than BDDA*, the currently most efficient BDD-based implementation of A*.

The ubiquitous World Wide Web presents a simple interface for a vast array of heterogeneous information.
We see the Web as one enabler for what we call superimposed information.
Superimposed information serves to highlight, annotate, connect and supplement information in a base information space.
Superimposed information is already pervasive for the Web, with a variety of models and accompanying capabilities.
::: ::: In this paper, we introduce superimposed information and analyze a range of conceptual models for it using a three-part feature space consisting of information elements, links, and marks.
Information elements in the superimposed layer and links among those information elements are analogous to the classical entities and relationships of database models.
The novelty is in the marks that reference underlying information elements.
Superimposed information can serve as proxies for underlying information elements, can provide new access paths, and can introduce new information as well as new links among existing information elements.
We conclude with a discussion of open research questions regarding superimposed information.

The introduction of extended producer responsibility forces Original Equipment Manufacturers to set up a logistic network for take back, processing and recovery of discarded products.
In this paper, we discuss a business case study carried out at Oce, a copier firm in Venlo (NL).
It concerns the installment of remanufacturing processes.
There is a choice from two locations in Venlo (NL) and one in Prague (Czech Republic), where assignments are subjected to managerial constraints.
The study is meant to verify whether the strategic decision of Oce to move remanufacturing activities to the Czech Republic is also economically feasible.
We limit ourselves to an optimisation of the HV02-machine network.
We follow our general approach, in which we first determine how return products are processed (recovery strategy) and subsequently optimise the reverse logistic network design.
We optimise on total operational costs over all possibilities and also compare three pregiven managerial solutions (=network designs) with a Mixed Integer Linear Programming model.
Differences in economic costs appear to be very small, hence installing recovery activities in Prague for the HV02-machine must be well motivated from a strategic point of view.
Moreover, we argue that besides cost minimisation, Oce should include performance indicators, such as JIT, reliability, in logistic optimisation to support its quality oriented business strategy.
In addition, we discuss aspects regarding specific modelling elements in this case situation, the definition of cost functions, the possibility of optimising the forward and reverse logistic network and the use of LP- versus MILP-models in this kind of situations.

We focus on the investors' opinion in stock message board.Extending six basic emotion words increases classifying accuracy.Opinion classifying based on ESM is effective and fast.We compare ESM with traditional SVM information gain and mutual information.
The online stock message is known to have impacts on the trend of the stock market.
Understanding investor opinions in stock message boards is important, and the automatic classification of the investors' opinions is one of the key methods for the issue.
Traditional opinion classification methods mainly use terms and their frequency, part of speech, rule of opinions and sentiment shifters.
But semantic information is ignored in term selection, and it is also hard to find the complete rules.
In this paper, based on the classification of human emotions proposed by Ekman, we extend the traditional positive-negative analysis to the six important emotion states to build an extremely low dimensional emotion space model (ESM).
It enables the prediction of investors' emotions in public.
Specifically, we use lexical semantic extension and correlation analysis methods to extend the scale of emotion words, which can capture more words with strong emotions for ad hoc domain, like network emotion symbols.
We apply our ESM on messages of a famous stock message board TheLion.
We also compare our model with traditional methods information gain and mutual information.
The results show that ESM is not parameter sensitive.
Besides, ESM is efficient for modeling sentiment classifying and can achieve higher classification accuracy than traditional ones.

Purpose: The Shared Hospital Electronic Library of Southern Indiana (SHELSI) research project was designed to determine whether access to a virtual health sciences library and training in its use would support medical decision making in rural southern Indiana and achieve the same level of impact seen by targeted information services provided by health sciences librarians in urban hospitals.
::: ::: Methods: Based on the results of a needs assessment, a virtual medical library was created; various levels of training were provided.
Virtual library users were asked to complete a Likert-type survey, which included questions on intent of use and impact of use.
At the conclusion of the project period, structured interviews were conducted.
::: ::: Results: Impact of the virtual health sciences library showed a strong correlation with the impact of information provided by health sciences librarians.
Both interventions resulted in avoidance of adverse health events.
Data collected from the structured interviews confirmed the perceived value of the virtual library.
::: ::: Conclusion: While librarians continue to hold a strong position in supporting information access for health care providers, their roles in the information age must begin to move away from providing information toward selecting and organizing knowledge resources and instruction in their use.

The paper demonstrates a relatively new perspective application of data envelopment analysis (DEA) in the optimization of urban public transport (UPT) systems.
For this goal a special two-phase approach is used.
The first phase consists of applying a special adjusted DEA model—input oriented hybrid model with non-controllable and bounded inputs with categorical decision making units under constant returns to scale—for evaluating efficiency of UPT lines and for obtaining recommendations for improving their efficiency.
Based on the recommendations from the first phase, in the second phase a possible assignment of dispatched vehicles of inefficient UPT lines is proposed.
The results of the conducted analysis offer suggestions to improve efficiency of the UPT so that the balance between supply and demand for transport services is maintained.
The proposed method is applied to the UPT system in a Slovak city, Banska Bystrica.
Copyright Springer-Verlag Berlin Heidelberg 2015

Abstract We investigated the neural generators of N1 and P1 components of visual magnetic responses through the concomitant study of low (1–15 Hz)- and high (15–30 Hz)-frequency brain activities phase-locked to stimulus and elicited by pattern reversal visual stimuli.
Whole helmet magnetic recordings and dipole modeling technique with support of functional magnetic resonance imaging (fMRI) were used to characterize locations and orientations of N1 and P1 sources as a function of four stimulated visual field quadrants.
A comparison between low- and high-frequency activities revealed fundamental differences among orientations of the quadrants dipoles thus suggesting partly distinct neural populations underlying low- and high-frequency responses to transient contrast visual stimuli.
Moreover, for both low- and high-frequency bands the specific study of locations and orientations of N1 and P1 sources indicated V1/V2 cortex as the neural substrate generating the two components.
In summary, we provided strong support for a cortical genesis of human oscillatory mass activity following transient contrast stimuli with specific neural districts active in the low- and high-frequency bands.
The converging results obtained from the concomitant investigation of probably different brain activities provided new evidences for a striate genesis of N1 and P1 components of the broadband visual-evoked responses following pattern reversal.

This is the official SunSoft documentation that describes the primary facilities for implementing multi-threaded applications.
It describes the services and capabilities of the multithreaded library, covering topics such as thread basics, thread creation and scheduling, synchronization, signals, threads and process resources, advanced topics, using threads, the threads environment, debugging techniques, and example applications.

The finite powerset construction upgrades an abstract domain by allowing for the representation of finite disjunctions of its elements.
While most of the operations on the finite powerset abstract domain are easily obtained by “lifting” the corresponding operations on the base-level domain, the problem of endowing finite powersets with a provably correct widening operator is still open.
In this paper we define three generic widening methodologies for the finite powerset abstract domain.
The widenings are obtained by lifting any widening operator defined on the base-level abstract domain and are parametric with respect to the specification of a few additional operators that allow all the flexibility required to tune the complexity/precision trade-off.
As far as we know, this is the first time that the problem of deriving non-trivial, provably correct widening operators in a domain refinement is tackled successfully.
We illustrate the proposed techniques by instantiating our widening methodologies on powersets of convex polyhedra, a domain for which no non-trivial widening operator was previously known.

The purpose of this study was to investigate the influence of mental models on the performance differences between expert and novice service technicians who troubleshoot faulty technical equipment.
This study involved the use of behavioral and cognitive methods of analysis to identify and develop the ideal mental models that are used by expert technical troubleshooters.
These two methods of analysis involved the identification of the specific tasks used during troubleshooting and the collection and analysis of the verbal protocols of expert and novice technicians as they attempted to identify faults in technical equipment.
::: ::: ::: ::: The results of the study showed definite differences between experts and novices.
The experts were able to acquire and correctly interpret the initial symptoms of the problem, and from the interpretation of the initial symptoms the experts were able to represent the problem through the development of an accurate problem space.
The sequences used by the experts to find faults in technical systems involved the generation of hypotheses that were used to reduce the size of the problem space and, therefore, to narrow the number of potential locations for the fault.
In contrast, the novice subjects were unable to accurately interpret the initial symptoms of the problem.
They were also unable to represent the problem through the development of an accurate problem space.
The lack of an accurate problem representation resulted in the apparent random search for faults in the system.

This paper considers the scheduling problem in a Flexible Manufacturing System.
The objective is to minimize the makespan of a set of jobs.
The problem resembles a job-shop scheduling problem.
However, it is complicated by the ability to perform several of the operations on more than one machine, and the disability of the material handling system to handle more than a fixed number of jobs at the same time.
A hierarchical algorithm based on the similarities with the job-shop scheduling problem is proposed and satisfactory computational results are provided.

The growing capabilities of computers and multimedia technology provide an exciting opportunity for boosting technology-based education in schools, universities, and corporate training.
Multimedia modules provide the advantage of visualization of highly mathematical subjects and abstract concepts, intuitive learning and conceptual understanding, and the ability to participate in “what-if” scenarios.
Graphics, animation, video clips, virtual labs, and guided use of simulation software introduce students to a challenging new world with all the advantages of learn-by-doing possibilities.
The CAEME Center at the University of Utah was established to focus on the development of highly interactive multimedia lessons in science and engineering education.
This article provides an overview of the Center's products and describes a new multimedia lesson on antenna theory and design.
© 2000 John Wiley & Sons, Inc. Comput Appl Eng Educ 8: 11–17, 2000

Abstract This paper provides an integrative summary of a three-year research program investigating various factors pertinent to human operator adaptation in process-control systems.
Four longitudinal experiments were conducted with a simplified but representative thermal-hydraulic process simulation.
These experiments investigated the impact of four behaviour-shaping constraints that can influence operator adaptation: interface content, interface form, type of training, and pre-existing competencies.
The findings obtained from the research program are summarized, and a number of implications for the design and operation of process-control systems are suggested.

Abstract This paper describes a new power network topology processor which has been successfully used in two practical power system application programs.
Artificial intelligence (AI) techniques have been used and this provides several advantages.
AI's frame representation method is used to represent electric network configuration.
This avoids manual numbering on a large amount of circuit breaker terminals/circuits.
Rule-based method has been adopted for substation's bus configuration analysis instead of the search method.
This makes the analysis simple and adaptable to new types of bus configurations just by adding corresponding rules.
AI's search techniques have been applied for network graph search making the search algorithm simpler and easier to be programmed.
Some results from the application of the proposed topology processor are also included in the paper.

The reported CMOS microsystem is the key element for accurate angle measurements.
In combination with a permanent magnet, it is used for various wear free angular positioning control systems for automotive and industrial applications covering the full 360/spl deg/ range.
The integrated system includes a two-dimensional (2-D) magnetic microsensor (30/spl times/30 /spl mu/m/sup 2/ active area), offset compensation, and signal conditioning circuitry.
A novel approach for the angle calculation is presented using an on-board incremental ADC.
A bitstream representing the angular position of the applied permanent magnet is provided at the system output.
The system achieves a 1/spl deg/ angular resolution with 9 mW power consumption and a permanent magnet of 100 mT.
The chip is fabricated in a generic 2-/spl mu/m, double-poly, double-metal CMOS process and covers an area of 2.6/spl times/4.1 mm/sup 2/.

The availability of electronic media challenges our approaches to student learning support in a variety of ways.
In spite of much work into online course design and delivery, we have tended to neglect the contribution of other modes and media used in learner support, and there has been a critical lack of overt integration of online use with other good learning support systems.
In the SOLACE project, we have been exploring current practices for learner support and the degree to which tutor interventions and interactions with their students map to their use of communication media.
The main focus for this study is ‘traditional distance’ tutors at the Open University, and contrasts are drawn with three courses taught within a ‘traditional campus’ setting at the University of Glasgow.
Preliminary findings from the first phase of this project reflect present practice in learner support for a range of courses, and describe factors influencing the use of media in a blended solution to learner support.

This paper presents a new self-routing packet network called the plane interconnected parallel network (PIPN).
In the proposed design, the traffic arriving at the network is shaped and routed through two banyan network based interconnected planes.
The interconnections between the planes distribute the incoming load more homogeneously over the network.
The throughput of the network under uniform and heterogeneous traffic requirements is studied analytically and by simulation.
The results are compared with the results of the baseline network and another banyan network based parallel interconnection network.
It is shown that, for the proposed design, a higher degree of heterogeneity results in better performance.

Abstract In the study of extended Runge–Kutta–Nystrom (abbr.
ERKN) methods for the integration of multi-frequency oscillatory systems, a quite complicated set of algebraic conditions arises which must be satisfied for a method to achieve some specified order.
A theory of tri-colored tree was proposed by Yang et al.
(2009), for achieving the order conditions of ERKN methods which are designed specially for multi-frequency and multidimensional perturbed oscillators.
The tri-colored tree theory for the order conditions in that paper is useful, but not completely satisfactory due to the existence of redundant trees.
In this paper, a simplified tri-colored theory and the order conditions for ERKN integrators are developed by constructing a set of simplified special extended Nystrom trees (abbr.
SSENT) and defining some real-valued mappings on it.
In order to simplify the tri-colored tree theory, two special mappings, the extended elementary differential and the sign mapping for a tree are investigated in detail.
This leads to a novel Nystrom-tree theory for the order conditions for ERKN methods without any redundant trees, which simplifies the tri-colored theory.

We give a novel analysis of the steady-state probabilities of a class of infinite Markov chains.
Markov chains of this type appear in the study of bulk queues and a variety of other stochastic models.
We present algorithms that involve only real arithmetic and avoid the traditional analysis based on Rouche's theorem.

The problem of scheduling in permutation flowshops is considered in this paper with the objectives of minimizing the sum of weighted flowtime/sum of weighted tardiness/sum of weighted flowtime and weighted tardiness/sum of weighted flowtime, weighted tardiness and weighted earliness of jobs, with each objective considered separately.
Lower bounds on the given objective (corresponding to a node generated in the scheduling tree) are developed by solving an assignment problem.
Branch-and-bound algorithms are developed to obtain the best permutation sequence in each case.
Our algorithm incorporates a job-based lower bound (integrated with machine-based bounds) with respect to the weighted flowtime/weighted tardiness/weighted flowtime and weighted tardiness, and a machine-based lower bound with respect to the weighted earliness of jobs.
The proposed algorithms are evaluated by solving many randomly generated problems of different problem sizes.
The results of an extensive computational investigation for various problem sizes are presented.
In addition, one of the proposed branch-and-bound algorithms is compared with a related existing branch-and-bound algorithm.

Prior work on TLB power optimization considered circuit and architectural techniques.
A recent software-based technique for data TLBs has considered the possibility of storing the frequently used virtual-to-physical address translations in a set of translation registers (TRs), and using them when necessary instead of going to the data TLB.
This paper presents a compiler-based strategy for increasing the effectiveness of TRs.
The idea is to restructure the application code in such a fashion that once a TR is loaded, its contents are reused as much as possible.
Our experimental evaluation with six array-based benchmarks from the Spec2000 suite indicates that the proposed TR reuse strategy brings significant reductions in data TLB energy over an alternate strategy that employs TRs but does not restructure the code for TR reuse

This article addresses calculations of the standard free energy of binding from molecular simulations in which a bound ligand is extracted from its binding site by steered molecular dynamics (MD) simulations or equilibrium umbrella sampling (US).
Host-guest systems are used as test beds to examine the requirements for obtaining the reversible work of ligand extraction.
We find that, for both steered MD and US, marked irreversibilities can occur when the guest molecule crosses an energy barrier and suddenly jumps to a new position, causing dissipation of energy stored in the stretched molecule(s).
For flexible molecules, this occurs even when a stiff pulling spring is used, and it is difficult to suppress in calculations where the spring is attached to the molecules by single, fixed attachment points.
We, therefore, introduce and test a method, fluctuation-guided pulling, which adaptively adjusts the spring's attachment points based on the guest's atomic fluctuations relative to the host.
This adaptive approach is found to substantially improve the reversibility of both steered MD and US calculations for the present systems.
The results are then used to estimate standard binding free energies within a comprehensive framework, termed attach-pull-release, which recognizes that the standard free energy of binding must include not only the pulling work itself, but also the work of attaching and then releasing the spring, where the release work includes an accounting of the standard concentration to which the ligand is discharged.

A design approach for a secure multilevel object-oriented database system is proposed by which a multilevel object-oriented system can be implemented on a conventional mandatory security kernel.
Each object is assigned a single security level that applies to all its contents (variables and methods).
The informal security policy model includes properties such as compatibility of security level assignments with the class hierarchy.
After discussing the essential features of a general object system model, and then extending the object model to incorporate mandatory label-based security, it is shown how typical database security and integrity policies can be supported by this model, with special attention to inference problems and integrity constraints.
The representation of integrity constraints and classification constraints are illustrated.
>

This paper presents a global optimization algorithm for a class of multiplicative programming with exponent under multiplicative constraints (MPE).
By utilizing equivalent problem of MPE in the sense that they have the same optimal solution, tangential hypersurfaces and concave envelope approximations a linear relaxation of equivalent problem is received.
Thus the initial nonconvex programming problem (MPE) is reduced to a sequence of linear programming problems through the successive refinement of a linear relaxation of feasible region of the objective function.
The proposed algorithm is convergent to the globally optimal solution of MPE by means of the subsequent solutions of a series of linear programming problems.
Numerical results indicate that the proposed algorithm is extremely robust and can be used successfully to solve global minimum of MPE on microcomputer.

Abstract In a recent paper, Xie et al.
[Reliab Eng Syst Saf 2002;76:279–85] proposed a modification of the Weibull distribution to allow it to exhibit bathtub-shaped failure rate functions.
In this note, we derive explicit algebraic formulas for the k th moment of the distribution.
The formulas allow one to calculate the moments for a wide range of values of k and the shape parameter of the modified Weibull.
They also allow one to calculate the moments more easily with less computational time.
Besides being of interest in their own, we believe that these formulas will be useful for any follow-up mathematical study of the proposed distribution.

Sets of biological features with genome coordinates (e.g., genes and promoters) are a particularly common form of data in bioinformatics today.
Accordingly, an increasingly important processing step involves comparing coordinates from large sets of features to find overlapping feature pairs.
This paper presents fjoin, an efficient, robust, and simple algorithm for finding these pairs, and a downloadable implementation.
For typical bioinformatics feature sets, fjoin requires O(n log(n)) time (O(n) if the inputs are sorted) and uses O(1) space.
The reference implementation is a stand-alone Python program; it implements the basic algorithm and a number of useful extensions, which are also discussed in this paper.

The probability of transient faults increases with the evolution of technologies.
There is a corresponding increased demand for an early analysis of erroneous behaviours.
This paper discusses alternative approaches to perform transient fault injection in circuits described in a high level language such as VHDL.
In the proposed analysis flow, a behavioural model is generated, allowing the designer to identify the detailed error propagation paths in the circuit.
This paper also reports on results obtained with SEU-like fault injections in VHDL descriptions of digital circuits.
Several circuit description levels are considered, as well as several fault modelling levels.
These results show that an analysis performed at a very early stage in the design process can actually give a helpful insight into the response of a circuit when a fault occurs.

A power factor controller, a method of controlling power factor and a power converter employing either the controller or the method.
The controller includes: (1) a ramp circuit that senses an output voltage of a converter being controlled and generates an intermediate waveform that rises as a function of a magnitude of the output voltage and (2) a drive circuit, coupled to the ramp circuit, that senses a current in the converter and causes the intermediate waveform to fall at a time that is a function of a magnitude of the current, the drive circuit generating a drive signal for the converter from the intermediate waveform.

Two software-based techniques for online detection of control flow errors were evaluated by fault injection.
One technique, called block signature self-checking (BSSC), checks the control flow between program blocks.
The other, called error capturing instructions (ECIs), inserts ECIs in the program area, the data area, and the unused area of the memory.
To demonstrate these techniques, a program has been developed which modifies the executable code for the MC6809E 8-b microprocessor.
The error detection techniques were evaluated using two fault injection techniques: heavy-ion radiation from a californium-252 source and power supply disturbances.
Combinations of the two error detection techniques were tested for three different workloads.
A combination BSSC, ECIs, and a watchdog timer was also evaluated.
>

John Nash's work laid the foundations for evolutionary game theory as well as the theory of games with rational agents.
The Nash bargaining solution emerges as a natural solution concept in both of these settings.

Abstract A Monte-Carlo approach to error propagation from input parameters of known variance (and covariance if available) properties through an arbitrarily complicated analytic or numerical transformation to output parameters is discussed.
A simple random-number generator for a rectangular distribution function is shown to provide an econimical and fairly efficient means of simulating the effects of using a normal distribution function.

Based on literature review and on the study of the most known and referred Crowdsourcing brokers, there's a clear trend to implement this model by large companies and mainly within the North American context.
Our research team is focused in bringing this approach closer to the European culture, more specifically the cultural factors underlying the dynamics and motivation of communities available to solve the innovation challenges of Small and Medium Enterprises (SMEs), that we call Crowdsourcing Innovation.
We believe that, due to the common lack of resources for innovation in these companies, a service capable of involving them in large networks filled with useful and reachable knowledge, and capable of supporting these companies through all the innovation process, is crucial to the future competitiveness of the European SMEs.
Although our team is focusing on several aspects related to Crowdsourcing, my main research focuses the information services and supporting applications to create a web platform adapted to the key economical, organizational, legal and cultural differences that make current Crowdsourcing Innovation businesses less popular among European SMEs than in North America.

This paper presents a ZCZ code which are combinedly used for spreading sequences and a synchronization symbol in quasi-synchronous CDMA systems using PSK, ASK or BFSK.
Furthermore a simple matched filter is presented, which simultaneously calculates correlations with any sequences in the ZCZ code.

Distributed Denial-of-service (DDoS) attack is one of the most dangerous threats that could cause devastating effects on the Internet.
DDoS mainly started in 1998 but the influence of it was realized by the people only when the big organizations and corporations were hit by DDoS attacks in July 1999.
Since then several DDoS attack tools such as Trinoo, Shaft, Tribe flood network (TFN), Tribe flood network 2000 (TFN2K) and Stacheldraht are identified and analyzed.
All these tools could launch DDoS attacks from thousands of compromised host and take down virtually any connection, any network on the Internet by just a few command keystrokes.
This survey paper deals with the introduction of DDoS attacks, DDoS attack history and incidents, DDoS attack strategy, DDoS attack tools, and classification of various attack and defense mechanisms.
Finally, direction for future research work has been pointed out.

Let G = (V,E) be an undirected graph.
A subset F of E is a matching cutset of G if no two edges of F are incident with the same point, and G-F has more components than G. Chvatal [2] proved that it is NP-complete to recognize graphs with a matching cutset even if the input is restricted to graphs with maximum degree 4.
We prove the following: (a) Every connected graph with maximum degree ⩽3 and on more than 7 points has a matching cutset.
(In particular, there are precisely two connected cubic graphs without a matching cutset).
(b) Line graphs with a matching cutset can be recognized in O(|E|) time.
(c) Graphs without a chordless circuit of length 5 or more that have a matching cutset can be recognized in O(|V||E|3) time.

Abstract Hyperspectral image(HSI) classification has been a hot topic in the remote sensing community.
A large number of methods have been proposed for HSI classification.
However, most of them are based on the extraction of spectral feature, which leads to information loss.
Moreover, they rarely consider the correlation among the spectrums.
In this paper, we see spectral information as a sequential data which is relevant with each other.
We introduce long short-term memory model, which is a typical recurrent neural network (RNN), to deal with HSI classification.
In order to solve the problem of difficult to reach the steady state of the model, we proposed a novel guided filter based RNN model.
Also, we proposed a method for modeling hyperspectral sequential data, which is very useful for future research work.
The experimental results show that our proposed method can improve the classification performance as compared to other methods in two popular hyperspectral datasets.

An algorithm is presented for the detection of textured areas in natural images.
Texture detection has potential application to image enhancement, tone correction, defect detection, content classification, and image segmentation.
For example, texture detection may be useful for object detection when combined with color models and other descriptors.
Sky, e.g., is generally smooth, and foliage is textured.
The texture detector presented here is based on the intuition that texture in a natural image is comprised of many components.
The measure we develop examines the structure of local regions of the image.
This structural approach enables us to detect both structured and unstructured textures at many scales.
Furthermore, it distinguishes between edges and texture, and also between texture and noise.
Automatic detection results are shown to match human classification of corresponding image areas.

The MPP Apprentice™ performance tool is designed to help users tune the performance of their Cray T3D, applications.
By presenting performance information from the perspective of the user’s original source code, the MPP Apprentice tool helps users rapidly identify the location and cause of the most significant performance problems.
Unlike many trace-based performance tools, the data collection mechanism permits fine-grained performance statistics to be collected with a low level of intrusion for work sharing, data parallel, and message passing codes.
The low level of intrusion also permits the mechanism to scale to permit performance analysis on long running codes on thousands of processors.
Information displayed within the tool includes total time through regions of code, instruction counts, time spent in overheads related to shared memory, time spent in message passing routines, calling tree information, and performance measures.
We will demonstrate the benefits of the data collection mechanism of the MPP Apprentice tool and how it guides the user’s identification of performance problems using application examples from benchmarks and industry.

This article reviews enhanced wireless access technologies and experimental evaluations of the wideband DS-CDMA physical layer employing intercell asynchronous operation with a three-step fast cell search method, pilot symbol-assisted coherent links, signal-to-interference plus background noise power ratio-based fast transmit power control, site diversity (soft/softer handover), and transmit diversity in the forward link.
The article also presents link-capacity-enhancing techniques such as using an interference canceller and adaptive antenna array diversity receiver/transmitter, and experimental results in a real multipath fading channel.
The laboratory and field experiments exemplify superior techniques of the W-CDMA physical layer and the potential of the IC and AAAD transceiver to decrease the mobile transmit power in the reverse link and multipath interference from high-rate users with large transmit power in the forward link.

The discretization approach for solving semi-infinite optimization problems is considered.
We are interested in the convergence rate of the error between the solution of the semi-infinite problem and the solution of the discretized program depending on the discretization mesh-size.
It will be shown how this rate depends on whether the minimizer is strict of order one or two and on whether the discretization includes boundary points of the index set in a specific way.
This is done for ordinary and for generalized semi-infinite problems.

A new two-point boundary value problem algorithm based upon the MATLAB bvp4c package of Kierzenka and Shampine is described.
The algorithm, implemented in a new package bvp6c, uses the residual control framework of bvp4c (suitably modifled for a more accurate flnite difierence approximation) to maintain a user specifled accuracy.
The new package is demonstrated to be as robust as the existing software, but more e‐cient for most problems, requiring fewer internal mesh points and evaluations to achieve the required accuracy.

Standard bioelectric field models assume that the tissue is purely resistive and frequency independent, and that capacitance, induction, and propagation effects can be neglected.
However, real tissue properties are frequency dependent, and tissue capacitance can be important for problems involving short stimulation pulses.
A straightforward interpolation scheme is introduced here that can account for frequency-dependent effects, while reducing runtime over a direct computation by several orders of magnitude.
The exact Helmholtz solution is compared to several approximate field solutions and is used to study neural stimulation.
Results show that frequency-independent tissue capacitance always acts to attenuate the stimulation pulse, thereby increasing firing thresholds, while the dispersion effects introduced by frequency-dependent capacitance may decrease firing thresholds.

There are several techniques for the analysis of the dynamics of timed Petri nets.
When the net is not decision free, the existing methods come short of an efficient analysis.
In this paper, we present a method, we call "relative temporal analysis", to analyze the dynamics of a system with two processes and a shared resource.
It is possible to find the resource utilization sequence, the waiting time period for each process and identify possible conflicts through this technique.
Based on this technique we also build control charts which can be used for control purposes such as obtaining "optimal" conflict resolution schemes.

This paper is concerned with certain applications of the estimation theory of Fisher and Cramer[1] to the problem of estimating signal parameters in the presence of noise.
Specifically, the situation to be treated is as follows.
A received signal

A robust state-predictive control strategy is proposed for discrete-time multi-input/output systems with non-equal delays on signal buses.
It is based on a separation property between control and prediction.
The input delays are taken into account in the control law synthesis and the output delays are handled using an original steady-state Kalman predictor of order equal to the length of the state of the system without delays.
Robustness, especially against uncertain delays, is improved using a state extension in the predictive gain synthesis.
The classic optimal separation theorem (in the case of an one step predictor) is extended to the general case of a d+1 step prediction for a delayed discrete-time system.

Abstract In this paper, we propose a numerical method for delayed partial differential equations that describe the dynamics of viral infections such as the human immunodeficiency virus (HIV) and the hepatitis B virus (HBV).
We first prove that the proposed numerical method preserves the positivity and boundedness of solutions in order to ensure the well-posedness of the problem.
By constructing appropriate discrete Lyapunov functionals, we show that the proposed method also preserves the global stability of equilibria of the corresponding continuous system with no restriction on the space and time step sizes.
Moreover, the discrete model and main results presented in Qin et al.
(2014) are extended and generalized.

This paper presents a backward state reduction dynamic programming algorithm for generating the exact Pareto frontier for the bi-objective integer knapsack problem.
The algorithm is developed addressing a reduced problem built after applying variable fixing techniques based on the core concept.
First, an approximate core is obtained by eliminating dominated items.
Second, the items included in the approximate core are subject to the reduction of the upper bounds by applying a set of weighted-sum functions associated with the efficient extreme solutions of the linear relaxation of the multi-objective integer knapsack problem.
Third, the items are classified according to the values of their upper bounds; items with zero upper bounds can be eliminated.
Finally, the remaining items are used to form a mixed network with different upper bounds.
The numerical results obtained from different types of bi-objective instances show the effectiveness of the mixed network and associated dynamic programming algorithm.

C-arm fluoroscopy is modelled as a perspective projection, the parameters of which are estimated through a calibration procedure.
It has been universally accepted that precise intra-procedural calibration is a prerequisite for accurate quantitative C-arm fluoroscopy guidance.
Calibration, however, significantly adds to system complexity, which is a major impediment to clinical practice.
We challenge the status quo by questioning the assumption that precise intra-procedural calibration is really necessary.
We derived theoretical bounds for the sensitivity of 3D measurements to mis-calibration.
Experimental results corroborated the theory in that mis-calibration in the focal spot by as much as 50 mm still allows for tracking with an accuracy of 0.5 mm in translation and 0.65o in rotation, and such mis-calibration does not impose any additional error on the reconstruction of small objects.

This paper proposes a fast convergence adaptive algorithm for identifying a sparse impulse response that is rich in spectral content.
A sparse impulse response is referred here as a discrete time impulse response that has a large number of zero or near zero coefficients.
The basic idea for rapid identification is to automatically determine the locations of the nonzero impulse response coefficients for their adaptation and eliminate the unnecessary adaptation of zero coefficients.
The proposed method, which is called the Haar-Basis algorithm, employs a transform approach by modeling the sparse impulse response in the Haar domain.
The Haar transform has many basis sets and each of them contains basis vectors that span the entire time domain range.
This special nature of the Haar transform allows for the selection of one small subset of adaptive filter coefficients whose basis vectors span the entire range of the impulse response.
These coefficients are adapted at the beginning and are then used subsequently to identify, from the hierarchical structure of the Haar transform, the rest of the filter coefficients that must be adapted to correctly model the unknown sparse impulse response.
The consequence is avoiding adaptation of many zero coefficients, leading to a dramatic improvement in either convergence speed or steady state excess mean-square error (EASE), while requiring no a priori knowledge such as the number of nonzero coefficients in the unknown sparse impulse response.
The proposed algorithm has been tested with a variety of unknown sparse systems using both white noise input and colored input whose spectrum closely resembles that of speech.
Simulation results show that the new approach provides promising results.

The graph coloring problem is NP-hard in the set of all graphs.
In this note we combine two problem reductions to provide a collection of hereditary classes of graphs for which the graph coloring problem remains NP-hard when restricted to it.

Applying a benchmarking approach to conflict resolution problems is a hard task, as the analytical form of the constraints is not simple.
This is especially the case when using realistic dynamics and models, considering accelerating aircraft that may follow flight paths that are not direct.
Currently, there is a lack of common problems and data that would allow researchers to compare the performances of several conflict resolution algorithms.
The present paper introduces a benchmarking approach that can provide researchers with common problems, in order to compare the performances of several conflict resolution algorithms.
A comparison between three resolution methods is drawn over several problems and highlights assets and weaknesses for each of them.

Development of high-quality synthesizers is typically dependent on having a large corpus of speech that has an accurate, time-aligned, phonetic transcription.
Producing such transcriptions has been difficult, slow, and expensive.
Here we describe the operation and performance of a new software tool that automates much of the transcription process and requires far less training and expertise to be used successfully.

Pertubations of the human granulocytopoiesis for which many different reasons can be named have been described with the aid of a biomathematical model by Fliedner & Steinbach [1].
Their model consists of a set of coupled nonlinear differential equations describing the time evolution of the cell content of 7 different cell compartments.
The most fundamental of these compartments is the stem cell pool because in case of a pertubation the number of remaining stem cells influences the whole process of recovery.
According to Fliedner & Steinbach only a small amount of surviving stem cells is sufficient for complete restauration.
Thus, it is a very important issue to estimate the stem cell pool size in order to predict the future behaviour of the granulocytopoiesis.
Work towards this direction has already been done by Fliedner, Steinbach and Szepesi [2].

Our system of Mixed Reality and 3D Live with Ambient Intelligence (AmI) is indented to bring performance art to the people while offering to the performance artists a creative tool to extend the grammar of the traditional theatre.
Actors and dancers at different places are captured by multiple cameras and their images are rendered in 3D form in such a way that they can play and dance together on the same place in real-time.
Our Quanticum Man is an allegory of the time of the General Relativity and the matter of the Quantum Mechanics.
The new type of interactive theatre enables social networking by supporting simultaneous participants in human-to-human social manner.

A novel superatom species with 20-electron system, Six Gey M(+) (x + y = 4; M = Nb, Ta), was properly proposed.
The trigonal bipyramid structures for the studied systems were identified as the putative global minimum by means of the density functional theory calculations.
The high chemical stability can be explained by the strong p-d hybridization between transition metal and mixed Si-Ge tetramers, and closed-shell valence electron configuration [1S(2) 1P(6) 2S(2) 1D(10) ].
Meanwhile, the chemical bondings between metal atom and the tetramers can be recognized by three localized two-center two-electron (2c-2e) and delocalized 3c-2e σ-bonds.
For all the doped structures studied here, it was found that the π- and σ-electrons satisfy the 2(N + 1)(2) counting rule, and thus these clusters possess spherically double (π and σ) aromaticity, which is also confirmed by the negative nucleus-independent chemical shifts values.
Consequently, all the calculated results provide a further understanding for structural stabilities and electronic properties of transition metal-doped semiconductor clusters.
© 2016 Wiley Periodicals, Inc.

Error probability analyses are performed for a coded M-ary frequency-shift keying system (MFSK) using L hops per M-ary word frequency-hopping spread-spectrum waveforms transmitted over a partial-band Gaussian noise jamming channel.
The bit error probabilities are obtained for a square-law adaptive gain control receiver with forward-error-control coding under conditions of worst-case partial-band noise jamming.
Both thermal noise and jamming noise are included in the analyses.
Performance curves are obtained for both block codes and convolutional codes with both binary and M-ary channel modulations.
The results show that thermal noise cannot be neglected in the analysis if correct determinations of the optimum order of diversity and the worst-case jamming fraction are to be obtained.
It is shown that the combination of nonlinear combining, M-ary modulation, and forward-error-control coding is effective against worst-case partial-band noise jamming.
>

Jacobi methods for computing the singular value decomposition (SVD) of a matrix are ideally suited for multiprocessor environments due to their inherent parallelism.
In this paper we show how a block version of the two-sided Jacobi method can be used to compute the SVD efficiently on a distributed architecture.
We compare two variants of this method that differ mainly in the degree to which they diagonalize a given subproblem.
The first method is a true block generalization of the scalar scheme in that each off-diagonal block is completely annihilated.
The second method is a scalar Jacobi algorithm reorganized in such a manner that it conforms to the block decomposition of the problem.
We have performed experiments on the Loosely Coupled Array Processor (LCAP) system at IBM Kingston which for the purposes of this article can be viewed as a ring of up to ten FPS-164/MAX array processors.
These experiments show that the block Jacobi algorithm performs well on a distributed system, especially when the processors have vector processing hardware.
As an example, we were able to achieve a sustained performance of 159 MFlops on a 960-by-720 SVD problem using eight processors.
A surprising outcome of these experiments is that the determining factor for the performance of the algorithm on a high-performance architecture is the subproblem solver, not the communication overhead of the algorithm.

Designing a constant false alarm rate detector to manage successfully interfering targets and clutter transitions has presented researchers with some serious design challenges.
One approach has been the application of test cell analysis, using a switching detection mechanism.
Such a process has resulted in detector performance improvement in a number of clutter model scenarios.
Here a switching based detector is developed for targets embedded within Pareto distributed clutter.
It is shown that this new detector can rectify many of the performance issues of detectors introduced recently for target detection in such a clutter environment.

From the point of view of safety applications, accurate and reliable positioning system for road users is the most important part.
Although GPSs are the most widely utilized technology today, they still have the problems of performance degradation caused by multipath propagation in urban canyons.
This study proposes an approach to estimate position by searching around the result of GPS.
The proposed algorithm evaluates the pseudoranges of the possible multipath signals by referring to the building geometry.
The assumed position is estimated by using received pseudoranges and is evaluated by the likelihood of the possible positioning error and filtering algorithm.
The proposed method was verified through field experiments in urban canyons in Tokyo.

Abstract This paper is concerned with the computation of transfer function matrices of linear multivariable systems described by their state-space equations.
The algorithm proposed here performs orthogonal similarity transformations to find the minimal order subsystem corresponding to each input-output pair and uses some determinant identities to determine the elements of the transfer function matrices.
Numerical examples are included to illustrate the performance of the proposed algorithm.

From the Preface: ::: ::: For practicing organic chemists the simple, linear-combination-of-atomic-orbitals (LCAO), molecular-orbital method permits useful calculations of semi-empirical electronic energies of unsaturated molecules with no more than high school algebra.
Anyone who can find the roots of x^4 - 5x^2 + 4x = 0 graphically, analytically, or by successive substitutions can obtain the energy levels and calculate the [Greek pi]-electron energy of bicyclo[1.1.0]butadiene.
::: ::: [figure of bicyclobutadiene] ::: ::: If in addition he can solve x^4 - 4x^2 = 0, then he can compare bicyclobutadiene with cyclobutadiene and predict what changes the 1, 3 bond would make in the a-electron energies.
With no more advanced mathematics, one can compute the bond orders, charge distributions, and reactivity parameters for both free-radical and polar processes.
The results may be crude, but they are often highly suggestive; there is no excuse for a modern organic chemist not to be able to use the LCAO method.
::: ::: The notes that make up this book have been used for many years at the California Institute of Technology to introduce seniors and graduate students to the elements of the simple LCAO method.
A fairly large number of exercises are interspersed in the text to illustrate important points.
It is recommended that these be solved as encountered.
Some of the problems are hoped to be suggestive of possible research problems in the field.
::: ::: These Notes are not intended as a complete course of study and should be supplemented by the reference works listed in the Bibliography.
No attempt has been made to survey the recent literature.
The purpose has been to provide a practical introduction.
As a result no appropriate acknowledgement to either the priority of ideas or to their development has been given.
::: ::: This set of notes would never have been written without the generous contributions of Professor W. G. McMillan and Dr. V. Schomaker to the author's education in the subject matter.
Camera copy was prepared by Mrs. Allene Luke with the aid of Miss Joy Matsumoto.
::: ::: JOHN D. ROBERTS

This chapter discusses current trends and key drivers that affect patient flow and efficiency and analyzes the most common myths of resource allocation for healthcare.
Providers are now finding that simply adding beds or staffs will not solve the commonplace problems of long waits and delays.
Contemporary physical design concepts that improve flow for healthcare are described in this chapter, and seven high-leverage steps that can significantly improve flow and expand capacity, and thus limit delays and waiting, are recommended.

Getting Started with a SIMPLIS Approach is particularly appropriate for those users who are not experts in statistics, but have a basic understanding of multivariate analysis that would allow them to use this handbook as a good first foray into LISREL.
Part I introduces the topic, presents the study that serves as the background for the explanation of matters, and provides the basis for Parts II and III, which, in turn, explain the process of estimation of the measurement model and the structural model, respectively.
In each section, we also suggest essential literature to support the utilization of the handbook.
After having read the book, readers will have acquired a basic knowledge of structural equation modeling, namely using the LISREL program, and will be prepared to continue with the learning process.

Makey Makey is a new platform for improvising tangible user interfaces.
It enables people to make nature-based interfaces, it is compatible with all software, and it does not require the user to program or to assemble electronics.
It is designed for a wide range of audiences, supporting ideation for experts and access for beginners.
In the studio, participants will rapidly create several different user interfaces incorporating a wide variety of found objects, both physical and digital.
There will also be opportunities to test out the newly created interfaces with each other, and reflect on the design of UI prototyping toolkits.

Orthogonality constrained problems are widely used in science and engineering.
However, it is challenging to solve these problems efficiently due to the non-convex constraints.
In this paper, a splitting method based on Bregman iteration is represented to tackle the optimization problems with orthogonality constraints.
With the proposed method, the constrained problems can be iteratively solved by computing the corresponding unconstrained problems and orthogonality constrained quadratic problems with analytic solutions.
As applications, we demonstrate the robustness of our method in several problems including direction fields correction, noisy color image restoration and global conformal mapping for genus-0 surfaces construction.
Numerical comparisons with existing methods are also conducted to illustrate the efficiency of our algorithms.

Finding a global minimizer of the Tikhonov function is in general not an easy task.
Numerical experience shows that the Tikhonov function has usually many local minima and a descent method for solving the optimization problem may tend to get stuck especially for severely ill-posed problems.
Since furthermore, the computation of an appropriate regularization parameter can require high computational effort, iterative regularization methods are an attractive alternative.

Abstract In this article the impact of a soft OR-method on problem structuring is investigated.
A game/simulation was used to test the impact of the method in a quasi-experimental setting.
The game was run several times with representatives from different municipalities.
In three municipalities the method was applied (experimental group) and in the other three municipalities the game/simulation was run without the intervention (control group).
On the basis of different types of measurements hardly any positive effects of the problem-structuring method were found.
A secondary analysis on available data was conducted to explain the little impact of the method.
It shows that the impact of the method is related to the type of problem-structuring process.
The soft OR-method had little impact when conflicts about tasks or interests were dominating, but it had more impact when the problem-structuring process focused on information exchange between participants.

This book is written as an introduction to annotated logics.
It provides logical foundations for annotated logics, discusses some interesting applications of these logics and also includes the authors' contributions to annotated logics.
The central idea of the book is to show how annotated logic can be applied as a tool to solve problems of technology and of applied science.
The book will be of interest to pure and applied logicians, philosophers and computer scientists as a monograph on a kind of paraconsistent logic.
But, the layman will also take profit from its reading.

It is a common problem to compute the derivative of a signal in image processing and computer vision.
So far, in most of the computational methods, the nth order derivative of a noisy signal is obtained by filtering the signal by a nth Order Derivative Filter (NODF), which is the nth order derivative of a smooth filter.
In order to do this, the given smooth filter has to be an Analytic Smooth Function Derivable up to nth order (ASFDN).

Abstract It is well established that seeing color activates the ventral occipital cortex, including the fusiform and lingual gyri, but less is known about whether the region directly relates to conscious color perception.
We investigated the neural correlates of conscious color perception in the ventral occipital cortex.
To vary conscious color perception with the stimuli-remaining constant, we took advantage of the McCollough effect, an illusory color effect that is contingent on the orientation of grating stimuli.
Subjects were exposed to a specific combination of chromatic grating patterns for 10 min to induce the McCollough effect.
We compared brain activities measured while the subjects viewed achromatic grating stimuli before (PRE) and after the induction of the McCollough effect (POST) using functional magnetic resonance imaging (fMRI).
There were two groups: one group was informed that they would perceive illusory color during the session (INFORMED group), whereas the other group was not informed (UNINFORMED group).
The successful induction of the McCollough effect was confirmed in all subjects after the fMRI experiment; nevertheless, only approximately half of the UNINFORMED subjects had been aware of the color during the POST session, while the other half had not.
The left anterior portion of the color-selective area in the ventral occipital cortex, presumably V4α, was significantly active in subjects who had consciously perceived the color during MR scan.
This study demonstrates the activity in a subregion of the color-selective area in the ventral occipital cortex directly related to conscious color perception.

For artificial entities to achieve true autonomy and display complex life-like behaviour they will need to exploit appropriate adaptable learning algorithms.
In this sense adaptability implies flexibility guided by the environment at any given time and an open-ended ability to learn novel behaviours.
This paper explores the potential of using constructivism within the neural classifier system architecture as an approach to realise such behaviour.
The system uses a rule structure in which each is represented by an artificial neural network.
Results are presented which suggest it is possible to allow appropriate internal rule complexity to emerge during learning and that the structure indicates underlying features of the task.

Digital acquisition of the clinical situation in the mouth seems much more logical than digitizing the impressions.
Intraoral scanning systems are laser supported or video based.
The retraction of the gingiva plays a special part in the accurate display of the preparation border.
The working procedures in the dental practice and in the dental laboratory have been changed fundamentally by this new technology.
Workable solutions are already available and rapid further developments can be expected in the near future.

Vasculatures are imperative structures in a human retina and their varied manifestations are often associated with abnormal state of many disorders.
Automatic detection and analysis of these structures assist in diagnosis of many diseases such as diabetes, hypertension and arteriosclerosis.
In this work, human retinal images are analysed using image processing techniques and transform based method.
Normal and abnormal digital fundus images recorded under controlled protocol are employed for the study.
The acquired images are subjected to Slantlet transform and the corresponding zero moments and statistical parameters from those features were derived for analysis.
The derived parameters are correlated with vessel density index to identify vascular density and its variations.
Results demonstrate that Slantlet transform is capable of extracting variations in vascular density in normal and abnormal images.
Varying magnitudes of positive and negative peaks are observed for normal images whereas variations are found to be uniform for abnormal images.
The values of second zero moment, skewness and kurtosis are found to correlate with vessel to vessel free area index.
The correlation values were high for abnormal images than normal images.
It appears that Slantlet transform based study carried out in this work seems to differentiate normal and pathological retina.
As the analysis of retinal vessel features are important for pathological states related with retinal vein occlusion and tortuosity, these studies seems to be clinically relevant.
In this paper, the objectives, methodology, results and the correlation analysis of all the derived parameters are presented in detail.

For the purpose of reducing the degradation of sensitivity performance due to the serious Squaring-Loss in interferometric processing, a differential coherent algorithm for interferometric acquisition of GNSS-R software receiver is explored in this paper.
The sensitivity performance is investigated and compared with the conventional non-coherent algorithm.
Analysis and simulation show that the proposed algorithm offers an obvious sensitivity gain over conventional noncoherent integration.

Abstract.
The National Air Quality Forecast Capability (NAQFC) project provides the US with operational and experimental real-time ozone predictions using two different versions of the three-dimensional Community Multi-scale Air Quality (CMAQ) modeling system.
Routine evaluation using near-real-time AIRNow ozone measurements through 2011 showed better performance of the operational ozone predictions.
In this work, quality-controlled and -assured Air Quality System (AQS) ozone and nitrogen dioxide (NO2) observations are used to evaluate the experimental predictions in 2010.
It is found that both ozone and NO2 are overestimated over the contiguous US (CONUS), with annual biases of +5.6 and +5.1 ppbv, respectively.
The annual root mean square errors (RMSEs) are 15.4 ppbv for ozone and 13.4 ppbv for NO2.
For both species the overpredictions are most pronounced in the summer.
The locations of the AQS monitoring sites are also utilized to stratify comparisons by the degree of urbanization.
Comparisons for six predefined US regions show the highest annual biases for ozone predictions in Southeast (+10.5 ppbv) and for NO2 in the Lower Middle (+8.1 ppbv) and Pacific Coast (+7.1 ppbv) regions.
The spatial distributions of the NO2 biases in August show distinctively high values in the Los Angeles, Houston, and New Orleans areas.
In addition to the standard statistics metrics, daily maximum eight-hour ozone categorical statistics are calculated using the current US ambient air quality standard (75 ppbv) and another lower threshold (70 ppbv).
Using the 75 ppbv standard, the hit rate and proportion of correct over CONUS for the entire year are 0.64 and 0.96, respectively.
Summertime biases show distinctive weekly patterns for ozone and NO2.
Diurnal comparisons show that ozone overestimation is most severe in the morning, from 07:00 to 10:00 local time.
For NO2, the morning predictions agree with the AQS observations reasonably well, but nighttime concentrations are overpredicted by around 100%.

This study investigates the multiple attribute decision making under triangular fuzzy environment in which the attributes and experts are in different priority level.
By combining the idea of quasi arithmetic mean and prioritized weighted average (PWA) operator, we first propose two new prioritized aggregation operators called quasi fuzzy prioritized weighted average (QFPWA) operator and the quasi fuzzy prioritized weighted ordered weighted average (QFPWOWA) operator for aggregating triangular fuzzy information.
The properties of the new aggregation operators are studied in detail and their special cases are examined.
Furthermore, based on the QFPWA operator and QFPWOWA operator, an approach to deal with multiple attribute decision-making problems under triangular fuzzy environments is developed.
Finally, a practical example is provided to illustrate the multiple attribute decision making process.

Cells have developed complex control networks which allow them to sense and response to changes in their environment.
Although they have different underlying biochemical mechanisms, signal transduction units in prokaryotes and eukaryotes fulfill similar tasks, such as switching on or off a required process or amplifying a certain signal.
The growing amount of data available allows the development of increasingly complex models which offer a detailed picture of signaling networks, but the properties of these systems as a whole become difficult to grasp.
A sound strategy to untangle this complexity is a decomposition into smaller units or modules.
How modules should be delimited, however, remains an unanswered question.
We propose that units without retroactive effects might be an interesting criterion.
In this contribution, this issue will be explored through several examples, starting with a simple two-component system in Escherichia coli up to the complex epidermal growth factor signaling pathway in human cells.

A tendon-driven manipulator, the CT Arm, has a specific tendon traction transmission mechanism, in which a pair of tendons for driving a joint is pulled from base actuators via pulleys mounted on the based-side joints.
The mechanism makes the most of the coupled drive function of the tendon tractions and thus enables the lightweight manipulator to exhibit enormous payload capability.
By this tendon-driven mechanism, a multijoint manipulator with super-redundant degrees of freedom could be realized, which is suitable to the maintenance of nuclear reactors.
In this article, we introduce the CT Arm and discuss the possibility of generating a multijoint maintenance manipulator for nuclear reactors, which must have super-redundant degrees of freedom.
A position-coordination approach for a hyper-redundant manipulator to carry tools or inspection equipment passing through a hole to a work location in a nuclear reactor is also proposed.
Computer simulation has been used to show the validity of the hyper-redundant...

The maximum robustness design of envelope-constrained filters with uncertain input (ECUI) is formulated as a minimax optimization problem in which the set of feasible filters is characterized by nonsmooth matrix inequalities.
An efficient design algorithm is developed to solve this problem by use of a newly developed transformation technique.
The attractive feature of this algorithm is that the ECUI filter thus designed can allow for the greatest uncertainty in the input signal while still achieving the acceptable filter performance.

This paper investigates why modem state-space system realization algorithms fail to identify modal models for structures with significant modes near the Nyquist frequency.
We show that several fundamental effects limit the ability of discrete state-space models to approximate the data.
These effects include placing multiple modes where only one mode appears in the data and failing to identify some modes without massive order overspecification.
This is complicated by the fact that modes above the Nyquist frequency significantly affect the frequency response function within the experimental bandwidth, especially near the frequency response function zeroes.
These problems are more prevalent in structures with high modal density, especially when they contain significant modes near the Nyquist frequency.

Popular conventional wisdom tells us that the prevalence of projects is on the increase and anyone can observe that the society that surrounds us is already “projectified”, at least in a partial sense.
However, at the present time there are very few macro oriented and measurable variables available to verify or invalidate such a statement.
In this chapter we therefore delve into thoughts that might be useful in conceptualising project aspects of society.
We aspire to foundations for an “outside” theory of temporary organisations.
We do that by attempting to combine a population ecology approach with notions of institutional processes.

In this paper we present a new paradigm for the generation and retargeting of facial animation.
Like a vast majority of the approaches that have adressed these topics, our formalism is built on blendshapes.
However, where prior works have generally encoded facial geometry using a low dimensional basis of these blendshapes, we propose to encode facial dynamics by looking at blendshapes as a basis of forces rather than a basis of shapes.
We develop this idea into a dynamic model that naturally combines the blendshapes paradigm with physics-based techniques for the simulation of deforming meshes.
Because it escapes the linear span of the shape basis through time-integration and physics-inspired simulation, this approach has a wider expressive range than previous blendshape-based methods.
Its inherent physically-based formulation also enables the simulation of more advanced physical interactions, such as collision responses on lip contacts.

The field of Semantic Web Services SWS has been recognized as one of the most promising areas of emergent research within the Semantic Web initiative, exhibiting an extensive commercial potential and attracting significant attention from both industry and the research community.
Currently, there exist several different frameworks and languages for formally describing a Web Service: Web Ontology Language for Services OWL-S, Web Service Modelling Ontology WSMO and Semantic Annotations for the Web Services Description Language SAWSDL are the most important approaches.
To the inexperienced user, choosing the appropriate platform for a specific SWS application may prove to be challenging, given a lack of clear separation between the ideas promoted by the associated research communities.
In this paper, we systematically compare OWL-S, WSMO and SAWSDL from various standpoints, namely, that of the service requester and provider as well as the broker-based view.
The comparison is meant to help users to better understand the strengths and limitations of these different approaches to formalizing SWS, and to choose the most suitable solution for a given application.
Copyright © 2015John Wiley & Sons, Ltd.

A finite undirected graph is called chordal if every simple circuit has a chord.
Given a chordal graph, we present, ways for constructing efficient algorithms for finding a minimum coloring, a minimum covering by cliques, a maximum clique, and a maximum independent set.
The proofs are based on a theorem of D. Rose [3] that a finite graph is chordal if and only if it has some special orientation called an R-orientation.
In the last part of this paper we prove that an infinite graph is chordal if and only if it has an R-orientation.

Semi-supervised or unsupervised, incremental learning approaches based on online boosting are very popular for object detection.
However, in the course of online learning, since the positive examples labelled by the current classifier may actually not be ''correct'', the optimal weak classifier is unlikely to be selected by previous approaches.
This would directly lead to a decline in algorithm performance.
In this paper, we present an improved online multiple instance learning algorithm based on boosting (called OMILBoost) for object detection.
It can pick out the real correct image patch around labelled example with high possibility and thus, avoid drifting problem effectively.
Furthermore, our method shows high performance when dealing with partial occlusions.
Effectiveness is experimentally demonstrated on six representative video sequences.

&The present study investigated the effects of question prompts and online peer collaborations on solving illstructured problems.
Sixty undergraduate students were randomly assigned to one of the four treatment groups: collaboration with question prompts, individual with question prompts, collaboration without question prompts, and individual without question prompts.
They were asked to solve real-world ill-structured problems in a case scenario.
The results revealed significant effects of question prompts in ill-structured problem solving at both overall and univariate levels.
However, there was no significant effect of online peer collaboration and no significant interaction.
This study has implications for instructional designers and educators in designing collaborative learning activities with technology support.

As the demand for higher bandwidth has lead to the development of increasingly complex wireless technologies, an understanding of both wireless networking technologies and radio frequency (RF) principles is essential for implementing high performance and cost effective wireless networks.Wireless Networking Technology clearly explains the latest wireless technologies, covering all scales of wireless networking from personal (PAN) through local area (LAN) to metropolitan (MAN).
Building on a comprehensive review of the underlying technologies, this practical guide contains ‘how to’ implementation information, including a case study that looks at the specific requirements for a voice over wireless LAN application.
This invaluable resource will give engineers and managers all the necessary knowledge to design, implement and operate high performance wireless networks.

Abstract F. Cohen raised the following question: Determine or estimate a function F(d) so that if we split the integers into two classes at least one class contains, for infinitely many values of d, an arithmetic progression of difference d and length F(d).
We prove F(d) ⩽ (1 + e) log2 d.

Abstract We present a tutorial overview showing how one can achieve scalable performance with R. We do so by utilizing several package extensions, including those from the pbdR project.
These packages consist of high performance, high-level interfaces to and extensions of MPI, PBLAS, ScaLAPACK, I/O libraries, profiling libraries, and more.
While these libraries shine brightest on large distributed platforms, they also work rather well on small clusters and often, surprisingly, even on a laptop with only two cores.
Our tutorial begins with recommendations on how to get more performance out of your R code before considering parallel implementations.
Because R is a high-level language, a function can have a deep hierarchy of operations.
For big data, this can easily lead to inefficiency.
Profiling is an important tool to understand the performance of an R code for both serial and parallel improvements.
The pbdR packages provide a highly scalable capability for the development of novel distributed data analysis algorithms.
This level of scalability is unmatched in other analysis software.
Interactive speeds (seconds) are achieved for complex analysis algorithms on data 100 GB and more.
This is possible because the interfaces add little overhead to the scalable libraries and their extensions.
Furthermore, this is often achieved with little or no change to serial R codes.
Our overview includes codes of varying complexity, illustrating reading data in parallel, the process of changing a serial code to a distributed parallel code, and how to engage distributed matrix computation from within R.

Abstract ::: A specific oligonucleotide has been used to isolate a cDNA prepared from the mRNA for a trout High Mobility Group (HMG) protein closely related to trout HMG-T and bovine HMG 1 and 2 proteins.
The sequence isolated more closely resembles bovine HMG-1 than the previously sequenced HMG-T protein in regions corresponding to the N terminal half of the protein.
Northern blot analysis at low stringency indicated that 2 related sequences are expressed in total trout testis mRNA.
Southern blots of total trout DNA indicate that several different forms of the homologous sequence are present in the trout genome and an estimate of copy number by dot-blot shows 4 HMG-T genes per trout sperm DNA equivalent.
Analysis of mRNA from several trout tissues, including testis, liver and kidney indicates that expression of genes for histones and the larger HMG proteins in trout is not closely coupled.

Abstract The paper describes the foundations of a methodology for natural language-based knowledge acquisition.
It concentrates on a special type of context: the case in which an analyst interviews an informant who is a domain expert and the text of the discussion is carefully recorded.
In this context the following paradox arises: the analyst is after knowledge, but all he gets are words.
Matching concepts to the words—or, more precisely, constructing conceptual structures which model the mental models of the informant—is the task of the analyst.
The conceptual structures are to be specified as sets of conceptual graphs.
To carry out this task, the clear specification of the domain of discourse in terms of an ontology and an inventory becomes necessary.
The discourse is considered to include not only the text of the discussion between the analyst and the informant, but also the ever-changing mental models of both parties.
The mental models are construed as modelling some object domain “out there”, but the domain of discourse is created through discourse.
A step-by-step technique is given for specifying the domain of discourse with careful attention paid to version control.
It is noted that different interviews about the “same” object domain may give rise to several different domains of discourse.

In this paper, I will argue that the process of ageing in scientific publications on the one hand, and the process of obsolescence and forgetting to which all kinds of phenomena, people and events are exposed on the other, develop with the same speed.
Whereas in the literature on the subject it is stated that the speed of the ageing of scientific literature is exponential, it is shown that the decay from 'age 4' is best described by an inverse function, as was already brought to light in reference to forgetting of people and events as measured by the frequencies of calendar years in large text corpora.
The empirical bases are SCI data as presented by Nakamoto and various files of reference data collected by the author.
It is shown that the decay curve of the reference frequencies from 'age 4' backwards is independent of time.

We harmonize many time-complexity classes DTIMEF ( f ( n )) ( f ( n ) [ges ] n ) with the PR functions (at and above the elementary level) in a transfinite hierarchy of classes of functions [Tscr ] α .
Class [Tscr ] α is obtained by means of unlimited operators, namely: a variant Π of the predicative or safe recursion scheme, introduced by Leivant, and by Bellantoni and Cook, if α is a successor; and constructive diagonalization if α is a limit.
Substitution ( SBST ) is discarded because the time complexity classes are not closed under this scheme.
[Tscr ] α is a structure for the PR functions finer than [Escr ] α , to the point that we have [Tscr ] e 0 = [Escr ] 3 (elementary functions).
Although no explicit use is made of hierarchy functions, it is proved that f ( n ) ∈ [Tscr ] α implies f ( n ) [les ] n G α ( n ) , where G α belongs to the slow growing hierarchy (of functions) studied, in particular, by Girard and Wainer.

Propose a hierarchical trajectory clustering framework for periodic pattern mining.Propose a trajectory clustering approach that considers additional semantics.Extend the proposed clustering to take into account the sequence of trajectory.Overcome the drawbacks of traditional periodic pattern mining.Provide experimental results to demonstrate the versatility of proposed framework.
Spatio-temporal periodic pattern mining is to find temporal regularities for interesting places.
Many real world spatio-temporal phenomena present sequential and hierarchical nature.
However, traditional spatio-temporal periodic pattern mining ignores the consideration of sequence, and fails to take into account inherent hierarchy.
This paper proposes a hierarchical trajectory clustering based periodic pattern mining that overcomes the two common drawbacks from traditional approaches: hierarchical reference spots and consideration of sequence.
We propose a new trajectory clustering algorithm which considers semantic spatio-temporal information such as direction, speed and time based on Traclus and present comparative experimental results with three popular clustering methods: Kernel function, Grid-based, and Traclus.
We further extend the proposed trajectory clustering to hierarchical clustering with the use of the single linkage approach to generate a hierarchy of reference spots.
Experimental results reveal various hierarchical periodic patterns, and demonstrate that our algorithm outperforms traditional reference spot detection algorithms.

We introduce a new cryptographic primitive, called proxy re-encryption with keyword search, which is motivated by the following scenario in email systems: Charlie sends an encrypted email, which contains some keywords, such as ''urgent'', to Alice under Alice's public key, and Alice delegates her decryption rights to Bob via her mail server.
The desired situations are: (1) Bob can decrypt mails delegated from Alice by using only his private key, (2) Bob's mail gateway, with a trapdoor from Bob, can test whether the email delegated from Alice contains some keywords, such as ''urgent'', (3) Alice and Bob do not wish to give the mail server or mail gateway the access to the content of emails.
The function of proxy re-encryption with keyword search (PRES) is the combination of proxy re-encryption (PRE) and public key encryption with keyword search (PEKS).
However, a PRES scheme cannot be obtained by directly combining those two schemes, since the resulting scheme is no longer proven secure in our security model.
In this paper, a concrete construction is proposed, which is proven secure in the random oracle model, based on the modified Decisional Bilinear Diffie-Hellman assumption.

To decide which norms can be removed from a system, we need to know when a norm is redundant.
After shifting the focus of attention in deontic logic from detachment of obligations and permissions to deontic redundancy, I discuss in this paper five benchmark examples of deontic redundancy in reasoning about permissions, intermediate concepts and constitutive norms, deontic dilemmas, temporal deontic reasoning and contrary-to-duty reasoning.
Then I discuss those benchmark examples in four formal approaches to deontic reasoning: traditional model logic, dynamic approaches, violation oriented or diagnostic systems, and imperativist or norm based approaches.

E-procurement has become an important function of enterprise information systems.
The process of e-procurement includes the automatic definition of product requirements, search and selection for suppliers, negotiation and contracting with suppliers.
However, the adoption of e-procurement encounters various uncertainties from internal and external environments, such as inventory failure, sharp increased demand, and delivery delay.
In this paper, we propose a novel agent-based architecture for an e-procurement system in which agent technology is applied to deal with the internal and external uncertainties.
Through the collaboration and interaction between different agents, the architecture that we propose can enhance the flexibility to handle unexpected exceptions, thus leading to agile procurement management.
To valid the feasibility of our approach, a case study has been conducted to investigate how our agents collaborate to manage the inventory failure exception which occurs in restaurant e-procurement.

Contents: E.A.
Fleishman, Introduction.
W.C. Howell, An Overview of Models, Methods, and Problems.
B.H.
Kantowitz, Interfacing Human Information Processing and Engineering Psychology.
R.C.
Williges, Applying the Human Information Processing Approach to Human-Operator/Computer Interactions.
D.A.
Lane, Limited Capacity, Attention Allocation, and Productivity.
P. Slovic, Toward Understanding and Improving Decisions.

A Bloom filter is a compact data structure that supports membership querie s on a set, allowing false positives.
The simplicity and the excellent performance of a Bloom filter make it a standard data str ucture of great use in many network applications.
In reducing the false positive rate of a Bloom filter, it is well known that the size of a Bloom filter and accordingly the number of hash indices should be increased.
In this paper, we propose a new architecture reducing the false positive rate of a Bloom filter more efficiently.
The proposed architecture uses cross-checking Bloom filters that are queried in case of positives of a main Bloom filter to cross-check the results.
If every cross-checking Bloom filters produce negatives , the positive of the main Bloom filter can be determined as a false positive.
The main Bloom filter is not necessarily large to reduce the false p ositive rate, since more numbers of the false positives of the main Bloom filter are identified by cross-checking Bloom filters.
Simulation re sults show that the false positive of the proposed scheme converges to zero faster, while requiring the total memory size for Bloom filters smaller, than that of a single Bloom filter architecture.

To deliver sample estimates provided with the necessary probability foundation to permit generalization from the sample data subset to the whole target population being sampled, probability sampling strategies are required to satisfy three necessary not sufficient conditions: 1) All inclusion probabilities be greater than zero in the target population to be sampled.
If some sampling units have an inclusion probability of zero, then a map accuracy assessment does not represent the entire target region depicted in the map to be assessed.
2) The inclusion probabilities must be: a) knowable for nonsampled units and b) known for those units selected in the sample: since the inclusion probability determines the weight attached to each sampling unit in the accuracy estimation formulas, if the inclusion probabilities are unknown, so are the estimation weights.
This original work presents a novel (to the best of these authors' knowledge, the first) probability sampling protocol for quality assessment and comparison of thematic maps generated from spaceborne/airborne very high resolution images, where: 1) an original Categorical Variable Pair Similarity Index (proposed in two different formulations) is estimated as a fuzzy degree of match between a reference and a test semantic vocabulary, which may not coincide, and 2) both symbolic pixel-based thematic quality indicators (TQIs) and sub-symbolic object-based spatial quality indicators (SQIs) are estimated with a degree of uncertainty in measurement in compliance with the well-known Quality Assurance Framework for Earth Observation (QA4EO) guidelines.
Like a decision-tree, any protocol (guidelines for best practice) comprises a set of rules, equivalent to structural knowledge, and an order of presentation of the rule set, known as procedural knowledge.
The combination of these two levels of knowledge makes an original protocol worth more than the sum of its parts.
The several degrees of novelty of the proposed probability sampling protocol are highlighted in this paper, at the levels of understanding of both structural and procedural knowledge, in comparison with related multi-disciplinary works selected from the existing literature.
In the experimental session, the proposed protocol is tested for accuracy validation of preliminary classification maps automatically generated by the Satellite Image Automatic Mapper (SIAM™) software product from two WorldView-2 images and one QuickBird-2 image provided by DigitalGlobe for testing purposes.
In these experiments, collected TQIs and SQIs are statistically valid, statistically significant, consistent across maps, and in agreement with theoretical expectations, visual (qualitative) evidence and quantitative quality indexes of operativeness (OQIs) claimed for SIAM™ by related papers.
As a subsidiary conclusion, the statistically consistent and statistically significant accuracy validation of the SIAM™ pre-classification maps proposed in this contribution, together with OQIs claimed for SIAM™ by related works, make the operational (automatic, accurate, near real-time, robust, scalable) SIAM™ software product eligible for opening up new inter-disciplinary research and market opportunities in accordance with the visionary goal of the Global Earth Observation System of Systems initiative and the QA4EO international guidelines.

As indicated in Section 2.5, today’s Advanced Planning Systems provide sophisticated planning functionality.
However, since APS providers are not willing to disclose the used lot-sizing and scheduling models nor the applied solution methods, the planner can barely know how the results are produced.

Abstract The essence of interactivity is exchange.
Degree of contingency and frequency are among the more important dimensions of a marketing exchange.
Technology enables and enhances these dimensions on a scale and scope unprecedented in human history.
The author argues that information technology and the collaborative potential of the Internet may eventually change human cognitive processes as much or more than writing technology did.
He offers anecdotal evidence that technology-assisted human collaboration can systematically lead to synergetic, “previously unthinkable,” (and therefore unpredictable) breakthroughs.
While it is therefore not reasonable to expect accurate predictions of what the breakthroughs in interactive marketing and “e-business” will be, it is possible to think about ways to discover them earlier.
Two areas of research are suggested as being of special interest: whether or not the collaborative potential of technology is likely to transform business-as-games-against-competitors into business-as-games-with-customers; and what are the necessary and sufficient conditions for interactive technology to lead to breakthrough creativity.

Abstract Fuzzy neural network (FNN) can be trained with crisp and fuzzy data.
This paper presents a novel approach to solve system of fuzzy differential equations (SFDEs) with fuzzy initial values by applying the universal approximation method (UAM) through an artificial intelligence utility in a simple way.
The model finds the approximated solution of SFDEs inside of its domain for the close enough neighborhood of the fuzzy initial points.
We propose a learning algorithm from the cost function for adjusting of fuzzy weights.
At the same time, some examples in engineering and economics are designed.

Abstract A parallel localized algorithm to calculate the Euler number of a given binary image on a square grid, is presented.
The algorithm is proven by employing graph theory.
Examples are given to show the validity of the algorithm.

Variational interpolation in curved geometries has many applications, so there has always been demand for geometrically meaningful and efficiently computable splines in manifolds.
We extend the definition of the familiar cubic spline curves and splines in tension, and we show how to compute these on parametric surfaces, level sets, triangle meshes, and point samples of surfaces.
This list is more comprehensive than it looks, because it includes variational motion design for animation, and allows the treatment of obstacles via barrier surfaces.
All these instances of the general concept are handled by the same geometric optimization algorithm, which minimizes an energy of curves on surfaces of arbitrary dimension and codimension.

Channel measurements and modelling have been long considered as the foundation for effective and efficient wireless communication system designs.
Recently, there has been an explosive growth of research work dedicated to the so-called device-to-device (D2D) communications.
In the mean time, however, measurements and modelling of D2D channels seem to somewhat fall behind.
To promote research on these aspects, in this study, the authors provide a critical overview of the current state of research on D2D channels, and comprehensively discuss future trends and research directions.

An estimation method of pseudo-random (PN) codes in the periodic long code direct sequence spread spectrum signals using a pair of spreading code and scrambling code [i.e.
long scrambling code direct sequence spread spectrum (LSC-DSSS)] is investigated in this study.
Via the investigation of properties of triple correlation function (TCF) of m-sequences, the existence of common peaks in the TCFs of different m-sequences is proved, and the corresponding relationship between common peaks and primitive polynomials is further investigated.
Four theorems are proposed as supplements of triple correlation theory and a novel estimation algorithm of the PN codes in LSC-DSSS signals is put forward on the basis of the theorems.
With certain carrier frequency and chip rate of spreading code, this algorithm first eliminates the influence of information codes through delay-and-multiply operation.
Then the TCF of signal is calculated, and the two PN codes in signal are successfully estimated finally by searching and using the common peak coordinates in the TCF.
Simulation results show that the proposed algorithm exhibits excellent performance in estimating PN codes in LSC-DSSS signals.

Schedulers play a vital role in the design of distributed algorithms.
This paper introduces priority scheduling that can be used to design as well as reason about correctness proofs at a higher level of abstraction.
The application of priority scheduling in solving problems related to stabilization and fault containment in spanning tree generation is presented.
This paper also presents an implementation of the priority scheduling scheme.

A survey is presented of techniques for verifying correctness properties of communications protocol design based on finite-state-machine (FSM) models.
The conventional reachability analysis is first described, giving advantages and limitations.
One major limitation is the so-called state-space explosion problem.
To approach this and other problems, a survey of different approaches is presented.
The author classifies the various techniques into categories.
These include closed covers, localized protocol verification, divide-and-conquer, modified reachability analysis, and partial state exploration.
Each technique is described in detail, including an analysis of its strengths and weaknesses.
Based on this analysis, one technique is proposed as the basis for further work on a protocol verifier and analyzer workstation which is being designed as a protocol development tool.
>

This paper examines the achievable capacity of a direct-sequence spread-spectrum multiple-access system based on an adaptive minimum mean-square-error (MMSE), single-user receiver over a Gaussian channel.
The effect of using both trellis and convolutional codes of varying rates and complexity is investigated in relation to the effect on the maximum number of users and the achievable bit-error rate for a given number of users.
A system based on a matched filter receiver is presented for the purposes of comparison.
It has been found that, over the band-limited channel considered, the use of convolutional coding reduces the total system capacity for the high efficiency MMSE receiver while still providing a coding gain for a small numbers of users.
The use of trellis codes however leads to increased system capacity.

The nature of the multivariate relationship between six characteristics of planning systems and three different conceptualizations of planning effectiveness is examined using canonical correlation analysis.
The analysis indicates that the organizational context of planning—captured here by two key characteristics—resistance to planning and resources provided for planning—is the dominant impact on planning system effectiveness, broadly construed.
While individual design elements of the planning system such as use of techniques and external orientation do influence effectiveness, the contextual dimensions appear to be overriding.
Implications for further research on planning systems and directions for future extensions of this study are discussed.

In this paper we present a trust model for an enhanced version of MR2 micropayment scheme.
We named this scheme TMR2.
TMR2's light-weight trust model is based on user polling, sales volume and vendor's reputation.
We claim that TMR2 can solve the mind barrier problem, which will result in the expansion of micropayment usage.
The new token proposed to handle trust is a certificate called Rating certificate.
We propose a new Rating certificate for merchants to support users' trust.
Our proposed purchase process needs validation of this certificate, therefore, compared to MR2, the proposed scheme needs only one extra on-line digital signature validation.
This validation adds less than 4 percent computational overhead in the purchase process, which is acceptable, considering the benefits that users will gain.

This paper examines how one can use Riemann-Liouville fractional Brownian motion (in contrast to complex-valued fractional Brownian motion) to take account of randomness in some biological systems, and the kind of results one may expect to obtain from this.
Loosely speaking, the Hurst parameter of the fractional Brownian motion appears to be quite suitable for describing the aggressiveness of some biological processes.
After a summary on fractional calculus completed by a new derivation of Taylor's series of fractional order, we propose a new solution for the stochastic differential equation of the Malthusian growth model with fractional random growth rate, and apply it to the stability analysis of some nonlinear systems (the logistic law of growth, for instance).
Then we derive, as a discrete space model, the equation for the birth-and-death process of fractional order; then its companion Poissonian process (of fractional order) is considered in a fully detailed way, including the fractional partial differential equation of its generating function, which is solved by using a new technique.
Lastly, we consider a model of optimal management of two species populations in the presence of fractal noises, which is an application of stochastic optimal control in the presence of fractional noises.
We show how one can solve this problem by using the Lagrange variational approach applied to the dynamical equations of the state moments of the system.

A scroll chaotic system containing a HP memristor model and triangular wave sequence is proposed in this article.
Because the memristor is both a nonlinear element and a memory element intrinsically, it is considered a potential candidate to reduce system power consumption and circuit size.
A reasonable mathematical structure of triangular wave sequence and the selection of appropriate amplitude, balance point and turning point reduce the dynamic range of signal input caused by the integrator.
The proposed system produces a wealth of chaos, just by changing one parameter.
Circuit simulations are conducted and the chaotic attractors can be observed.
Theoretical analysis, computer simulation and calculation of maximum Lyapunov exponent have been used to research the basic dynamics of this system.
The consistency of circuit implementation and computer simulations verifies the effectiveness of the system design.

The authors develop a mathematical model of the effects of synaptic arithmetic noise in multilayer perceptron training.
Predictions are made regarding enhanced fault-tolerance and generalization ability and improved learning trajectory.
These predictions are subsequently verified by simulation.
The results are perfectly general and have profound implications for the accuracy requirements in multilayer perceptron (MLP) training, particularly in the analog domain.
>

Multiple applications executing concurrently on a multicore system interfere with each other at different shared resources such as main memory and shared caches.
Such inter-application interference, if uncontrolled, results in high system performance degradation and unpredictable application slowdowns.
While previous work has proposed application-aware memory scheduling as a solution to mitigate inter-application interference and improve system performance, previously proposed memory scheduling techniques incur high hardware complexity and unfairly slowdown some applications.
Furthermore, previously proposed memory-interference mitigation techniques are not designed to precisely control application performance.
::: This dissertation seeks to achieve high and controllable performance in multicore systems by mitigating and quantifying the impact of shared resource interference.
First, towards mitigating memory interference and achieving high performance, we propose the Blacklisting memory scheduler that achieves high performance and fairness at low complexity.
Next, towards quantifying the impact of memory interference and achieving controllable performance in the presence of memory bandwidth interference, we propose the Memory Interference induced Slowdown Estimation (MISE) model.
We propose and demonstrate two use cases that can leverage MISE to provide soft performance guarantees and high overall performance/fairness.
Finally, we seek to quantify the impact of shared cache interference on application slowdowns, in addition to memory bandwidth interference.
Towards this end, we propose the Application Slowdown Model (ASM).
We propose and demonstrate several use cases of ASM that leverage it to provide soft performance guarantees and improve performance and fairness.

Abstract A face sculpturing robot system has been developed that obtains the 3D contour of a person's face using projection type multislit beam topography, so that the person feels comfortable and can remain still while being measured.
The facial features of the person are extracted from the contour of the face using a model face.
All the facial features are defined a priori for the model face.
Then, the recognized face data is affine transformed at the feature level to show different facial expressions.
From the transformed data, a robot program sculpturing the person's face is automatically generated with different cutting conditions for individual features.
The face sculpturing robot system conveys the concept of cad/cam integration to the public effectively in the friendly context of sculpturing a human face.

Abstract In this paper a novel 3D electro-thermal simulator for high power devices operating in avalanche condition will be presented.
The proposed solution is based on two coupled systems: a 3D-FEM thermal simulator and a 2D electrical solver.
The simulator has been implemented in MATLAB language.
It is capable of simulating a large number of macro-cells composing a wide-area power devices operating in avalanche condition.
The electrical solver uses a SPICE-like algorithm with a look-up-table description for every cell.
The thermal problem is solved by a finite element method (FEM) in an iterative scheme with the electrical simulator.
In order to prove the effectiveness of the simulator, we will present electro-thermal simulations in Unclamped Inductive Switching (UIS) conditions for a high power Trench-IGBT.

By a systems approach to management is meant the study of a firm in its totality so that the men and material resources of the firm can be organized to realize the firm's overall objectives as efficiently as possible.
This approach is now becoming essential because of the growth of complexity of firms and the increasing potential of automatic computers.
The systems approach to management implies that every manager should be much more precise about decision-making and information flow.
For this to be effective, a company should have an overall system of corporate objectives.
First a detailed systems study will be necessary to decide on the best objectives and then subsidiary systems must be set up to realize these objectives as efficiently as possible.
This paper is directed at senior management and is a simple attempt to emphasize the urgent need to apply the systems approach, both in government and industry.

Multilevel and analogue memory cells have the capability of storing considerably more than one bit of information per cell.
The number of levels the memory cells can store is limited by noise and bit error probability.
The authors propose using error correction coding and digital modulation techniques used in communication systems to increase the storage capacity of multilevel and analogue memory cells.
They evaluate these techniques and find that compared to the methods that do not use coded modulation techniques, typically, one extra bit per cell (i.e.
effectively doubling the number of levels per cell) in storage capacity can be achieved.
The techniques can also be applied to improve readout bit-error probability and provide unequal error protection for the bits to be stored.
The latter technique is important for the efficient storage of digitised multimedia signals such as speech, audio, image and video signals.
One- and two-dimensional coded modulation schemes as well as uncoded multilevel digital modulation schemes are presented in detail.
Complexity and speed issues are also considered.

Abstract MapReduce is a popular parallel programming model used to solve wide range of BigData applications in cloud computing environment.
Hadoop is an open source implementation MapReduce and widely used by vast amount of users.
It provides an abstracted environment for running large scale data intensive applications in a scalable and fault tolerant manner.
There are several Hadoop scheduling algorithms are proposed in the literature with various performance goals.
In this paper, a new optimal task selection scheme is introduced in to assist the scheduler when multiple local tasks are available for a node.
To improve the probability of percentage of local tasks launched for a job in future, the task which has least number of replicas of input, individual load of disks attached to the node and maximum expected time to wait for next local node is launched among the available local tasks for a node.
The proposed method was evaluated by extensive experiments and it has been observed that the method improves the performance significantly.
From the experiments, around 20% of improvements achieved in terms of locality and fairness.

The authors present a switch level test generation system called SWiTEST.
SWiTEST deals with bridging, breaking, stuck-open/on and stuck-at-faults.
It employs both logic and current monitoring and takes into account the invalidation problem associated with stuck-open tests.
The framework for SWiTEST is based on the PODEM algorithm.
Some experimental results are presented and discussed.
The experimental results imply that switch level test generation can be done in CPU time that is within an order of magnitude of that required for gate level test generation.
>

A new method for planning the routes of a fleet of carriers subject to a maximum load restriction is outlined.
It is derived from a combination of the well-known "savings" heuristic rule and Monte Carlo simulation.
Without increasing the level of complexity of the search routine beyond that already employed in "savings" based programs a marked reduction in total route length can be obtained.
This is demonstrated with the aid of three much-considered problems, and for one of these, distances have been found that are below any previously recorded, even for those algorithms with the support of more elaborate logistics.

The encoding of a VQ-based image coding requires a full codebook search for each input vector to find out the best matched codeword.
It is a time consuming process.
A fast algorithm for vector quantising image data is proposed.
The algorithm is proved powerful.

This paper describes modeling, simulation, algorithm, and flight validation activities done to explore the feasibility and highlight the issues with precision cargo delivery from an unmanned helicopter onto or off-of a moving platform.
The challenge is to precisely control a slung load under a helicopter with a high enough bandwidth to allow for the rendezvous with a stochastically accelerating ground target.
To aide this effort, models are developed of the slung load and the moving platform.

Artifacts in engineering design are structurally complex and may be represented in software as recursively composite objects.
Due to the evolutionary nature of the design process each artifact and its components may evolve through several versions.
The paper describes enhanced database system facilities that are used to group mutually consistent component versions together into useful configurations.
The versioning system includes integrity management facilities that allow evolving design constraints to be captured flexibly at individual component/object level.
In order to permit evolution, integrity constraints are represented within versionable objects, so-called constraint version objects (CVOs).
Inter-dependency constraints can be modelled to express the consistency semantics necessary to combine component object versions into useful configurations.
The evolution of these configurations can be captured in the database, as configurations are also represented as versioned objects.
As a result of the hierarchical structure of composite artifact objects, subconfigurations may be combined to form higher-level configurations.
To overcome the difficulties associated with combining object versions and subconfigurations into meaningful configurations, the computer system provides a graphical user interface which greatly eases the task of the designer.
Selection of the mutually consistent object versions making up a configuration and the associated integrity validation is facilitated by the system, thus removing the need for the designer to become involved in any form of computer programming.

UDDI is a standard for publishing and discovery of web services.
UDDI registries provide keyword searches for web services.
The search functionality is very simple and fails to account for relationships between web services.
In this paper, we propose an algorithm which retrieves closely related web services.
The proposed algorithm is based on singular value decomposition (SVD) in linear algebra, which reveals semantic relationships among web services.
The preliminary evaluation shows the effectiveness and feasibility of the algorithm.

Solving optimally large instances of combinatorial optimisation problems using Branch and Bound (B&B) algorithms is CPU-time intensive and requires a large number of computational resources.
To harness such huge amount of resources Peer-to-Peer (P2P) communications must be allowed between resources, and adaptive load balancing and fault-tolerance have to be dealt with when designing and implementing a B&B algorithm.
In this paper, we propose a P2P design and implementation of a parallel B&B algorithm on top of the ProActive grid middleware.
Load distribution and fault-tolerance strategies are proposed to deal with the dynamic and heterogeneous characteristics of the computational grid.
The approach has been promisingly applied to the Flow-Shop scheduling problem and experimented on a computational pool of 1500 CPUs from the GRID'5000 Nation-wide experimental Grid.

This article introduces corrigenda and addenda for tolerance near sets and image correspondence (Peters, 2009).
The principal problem considered in this article is how to solve the image correspondence problem using a bio-inspired approach in the study of representative spaces (inspired by J.H.
Poincare's work during the 1890s) and tolerance spaces (introduced by E.C.
Zeeman during the 1960s).
One solution to this problem is to consider a tolerance space form of near sets that model human perception.
Near sets are disjoint sets that resemble each other, especially resemblance defined within perceptual representative spaces (a.k.a., tolerance spaces).
The contribution of this article is threefold.
First, corrigenda and addenda for the original IJBIC article are presented.
Second, similarities between digital images are viewed within the context of perceptual representative spaces introduced in this article.
Third, an approach to quantifying the nearness of digital images is shown using the Henry-Peters nearness measure.

Abstract This paper presents a Kalman filter approach for accurately estimating the 3-D position and orientation of a moving object from a sequence of stereo images.
Emphasis is given to finding a solution for the following problem incurred by the use of a long sequence of images: the images taken from a longer distance suffer from a larger noise-to-signal ratio, which results in larger errors in 3-D reconstruction and, thereby, causes a serious degradation in motion estimation.
To this end, we have derived a new set of discrete Kalman filter equations for motion estimation: (1) The measurement equation is obtained by analyzing the effect of white Gaussian noise in 2-D images on 3-D positional errors (instead of directly assigning Gaussian noise to 3-D feature points) and by incorporating an optimal 3-D reconstruction under the constraints of consistency satisfaction among 3-D feature points.
(2) The state propagation equation, or the system dynamic equation, is formulated by describing the rotation between two consecutive 3-D object poses, based on quaternions and representing the error between the true rotation and the nominal rotation (obtained by 3-D reconstruction) in terms of the measurement noise in 2-D images.
Furthermore, we can estimate object position from the estimation of object orientation in such a way that an object position can be directly computed once the estimation of an object orientation is obtained.
Simulation results indicate that the Kalman filter equations derived in this paper represent an accurate model for 3-D motion estimation in spite of the first-order approximation used in the derivation.
The accuracy of this model is demonstrated by the significant error reduction in the presence of large triangulation errors in a long sequence of images and by a shorter transition period for convergence to the true values.

A class of preconditioning techniques for sparse matrices is considered, based on computing an approximation of the Schur complement of a (suitably ordered) matrix.
The techniques generalize the reduced system methodology for 2-cyclic matrices to non-2-cyclic matrices, and in addition, they are well suited to parallel architectures.
Their effectiveness with numerical experiments on a nine-point finite-difference operator is demonstrated, and an analysis showing that they can be implemented efficiently on multiprocessors is presented.

In this paper, we describe a Block Lanczos method for computing a few of the least or greatest eigenvalues of a sparse symmetric matrix.
A basic result of Kaniel and Paige describing the rate of convergence of Lanczos' method will be extended to the Block Lanczos method.
The results of experiments conducted with this method will be presented and discussed.

The Internet was originally designed to facilitate communication and research activities.
However, the dramatic increase in the use of the Internet in recent years has led to pathological use (Internet addiction).
This study is a preliminary investigation of the extent of Internet addiction in school children 16-18 years old in India.
The Davis Online Cognition Scale (DOCS) was used to assess pathological Internet use.
On the basis of total scores obtained (N = 100) on the DOCS, two groups were identified—dependents (18) and non-dependents (21), using mean ± h SD as the criterion for selection.
The UCLA loneliness scale was also administered to the subjects.
Significant behavioral and functional usage differences were revealed between the two groups.
Dependents were found to delay other work to spend time online, lose sleep due to late-night logons, and feel life would be boring without the Internet.
The hours spent on the Internet by dependents were greater than those of non-dependents.
On the lonel...

Alleles, individuals, and species are all examples of entities possessing variation in the properties that underlie natural selection: branching (reproduction), persistence (survivorship), and heritability of characters.
This suggests that the logic embodied in the theory of natural selection can be abstracted from its usual application to the level of individuals to encompass selection operating among any biological entities for which these essential properties can be meaningfully defined.
This approach leads to a unified perspective of adaptation, selection, and fitness at all levels.
Expanded versions of the Price covariance selection equations provide a convenient and useful conceptual vehicle for this discussion.
The advantages of a hierarchical approach are twofold: it permits exploration of concepts and ideas across levels by analogy, and it focuses attention upon the mechanisms that account for different evolutionary dynamics at each level rather than obscuring these biologically unique properties with argument by extension from a single “special” level.
We point out that the choice of a single measure of evolutionary change restricts the context in which “other level” processes will be perceived.
We illustrate the limited forms in which higher and lower level selection can be recognized from the unique perspective provided by any given level through extensions of Price's formula.
An exploration of the implications of such an approach leads us to the assertion that the development of a unified theory of evolution demands the recognition and incorporation of hierarchical structure as a conceptual foundation.

CSRE (The Canadian Speech Research Environment) is designed as a comprehensive, integrated, inexpensive, micro-computer-based workstation to support speech research.
The focus of the project has been to develop the basic functionality required by speech researchers, using mass-produced, widely-available hardware.
In its present form, CSRE includes: (1) waveform editors to capture, cut, paste, and measure signals; (2) facilities to produce colour spectrograms and waterfall displays from sampled data using a variety of spectral analysis techniques; (3) a parametric digital speech synthesizer, based on that developed by Klatt (1980); and (4) an experiment generator, developed to speed the programming of experimental sequences.
The system requires an IBM/AT compatible computer (typically, Zenith Model 386 or Model 248, with math coprocessor and EGA or VGA graphics), a data acquisition system (e.g.
Data Translation DT2801-A or ARIEL DSP-16, plus microphone, preamplifier and filters), and a Microsoft-compatible mouse

In the past, many studies on business survival have been conducted but very few have explained the survival of online stores operating in the ''long tail'' market.
To address this paucity, this study proposes a theoretical model, hypothesizing that online social networks, structural assurance, and online word-of-mouth (WOM) affect online store survival.
An empirical study on 5772 online stores in two industries was conducted to validate the model.
For social network factors, socialization efforts in SNS (social network site) personal space and online discussion forums were not found to be related to store survival.
