title,abstract,score,comment
Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,"We introduce positive-only projection (PoP), a novel technique for constructing semantic spaces and word embeddings. The PoP method is based on random projections. Hence, it is highly scalable and computationally efficient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R)=0), the proposed method uses R with E(R)>0. We use Kendall's tau_b distance to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R), weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art top-performing algorithms.","{'IMPACT': '2', 'SUBSTANCE': '3', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '2', 'SOUNDNESS_CORRECTNESS': '3', 'ORIGINALITY': '3', 'CLARITY': '3'}","I am buying some of the motivation: the proposed method is much faster to train
than it is to train a neural network. Also, it keeps some properties of the
distribution when going to lower dimensionality. 

However, I am not convinced why it is so important for vectors to be
transformable with PPMI.

Most importantly, there is no direct comparison to related work.

Detailed comments:

- p.3: The definition of Kendall's tau that the authors use is strange. This is
NOT the original formula; I am not sure what it is and where it comes from.

- p.3: Why not use Spearman correlation as is standard in semantic tasks (and
as teh authors do at evaluation time)?

- The datasets chosen for evaluation are not the standard ones for measuring
semantic relatedness that the NLP community prefers. It is nice to try other
sets, but I would recommend to also include results on the standard ones.

- I can only see two lines on Figure 1. Where is the third line?

- There is no direct comparison to related work, just a statement that 

Some typos:

- large extend -- extent"
Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,"We introduce positive-only projection (PoP), a novel technique for constructing semantic spaces and word embeddings. The PoP method is based on random projections. Hence, it is highly scalable and computationally efficient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R)=0), the proposed method uses R with E(R)>0. We use Kendall's tau_b distance to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R), weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art top-performing algorithms.","{'IMPACT': '3', 'SUBSTANCE': '4', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '4', 'SOUNDNESS_CORRECTNESS': '3', 'ORIGINALITY': '4', 'CLARITY': '2'}","The paper presents a positive-only projection (PoP) word embedding method. This
is a random projection method with a random projection matrix whose expected
value is positive. The authors argue that this enables the application of PPMI
which is not possible with an expected value of 0 and that being a random
projection method, their computation is efficient.

My main reservation about this paper has to do with its clarity. Particularly:

1. I could not understand the core difference between the method proposed in
the paper and previous random projection methods. Hence, I could not understand
how (and whether) the advantages the authors argue to achieve hold.

2. It was hard to follow the arguments of the paper starting from the
introduction. 

3. Some of the arguments of the paper are not supported: 

- Line 114: Sentence starts with ""in addition""

- Line 137: Sentence starts with ""Since""

- Line 154: Sentence starts with ""thus""

4. While I have worked on vector space modeling (who hasn't ?), I am not an
expert to random projections and have not used them in my research. It was hard
for me to understand the logic behind this research avenue from the paper. I
believe that a paper should be self contained and possible to follow by people
with some experience in the field.

5. The paper has lots of English mistakes (86: ""To a large extend"", 142: ""such
PPMI"").

In addition, I cannot see why the paper is evaluating only on MEN. There are a
couple of standard benchmarks (MEN, WordSeim, SimLex and a couple of others) -
if you present a new method, I feel that it is insufficient to evaluate only on
one dataset unless you provide a good justification.

I recommend that the authors will substantially improve the presentation in the
paper and will resubmit to another conference."
