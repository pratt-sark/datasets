title,abstract,comment,score
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Very good paper, I hope it will be accepted. I keep my original evaluation.","{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. 

With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. 

I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ","{'APPROPRIATENESS': 2, 'MEANINGFUL_COMPARISON': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",,"{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.
","{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","SUMMARY 
This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. 

PROS 
Theory, good questions, nice answers. 
Makes an interesting use of concepts form analysis and differential topology. 
Proposes avenues to avoid instability in GANs. 

CONS 
A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). 

MINOR COMMENTS

- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. 

- Section 3 provides a nice, intuitive, simple solution. 

- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. 

- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  

- Lemma 1 would also hold in more generality. 

- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). 

- In Theorem 2.4, it would be good to remind the reader about p(z). 

- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). 
Specify the domain of the random variables. 

- relly - > rely 

- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)

- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. 

- Theorem 2.5 ``Therefore'' -> `Then'? 

- Theorem 2.6 ``Is a... '' -> `is a' ? 

- The number of the theorems is confusing. 
","{'MEANINGFUL_COMPARISON': 4, 'SOUNDNESS_CORRECTNESS': 4, 'CLARITY': 4}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",,"{'MEANINGFUL_COMPARISON': 4, 'SOUNDNESS_CORRECTNESS': 4, 'CLARITY': 4}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","Very good paper, I hope it will be accepted. I keep my original evaluation.","{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure. 

With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant. More importantly, they also provide a way to avoid such difficulties by introducing perturbation. I believe this paper will inspire more principled research in this direction. 

I am very interested in the perturbation trick to avoid the gradient instability and vanishment. In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution. It will be great if the connection can be discussed.  Besides the theoretical analysis, is there any empirical study to justify this trick? Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison? ","{'APPROPRIATENESS': 2, 'MEANINGFUL_COMPARISON': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",,"{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks. The authors analyze theoretically the convergence of GANs and discuss the stability of GANs. Both are very important. To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture. The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice. Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting. I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.
","{'IMPACT': 3, 'ORIGINALITY': 2}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.","SUMMARY 
This paper addresses important questions about the difficulties in training generative adversarial networks. It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs. Then it proposes an alternative using a smoothening approach. 

PROS 
Theory, good questions, nice answers. 
Makes an interesting use of concepts form analysis and differential topology. 
Proposes avenues to avoid instability in GANs. 

CONS 
A bit too long, technical. Some parts and consequences still need to be further developed (which is perfectly fine for future work). 

MINOR COMMENTS

- Section 2.1 Maybe shorten this section a bit. E.g., move all proofs to the appendix. 

- Section 3 provides a nice, intuitive, simple solution. 

- On page 2 second bullet. This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero. 

- On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''.  

- Lemma 1 would also hold in more generality. 

- Theorem 2.1 seems to be basic analysis. (In other words, a reference could spare the proof). 

- In Theorem 2.4, it would be good to remind the reader about p(z). 

- Lemma 2 seems to be basic analysis. (In other words, a reference could spare the proof). 
Specify the domain of the random variables. 

- relly - > rely 

- Theorem 2.2 the closed manifolds have boundary or not? (already in the questions)

- Corollary 2.1, ``assumptions of Theorem 1.3''. I could not find Theorem 1.3. 

- Theorem 2.5 ``Therefore'' -> `Then'? 

- Theorem 2.6 ``Is a... '' -> `is a' ? 

- The number of the theorems is confusing. 
","{'MEANINGFUL_COMPARISON': 4, 'SOUNDNESS_CORRECTNESS': 4, 'CLARITY': 4}"
Towards Principled Methods for Training Generative Adversarial Networks,"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",,"{'MEANINGFUL_COMPARISON': 4, 'SOUNDNESS_CORRECTNESS': 4, 'CLARITY': 4}"
