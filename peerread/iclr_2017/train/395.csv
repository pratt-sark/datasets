title,abstract,comment,score
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","The paper introduces Edward, a probabilistic programming language
built over TensorFlow and Python, and supporting a broad range of most
popular contemporary methods in probabilistic machine learning.


Quality:

The Edward library provides an extremely impressive collection of
modern probabilistic inference methods in an easily usable form.
The paper provides a brief review of the most important techniques
especially from a representation learning perspective, combined with
two experiments on implementing various modern variational inference
methods and GPU-accelerated HMC.

The first experiment (variational inference) would be more valuable if
there was a clear link to complete code to reproduce the results
provided. The HMC experiment looks OK, except the characterising Stan
as a hand-optimised implementation seems unfair as the code is clearly
not hand-optimised for this specific model and hardware configuration.
I do not think anyone doubts the quality of your implementation, so
please do not ruin the picture by unsubstantiated sensationalist
claims. Instead of current drama, I would suggest comparing
head-to-head against Stan on single core and separately reporting the
extra speedups you gain from parallelisation and GPU. These numbers
would also help the readers to estimate the performance of the method
for other hardware configurations.


Clarity:

The paper is in general clearly written and easy to read. The numerous
code examples are helpful, but also difficult as it is sometimes
unclear what is missing. It would be very helpful if the authors could
provide and clearly link to a machine-readable companion (a Jupyter
notebook would be great, but even text or HTML would be easier to
copy-paste from than a pdf like the paper) with complete runnable code
for all the examples.


Originality:

The Edward library is clearly a unique collection of probabilistic
inference methods. In terms of the paper, the main threat to novelty
comes from previous publications of the same group. The main paper
refers to Tran et al. (2016a) which covers a lot of similar material,
although from a different perspective. It is unclear if the other
paper has been published or submitted somewhere and if so, where.


Significance:

It seems very likely Edward will have a profound impact on the field
of Bayesian machine learning and deep learning.


Other comments:

In Sec. 2 you draw a clear distinction between specialised languages
(including Stan) and Turing-complete languages such as Edward. This
seems unfair as I believe Stan is also Turing complete. Additionally
no proof is provided to support the Turing-completeness of Edward.
","{'IMPACT': 3, 'ORIGINALITY': 2, 'CLARITY': 1}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?","{'IMPACT': 4, 'SOUNDNESS_CORRECTNESS': 5, 'ORIGINALITY': 4, 'CLARITY': 3}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",,"{'IMPACT': 3, 'ORIGINALITY': 2, 'CLARITY': 1}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",,"{'IMPACT': 4, 'SOUNDNESS_CORRECTNESS': 5, 'ORIGINALITY': 4, 'CLARITY': 3}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","The paper introduces Edward, a probabilistic programming language
built over TensorFlow and Python, and supporting a broad range of most
popular contemporary methods in probabilistic machine learning.


Quality:

The Edward library provides an extremely impressive collection of
modern probabilistic inference methods in an easily usable form.
The paper provides a brief review of the most important techniques
especially from a representation learning perspective, combined with
two experiments on implementing various modern variational inference
methods and GPU-accelerated HMC.

The first experiment (variational inference) would be more valuable if
there was a clear link to complete code to reproduce the results
provided. The HMC experiment looks OK, except the characterising Stan
as a hand-optimised implementation seems unfair as the code is clearly
not hand-optimised for this specific model and hardware configuration.
I do not think anyone doubts the quality of your implementation, so
please do not ruin the picture by unsubstantiated sensationalist
claims. Instead of current drama, I would suggest comparing
head-to-head against Stan on single core and separately reporting the
extra speedups you gain from parallelisation and GPU. These numbers
would also help the readers to estimate the performance of the method
for other hardware configurations.


Clarity:

The paper is in general clearly written and easy to read. The numerous
code examples are helpful, but also difficult as it is sometimes
unclear what is missing. It would be very helpful if the authors could
provide and clearly link to a machine-readable companion (a Jupyter
notebook would be great, but even text or HTML would be easier to
copy-paste from than a pdf like the paper) with complete runnable code
for all the examples.


Originality:

The Edward library is clearly a unique collection of probabilistic
inference methods. In terms of the paper, the main threat to novelty
comes from previous publications of the same group. The main paper
refers to Tran et al. (2016a) which covers a lot of similar material,
although from a different perspective. It is unclear if the other
paper has been published or submitted somewhere and if so, where.


Significance:

It seems very likely Edward will have a profound impact on the field
of Bayesian machine learning and deep learning.


Other comments:

In Sec. 2 you draw a clear distinction between specialised languages
(including Stan) and Turing-complete languages such as Edward. This
seems unfair as I believe Stan is also Turing complete. Additionally
no proof is provided to support the Turing-completeness of Edward.
","{'IMPACT': 3, 'ORIGINALITY': 2, 'CLARITY': 1}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.","Thank you for an interesting read.

I found this paper very interesting. Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints. I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference. Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.

The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions). It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP. However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?","{'IMPACT': 4, 'SOUNDNESS_CORRECTNESS': 5, 'ORIGINALITY': 4, 'CLARITY': 3}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",,"{'IMPACT': 3, 'ORIGINALITY': 2, 'CLARITY': 1}"
Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",,"{'IMPACT': 4, 'SOUNDNESS_CORRECTNESS': 5, 'ORIGINALITY': 4, 'CLARITY': 3}"
