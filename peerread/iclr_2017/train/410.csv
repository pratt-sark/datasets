title,abstract,comment,score
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.","The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","{'MEANINGFUL_COMPARISON': 4, 'ORIGINALITY': 4}"
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.","The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.

It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.

It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.

In section 4, the description of the auxiliary decoder setup might benefit from more detail.

There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 
1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.

2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.

3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013.","{'IMPACT': 2, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",,"{'MEANINGFUL_COMPARISON': 4, 'ORIGINALITY': 4}"
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.","The paper address the problem of detecting if an example is misclassified or out-of-distribution. This is an very important topic and the study provides a good baseline. Although it misses strong novel methods for the task, the study contributes to the community.","{'MEANINGFUL_COMPARISON': 4, 'ORIGINALITY': 4}"
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.","The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.

It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.

It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.

In section 4, the description of the auxiliary decoder setup might benefit from more detail.

There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. 
1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.

2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.

3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013.","{'IMPACT': 2, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,"We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",,"{'MEANINGFUL_COMPARISON': 4, 'ORIGINALITY': 4}"
