title,abstract,comment,score
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.

The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.

The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.

This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.

Overall, the paper is well communicated and a novel idea.

The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.

The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.

Minor issues:
Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.
Figures on page 8 are difficult to follow.
","{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","The paper proposes a new memory access scheme based on Lie group actions for NTMs.

Pros:
* Well written
* Novel addressing scheme as an extension to NTM.
* Seems to work slightly better than normal NTMs.
* Some interesting theory about the novel addressing scheme based on Lie groups.

Cons:
* In the results, the LANTM only seems to be slightly better than the normal NTM.
* The result tables are a bit confusing.
* No source code available.
* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.
* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.
* No tests on real-world tasks, only some toy tasks.
* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (","{'IMPACT': 5, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.",,"{'IMPACT': 5, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly.","{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.",,"{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.

The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.

The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.

This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.

Overall, the paper is well communicated and a novel idea.

The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.

The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.

Minor issues:
Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.
Figures on page 8 are difficult to follow.
","{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","The paper proposes a new memory access scheme based on Lie group actions for NTMs.

Pros:
* Well written
* Novel addressing scheme as an extension to NTM.
* Seems to work slightly better than normal NTMs.
* Some interesting theory about the novel addressing scheme based on Lie groups.

Cons:
* In the results, the LANTM only seems to be slightly better than the normal NTM.
* The result tables are a bit confusing.
* No source code available.
* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.
* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.
* No tests on real-world tasks, only some toy tasks.
* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (","{'IMPACT': 5, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.",,"{'IMPACT': 5, 'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.","I struggle to understand figure 2, despite the length of the caption. Perhaps labelling the images themselves a bit more clearly.","{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
Lie-Access Neural Turing Machines,"External neural memory structures have recently become a popular tool for   algorithmic deep learning   (Graves et al. 2014; Weston et al. 2014).  These models   generally utilize differentiable versions of traditional discrete   memory-access structures (random access, stacks, tapes) to provide   the storage necessary for computational tasks.  In   this work, we argue that these neural memory systems lack specific   structure important for relative indexing, and propose an   alternative model, Lie-access memory, that is explicitly designed   for the neural setting.  In this paradigm, memory is accessed using   a continuous head in a key-space manifold. The head is moved via Lie   group actions, such as shifts or rotations, generated by a   controller, and memory access is performed by linear smoothing in   key space. We argue that Lie groups provide a natural generalization   of discrete memory structures, such as Turing machines, as they   provide inverse and identity operators while maintaining   differentiability. To experiment with this approach, we implement   a simplified Lie-access neural Turing machine (LANTM) with   different Lie groups.  We find that this approach is able to perform   well on a range of algorithmic tasks.",,"{'APPROPRIATENESS': 2, 'SOUNDNESS_CORRECTNESS': 2, 'ORIGINALITY': 2}"
