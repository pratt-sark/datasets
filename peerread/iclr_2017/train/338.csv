title,abstract,comment,score
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.","Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 
                                                                                                                                                                                                          
Pros                                                                                                                                                                                                      
- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 
- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    
                                                                                                                                                                                                          
Cons                                                                                                                                                                                                      
- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         
- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   ","{'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.","This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.

","{'IMPACT': 5, 'SUBSTANCE': 3, 'SOUNDNESS_CORRECTNESS': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",,"{'IMPACT': 5, 'SUBSTANCE': 3, 'SOUNDNESS_CORRECTNESS': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",,"{'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.","Thank you for an interesting angle on highway and residual networks. This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models. Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.                                 
                                                                                                                                                                                                          
Pros                                                                                                                                                                                                      
- the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.                                                                 
- Figure 3 gave a clear visualization for the iterative unrolling view                                                                                                                                    
                                                                                                                                                                                                          
Cons                                                                                                                                                                                                      
- Even though, the perspective is interesting few empirical results were shown to support the argument. The major experiments are image classification and language models trained on mutations of character-aware neural language models.                                                                                                                                                                         
- Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.   ","{'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.","This paper provides a new perspective to understanding the ResNet and Highway net. The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations. Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.

The pros of the paper are:
1. A novel perspective to understand the recent progress of neural network is proposed.
2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.

The main cons of the paper is that the experiments are not sufficient. For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."". Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption. 

Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net. The lack of experiments on this point is the main concern from myself.

","{'IMPACT': 5, 'SUBSTANCE': 3, 'SOUNDNESS_CORRECTNESS': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",,"{'IMPACT': 5, 'SUBSTANCE': 3, 'SOUNDNESS_CORRECTNESS': 3}"
Highway and Residual Networks learn Unrolled Iterative Estimation,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks.  Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",,"{'SOUNDNESS_CORRECTNESS': 3, 'ORIGINALITY': 3}"
