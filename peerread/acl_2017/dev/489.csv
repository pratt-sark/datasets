title,abstract,score,comment
Obtaining referential word meanings from visual and distributional information: Experiments on object naming,"We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.  We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.","{'IMPACT': '3', 'SUBSTANCE': '5', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '3', 'SOUNDNESS_CORRECTNESS': '5', 'ORIGINALITY': '5', 'CLARITY': '3'}","COMMENTS AFTER AUTHOR RESPONSE:

Thanks for your response, particularly for the clarification wrt the
hypothesis. I agree with the comment wrt cross-modal mapping. What I don't
share is the kind of equation ""visual = referential"" that you seem to assume. A
referent can be visually presented, but visual information can be usefully
added to a word's representation in aggregate form to encode perceptual aspects
of the words' meaning, the same way that it is done for textual information;
for instance, the fact that bananas are yellow
will not frequently be mentioned in text, and adding visual information
extracted from images will account for this aspect of the semantic
representation of the word. This is kind of technical and specific to how we
build distributional models, but it's also relevant if you think of human
cognition (probably our representation for ""banana"" has some aggregate
information about all the bananas we've seen --and touched, tasted, etc.). 
It would be useful if you could discuss this issue explicitly, differentiating
between multi-modal distributional semantics in general and the use of
cross-modal mapping in particular.

Also, wrt the ""all models perform similarly"" comment: I really
urge you, if the paper is accepted, to state it in this form, even if it
doesn't completely align with your hypotheses/goals (you have enough results
that do). It is a better description of the results, and more useful for the
community, than clinging to the
n-th digit difference (and this is to a large extent independent of whether the
difference
is actually statistical significant or not: If one bridge has 49% chances of
collapsing and another one 50%, the difference may be statistically
significant, but that doesn't really make the first bridge a better bridge to
walk on).

Btw, small quibble, could you find a kind of more compact and to the point
title? (More geared towards either generally what you explore or to what you
find?)

----------

The paper tackles an extremely interesting issue, that the authors label
""referential word meaning"", namely, the connection between a word's meaning and
the referents (objects in the external world) it is applied to. If I understood
it correctly, they argue that
this is different from a typical word meaning representation as obtained e.g.
with distributional
methods, because one thing is the abstract ""lexical meaning"" of a word and the
other which label is appropriate for a given referent with specific properties
(in a specific context, although context is something they explicitly leave
aside in this paper). This hypothesis has been previously explored in work by
Schlangen and colleagues (cited in the paper). The paper explores referential
word meaning empirically on a specific version of the task of Referential
Expression Generation (REG), namely, generating the appropriate noun for a
given visually represented object.

- Strengths:

1) The problem they tackle I find extremely interesting; as they argue, REG is
a problem that had previously been addressed mainly using symbolic methods,
that did not easily allow for an exploration of how speakers choose the names
of the objects. The scope of the research goes beyond REG as such, as it
addresses the link between semantic representations and reference more broadly.

2) I also like how they use current techniques and datasets (cross-modal
mapping and word classifiers, the ReferIt dataset containing large amounts of
images with human-generated referring expressions) to address the problem at
hand. 

3) There are a substantial number of experiments as well as analysis into the
results. 

- Weaknesses:

1) The main weakness for me is the statement of the specific hypothesis, within
the general research line, that the paper is probing: I found it very
confusing.  As a result, it is also hard to make sense of the kind of feedback
that the results give to the initial hypothesis, especially because there are a
lot of them and they don't all point in the same direction.

The paper says:

""This paper pursues the hypothesis that an accurate
model of referential word meaning does not
need to fully integrate visual and lexical knowledge
(e.g. as expressed in a distributional vector
space), but at the same time, has to go beyond
treating words as independent labels.""

The first part of the hypothesis I don't understand: What is it to fully
integrate (or not to fully integrate) visual and lexical knowledge? Is the goal
simply to show that using generic distributional representation yields worse
results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are
showing: Specifically, word classifiers must be trained on the dataset itself
and only word classifiers with a sufficient amount of items in the dataset can
be obtained, whereas word vectors are available for many other words and are
obtained from an independent source (even if the cross-modal mapping itself is
trained on the dataset); moreover, they use the simplest Ridge Regression,
instead of the best method from Lazaridou et al. 2014, so any conclusion as to
which method is better should be taken with a grain of salt. However, I'm
hoping that the research goal is both more constructive and broader. Please
clarify. 

2) The paper uses three previously developed methods on a previously available
dataset. The problem itself has been defined before (in Schlangen et al.). In
this sense, the originality of the paper is not high. 

3) As the paper itself also points out, the authors select a very limited
subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm
not even sure why they limited it this way (see detailed comments below).

4) Some aspects could have been clearer (see detailed comments).

5) The paper contains many empirical results and analyses, and it makes a
concerted effort to put them together; but I still found it difficult to get
the whole picture: What is it exactly that the experiments in the paper tell us
about the underlying research question in general, and the specific hypothesis
tested in particular? How do the different pieces of the puzzle that they
present fit together?

- General Discussion: [Added after author response]

Despite the weaknesses, I find the topic of the paper very relevant and also
novel enough, with an interesting use of current techniques to address an ""old""
problem, REG and reference more generally, in a way that allows aspects to be
explored that have not received enough attention. The experiments and analyses
are a substantial contribution, even though, as mentioned above, I'd like the
paper to present a more coherent overall picture of how the many experiments
and analyses fit together and address the question pursued.

- Detailed comments:

Section 2 is missing the following work in computational semantic approaches to
reference:

Abhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  
Distributional                                            vectors  encode 
referential        

attributes.
Proceedings of
EMNLP,
12-21

Aurelie Herbelot and Eva Maria Vecchi.                                           
2015. 
Building
a
shared
world:
mapping
distributional to model-theoretic semantic spaces. Proceedings of EMNLP,
22â€“32.

142 how does Roy's work go beyond early REG work?

155 focusses links

184 flat ""hit @k metric"": ""flat""?

Section 3: please put the numbers related to the dataset in a table, specifying
the image regions, number of REs, overall number of words, and number of object
names in the original ReferIt dataset and in the version you use. By the way,
will you release your data? I put a ""3"" for data because in the reviewing form
you marked ""Yes"" for data, but I can't find the information in the paper.

229 ""cannot be considered to be names"" ==> ""image object names""

230 what is ""the semantically annotated portion"" of ReferIt?

247 why don't you just keep ""girl"" in this example, and more generally the head
nouns of non-relational REs? More generally, could you motivate your choices a
bit more so we understand why you ended up with such a restricted subset of
ReferIt?

258 which 7 features? (list) How did you extract them?

383 ""suggest that lexical or at least distributional knowledge is detrimental
when learning what a word refers to in the world"": How does this follow from
the results of Frome et al. 2013 and Norouzi et al. 2013? Why should
cross-modal projection give better results? It's a very different type of
task/setup than object labeling.

394-395 these numbers belong in the data section

Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods
perform similarly"" seems unwarranted to me. Especially the ""This suggests...""
part (407). 

Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost
identical to wac); this is counter-intuitive given the @1 and @2 results. Any
idea of what's going on?

Section 5.2: Why did you define your ensemble classifier by hand instead of
learning it? Also, your method amounts to majority voting, right? 

Table 2: the order of the models is not the same as in the other tables + text.

Table 3: you report cosine distances but discuss the results in terms of
similarity. It would be clearer (and more in accordance with standard practice
in CL imo) if you reported cosine similarities.

Table 3: you don't comment on the results reported in the right columns. I
found it very curious that the gold-top k data similarities are higher for
transfer+sim-wap, whereas the results on the task are the same. I think that
you could squeeze more information wrt the phenomenon and the models out of
these results.

496 format of ""wac""

Section 6 I like the idea of the task a lot, but I was very confused as to how
you did and why: I don't understand lines 550-553. What is the task exactly? An
example would help. 

558 ""Testsets""

574ff Why not mix in the train set examples with hypernyms and non-hypernyms?

697 ""more even"": more wrt what?

774ff ""Previous cross-modal mapping models ... force..."": I don't understand
this claim.

792 ""larger test sets"": I think that you could even exploit ReferIt more (using
more of its data) before moving on to other datasets."
Obtaining referential word meanings from visual and distributional information: Experiments on object naming,"We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.  We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.","{'IMPACT': '3', 'SUBSTANCE': '4', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '3', 'SOUNDNESS_CORRECTNESS': '5', 'ORIGINALITY': '5', 'CLARITY': '4'}","AFTER AUTHOR RESPONSE

I accept the response about emphasizing novelty of the task and comparison with
previous work. Also increase ratings for the dataset and software that are
promised to become public before the article publishing.

======================

GENERAL 
The paper presents an interesting empirical comparison of 3 referring
expression generation models. The main novelty lies in the comparison of a yet
unpublished model called SIM-WAP (in press by Anonymous). The model is
described in SECTION 4.3 but it is not clear whether it is extended or modified
anyhow in the current paper.  

The novelty of the paper may be considered as the comparison of the unpublished
SIM-WAP model to existing 2 models. This complicates evaluation of the novelty
because similar experiments were already performed for the other two models and
it is unclear why this comparison was not performed in the paper where SIM-WAP
model was presented. A significant novelty might be the combined model yet this
is not stated clearly and the combination is not described with enough details.

The contribution of the paper may be considered the following: the side-by-side
comparison of the 3 methods for REG; analysis of zero-shot experiment results
which mostly confirms similar observations in previous works; analysis of the
complementarity of the combined model.                     

WEAKNESSES
Unclear novelty and significance of contributions. The work seems like an
experimental extension of the cited Anonymous paper where the main method was
introduced.    

Another weakness is the limited size of the vocabulary in the zero-shot
experiments that seem to be the most contributive part. 

Additionally, the authors never presented significance scores for their
accuracy results. This would have solidified the empirical contribution of the
work which its main value.   

My general feeling is that the paper is more appropriate for a conference on
empirical methods such as EMNLP. 

Lastly, I have not found any link to any usable software. Existing datasets
have been used for the work.  

Observations by Sections: 

ABSTRACT
""We compare three recent models"" -- Further in the abstract you write that you
also experiment with the combination of approaches. In Section 2 you write that
""we present a model that exploits distributional knowledge for learning
referential word meaning as well, but explore and compare different ways of
combining visual and lexical aspects of referential word meaning"" which
eventually might be a better summarization of the novelty introduced in the
paper and give more credit to the value of your work. 

My suggestion is to re-write the abstract (and eventually even some sections in
the paper) focusing on the novel model and results and not just stating that
you compare models of others.                  

INTRODUCTION 
""Determining such a name is is"" - typo 
""concerning e.g."" -> ""concerning, e.g.,"" 
""having disjunct extensions."" - specify or exemplify, please 
""building in Figure 1"" -> ""building in Figure 1 (c)""

SECTION 4
""Following e.g. Lazaridou et al. (2014),"" - ""e.g."" should be omitted  

SECTION 4.2
""associate the top n words with their corresponding distributional vector"" -
What are the values of N that you used? If there were any experiments for
finding the optimal values, please, describe because this is original work. The
use top N = K is not obvious and not obvious why it should be optimal (how
about finding similar vectors to each 5 in top 20?)    

SECTION 4.3 
""we annotate its training instances with a fine-grained similarity signal
according to their object names."" - please, exemplify. 

LANGUAGE   
Quite a few typos in the draft. Generally, language should be cleaned up (""as
well such as""). 
Also, I believe the use of American English spelling standard is preferable
(e.g., ""summarise"" -> ""summarize""). Please, double check with your conference
track chairs."
