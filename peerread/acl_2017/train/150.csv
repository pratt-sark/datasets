title,abstract,score,comment
Deep Character-Level Neural Machine Translation By Learning Morphology,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology.","{'IMPACT': '4', 'SUBSTANCE': '4', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '4', 'SOUNDNESS_CORRECTNESS': '4', 'ORIGINALITY': '3', 'CLARITY': '4'}","- Strengths:

The authors present a novel adaptation of encoder-decoder neural MT using an
approach that starts and ends with characters, but in between works with
representations of morphemes and characters. 

The authors release both their code as well as their final learned models for
fr-en, cs-en, and en-cs. This is helpful in validating their work, as well as
for others looking to replicate and extends this work.

The system reported appears to produce translation results of reasonable
quality even after the first training epoch, with continued progress in future
epochs.

The system appears to learn reasonable morphological tokenizations, and appears
able to handle previously unseen words (even nonce words) by implicitly backing
off to morphemes.

- Weaknesses:

In the paper, the authors do not explicitly state which WMT test and dev sets
their results are reported on. This is problematic for readers wishing to
compare the reported results to existing work (for example, the results at
matrix.statmt.org). The only way this reviewer found to get this information
was to look in the README of the code supplement, which indicates that the test
set was newstest2015 and the dev test was newstest2013. This should have been
explicitly described in the paper.

The instructions given in the software README are OK, but not great. The
training and testing sections each could be enhanced with explicit examples of
how to run the respective commands. The software itself should respond to a
--help flag, which it currently does not.

The paper describes a 6-level architecture, but the diagram in Figure 2 appears
to show fewer than 6 layers. What's going on? The caption should be more
explicit, and if this figure is not showing all of the layers, then there
should be a figure somewhere (even if it's in an appendix) showing all of the
layers.

The results show comparison to other character-based neural systems, but do not
show state-of-the-art results for other types of MT system. WMT (and
matrix.statmt.org) has reported results for other systems on these datasets,
and it appears that the state-of-the-art is much higher than any of the results
reported in this paper. That should be acknowledged, and ideally should be
discussed.

There are a handful of minor English disfluencies, misspellings, and minor
LaTeX issues, such as reverse quotation marks. These should be corrected.

- General Discussion:

Paper is a nice contribution to the existing literature on character-based
neural MT."
Deep Character-Level Neural Machine Translation By Learning Morphology,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology.","{'IMPACT': '3', 'SUBSTANCE': '4', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '3', 'SOUNDNESS_CORRECTNESS': '4', 'ORIGINALITY': '3', 'CLARITY': '3'}","- Strengths: In general, the paper is well structured and clear. It is possible
to follow most of the explanation, the ideas presented are original and the
results obtained are quite interesting.

- Weaknesses: I have some doubts about the interpretation of the results. In
addition, I think that some of the claims regarding the capability of the
method proposed to learn morphology are not propperly backed by scientific
evidence.

- General Discussion:

This paper explores a complex architecture for character-level neural machine
translation (NMT). The proposed architecture extends a classical
encoder-decoder architecture by adding a new deep word-encoding layer capable
of encoding the character-level input into sub-word representations of the
source-language sentence. In the same way, a deep word-decoding layer is added
to the output to transform the target-language sub-word representations into a
character sequence as the final output of the NMT system. The objective of such
architecture is to take advantage of the benefits of character-level NMT
(reduction of the size of the vocabulary and flexibility to deal with unseen
words) and, at the same time, improving the performance of the whole system by
using an intermediate representation of sub-words to reduce the size of the
input sequence of characters. In addition, the authors claim that their deep
word-encoding model is able to learn morphology better than other
state-of-the-art approaches.

I have some concerns regarding the evaluation. The authors compare their
approach to other state-of-the-art systems taking into account two parameters:
training time and BLEU score. However, I do not clearly see the advantage of
the model proposed (DCNMT) in front of other approaches such as bpe2char. The
difference between both approaches as regards BLEU score is very small (0.04 in
Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming
the other one without statistical significance information: has statistical
significance been evaluated? As regards the training time, it is worth
mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs
training time is not provided (why not?) and for En-Fr bpe2char is not
evaluated. I think that a more complete comparison with this system should be
carried out to prove the advantages of the model proposed.

My second concern is on the 5.2 Section, where authors start claiming that they
investigated about the ability of their system to learn morphology. However,
the section only contains a examples and some comments on them. Even though
these examples are very well chosen and explained in a very didactic way, it is
worth noting that no experiments or formal evaluation seem to have been
carried out to support the claims of the authors. I would definitely encourage
authors to extend this very interesting part of the paper that could even
become a different paper itself. On the other hand, this Section does not seem
to be a critical point of the paper, so for the current work I may suggest just
to move this section to an appendix and soften some of the claims done
regarding the capabilities of the system to learn morphology.

Other comments, doubts and suggestions:

 - There are many acronyms that are used but are not defined (such as LSTM,
HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN
or BPE). Even though some of these acronyms are well known in the field of deep
learning, I would encourage the authors to defined them to improve clearness.

 - The concept of energy is mentioned for the first time in Section 3.1. Even
though the explanation provided is enough at that point, it would be nice to
refresh the idea of energy in Section 5.2 (where it is used several times) and
providing some hints about how to interpret it: a high energy on a character
would be indicating that the current morpheme should be split at that point? In
addition, the concept of peak (in Figure 5) is not described.

 - When the acronym BPE is defined, capital letters are used, but then, for the
rest of mentions it is lower cased; is there a reason for this?

 - I am not sure if it is necessary to say that no monolingual corpus is used
in Section 4.1.

 - It seems that there is something wrong with Figure 4a since the colours for
the energy values are not shown for every character.

 - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not
taken from the papers, since they are not reported. If the authors computed
these results by themselves (as it seems) they should mention it.

 - I would not say that French is morphologically poor, but rather that it is
not that rich as Slavic languages such as Czech.

 - Why a link is provided for WMT'15 training corpora but not for WMT'14?

 - Several references are incomplete

Typos:

  - ""..is the bilingual, parallel corpora provided..."" -> ""..are the bilingual,
parallel corpora provided...""

  - ""Luong and Manning (2016) uses"" -> ""Luong and Manning (2016) use""

  - ""HGRU (It is"" -> ""HGRU (it is""

  - ""coveres"" -> ""covers""

  - ""both consists of two-layer RNN, each has 1024"" -> ""both consist of
two-layer RNN, each have 1024""

  - ""the only difference between CNMT and DCNMT is CNMT"" -> ""the only
difference between CNMT and DCNMT is that CNMT"""
