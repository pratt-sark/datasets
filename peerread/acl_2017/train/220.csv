title,abstract,score,comment
Vancouver Welcomes You! Minimalist Location Metonymy Resolution,"Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.","{'IMPACT': '3', 'SUBSTANCE': '4', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '3', 'SOUNDNESS_CORRECTNESS': '3', 'ORIGINALITY': '3', 'CLARITY': '5'}","- Strengths: Great paper: Very well-written, interesting results, creative
method, good and enlightening comparisons with earlier approaches. In addition,
the corpus, which is very carefully annotated, will prove to be a valuable
resource for other researchers. I appreciated the qualitative discussion in
section 5. Too many ML papers just give present a results table without much
further ado, but the discussion in this paper really provides insights for the
reader. 

- Weaknesses: In section 4.1, the sentence ""The rest of the modelâ€™s input is
set to zeroes..."" is quite enigmatic until you look at Figure 2. Some extra
sentence here explaining what is going on would be helpful. Furthermore, in
Figure 2, in the input layers to the LSTMs it says ""5*Embeddings(50D)"" also for
the networks taking dependency labels as input. Surely this is wrong? (Or if it
is correct, please explain what you mean). 

- General Discussion: Concerning the comment in 4.2 ""LSTMs are excellent at
modelling language sequences ... which is why we use this type of model."". This
comment seems strange to me. This is not a sequential problem in that sense.
For each datapoint, you feed the network all 5 words in an example in one go,
and the next example has nothing to do with the preceding one. The LSTM
architecture could still be superior, of course, but not for the reason you
state. Or have I misunderstood something? I'd be interested to hear the
authors' comments on this point."
