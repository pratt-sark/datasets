title,abstract,score,comment
Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,"This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ``soft'' posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.","{'IMPACT': '3', 'SUBSTANCE': '3', 'APPROPRIATENESS': '5', 'MEANINGFUL_COMPARISON': '5', 'SOUNDNESS_CORRECTNESS': '3', 'ORIGINALITY': '4', 'CLARITY': '4'}","This paper presents a dialogue agent where the belief tracker and the dialogue
manager are jointly optimised using the reinforce algorithm. It learns from
interaction with a user simulator. There are two training phases. The first is
an imitation learning phase where the system is initialised using supervising
learning from a rule-based model. Then there is a reinforcement learning phase
where the system has jointly been optimised using the RL objective.

- Strengths: This paper presents a framework where a differentiable access to
the KB is integrated in the joint optimisation. This is the biggest
contribution of the paper. 

- Weaknesses: Firstly, this is not a truly end-to-end system considering the
response generation was handcrafted rather than learnt. Also, their E2E model
actually overfits to the simulator and performs poorly in human evaluation.
This begs the question whether the authors are actually selling the idea of E2E
learning or the soft-KB access. The soft-KB access actually brings consistent
improvement, however the idea of end-to-end learning not so much. The authors
tried to explain the merits of E2E in Figure 5 but I also fail to see the
difference. In addition, the authors didn't motivate the reason for using the
reinforce algorithm which is known to suffer from high variance problem. They
didn't attempt to improve it by using a baseline or perhaps considering the
natural actor-critic algorithm which is known to perform better.

- General Discussion: Apart from the mentioned weaknesses, I think the
experiments are solid and this is generally an acceptable paper. However, if
they crystallised the paper around the idea which actually improves the
performance (the soft KB access) but not the idea of E2E learning the paper
would be better."
